@prefix mcro: <http://purl.obolibrary.org/obo/MCRO_> .
@prefix obo: <http://purl.obolibrary.org/obo/> .
@prefix prov1: <https://www.w3.org/ns/prov#> .

mcro:AlibabaNLPgteQwen27Binstruct a mcro:Model ;
    mcro:hasCitation mcro:AlibabaNLPgteQwen27Binstruct-CitationInformationSection ;
    mcro:hasLicense mcro:AlibabaNLPgteQwen27Binstruct-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:AlibabaNLPgteQwen27Binstruct-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:AlibabaNLPgteQwen27Binstruct-UseCaseInformationSection .

mcro:AlibabaNLPgtebaseenv15 a mcro:Model ;
    mcro:hasConsideration mcro:AlibabaNLPgtebaseenv15-Consideration ;
    mcro:hasDataset mcro:AlibabaNLPgtebaseenv15-Dataset ;
    mcro:hasModelArchitecture mcro:AlibabaNLPgtebaseenv15-ModelArchitecture ;
    mcro:hasModelDetail mcro:AlibabaNLPgtebaseenv15-ModelDetail ;
    mcro:hasModelParameter mcro:AlibabaNLPgtebaseenv15-ModelParameter ;
    mcro:hasQuantativeAnalysis mcro:AlibabaNLPgtebaseenv15-QuantativeAnalysis ;
    mcro:hasUseCase mcro:AlibabaNLPgtebaseenv15-UseCase .

mcro:AlibabaNLPgtemultilingualbase a mcro:Model ;
    mcro:hasCitation mcro:AlibabaNLPgtemultilingualbase-Citation ;
    mcro:hasModelArchitecture mcro:AlibabaNLPgtemultilingualbase-Architecture ;
    mcro:hasUseCase mcro:AlibabaNLPgtemultilingualbase-UseCase .

mcro:AudioSpectrogramTransformerfinetunedonAudioSet a mcro:Model ;
    mcro:hasCitation mcro:AudioSpectrogramTransformerfinetunedonAudioSet-Citation ;
    mcro:hasModelArchitecture mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture ;
    mcro:hasModelDetail mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelDetail ;
    mcro:hasUseCase mcro:AudioSpectrogramTransformerfinetunedonAudioSet-UseCase .

mcro:Auralis a mcro:Model ;
    mcro:hasModelDetail mcro:Auralis-ModelDetail ;
    mcro:hasUseCase mcro:Auralis-UseCase .

mcro:BAAIbgebaseen a mcro:Model ;
    mcro:hasCitation mcro:BAAIbgebaseen-Citation ;
    mcro:hasDataset mcro:BAAIbgebaseen-Dataset ;
    mcro:hasIntendedUseCase mcro:BAAIbgebaseen-UseCase ;
    mcro:hasLicense mcro:BAAIbgebaseen-License ;
    mcro:hasModelArchitecture mcro:BAAIbgebaseen-ModelArchitecture .

mcro:BAAIbgelargeen a mcro:Model ;
    mcro:hasCitation mcro:BAAIbgelargeen-Citation ;
    mcro:hasLicense mcro:BAAIbgelargeen-License .

mcro:BAAIbgem3 a mcro:Model ;
    mcro:hasDataset mcro:BAAIbgem3-Dataset ;
    mcro:hasLicense mcro:BAAIbgem3-License ;
    mcro:hasModelArchitecture mcro:BAAIbgem3-ModelArchitecture ;
    mcro:hasModelDetail mcro:BAAIbgem3-ModelDetail ;
    mcro:hasUseCase mcro:BAAIbgem3-UseCase .

mcro:BAAIbgereRankerBase a mcro:Model ;
    mcro:hasCitation mcro:BAAIbgereRankerBase-CitationInformationSection ;
    mcro:hasDataset mcro:BAAIbgereRankerBase-DatasetInformationSection ;
    mcro:hasLicense mcro:BAAIbgereRankerBase-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:BAAIbgereRankerBase-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:BAAIbgereRankerBase-UseCaseInformationSection .

mcro:BAAIbgererankerlarge a mcro:Model ;
    mcro:hasCitation mcro:BAAIbgererankerlarge-Citation ;
    mcro:hasDataset mcro:BAAIbgererankerlarge-Dataset ;
    mcro:hasIntendedUseCase mcro:BAAIbgererankerlarge-UseCase ;
    mcro:hasLicense mcro:BAAIbgererankerlarge-License ;
    mcro:hasModelArchitecture mcro:BAAIbgererankerlarge-ModelArchitecture .

mcro:BAAIbgererankerv2m3 a mcro:Model ;
    mcro:hasCitation mcro:BAAIbgererankerv2m3-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:BAAIbgererankerv2m3-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:BAAIbgererankerv2m3-UseCaseInformationSection .

mcro:BAAIbgesmallen a mcro:Model ;
    mcro:hasCitation mcro:BAAIbgesmallen-CitationInformationSection ;
    mcro:hasDataset mcro:BAAIbgesmallen-DatasetInformationSection ;
    mcro:hasLicense mcro:BAAIbgesmallen-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:BAAIbgesmallen-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:BAAIbgesmallen-UseCaseInformationSection .

mcro:BEiTbasesizedmodelfinetunedonImageNet22k a mcro:Model ;
    mcro:hasDataset mcro:BEiTbasesizedmodelfinetunedonImageNet22k-TrainingDataInformationSection ;
    mcro:hasIntendedUseCase mcro:BEiTbasesizedmodelfinetunedonImageNet22k-UseCaseInformationSection ;
    mcro:hasModelDetail mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelDetailSection .

mcro:BRIA_Background_Removal_v14 a mcro:Model ;
    mcro:hasDataset mcro:BRIA_Background_Removal_v14-Dataset ;
    mcro:hasIntendedUseCase mcro:BRIA_Background_Removal_v14-UseCase ;
    mcro:hasLicense mcro:BRIA_Background_Removal_v14-License ;
    mcro:hasModelArchitecture mcro:BRIA_Background_Removal_v14-Architecture .

mcro:Bertbasemultilingualcasednerhrl a mcro:Model ;
    mcro:hasIntendedUseCase mcro:Bertbasemultilingualcasednerhrl-IntendedUseCase ;
    mcro:hasLimitation mcro:Bertbasemultilingualcasednerhrl-Limitation ;
    mcro:hasModelArchitecture mcro:Bertbasemultilingualcasednerhrl-ModelArchitecture ;
    mcro:hasTrainingData mcro:Bertbasemultilingualcasednerhrl-TrainingData .

mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation a mcro:Model ;
    mcro:hasArchitecture mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-ModelArchitectureInformationSection ;
    mcro:hasCitation mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-CitationInformationSection ;
    mcro:hasDataset mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-DatasetInformationSection ;
    mcro:hasLicense mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-LicenseInformationSection .

mcro:BiomedParse a mcro:Model ;
    mcro:hasCitation mcro:BiomedParse-CitationInformationSection ;
    mcro:hasDataSpecification mcro:BiomedParse-DataSpecificationSection ;
    mcro:hasEthicalConsideration mcro:BiomedParse-EthicalConsiderationSection ;
    mcro:hasModelDetail mcro:BiomedParse-ModelDetailSection ;
    mcro:hasUseCase mcro:BiomedParse-UseCaseInformationSection .

mcro:BridgeTowerbridgetowerlargeitmmlmitc a mcro:Model ;
    mcro:hasCitation mcro:BridgeTowerbridgetowerlargeitmmlmitc-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:BridgeTowerbridgetowerlargeitmmlmitc-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:BridgeTowerbridgetowerlargeitmmlmitc-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:BridgeTowerbridgetowerlargeitmmlmitc-UseCaseInformationSection .

mcro:CLIPSegModel a mcro:Model ;
    mcro:hasCitation mcro:CLIPSegModel-CitationInformationSection ;
    mcro:hasUseCase mcro:CLIPSegModel-UseCaseInformationSection .

mcro:CLIPViTB32LAION2B a mcro:Model ;
    mcro:hasModelDetail mcro:CLIPViTB32LAION2B-ModelDetail ;
    mcro:hasUseCase mcro:CLIPViTB32LAION2B-UseCase .

mcro:CLIPViTH14LAION2B a mcro:Model ;
    mcro:hasCitation mcro:CLIPViTH14LAION2B-Citation ;
    mcro:hasModelDetail mcro:CLIPViTH14LAION2B-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:CLIPViTH14LAION2B-Evaluation ;
    mcro:hasTrainingDataInformationSection mcro:CLIPViTH14LAION2B-TrainingDetails ;
    mcro:hasUseCaseInformationSection mcro:CLIPViTH14LAION2B-UseCase .

mcro:CLIPViTbigG14LAION2B a mcro:Model ;
    mcro:hasEvaluation mcro:CLIPViTbigG14LAION2B-Evaluation ;
    mcro:hasModelArchitecture mcro:CLIPViTbigG14LAION2B-Architecture ;
    mcro:hasModelDetail mcro:CLIPViTbigG14LAION2B-ModelDetail ;
    mcro:hasTrainingData mcro:CLIPViTbigG14LAION2B-TrainingData ;
    mcro:hasUseCase mcro:CLIPViTbigG14LAION2B-UseCase .

mcro:ChatGLM2-6B a mcro:Model ;
    mcro:hasCitation mcro:ChatGLM2-6B-Citation ;
    mcro:hasDataset mcro:ChatGLM2-6B-Dataset ;
    mcro:hasLicense mcro:ChatGLM2-6B-License ;
    mcro:hasModelArchitecture mcro:ChatGLM2-6B-Arch ;
    mcro:hasUseCase mcro:ChatGLM2-6B-UseCase .

mcro:CryptoBERT a mcro:Model ;
    mcro:hasCitation mcro:CryptoBERT-CitationInformationSection ;
    mcro:hasDataset mcro:CryptoBERT-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:CryptoBERT-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:CryptoBERT-UseCaseInformationSection .

mcro:DFN5BCLIPViTH14378 a mcro:Model ;
    mcro:hasModelDetail mcro:DFN5BCLIPViTH14378-ModelDetail ;
    mcro:hasUseCase mcro:DFN5BCLIPViTH14378-UseCase .

mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention a mcro:Model ;
    mcro:hasCitation mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-Citation ;
    mcro:hasModelArchitecture mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-ModelArchitecture ;
    mcro:hasUseCase mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-UseCase .

mcro:Distilbertbasemultilingualcasednerhrl a mcro:Model ;
    mcro:hasDataset mcro:Distilbertbasemultilingualcasednerhrl-Dataset ;
    mcro:hasModelDetail mcro:Distilbertbasemultilingualcasednerhrl-ModelDetail ;
    mcro:hasUseCase mcro:Distilbertbasemultilingualcasednerhrl-UseCase .

mcro:Dreamshaper8inpainting a mcro:Model ;
    mcro:hasModelDetail mcro:Dreamshaper8inpainting-ModelDetail ;
    mcro:hasUseCase mcro:Dreamshaper8inpainting-UseCase .

mcro:ESM2 a mcro:Model ;
    mcro:hasCitation mcro:ESM2-Citation ;
    mcro:hasModelArchitecture mcro:ESM2-ModelArchitecture ;
    mcro:hasUseCase mcro:ESM2-UseCase .

mcro:ESMFold a mcro:Model ;
    mcro:hasCitation mcro:ESMFold-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:ESMFold-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:ESMFold-UseCaseInformationSection .

mcro:EleutherAIpythia70mdeduped a mcro:Model ;
    mcro:hasCitation mcro:EleutherAIpythia70mdeduped-Citation ;
    mcro:hasLimitation mcro:EleutherAIpythia70mdeduped-Limitation ;
    mcro:hasModelDetail mcro:EleutherAIpythia70mdeduped-ModelDetail ;
    mcro:hasTrainingData mcro:EleutherAIpythia70mdeduped-TrainingData .

mcro:EmergentMethodsglinermediumnewsv21 a mcro:Model ;
    mcro:hasCitation mcro:EmergentMethodsglinermediumnewsv21-Citation ;
    mcro:hasModelDetail mcro:EmergentMethodsglinermediumnewsv21-ModelDetail ;
    mcro:hasTrainingData mcro:EmergentMethodsglinermediumnewsv21-TrainingData ;
    mcro:hasUseCase mcro:EmergentMethodsglinermediumnewsv21-UseCase .

mcro:FacebookAIxlmrobertabase a mcro:Model ;
    mcro:hasLimitation mcro:FacebookAIxlmrobertabase-Limitation ;
    mcro:hasModelDetail mcro:FacebookAIxlmrobertabase-ModelDetail ;
    mcro:hasUseCase mcro:FacebookAIxlmrobertabase-UseCase .

mcro:FacebookAIxlmrobertalarge a mcro:Model ;
    mcro:hasCitation mcro:FacebookAIxlmrobertalarge-Citation ;
    mcro:hasModelArchitecture mcro:FacebookAIxlmrobertalarge-ModelArchitecture ;
    mcro:hasModelDetail mcro:FacebookAIxlmrobertalarge-ModelDetail ;
    mcro:hasUseCase mcro:FacebookAIxlmrobertalarge-UseCase .

mcro:Falconsainsfwimagedetection a mcro:Model ;
    mcro:hasModelDetail mcro:Falconsainsfwimagedetection-ModelDetail ;
    mcro:hasReference mcro:Falconsainsfwimagedetection-Reference ;
    mcro:hasTrainingData mcro:Falconsainsfwimagedetection-TrainingData ;
    mcro:hasUseCase mcro:Falconsainsfwimagedetection-UseCase .

mcro:FinBERT a mcro:Model ;
    mcro:hasModelArchitecture mcro:FinBERT-ModelArchitecture ;
    mcro:hasModelDetail mcro:FinBERT-ModelDetail ;
    mcro:hasUseCase mcro:FinBERT-UseCase .

mcro:FinetunedT5SmallTextSummarization a mcro:Model ;
    mcro:hasDataset mcro:FinetunedT5SmallTextSummarization-Dataset ;
    mcro:hasModelDetail mcro:FinetunedT5SmallTextSummarization-ModelDetail ;
    mcro:hasUseCase mcro:FinetunedT5SmallTextSummarization-UseCase .

mcro:FlagEmbedding a mcro:Model ;
    mcro:hasCitation mcro:FlagEmbedding-Citation,
        mcro:FlagEmbedding-CitationInformationSection ;
    mcro:hasDataset mcro:FlagEmbedding-Dataset ;
    mcro:hasLicense mcro:FlagEmbedding-License,
        mcro:FlagEmbedding-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:FlagEmbedding-ModelArchitecture ;
    mcro:hasUseCase mcro:FlagEmbedding-UseCase .

mcro:FsoftAICpiiphi a mcro:Model ;
    mcro:hasConsideration mcro:FsoftAICpiiphi-ConsiderationInformationSection ;
    mcro:hasUseCase mcro:FsoftAICpiiphi-UseCaseInformationSection .

mcro:GIT a mcro:Model ;
    mcro:hasIntendedUseCase mcro:GIT-UseCase ;
    mcro:hasModelDetail mcro:GIT-ModelDetail ;
    mcro:hasTrainingData mcro:GIT-TrainingData .

mcro:HelsinkiNLPopusmtzhen a mcro:Model ;
    mcro:hasArchitecture mcro:HelsinkiNLPopusmtzhen-Architecture ;
    mcro:hasEvaluationData mcro:HelsinkiNLPopusmtzhen-EvaluationData ;
    mcro:hasModelDetail mcro:HelsinkiNLPopusmtzhen-ModelDetail ;
    mcro:hasTrainingData mcro:HelsinkiNLPopusmtzhen-TrainingData ;
    mcro:hasUseCase mcro:HelsinkiNLPopusmtzhen-UseCase .

mcro:HuggingFaceH4zephyr7bbeta a mcro:Model ;
    mcro:hasCitation mcro:HuggingFaceH4zephyr7bbeta-Citation ;
    mcro:hasDataset mcro:HuggingFaceH4zephyr7bbeta-Dataset ;
    mcro:hasIntendedUseCase mcro:HuggingFaceH4zephyr7bbeta-IntendedUseCase ;
    mcro:hasLicense mcro:HuggingFaceH4zephyr7bbeta-License ;
    mcro:hasModelArchitecture mcro:HuggingFaceH4zephyr7bbeta-ModelArchitecture .

mcro:HuggingFaceM4idefics28b a mcro:Model ;
    mcro:hasCitation mcro:HuggingFaceM4idefics28b-Citation ;
    mcro:hasDataset mcro:HuggingFaceM4idefics28b-Dataset ;
    mcro:hasLicense mcro:HuggingFaceM4idefics28b-License ;
    mcro:hasModelArchitecture mcro:HuggingFaceM4idefics28b-ModelArchitecture ;
    mcro:hasUseCase mcro:HuggingFaceM4idefics28b-UseCase .

mcro:IDEAResearchgroundingdinobase a mcro:Model ;
    mcro:hasCitation mcro:IDEAResearchgroundingdinobase-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:IDEAResearchgroundingdinobase-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:IDEAResearchgroundingdinobase-UseCaseInformationSection .

mcro:IDEAResearchgroundingdinotiny a mcro:Model ;
    mcro:hasCitation mcro:IDEAResearchgroundingdinotiny-Citation ;
    mcro:hasModelArchitecture mcro:IDEAResearchgroundingdinotiny-ModelArchitecture ;
    mcro:hasUseCase mcro:IDEAResearchgroundingdinotiny-UseCase .

mcro:IP-Adapter-FaceID a mcro:Model ;
    mcro:hasModelDetail mcro:IP-Adapter-FaceID-ModelDetail ;
    mcro:hasUseCase mcro:IP-Adapter-FaceID-UseCaseInformationSection .

mcro:Inteldptlarge a mcro:Model ;
    mcro:hasCitation mcro:Inteldptlarge-Citation ;
    mcro:hasConsideration mcro:Inteldptlarge-Consideration ;
    mcro:hasEthicalConsideration mcro:Inteldptlarge-EthicalConsideration ;
    mcro:hasLicense mcro:Inteldptlarge-License ;
    mcro:hasModelDetail mcro:Inteldptlarge-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:Inteldptlarge-QuantativeAnalysis ;
    mcro:hasTradeoff mcro:Inteldptlarge-Tradeoff ;
    mcro:hasTrainingData mcro:Inteldptlarge-TrainingData ;
    mcro:hasUseCase mcro:Inteldptlarge-UseCase .

mcro:InternVL22B a mcro:Model ;
    mcro:hasCitation mcro:InternVL22B-Citation ;
    mcro:hasLicense mcro:InternVL22B-License ;
    mcro:hasModelDetail mcro:InternVL22B-ModelDetail .

mcro:InternVL378B a mcro:Model ;
    mcro:hasCitation mcro:InternVL378B-Citation ;
    mcro:hasLicense mcro:InternVL378B-License ;
    mcro:hasModelArchitecture mcro:InternVL378B-ModelArchitecture ;
    mcro:hasUseCase mcro:InternVL378B-UseCase .

mcro:JackFramllama68m a mcro:Model ;
    mcro:hasModelDetail mcro:JackFramllama68m-ModelDetail .

mcro:LTXVideo a mcro:Model ;
    mcro:hasConsideration mcro:LTXVideo-Consideration ;
    mcro:hasLimitation mcro:LTXVideo-Limitation ;
    mcro:hasModelDetail mcro:LTXVideo-ModelDetail .

mcro:Llama160m a mcro:Model ;
    mcro:hasCitation mcro:Llama160m-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:Llama160m-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:Llama160m-UseCaseInformationSection .

mcro:Llama3370BInstructAWQ a mcro:Model ;
    mcro:hasLicense mcro:Llama3370BInstructAWQ-License ;
    mcro:hasModelArchitecture mcro:Llama3370BInstructAWQ-ModelArchitecture .

mcro:M2M100418M a mcro:Model ;
    mcro:hasCitation mcro:M2M100418M-Citation ;
    mcro:hasModelArchitecture mcro:M2M100418M-ModelArchitecture ;
    mcro:hasUseCase mcro:M2M100418M-UseCase .

mcro:MERTv195M a mcro:Model ;
    mcro:hasCitation mcro:MERTv195M-Citation ;
    mcro:hasIntendedUseCase mcro:MERTv195M-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:MERTv195M-ModelArchitecture .

mcro:MahmoudAshrafmms300m1130forcedaligner a mcro:Model ;
    mcro:hasModelArchitecture mcro:MahmoudAshrafmms300m1130forcedaligner-ModelArchitecture .

mcro:Marqo-FashionSigLIP a mcro:Model ;
    mcro:hasModelDetail mcro:Marqo-FashionSigLIP-ModelDetail .

mcro:MaziyarPanahiMistral7BInstructv03GGUF a mcro:Model ;
    mcro:hasDescription mcro:MaziyarPanahiMistral7BInstructv03GGUF-Description .

mcro:MaziyarPanahiPhi35miniinstructGGUF a mcro:Model ;
    mcro:hasModelDetails mcro:MaziyarPanahiPhi35miniinstructGGUF-ModelDetail .

mcro:MaziyarPanahigemma31bitGGUF a mcro:Model ;
    mcro:hasCitation mcro:MaziyarPanahigemma31bitGGUF-CitationInformationSection ;
    mcro:hasLicense mcro:MaziyarPanahigemma31bitGGUF-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:MaziyarPanahigemma31bitGGUF-ModelArchitectureInformationSection .

mcro:ModelCardReport a mcro:Model ;
    mcro:hasCitation mcro:ModelCardReport-CitationInformationSection ;
    mcro:hasDataset mcro:ModelCardReport-DatasetInformationSection ;
    mcro:hasLicense mcro:ModelCardReport-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:ModelCardReport-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:ModelCardReport-UseCaseInformationSection .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli a mcro:Model ;
    mcro:hasConsideration mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Consideration ;
    mcro:hasModelDetail mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelDetail ;
    mcro:hasModelParameter mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelParameter ;
    mcro:hasUseCase mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-UseCase .

mcro:MyAwesomeModel a mcro:Model ;
    mcro:hasArchitecture mcro:MyAwesomeModel-Architecture ;
    mcro:hasCitation mcro:MyAwesomeModel-Citation ;
    mcro:hasDataset mcro:MyAwesomeModel-Dataset ;
    mcro:hasLicense mcro:MyAwesomeModel-License ;
    mcro:hasUseCase mcro:MyAwesomeModel-UseCase .

mcro:NLLB200 a mcro:Model ;
    mcro:hasArchitecture mcro:NLLB200-Arch ;
    mcro:hasCitation mcro:NLLB200-Citation ;
    mcro:hasConsideration mcro:NLLB200-Consideration ;
    mcro:hasDataset mcro:NLLB200-Dataset,
        mcro:NLLB200-EvalData,
        mcro:NLLB200-TrainData ;
    mcro:hasIntendedUseCase mcro:NLLB200-IntendedUse ;
    mcro:hasLicense mcro:NLLB200-License ;
    mcro:hasPerformanceMetric mcro:NLLB200-Metrics,
        mcro:NLLB200-PerfMetric ;
    mcro:hasTradeoff mcro:NLLB200-Tradeoff ;
    mcro:hasTrainingData mcro:NLLB200-TrainingData ;
    mcro:hasUseCase mcro:NLLB200-UseCase .

mcro:NepaliBERT a mcro:Model ;
    mcro:hasCitation mcro:NepaliBERT-CitationInformationSection ;
    mcro:hasDataset mcro:NepaliBERT-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:NepaliBERT-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:NepaliBERT-UseCaseInformationSection .

mcro:NusaBertnerv13 a mcro:Model ;
    mcro:hasModelDetail mcro:NusaBertnerv13-ModelDetail .

mcro:OrengutengLlama38BLexiUncensored a mcro:Model ;
    mcro:hasLicense mcro:OrengutengLlama38BLexiUncensored-License ;
    mcro:hasModelArchitecture mcro:OrengutengLlama38BLexiUncensored-ModelArchitecture ;
    mcro:hasQuantativeAnalysis mcro:OrengutengLlama38BLexiUncensored-QuantativeAnalysis ;
    mcro:hasUseCase mcro:OrengutengLlama38BLexiUncensored-UseCase .

mcro:ProtGPT2 a mcro:Model ;
    mcro:hasModelDetail mcro:ProtGPT2-ModelDetail ;
    mcro:hasUseCase mcro:ProtGPT2-UseCase .

mcro:Protbertmodel a mcro:Model ;
    mcro:hasEvaluationData mcro:Protbertmodel-EvaluationDataInformationSection ;
    mcro:hasModelDetail mcro:Protbertmodel-ModelDetail ;
    mcro:hasTrainingData mcro:Protbertmodel-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:Protbertmodel-UseCaseInformationSection .

mcro:QdrantallminiLML6v2withattentions a mcro:Model ;
    mcro:hasUseCase mcro:QdrantallminiLML6v2withattentions-UseCaseInformationSection .

mcro:Qwen1505BChat a mcro:Model ;
    mcro:hasCitation mcro:Qwen1505BChat-Citation ;
    mcro:hasModelArchitecture mcro:Qwen1505BChat-ModelArchitecture ;
    mcro:hasModelDetail mcro:Qwen1505BChat-ModelDetail .

mcro:Qwen2.5-1.5B a mcro:Model ;
    mcro:hasCitation mcro:Qwen2.5-1.5B-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:Qwen2.5-1.5B-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:Qwen2.5-1.5B-UseCaseInformationSection .

mcro:Qwen2.5-VL-32B-Instruct a mcro:Model ;
    mcro:hasModelDetail mcro:Qwen2.5-VL-32B-Instruct-ModelDetail ;
    mcro:hasUseCase mcro:Qwen2.5-VL-32B-Instruct-UseCase .

mcro:Qwen2505B a mcro:Model ;
    mcro:hasCitation mcro:Qwen2505B-Citation ;
    mcro:hasModelArchitecture mcro:Qwen2505B-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen2505B-UseCase .

mcro:Qwen2505BInstruct a mcro:Model ;
    mcro:hasDataset mcro:Qwen2505BInstruct-Dataset ;
    mcro:hasModelDetail mcro:Qwen2505BInstruct-ModelDetail,
        mcro:Qwen2505BInstruct-ModelDetailSection ;
    mcro:hasUseCase mcro:Qwen2505BInstruct-UseCaseInformationSection .

mcro:Qwen2514BInstruct a mcro:Model ;
    mcro:hasCitation mcro:Qwen2514BInstruct-Citation ;
    mcro:hasModelArchitecture mcro:Qwen2514BInstruct-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen2514BInstruct-UseCase .

mcro:Qwen2515BInstruct a mcro:Model ;
    mcro:hasModelArchitecture mcro:Qwen2515BInstruct-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:Qwen2515BInstruct-UseCaseInformationSection .

mcro:Qwen2532BInstruct a mcro:Model ;
    mcro:hasModelDetail mcro:Qwen2532BInstruct-ModelDetail .

mcro:Qwen253BInstruct a mcro:Model ;
    mcro:hasArchitecture mcro:Qwen253BInstruct-Architecture ;
    mcro:hasCitation mcro:Qwen253BInstruct-Citation ;
    mcro:hasUseCase mcro:Qwen253BInstruct-UseCase .

mcro:Qwen257B a mcro:Model ;
    mcro:hasCitation mcro:Qwen257B-Citation ;
    mcro:hasIntendedUseCase mcro:Qwen257B-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:Qwen257B-ModelArchitecture .

mcro:Qwen257BInstruct a mcro:Model ;
    mcro:hasCitation mcro:Qwen257BInstruct-Citation,
        mcro:Qwen257BInstruct-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:Qwen257BInstruct-ModelArchitecture,
        mcro:Qwen257BInstruct-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:Qwen257BInstruct-UseCase,
        mcro:Qwen257BInstruct-UseCaseInformationSection .

mcro:Qwen257BInstruct1M a mcro:Model ;
    mcro:hasCitation mcro:Qwen257BInstruct1M-Citation ;
    mcro:hasModelArchitecture mcro:Qwen257BInstruct1M-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen257BInstruct1M-UseCase .

mcro:Qwen25Coder7BInstruct a mcro:Model ;
    mcro:hasCitation mcro:Qwen25Coder7BInstruct-Citation ;
    mcro:hasModelArchitecture mcro:Qwen25Coder7BInstruct-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen25Coder7BInstruct-UseCase .

mcro:Qwen25Omni a mcro:Model ;
    mcro:hasCitation mcro:Qwen25Omni-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:Qwen25Omni-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:Qwen25Omni-UseCaseInformationSection .

mcro:Qwen2VL2BInstruct a mcro:Model ;
    mcro:hasCitation mcro:Qwen2VL2BInstruct-Citation ;
    mcro:hasLimitation mcro:Qwen2VL2BInstruct-Limitation ;
    mcro:hasModelArchitecture mcro:Qwen2VL2BInstruct-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen2VL2BInstruct-UseCase .

mcro:Qwen3-14B a mcro:Model ;
    mcro:hasModelArchitecture mcro:Qwen3-14B-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen3-14B-UseCase .

mcro:Qwen3-32B a mcro:Model ;
    mcro:hasCitation mcro:Qwen3-32B-Citation ;
    mcro:hasModelArchitecture mcro:Qwen3-32B-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen3-32B-UseCase .

mcro:Qwen3-4B a mcro:Model ;
    mcro:hasModelDetail mcro:Qwen3-4B-ModelDetail .

mcro:Qwen3-8B a mcro:Model ;
    mcro:hasCitation mcro:Qwen3-8B-Citation ;
    mcro:hasModelDetail mcro:Qwen3-8B-ModelDetail .

mcro:Qwen306B a mcro:Model ;
    mcro:hasCitation mcro:Qwen306B-Citation ;
    mcro:hasModelArchitecture mcro:Qwen306B-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen306B-UseCase .

mcro:Qwen317B a mcro:Model ;
    mcro:hasCitation mcro:Qwen317B-Citation ;
    mcro:hasModelDetail mcro:Qwen317B-ModelDetail ;
    mcro:hasUseCase mcro:Qwen317B-UseCase .

mcro:QwenQwen2505BInstruct a mcro:Model ;
    mcro:hasModelDetail mcro:QwenQwen2505BInstruct-ModelDetail ;
    mcro:hasUseCase mcro:QwenQwen2505BInstruct-UseCaseInformationSection .

mcro:QwenQwen2515BInstruct a mcro:Model ;
    mcro:hasModelDetail mcro:QwenQwen2515BInstruct-ModelDetail ;
    mcro:hasUseCase mcro:QwenQwen2515BInstruct-UseCase .

mcro:QwenQwen25Coder7BInstructGPTQInt4 a mcro:Model ;
    mcro:hasCitation mcro:QwenQwen25Coder7BInstructGPTQInt4-Citation ;
    mcro:hasModelArchitecture mcro:QwenQwen25Coder7BInstructGPTQInt4-ModelArchitecture ;
    mcro:hasUseCase mcro:QwenQwen25Coder7BInstructGPTQInt4-UseCase .

mcro:QwenQwen25VL3BInstruct a mcro:Model ;
    mcro:hasCitation mcro:QwenQwen25VL3BInstruct-Citation,
        mcro:QwenQwen25VL3BInstruct-Citation2,
        mcro:QwenQwen25VL3BInstruct-Citation3 ;
    mcro:hasModelArchitecture mcro:QwenQwen25VL3BInstruct-ModelArchitecture ;
    mcro:hasUseCase mcro:QwenQwen25VL3BInstruct-UseCase .

mcro:QwenQwen25VL7BInstruct a mcro:Model ;
    mcro:hasCitation mcro:QwenQwen25VL7BInstruct-Citation ;
    mcro:hasModelArchitecture mcro:QwenQwen25VL7BInstruct-ModelArchitecture ;
    mcro:hasUseCase mcro:QwenQwen25VL7BInstruct-UseCase .

mcro:QwenQwen2VL7BInstruct a mcro:Model ;
    mcro:hasCitation mcro:QwenQwen2VL7BInstruct-Citation ;
    mcro:hasLicense mcro:QwenQwen2VL7BInstruct-License ;
    mcro:hasLimitation mcro:QwenQwen2VL7BInstruct-Limitation ;
    mcro:hasModelDetail mcro:QwenQwen2VL7BInstruct-ModelDetail ;
    mcro:hasUseCase mcro:QwenQwen2VL7BInstruct-UseCase .

mcro:RoBERTaBaseModel a mcro:Model ;
    mcro:hasCitation mcro:RoBERTaBaseModel-Citation ;
    mcro:hasModelArchitecture mcro:RoBERTaBaseModel-ModelArchitecture ;
    mcro:hasTrainingData mcro:RoBERTaBaseModel-TrainingData ;
    mcro:hasUseCase mcro:RoBERTaBaseModel-UseCase .

mcro:RoBERTaLargeModel a mcro:Model ;
    mcro:hasCitation mcro:RoBERTaLargeModel-Citation ;
    mcro:hasIntendedUseCase mcro:RoBERTaLargeModel-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:RoBERTaLargeModel-ModelArchitecture ;
    mcro:hasTrainingData mcro:RoBERTaLargeModel-TrainingData .

mcro:RobertaLargeMnli a mcro:Model ;
    mcro:hasCitation mcro:RobertaLargeMnli-Citation ;
    mcro:hasEvaluationData mcro:RobertaLargeMnli-EvaluationData ;
    mcro:hasModelDetail mcro:RobertaLargeMnli-ModelDetail ;
    mcro:hasTrainingData mcro:RobertaLargeMnli-TrainingData ;
    mcro:hasUseCase mcro:RobertaLargeMnli-UseCase .

mcro:SWividF5TTS a mcro:Model ;
    mcro:hasCitation mcro:SWividF5TTS-Citation .

mcro:Salesforceblip2opt27b a mcro:Model ;
    mcro:hasConsideration mcro:Salesforceblip2opt27b-Consideration ;
    mcro:hasEthicalConsideration mcro:Salesforceblip2opt27b-EthicalConsideration ;
    mcro:hasModelDetail mcro:Salesforceblip2opt27b-ModelDetail .

mcro:Salesforceblipimagecaptioningbase a mcro:Model ;
    mcro:hasCitation mcro:Salesforceblipimagecaptioningbase-Citation ;
    mcro:hasConsideration mcro:Salesforceblipimagecaptioningbase-Consideration ;
    mcro:hasDataset mcro:Salesforceblipimagecaptioningbase-Dataset ;
    mcro:hasModelArchitecture mcro:Salesforceblipimagecaptioningbase-ModelArchitecture ;
    mcro:hasUseCase mcro:Salesforceblipimagecaptioningbase-UseCase .

mcro:Salesforceblipimagecaptioninglarge a mcro:Model ;
    mcro:hasCitation mcro:Salesforceblipimagecaptioninglarge-Citation ;
    mcro:hasConsideration mcro:Salesforceblipimagecaptioninglarge-Consideration ;
    mcro:hasDataset mcro:Salesforceblipimagecaptioninglarge-Dataset ;
    mcro:hasModelArchitecture mcro:Salesforceblipimagecaptioninglarge-ModelArchitecture ;
    mcro:hasUseCase mcro:Salesforceblipimagecaptioninglarge-UseCase .

mcro:Salesforceblipvqabase a mcro:Model ;
    mcro:hasCitation mcro:Salesforceblipvqabase-Citation ;
    mcro:hasConsideration mcro:Salesforceblipvqabase-Consideration ;
    mcro:hasModelArchitecture mcro:Salesforceblipvqabase-Architecture ;
    mcro:hasUseCase mcro:Salesforceblipvqabase-UseCase .

mcro:Salesforceblipvqacapfiltlarge a mcro:Model ;
    mcro:hasConsideration mcro:Salesforceblipvqacapfiltlarge-Consideration ;
    mcro:hasModelDetail mcro:Salesforceblipvqacapfiltlarge-ModelDetail ;
    mcro:hasUseCase mcro:Salesforceblipvqacapfiltlarge-UseCase .

mcro:Salesforcecodet5basemultisum a mcro:Model ;
    mcro:hasCitation mcro:Salesforcecodet5basemultisum-Citation ;
    mcro:hasDataset mcro:Salesforcecodet5basemultisum-Dataset ;
    mcro:hasModelArchitecture mcro:Salesforcecodet5basemultisum-Architecture ;
    mcro:hasUseCase mcro:Salesforcecodet5basemultisum-UseCase .

mcro:Segformerb0finetunedade512512 a mcro:Model ;
    mcro:hasUseCase mcro:Segformerb0finetunedade512512-UseCase .

mcro:SmolDocling256Mpreview a mcro:Model ;
    mcro:hasCitation mcro:SmolDocling256Mpreview-CitationInformationSection ;
    mcro:hasLicense mcro:SmolDocling256Mpreview-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:SmolDocling256Mpreview-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:SmolDocling256Mpreview-UseCaseInformationSection .

mcro:SmolLM2 a mcro:Model ;
    mcro:hasCitation mcro:SmolLM2-Citation ;
    mcro:hasDataset mcro:SmolLM2-Dataset ;
    mcro:hasLicense mcro:SmolLM2-License ;
    mcro:hasLimitation mcro:SmolLM2-Limitations ;
    mcro:hasModelArchitecture mcro:SmolLM2-ModelArchitecture ;
    mcro:hasTrainingData mcro:SmolLM2-Training ;
    mcro:hasUseCase mcro:SmolLM2-UseCase .

mcro:Snowflakesnowflakearcticembedm a mcro:Model ;
    mcro:hasLicense mcro:Snowflakesnowflakearcticembedm-License ;
    mcro:hasModelDetail mcro:Snowflakesnowflakearcticembedm-ModelDetail ;
    mcro:hasUseCase mcro:Snowflakesnowflakearcticembedm-UseCase .

mcro:Snowflakesnowflakearcticembedxs a mcro:Model ;
    mcro:hasDataset mcro:Snowflakesnowflakearcticembedxs-Dataset ;
    mcro:hasIntendedUseCase mcro:Snowflakesnowflakearcticembedxs-IntendedUseCase ;
    mcro:hasLicense mcro:Snowflakesnowflakearcticembedxs-License ;
    mcro:hasModelArchitecture mcro:Snowflakesnowflakearcticembedxs-ModelArchitecture ;
    mcro:hasModelDetail mcro:Snowflakesnowflakearcticembedxs-ModelDetail .

mcro:Stablediffusionsafetychecker a mcro:Model ;
    mcro:hasBiasRiskLimitationConsideration mcro:Stablediffusionsafetychecker-BiasRiskLimitationConsideration ;
    mcro:hasCitation mcro:Stablediffusionsafetychecker-Citation ;
    mcro:hasEnvironmentalImpact mcro:Stablediffusionsafetychecker-EnvironmentalImpact ;
    mcro:hasEvaluation mcro:Stablediffusionsafetychecker-Evaluation ;
    mcro:hasModelDetail mcro:Stablediffusionsafetychecker-ModelDetail ;
    mcro:hasTechnicalSpecification mcro:Stablediffusionsafetychecker-TechnicalSpecification ;
    mcro:hasTrainingDetail mcro:Stablediffusionsafetychecker-TrainingDetail ;
    mcro:hasUseCase mcro:Stablediffusionsafetychecker-UseCase .

mcro:Stablediffusionv14ModelCard a mcro:Model ;
    mcro:hasDataset mcro:Stablediffusionv14ModelCard-Dataset ;
    mcro:hasModelDetail mcro:Stablediffusionv14ModelCard-ModelDetailSection ;
    mcro:hasUseCase mcro:Stablediffusionv14ModelCard-UseCase .

mcro:StanfordAIMIstanforddeidentifierbase a mcro:Model ;
    mcro:hasCitation mcro:StanfordAIMIstanforddeidentifierbase-Citation ;
    mcro:hasUseCase mcro:StanfordAIMIstanforddeidentifierbase-UseCase .

mcro:StellaEn400Mv5 a mcro:Model ;
    mcro:hasCitation mcro:StellaEn400Mv5-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:StellaEn400Mv5-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:StellaEn400Mv5-UseCaseInformationSection .

mcro:Supabasegtesmall a mcro:Model ;
    mcro:hasLimitation mcro:Supabasegtesmall-Limitation ;
    mcro:hasModelArchitecture mcro:Supabasegtesmall-ModelArchitecture ;
    mcro:hasModelDetail mcro:Supabasegtesmall-ModelDetail ;
    mcro:hasUseCase mcro:Supabasegtesmall-UseCase .

mcro:Systranfasterwhisperbase a mcro:Model ;
    mcro:hasModelArchitecture mcro:Systranfasterwhisperbase-ModelArchitecture .

mcro:Systranfasterwhispersmall a mcro:Model ;
    mcro:hasModelArchitecture mcro:Systranfasterwhispersmall-ModelArchitecture .

mcro:T5basesummarizationclaimextractor a mcro:Model ;
    mcro:hasCitation mcro:T5basesummarizationclaimextractor-Citation ;
    mcro:hasModelArchitecture mcro:T5basesummarizationclaimextractor-ModelArchitecture ;
    mcro:hasModelDetail mcro:T5basesummarizationclaimextractor-ModelDetail ;
    mcro:hasUseCase mcro:T5basesummarizationclaimextractor-UseCase .

mcro:TRELLISImageLarge a mcro:Model ;
    mcro:hasCitation mcro:TRELLISImageLarge-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:TRELLISImageLarge-ModelArchitectureInformationSection .

mcro:TahaDouajidetrdoctabledetection a mcro:Model ;
    mcro:hasConsideration mcro:TahaDouajidetrdoctabledetection-Consideration ;
    mcro:hasModelDetail mcro:TahaDouajidetrdoctabledetection-ModelDetail ;
    mcro:hasTrainingData mcro:TahaDouajidetrdoctabledetection-TrainingData ;
    mcro:hasUseCase mcro:TahaDouajidetrdoctabledetection-UseCase .

mcro:TaiyiCLIPRoberta102MChinese a mcro:Model ;
    mcro:hasDataset mcro:TaiyiCLIPRoberta102MChinese-Dataset ;
    mcro:hasModelDetail mcro:TaiyiCLIPRoberta102MChinese-ModelDetail ;
    mcro:hasUseCase mcro:TaiyiCLIPRoberta102MChinese-UseCase .

mcro:TheBlokeMistral7BInstructv01GGUF a mcro:Model ;
    mcro:hasModelArchitecture mcro:TheBlokeMistral7BInstructv01GGUF-ModelArchitecture .

mcro:TheBlokephi2GGUF a mcro:Model ;
    mcro:hasCitation mcro:TheBlokephi2GGUF-Citation ;
    mcro:hasDataset mcro:TheBlokephi2GGUF-Dataset ;
    mcro:hasLicense mcro:TheBlokephi2GGUF-License ;
    mcro:hasModelArchitecture mcro:TheBlokephi2GGUF-ModelArchitecture ;
    mcro:hasUseCase mcro:TheBlokephi2GGUF-UseCase .

mcro:TinyLlama11BChatv10 a mcro:Model ;
    mcro:hasDataset mcro:TinyLlama11BChatv10-Dataset ;
    mcro:hasModelArchitecture mcro:TinyLlama11BChatv10-ModelArchitecture ;
    mcro:hasModelDetail mcro:TinyLlama11BChatv10-ModelDetail ;
    mcro:hasUseCase mcro:TinyLlama11BChatv10-UseCase .

mcro:UFNLPgatortronbase a mcro:Model ;
    mcro:hasCitation mcro:UFNLPgatortronbase-Citation ;
    mcro:hasDataset mcro:UFNLPgatortronbase-Dataset ;
    mcro:hasModelArchitecture mcro:UFNLPgatortronbase-ModelArchitecture .

mcro:USERbgem3 a mcro:Model ;
    mcro:hasLimitation mcro:USERbgem3-Limitation ;
    mcro:hasModelDetail mcro:USERbgem3-ModelDetail ;
    mcro:hasTrainingData mcro:USERbgem3-TrainingData ;
    mcro:hasUseCase mcro:USERbgem3-UseCase .

mcro:ViTSO400M14SigLIP384 a mcro:Model ;
    mcro:hasModelArchitecture mcro:ViTSO400M14SigLIP384-Architecture ;
    mcro:hasModelDetail mcro:ViTSO400M14SigLIP384-ModelDetail ;
    mcro:hasUseCase mcro:ViTSO400M14SigLIP384-UseCase .

mcro:VitTinyVitSmall a mcro:Model ;
    mcro:hasModelArchitecture mcro:VitTinyVitSmall-ModelArchitecture .

mcro:WDViTLargeTaggerV3 a mcro:Model ;
    mcro:hasDataset mcro:WDViTLargeTaggerV3-DatasetInformationSection ;
    mcro:hasPerformanceMetric mcro:WDViTLargeTaggerV3-PerformanceMetric .

mcro:WhereIsAIUAELargeV1 a mcro:Model ;
    mcro:hasCitation mcro:WhereIsAIUAELargeV1-Citation ;
    mcro:hasLicense mcro:WhereIsAIUAELargeV1-License ;
    mcro:hasUseCase mcro:WhereIsAIUAELargeV1-UseCase .

mcro:WhisperbaseenmodelforCTranslate2 a mcro:Model ;
    mcro:hasModelArchitecture mcro:WhisperbaseenmodelforCTranslate2-ModelArchitecture .

mcro:Whisperlargev2modelforCTranslate2 a mcro:Model ;
    mcro:hasModelArchitecture mcro:Whisperlargev2modelforCTranslate2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:Whisperlargev2modelforCTranslate2-UseCaseInformationSection .

mcro:Whisperlargev3model a mcro:Model ;
    mcro:hasModelDetail mcro:Whisperlargev3model-ModelDetail .

mcro:Whispertiny a mcro:Model ;
    mcro:hasModelDetail mcro:Whispertiny-ModelDetail .

mcro:XTTS a mcro:Model ;
    mcro:hasLicense mcro:XTTS-LicenseInformationSection ;
    mcro:hasUseCase mcro:XTTS-UseCaseInformationSection .

mcro:XenovaallMiniLML6v2 a mcro:Model ;
    mcro:hasModelArchitecture mcro:XenovaallMiniLML6v2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:XenovaallMiniLML6v2-UseCaseInformationSection .

mcro:Xenovabgebaseenv15 a mcro:Model ;
    mcro:hasLicense mcro:Xenovabgebaseenv15-LicenseInformationSection ;
    mcro:hasUseCase mcro:Xenovabgebaseenv15-UseCaseInformationSection .

mcro:Xlmrobertalargenerspanish a mcro:Model ;
    mcro:hasModelDetail mcro:Xlmrobertalargenerspanish-ModelDetail ;
    mcro:hasPerformanceMetric mcro:Xlmrobertalargenerspanish-Performance .

mcro:XuhuiToxDectrobertalarge a mcro:Model ;
    mcro:hasCitation mcro:XuhuiToxDectrobertalarge-Citation ;
    mcro:hasDataset mcro:XuhuiToxDectrobertalarge-Dataset ;
    mcro:hasIntendedUseCase mcro:XuhuiToxDectrobertalarge-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:XuhuiToxDectrobertalarge-ModelArchitecture .

mcro:YOLOWorldMirror a mcro:Model ;
    mcro:hasDocumentation mcro:YOLOWorldMirror-Documentation ;
    mcro:hasModelDetail mcro:YOLOWorldMirror-ModelDetail .

mcro:YOLOv8DetectionModel a mcro:Model ;
    mcro:hasConsideration mcro:YOLOv8DetectionModel-ConsiderationInformationSection ;
    mcro:hasDataset mcro:YOLOv8DetectionModel-DatasetInformationSection ;
    mcro:hasPerformanceMetric mcro:YOLOv8DetectionModel-PerformanceMetricInformationSection ;
    mcro:hasUseCase mcro:YOLOv8DetectionModel-UseCaseInformationSection .

mcro:Yehorw2vxlsruk a mcro:Model ;
    mcro:hasCitation mcro:Yehorw2vxlsruk-CitationInformationSection .

mcro:ai4bharatindictrans2indicen1B a mcro:Model ;
    mcro:hasCitation mcro:ai4bharatindictrans2indicen1B-Citation ;
    mcro:hasUseCase mcro:ai4bharatindictrans2indicen1B-UseCase .

mcro:aiforeversbertlargenluru a mcro:Model ;
    mcro:hasModelArchitecture mcro:aiforeversbertlargenluru-ModelArchitecture ;
    mcro:hasUseCase mcro:aiforeversbertlargenluru-UseCase .

mcro:alakxendermmsttsdivfinetunedmdm01 a mcro:Model ;
    mcro:hasModelDetail mcro:alakxendermmsttsdivfinetunedmdm01-ModelDetail .

mcro:albertbasev2 a mcro:Model ;
    mcro:hasArchitecture mcro:albertbasev2-Architecture ;
    mcro:hasCitation mcro:albertbasev2-Citation ;
    mcro:hasIntendedUseCase mcro:albertbasev2-IntendedUseCase ;
    mcro:hasLimitation mcro:albertbasev2-Limitation ;
    mcro:hasTrainingData mcro:albertbasev2-TrainingData .

mcro:all-distilroberta-v1 a mcro:Model ;
    mcro:hasIntendedUseCase mcro:all-distilroberta-v1-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:all-distilroberta-v1-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:all-distilroberta-v1-TrainingDataInformationSection .

mcro:allMiniLML12v2 a mcro:Model ;
    mcro:hasModelArchitecture mcro:allMiniLML12v2-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:allMiniLML12v2-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:allMiniLML12v2-UseCaseInformationSection .

mcro:allMiniLML6v2 a mcro:Model ;
    mcro:hasIntendedUseCase mcro:allMiniLML6v2-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:allMiniLML6v2-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:allMiniLML6v2-TrainingDataInformationSection .

mcro:allmpnetbasev2 a mcro:Model ;
    mcro:hasIntendedUseCase mcro:allmpnetbasev2-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:allmpnetbasev2-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:allmpnetbasev2-TrainingDataInformationSection .

mcro:allrobertalargev1 a mcro:Model ;
    mcro:hasBackground mcro:allrobertalargev1-BackgroundInformationSection ;
    mcro:hasModelArchitecture mcro:allrobertalargev1-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:allrobertalargev1-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:allrobertalargev1-UseCaseInformationSection .

mcro:answerdotaiModernBERTbase a mcro:Model ;
    mcro:hasLimitation mcro:answerdotaiModernBERTbase-Limitation ;
    mcro:hasModelArchitecture mcro:answerdotaiModernBERTbase-ModelArchitecture ;
    mcro:hasModelDetail mcro:answerdotaiModernBERTbase-ModelDetail ;
    mcro:hasUseCase mcro:answerdotaiModernBERTbase-UseCase .

mcro:appleOpenELM11BInstruct a mcro:Model ;
    mcro:hasCitation mcro:appleOpenELM11BInstruct-Citation ;
    mcro:hasDataset mcro:appleOpenELM11BInstruct-Dataset ;
    mcro:hasLicense mcro:appleOpenELM11BInstruct-License ;
    mcro:hasModelArchitecture mcro:appleOpenELM11BInstruct-ModelArchitecture ;
    mcro:hasUseCase mcro:appleOpenELM11BInstruct-UseCase .

mcro:arabertv1andv2 a mcro:Model ;
    mcro:hasCitation mcro:arabertv1andv2-Citation ;
    mcro:hasDataset mcro:arabertv1andv2-Dataset ;
    mcro:hasModelArchitecture mcro:arabertv1andv2-ModelArchitecture ;
    mcro:hasUseCase mcro:arabertv1andv2-UseCase .

mcro:atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct a mcro:Model ;
    mcro:hasModelArchitecture mcro:atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct-ModelArchitectureInformationSection .

mcro:audeeringwav2vec2largerobust12ftemotionmspdim a mcro:Model ;
    mcro:hasCitation mcro:audeeringwav2vec2largerobust12ftemotionmspdim-CitationInformationSection ;
    mcro:hasDataset mcro:audeeringwav2vec2largerobust12ftemotionmspdim-DatasetInformationSection ;
    mcro:hasLicense mcro:audeeringwav2vec2largerobust12ftemotionmspdim-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:audeeringwav2vec2largerobust12ftemotionmspdim-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:audeeringwav2vec2largerobust12ftemotionmspdim-UseCaseInformationSection .

mcro:autogluonchronosboltbase a mcro:Model ;
    mcro:hasModelArchitecture mcro:autogluonchronosboltbase-ModelArchitecture ;
    mcro:hasModelDetail mcro:autogluonchronosboltbase-ModelDetail ;
    mcro:hasModelParameter mcro:autogluonchronosboltbase-ModelParameter ;
    mcro:hasQuantativeAnalysis mcro:autogluonchronosboltbase-QuantativeAnalysis ;
    mcro:hasUseCase mcro:autogluonchronosboltbase-UseCase .

mcro:avsolatorioGISTsmallEmbeddingv0 a mcro:Model ;
    mcro:hasCitation mcro:avsolatorioGISTsmallEmbeddingv0-CitationInformationSection ;
    mcro:hasDataset mcro:avsolatorioGISTsmallEmbeddingv0-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:avsolatorioGISTsmallEmbeddingv0-ModelArchitectureInformationSection .

mcro:banglat5_banglaparaphrase a mcro:Model ;
    mcro:hasCitation mcro:banglat5_banglaparaphrase-CitationInformationSection ;
    mcro:hasDataset mcro:banglat5_banglaparaphrase-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:banglat5_banglaparaphrase-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:banglat5_banglaparaphrase-UseCaseInformationSection .

mcro:bartbase a mcro:Model ;
    mcro:hasModelDetail mcro:bartbase-ModelDetail ;
    mcro:hasUseCase mcro:bartbase-UseCaseInformationSection .

mcro:bartlargemnli a mcro:Model ;
    mcro:hasIntendedUseCase mcro:bartlargemnli-UseCase ;
    mcro:hasModelDetail mcro:bartlargemnli-ModelDetail .

mcro:bartlargesizedmodelfinetunedoncnndailymail a mcro:Model ;
    mcro:hasCitation mcro:bartlargesizedmodelfinetunedoncnndailymail-Citation ;
    mcro:hasIntendedUseCase mcro:bartlargesizedmodelfinetunedoncnndailymail-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:bartlargesizedmodelfinetunedoncnndailymail-ModelArchitecture .

mcro:bert-base-uncased a mcro:Model ;
    mcro:hasCitation mcro:bert-base-uncased-Citation ;
    mcro:hasDataset mcro:bert-base-uncased-Dataset ;
    mcro:hasLicense mcro:bert-base-uncased-License ;
    mcro:hasModelArchitecture mcro:bert-base-uncased-Arch ;
    mcro:hasUseCase mcro:bert-base-uncased-UseCase .

mcro:bertMiniatures a mcro:Model ;
    mcro:hasCitation mcro:bertMiniatures-Citation ;
    mcro:hasModelArchitecture mcro:bertMiniatures-Architecture ;
    mcro:hasUseCase mcro:bertMiniatures-UseCase .

mcro:bertbaseNER a mcro:Model ;
    mcro:hasEvalResults mcro:bertbaseNER-EvalResults ;
    mcro:hasModelDetail mcro:bertbaseNER-ModelDetail .

mcro:bertbasechinese a mcro:Model ;
    mcro:hasCitation mcro:bertbasechinese-Citation ;
    mcro:hasConsideration mcro:bertbasechinese-Consideration ;
    mcro:hasLicense mcro:bertbasechinese-License ;
    mcro:hasModelDetail mcro:bertbasechinese-ModelDetail ;
    mcro:hasModelParameter mcro:bertbasechinese-ModelParameter ;
    mcro:hasQuantativeAnalysis mcro:bertbasechinese-QuantativeAnalysis ;
    mcro:hasTrainingData mcro:bertbasechinese-TrainingData ;
    mcro:hasUseCase mcro:bertbasechinese-UseCase .

mcro:bertbasemodelcased a mcro:Model ;
    mcro:hasConsideration mcro:bertbasemodelcased-Consideration ;
    mcro:hasEvaluationData mcro:bertbasemodelcased-EvaluationData ;
    mcro:hasModelDetail mcro:bertbasemodelcased-ModelDetail ;
    mcro:hasTrainingData mcro:bertbasemodelcased-TrainingData ;
    mcro:hasUseCase mcro:bertbasemodelcased-UseCase .

mcro:bertbasemultilingualuncasedsentiment a mcro:Model ;
    mcro:hasModelDetail mcro:bertbasemultilingualuncasedsentiment-ModelDetail .

mcro:bertbaseuncased a mcro:Model ;
    mcro:hasConsideration mcro:bertbaseuncased-Consideration ;
    mcro:hasIntendedUseCase mcro:bertbaseuncased-UseCase ;
    mcro:hasModelDetail mcro:bertbaseuncased-ModelDetail ;
    mcro:hasTrainingData mcro:bertbaseuncased-TrainingData .

mcro:bertlargemodeluncased a mcro:Model ;
    mcro:hasModelDetail mcro:bertlargemodeluncased-ModelDetail ;
    mcro:hasTrainingData mcro:bertlargemodeluncased-TrainingData ;
    mcro:hasUseCase mcro:bertlargemodeluncased-UseCase .

mcro:bertmultilingualbasemodelcased a mcro:Model ;
    mcro:hasModelArchitecture mcro:bertmultilingualbasemodelcased-ModelArchitecture ;
    mcro:hasModelDetail mcro:bertmultilingualbasemodelcased-ModelDetail ;
    mcro:hasTrainingData mcro:bertmultilingualbasemodelcased-TrainingData ;
    mcro:hasUseCase mcro:bertmultilingualbasemodelcased-UseCase .

mcro:bertmultilingualbasemodeluncased a mcro:Model ;
    mcro:hasDataset mcro:bertmultilingualbasemodeluncased-Dataset ;
    mcro:hasModelArchitecture mcro:bertmultilingualbasemodeluncased-ModelArchitecture ;
    mcro:hasModelDetail mcro:bertmultilingualbasemodeluncased-ModelDetail ;
    mcro:hasTrainingData mcro:bertmultilingualbasemodeluncased-TrainingData ;
    mcro:hasUseCase mcro:bertmultilingualbasemodeluncased-UseCase .

mcro:bertsmalluncased a mcro:Model ;
    mcro:hasCitation mcro:bertsmalluncased-Citation ;
    mcro:hasModelArchitecture mcro:bertsmalluncased-ModelArchitecture ;
    mcro:hasUseCase mcro:bertsmalluncased-UseCase .

mcro:bertwithflashattention a mcro:Model ;
    mcro:hasModelParameter mcro:bertwithflashattention-ModelParameterSection .

mcro:bge-m3-onnx-o4 a mcro:Model ;
    mcro:hasUseCase mcro:bge-m3-onnx-o4-UseCase .

mcro:bigsciencebloomz560m a mcro:Model ;
    mcro:hasCitation mcro:bigsciencebloomz560m-Citation ;
    mcro:hasModelArchitecture mcro:bigsciencebloomz560m-ModelArchitecture ;
    mcro:hasUseCase mcro:bigsciencebloomz560m-UseCase .

mcro:bigvganv222khz80band256x a mcro:Model ;
    mcro:hasCitation mcro:bigvganv222khz80band256x-Citation ;
    mcro:hasDataset mcro:bigvganv222khz80band256x-Dataset ;
    mcro:hasIntendedUseCase mcro:bigvganv222khz80band256x-UseCase ;
    mcro:hasModelArchitecture mcro:bigvganv222khz80band256x-ModelArchitecture .

mcro:bigvganv244khz128band512x a mcro:Model ;
    mcro:hasCitation mcro:bigvganv244khz128band512x-CitationInformationSection ;
    mcro:hasDataset mcro:bigvganv244khz128band512x-DatasetInformationSection ;
    mcro:hasIntendedUseCase mcro:bigvganv244khz128band512x-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:bigvganv244khz128band512x-ModelArchitectureInformationSection .

mcro:blackforestlabsFLUX1Filldev a mcro:Model ;
    mcro:hasLicense mcro:blackforestlabsFLUX1Filldev-License ;
    mcro:hasLimitation mcro:blackforestlabsFLUX1Filldev-Limitation ;
    mcro:hasModelDetail mcro:blackforestlabsFLUX1Filldev-ModelDetail ;
    mcro:hasOutofScopeUseCase mcro:blackforestlabsFLUX1Filldev-OutofScopeUseCase ;
    mcro:hasUseCase mcro:blackforestlabsFLUX1Filldev-UseCase .

mcro:blackforestlabsFLUX1dev a mcro:Model ;
    mcro:hasArchitecture mcro:blackforestlabsFLUX1dev-Architecture ;
    mcro:hasLicense mcro:blackforestlabsFLUX1dev-License ;
    mcro:hasLimitation mcro:blackforestlabsFLUX1dev-Limitation ;
    mcro:hasUseCase mcro:blackforestlabsFLUX1dev-UseCase .

mcro:blackforestlabsFLUX1schnell a mcro:Model ;
    mcro:hasLicense mcro:blackforestlabsFLUX1schnell-LicenseInformationSection ;
    mcro:hasLimitation mcro:blackforestlabsFLUX1schnell-LimitationInformationSection ;
    mcro:hasOutOfScopeUse mcro:blackforestlabsFLUX1schnell-OutOfScopeUse ;
    mcro:hasUseCase mcro:blackforestlabsFLUX1schnell-UseCaseInformationSection .

mcro:byt5small a mcro:Model ;
    mcro:hasDataset mcro:byt5small-Dataset ;
    mcro:hasModelDetail mcro:byt5small-ModelDetail ;
    mcro:hasUseCase mcro:byt5small-UseCase .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltext a mcro:Model ;
    mcro:hasDataset mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Dataset ;
    mcro:hasModelDetail mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-ModelDetail ;
    mcro:hasUseCase mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-UseCase .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken a mcro:Model ;
    mcro:hasCitation mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Citation ;
    mcro:hasDataset mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Dataset ;
    mcro:hasModelArchitecture mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-ModelArchitecture .

mcro:canary1bflash a mcro:Model ;
    mcro:hasCitation mcro:canary1bflash-CitationInformationSection ;
    mcro:hasDataset mcro:canary1bflash-DatasetInformationSection ;
    mcro:hasLicense mcro:canary1bflash-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:canary1bflash-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:canary1bflash-UseCaseInformationSection .

mcro:chronos-t5-base a mcro:Model ;
    mcro:hasCitation mcro:chronos-t5-base-CitationInformationSection ;
    mcro:hasLicense mcro:chronos-t5-base-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:chronos-t5-base-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:chronos-t5-base-UseCaseInformationSection .

mcro:chronos-t5-small a mcro:Model ;
    mcro:hasCitation mcro:chronos-t5-small-Citation ;
    mcro:hasLicense mcro:chronos-t5-small-License ;
    mcro:hasModelArchitecture mcro:chronos-t5-small-ModelArchitecture ;
    mcro:hasUseCase mcro:chronos-t5-small-UseCase .

mcro:chronos-t5-tiny a mcro:Model ;
    mcro:hasCitation mcro:chronos-t5-tiny-CitationSection ;
    mcro:hasLicense mcro:chronos-t5-tiny-LicenseSection ;
    mcro:hasModelArchitecture mcro:chronos-t5-tiny-ArchitectureSection ;
    mcro:hasUseCase mcro:chronos-t5-tiny-UseCaseSection .

mcro:chronosboltbase a mcro:Model ;
    mcro:hasCitation mcro:chronosboltbase-Citation ;
    mcro:hasLicense mcro:chronosboltbase-License ;
    mcro:hasModelArchitecture mcro:chronosboltbase-ModelArchitecture ;
    mcro:hasUseCase mcro:chronosboltbase-UseCase .

mcro:chronosboltmini a mcro:Model ;
    mcro:hasCitation mcro:chronosboltmini-Citation ;
    mcro:hasDataset mcro:chronosboltmini-Dataset ;
    mcro:hasLicense mcro:chronosboltmini-License ;
    mcro:hasModelArchitecture mcro:chronosboltmini-ModelArchitecture ;
    mcro:hasUseCase mcro:chronosboltmini-UseCase .

mcro:chronosboltsmall a mcro:Model ;
    mcro:hasCitation mcro:chronosboltsmall-Citation ;
    mcro:hasDataset mcro:chronosboltsmall-Dataset ;
    mcro:hasLicense mcro:chronosboltsmall-License ;
    mcro:hasModelArchitecture mcro:chronosboltsmall-ModelArchitecture ;
    mcro:hasUseCase mcro:chronosboltsmall-UseCase .

mcro:chronosbolttiny a mcro:Model ;
    mcro:hasCitation mcro:chronosbolttiny-Citation ;
    mcro:hasDataset mcro:chronosbolttiny-Dataset ;
    mcro:hasLicense mcro:chronosbolttiny-License ;
    mcro:hasModelArchitecture mcro:chronosbolttiny-ModelArchitecture ;
    mcro:hasUseCase mcro:chronosbolttiny-UseCase .

mcro:clip a mcro:Model ;
    mcro:hasConsideration mcro:clip-Consideration ;
    mcro:hasDataset mcro:clip-Dataset ;
    mcro:hasModelDetail mcro:clip-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:clip-QuantativeAnalysis ;
    mcro:hasUseCase mcro:clip-UseCase .

mcro:clip-vit-large-patch14-336 a mcro:Model ;
    mcro:hasDataset mcro:clip-vit-large-patch14-336-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:clip-vit-large-patch14-336-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:clip-vit-large-patch14-336-UseCaseInformationSection .

mcro:clipvitlargepatch14 a mcro:Model ;
    mcro:hasConsideration mcro:clipvitlargepatch14-Consideration ;
    mcro:hasDataset mcro:clipvitlargepatch14-Dataset ;
    mcro:hasModelDetail mcro:clipvitlargepatch14-ModelDetail ;
    mcro:hasQuantitativeAnalysis mcro:clipvitlargepatch14-QuantitativeAnalysis ;
    mcro:hasUseCase mcro:clipvitlargepatch14-UseCase .

mcro:codebertbase a mcro:Model ;
    mcro:hasCitation mcro:codebertbase-Citation ;
    mcro:hasDataset mcro:codebertbase-Dataset ;
    mcro:hasModelArchitecture mcro:codebertbase-ModelArchitecture ;
    mcro:hasReference mcro:codebertbase-Reference,
        mcro:codebertbase-Reference2 .

mcro:cognitivecomputationsdolphin291yi1534b a mcro:Model ;
    mcro:hasIntendedUseCase mcro:cognitivecomputationsdolphin291yi1534b-UseCase ;
    mcro:hasLicense mcro:cognitivecomputationsdolphin291yi1534b-License ;
    mcro:hasModelArchitecture mcro:cognitivecomputationsdolphin291yi1534b-Arch ;
    mcro:hasTrainingData mcro:cognitivecomputationsdolphin291yi1534b-TrainingData .

mcro:cointegratedruberttiny2 a mcro:Model ;
    mcro:hasModelArchitecture mcro:cointegratedruberttiny2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:cointegratedruberttiny2-UseCaseInformationSection .

mcro:colbertv2 a mcro:Model ;
    mcro:hasCitation mcro:colbertv2-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:colbertv2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:colbertv2-UseCaseInformationSection .

mcro:conjunctsditre15 a mcro:Model ;
    mcro:hasCitation mcro:conjunctsditre15-Citation ;
    mcro:hasConsideration mcro:conjunctsditre15-Consideration ;
    mcro:hasEvaluationData mcro:conjunctsditre15-EvaluationData ;
    mcro:hasModelDetail mcro:conjunctsditre15-ModelDetail ;
    mcro:hasTechnicalSpecification mcro:conjunctsditre15-TechnicalSpecification ;
    mcro:hasTrainingData mcro:conjunctsditre15-TrainingData ;
    mcro:hasUseCase mcro:conjunctsditre15-UseCase .

mcro:convnextv2_nano.fcmae_ft_in22kin1k a mcro:Model ;
    mcro:hasModelDetail mcro:convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail ;
    mcro:hasUseCase mcro:convnextv2_nano.fcmae_ft_in22kin1k-UseCase .

mcro:crossencodermsmarcoMiniLML12v2 a mcro:Model ;
    mcro:hasDataset mcro:crossencodermsmarcoMiniLML12v2-Dataset ;
    mcro:hasPerformanceMetric mcro:crossencodermsmarcoMiniLML12v2-Performance ;
    mcro:hasUseCase mcro:crossencodermsmarcoMiniLML12v2-UseCase .

mcro:crossencodermsmarcoMiniLML4v2 a mcro:Model ;
    mcro:hasDataset mcro:crossencodermsmarcoMiniLML4v2-DatasetInformationSection ;
    mcro:hasUseCase mcro:crossencodermsmarcoMiniLML4v2-UseCaseInformationSection .

mcro:crossencodermsmarcoMiniLML6v2 a mcro:Model ;
    mcro:hasDataset mcro:crossencodermsmarcoMiniLML6v2-Dataset ;
    mcro:hasModelArchitecture mcro:crossencodermsmarcoMiniLML6v2-Architecture ;
    mcro:hasQuantativeAnalysis mcro:crossencodermsmarcoMiniLML6v2-QuantativeAnalysis ;
    mcro:hasUseCase mcro:crossencodermsmarcoMiniLML6v2-UseCase .

mcro:crossencodermsmarcoTinyBERTL2v2 a mcro:Model ;
    mcro:hasDataset mcro:crossencodermsmarcoTinyBERTL2v2-Dataset ;
    mcro:hasModelDetail mcro:crossencodermsmarcoTinyBERTL2v2-ModelDetail ;
    mcro:hasModelParameter mcro:crossencodermsmarcoTinyBERTL2v2-ModelParameter ;
    mcro:hasQuantativeAnalysis mcro:crossencodermsmarcoTinyBERTL2v2-QuantativeAnalysis ;
    mcro:hasUseCase mcro:crossencodermsmarcoTinyBERTL2v2-UseCase .

mcro:crossencodernlidebertav3base a mcro:Model ;
    mcro:hasModelDetail mcro:crossencodernlidebertav3base-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:crossencodernlidebertav3base-QuantativeAnalysis ;
    mcro:hasUseCase mcro:crossencodernlidebertav3base-UseCase .

mcro:czechwav2vec2xlsr300mcs250 a mcro:Model ;
    mcro:hasModelDetail mcro:czechwav2vec2xlsr300mcs250-ModelDetailSection .

mcro:deberta-v3-base-prompt-injection-v2 a mcro:Model ;
    mcro:hasLimitation mcro:deberta-v3-base-prompt-injection-v2-Limitation ;
    mcro:hasModelDetail mcro:deberta-v3-base-prompt-injection-v2-ModelDetail ;
    mcro:hasUseCase mcro:deberta-v3-base-prompt-injection-v2-UseCase .

mcro:debertaV3Small a mcro:Model ;
    mcro:hasCitation mcro:debertaV3Small-CitationInformationSection ;
    mcro:hasDataset mcro:debertaV3Small-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:debertaV3Small-ModelArchitectureInformationSection .

mcro:debertav3basetasksourcenli a mcro:Model ;
    mcro:hasCitation mcro:debertav3basetasksourcenli-Citation ;
    mcro:hasModelArchitecture mcro:debertav3basetasksourcenli-Arch ;
    mcro:hasModelDetail mcro:debertav3basetasksourcenli-ModelDetail ;
    mcro:hasOwner mcro:debertav3basetasksourcenli-Owner ;
    mcro:hasUseCase mcro:debertav3basetasksourcenli-UseCase .

mcro:deepseekaiDeepSeekR1 a mcro:Model ;
    mcro:hasCitation mcro:deepseekaiDeepSeekR1-Citation,
        mcro:deepseekaiDeepSeekR1-CitationInformationSection ;
    mcro:hasDataset mcro:deepseekaiDeepSeekR1-DatasetInformationSection ;
    mcro:hasIntendedUseCase mcro:deepseekaiDeepSeekR1-UseCase ;
    mcro:hasLicense mcro:deepseekaiDeepSeekR1-License,
        mcro:deepseekaiDeepSeekR1-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:deepseekaiDeepSeekR1-ModelArchitecture,
        mcro:deepseekaiDeepSeekR1-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:deepseekaiDeepSeekR1-UseCase,
        mcro:deepseekaiDeepSeekR1-UseCaseInformationSection .

mcro:deepseekaiDeepSeekR10528 a mcro:Model ;
    mcro:hasCitation mcro:deepseekaiDeepSeekR10528-Citation ;
    mcro:hasLicense mcro:deepseekaiDeepSeekR10528-License ;
    mcro:hasModelArchitecture mcro:deepseekaiDeepSeekR10528-ModelArchitecture ;
    mcro:hasUseCase mcro:deepseekaiDeepSeekR10528-UseCase .

mcro:deepseekaiDeepSeekR1DistillLlama8B a mcro:Model ;
    mcro:hasCitation mcro:deepseekaiDeepSeekR1DistillLlama8B-Citation ;
    mcro:hasDataset mcro:deepseekaiDeepSeekR1DistillLlama8B-Dataset ;
    mcro:hasIntendedUseCase mcro:deepseekaiDeepSeekR1DistillLlama8B-UseCase ;
    mcro:hasLicense mcro:deepseekaiDeepSeekR1DistillLlama8B-License ;
    mcro:hasModelArchitecture mcro:deepseekaiDeepSeekR1DistillLlama8B-ModelArchitecture .

mcro:deepseekaiDeepSeekR1DistillQwen14B a mcro:Model ;
    mcro:hasCitation mcro:deepseekaiDeepSeekR1DistillQwen14B-Citation ;
    mcro:hasDataset mcro:deepseekaiDeepSeekR1DistillQwen14B-Dataset ;
    mcro:hasLicense mcro:deepseekaiDeepSeekR1DistillQwen14B-License ;
    mcro:hasModelArchitecture mcro:deepseekaiDeepSeekR1DistillQwen14B-ModelArchitecture ;
    mcro:hasUseCase mcro:deepseekaiDeepSeekR1DistillQwen14B-UseCase .

mcro:deepseekaiDeepSeekR1DistillQwen7B a mcro:Model ;
    mcro:hasArchitecture mcro:deepseekaiDeepSeekR1DistillQwen7B-Architecture ;
    mcro:hasCitation mcro:deepseekaiDeepSeekR1DistillQwen7B-Citation ;
    mcro:hasLicense mcro:deepseekaiDeepSeekR1DistillQwen7B-License ;
    mcro:hasUseCase mcro:deepseekaiDeepSeekR1DistillQwen7B-UseCase .

mcro:deepseekaiDeepSeekV3 a mcro:Model ;
    mcro:hasDataset mcro:deepseekaiDeepSeekV3-Dataset,
        mcro:deepseekaiDeepSeekV3-Dataset2 ;
    mcro:hasModelDetail mcro:deepseekaiDeepSeekV3-ModelDetail .

mcro:deepseekaiDeepSeekV30324 a mcro:Model ;
    mcro:hasCitation mcro:deepseekaiDeepSeekV30324-Citation ;
    mcro:hasLicense mcro:deepseekaiDeepSeekV30324-License ;
    mcro:hasModelArchitecture mcro:deepseekaiDeepSeekV30324-Architecture ;
    mcro:hasUseCase mcro:deepseekaiDeepSeekV30324-UseCase .

mcro:depthanythingDepthAnythingV2Smallhf a mcro:Model ;
    mcro:hasCitation mcro:depthanythingDepthAnythingV2Smallhf-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:depthanythingDepthAnythingV2Smallhf-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:depthanythingDepthAnythingV2Smallhf-UseCaseInformationSection .

mcro:detoxify a mcro:Model ;
    mcro:hasCitation mcro:detoxify-CitationInformationSection ;
    mcro:hasDataset mcro:detoxify-DatasetInformationSection,
        mcro:detoxify-DatasetInformationSection_Multilingual,
        mcro:detoxify-DatasetInformationSection_Unintended_Bias ;
    mcro:hasLicense mcro:detoxify-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:detoxify-ModelArchitectureInformationSection,
        mcro:detoxify-ModelArchitectureInformationSection_Pytorch_Lightning ;
    mcro:hasUseCase mcro:detoxify-UseCaseInformationSection,
        mcro:detoxify-UseCaseInformationSection_content_moderators,
        mcro:detoxify-UseCaseInformationSection_fine_tuning .

mcro:detrresnet50 a mcro:Model ;
    mcro:hasModelDetail mcro:detrresnet50-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:detrresnet50-QuantativeAnalysis ;
    mcro:hasTrainingData mcro:detrresnet50-TrainingData ;
    mcro:hasUseCase mcro:detrresnet50-UseCase .

mcro:dima806fairfaceageimagedetection a mcro:Model ;
    mcro:hasUseCase mcro:dima806fairfaceageimagedetection-UseCaseInformationSection .

mcro:distilbartcnn126 a mcro:Model ;
    mcro:hasModelParameter mcro:distilbartcnn126-ModelParameterSection ;
    mcro:hasQuantitativeAnalysis mcro:distilbartcnn126-QuantitativeAnalysisSection .

mcro:distilbertbasemodeluncased a mcro:Model ;
    mcro:hasCitation mcro:distilbertbasemodeluncased-Citation ;
    mcro:hasIntendedUseCase mcro:distilbertbasemodeluncased-IntendedUseCase ;
    mcro:hasModelDetail mcro:distilbertbasemodeluncased-ModelDetail ;
    mcro:hasTrainingData mcro:distilbertbasemodeluncased-TrainingData .

mcro:distilbertbasemultilingualcased a mcro:Model ;
    mcro:hasConsideration mcro:distilbertbasemultilingualcased-Consideration ;
    mcro:hasEnvironmentalImpact mcro:distilbertbasemultilingualcased-EnvironmentalImpact ;
    mcro:hasModelDetail mcro:distilbertbasemultilingualcased-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:distilbertbasemultilingualcased-QuantativeAnalysis ;
    mcro:hasTrainingParameter mcro:distilbertbasemultilingualcased-TrainingParameter ;
    mcro:hasUseCase mcro:distilbertbasemultilingualcased-UseCase .

mcro:distilbertbasemultilingualcasedsentimentsstudent a mcro:Model ;
    mcro:hasIntendedUseCase mcro:distilbertbasemultilingualcasedsentimentsstudent-UseCase ;
    mcro:hasModelArchitecture mcro:distilbertbasemultilingualcasedsentimentsstudent-ModelArchitecture .

mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis a mcro:Model ;
    mcro:hasIntendedUseCase mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-UseCase ;
    mcro:hasModelDetail mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelDetail ;
    mcro:hasTrainingData mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-TrainingData .

mcro:distilgpt2 a mcro:Model ;
    mcro:hasModelDetail mcro:distilgpt2-ModelDetail ;
    mcro:hasTrainingData mcro:distilgpt2-TrainingData ;
    mcro:hasUseCase mcro:distilgpt2-UseCase .

mcro:distilrobertabase a mcro:Model ;
    mcro:hasConsideration mcro:distilrobertabase-Consideration ;
    mcro:hasDataset mcro:distilrobertabase-Dataset ;
    mcro:hasModelArchitecture mcro:distilrobertabase-ModelArchitecture ;
    mcro:hasModelDetail mcro:distilrobertabase-ModelDetail ;
    mcro:hasModelParameter mcro:distilrobertabase-ModelParameter ;
    mcro:hasQuantativeAnalysis mcro:distilrobertabase-QuantativeAnalysis ;
    mcro:hasUseCase mcro:distilrobertabase-UseCase .

mcro:distilwhisperdistillargev3 a mcro:Model ;
    mcro:hasCitation mcro:distilwhisperdistillargev3-Citation ;
    mcro:hasDataset mcro:distilwhisperdistillargev3-Dataset ;
    mcro:hasLicense mcro:distilwhisperdistillargev3-License ;
    mcro:hasModelArchitecture mcro:distilwhisperdistillargev3-Arch ;
    mcro:hasUseCase mcro:distilwhisperdistillargev3-UseCase .

mcro:doclingmodels a mcro:Model ;
    mcro:hasDataset mcro:doclingmodels-Dataset ;
    mcro:hasModelArchitecture mcro:doclingmodels-ModelArchitecture ;
    mcro:hasModelDetail mcro:doclingmodels-ModelDetail ;
    mcro:hasReference mcro:doclingmodels-Reference ;
    mcro:hasUseCase mcro:doclingmodels-UseCase .

mcro:e5-base-v2 a mcro:Model ;
    mcro:hasCitation mcro:e5-base-v2-Citation ;
    mcro:hasModelArchitecture mcro:e5-base-v2-ModelArchitecture .

mcro:e5largev2 a mcro:Model ;
    mcro:hasCitation mcro:e5largev2-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:e5largev2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:e5largev2-UseCaseInformationSection .

mcro:e5smallv2 a mcro:Model ;
    mcro:hasCitation mcro:e5smallv2-Citation ;
    mcro:hasLimitation mcro:e5smallv2-Limitation ;
    mcro:hasModelArchitecture mcro:e5smallv2-ModelArchitecture ;
    mcro:hasUseCase mcro:e5smallv2-UseCase .

mcro:edgenextsmallusiin1k a mcro:Model ;
    mcro:hasModelDetail mcro:edgenextsmallusiin1k-ModelDetail ;
    mcro:hasUseCase mcro:edgenextsmallusiin1k-UseCase .

mcro:efficientnet_b0.ra_in1k a mcro:Model ;
    mcro:hasModelDetail mcro:efficientnet_b0.ra_in1k-ModelDetail ;
    mcro:hasUseCase mcro:efficientnet_b0.ra_in1k-UseCase .

mcro:efficientnet_b3.ra2_in1k a mcro:Model ;
    mcro:hasModelDetail mcro:efficientnet_b3.ra2_in1k-ModelDetail ;
    mcro:hasUseCase mcro:efficientnet_b3.ra2_in1k-UseCase .

mcro:emilyalsentzerBioClinicalBERT a mcro:Model ;
    mcro:hasCitation mcro:emilyalsentzerBioClinicalBERT-Citation ;
    mcro:hasDataset mcro:emilyalsentzerBioClinicalBERT-DatasetInfo ;
    mcro:hasModelArchitecture mcro:emilyalsentzerBioClinicalBERT-ModelArch ;
    mcro:hasUseCase mcro:emilyalsentzerBioClinicalBERT-UseCase .

mcro:emimodel a mcro:Model ;
    mcro:hasLicense mcro:emimodel-LicenseInformationSection ;
    mcro:hasModelDetail mcro:emimodel-ModelDetailSection ;
    mcro:hasUseCase mcro:emimodel-UseCaseInformationSection .

mcro:emrecanbertbaseturkishcasedmeannlistsbtr a mcro:Model ;
    mcro:hasDataset mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Dataset ;
    mcro:hasModelArchitecture mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Architecture ;
    mcro:hasModelDetail mcro:emrecanbertbaseturkishcasedmeannlistsbtr-ModelDetail ;
    mcro:hasTrainingData mcro:emrecanbertbaseturkishcasedmeannlistsbtr-TrainingData ;
    mcro:hasUseCase mcro:emrecanbertbaseturkishcasedmeannlistsbtr-UseCase .

mcro:eng-zho a mcro:Model ;
    mcro:hasDataset mcro:eng-zho-Dataset ;
    mcro:hasModelArchitecture mcro:eng-zho-ModelArchitecture ;
    mcro:hasUseCase mcro:eng-zho-UseCase .

mcro:englishnerinflairfastmodel a mcro:Model ;
    mcro:hasCitation mcro:englishnerinflairfastmodel-Citation ;
    mcro:hasModelArchitecture mcro:englishnerinflairfastmodel-ModelArchitecture ;
    mcro:hasPerformanceMetric mcro:englishnerinflairfastmodel-PerformanceMetric ;
    mcro:hasUseCase mcro:englishnerinflairfastmodel-UseCase .

mcro:example_model a mcro:Model ;
    mcro:hasCitation mcro:example_model-Citation ;
    mcro:hasDataset mcro:example_model-Dataset ;
    mcro:hasLicense mcro:example_model-License ;
    mcro:hasModelArchitecture mcro:example_model-ModelArchitecture ;
    mcro:hasUseCase mcro:example_model-UseCase .

mcro:facebookcontriever a mcro:Model ;
    mcro:hasCitation mcro:facebookcontriever-CitationInformationSection ;
    mcro:hasUseCase mcro:facebookcontriever-UseCaseInformationSection .

mcro:facebookdinov2large a mcro:Model ;
    mcro:hasCitation mcro:facebookdinov2large-Citation ;
    mcro:hasModelDetail mcro:facebookdinov2large-ModelDetail .

mcro:facebookencodec24khz a mcro:Model ;
    mcro:hasDataset mcro:facebookencodec24khz-DatasetInformationSection ;
    mcro:hasModelDetail mcro:facebookencodec24khz-ModelDetailSection ;
    mcro:hasUseCase mcro:facebookencodec24khz-UseCaseInformationSection .

mcro:facebookesm2 a mcro:Model ;
    mcro:hasCitation mcro:facebookesm2-Citation ;
    mcro:hasIntendedUseCase mcro:facebookesm2-UseCase ;
    mcro:hasModelArchitecture mcro:facebookesm2-ModelArchitecture .

mcro:facebookesm2t363BUR50D a mcro:Model ;
    mcro:hasCitation mcro:facebookesm2t363BUR50D-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:facebookesm2t363BUR50D-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:facebookesm2t363BUR50D-UseCaseInformationSection .

mcro:facebookhubertlargels960ft a mcro:Model ;
    mcro:hasModelDetail mcro:facebookhubertlargels960ft-ModelDetail .

mcro:facebookmask2formerswintinycocoinstance a mcro:Model ;
    mcro:hasLicense mcro:facebookmask2formerswintinycocoinstance-License ;
    mcro:hasModelDetail mcro:facebookmask2formerswintinycocoinstance-ModelDetail ;
    mcro:hasUseCase mcro:facebookmask2formerswintinycocoinstance-UseCase .

mcro:facebookmmslid256 a mcro:Model ;
    mcro:hasDataset mcro:facebookmmslid256-Dataset ;
    mcro:hasModelDetail mcro:facebookmmslid256-ModelDetail ;
    mcro:hasUseCase mcro:facebookmmslid256-UseCase .

mcro:facebookmusicgenmedium a mcro:Model ;
    mcro:hasIntendedUseCase mcro:facebookmusicgenmedium-IntendedUseCase ;
    mcro:hasModelDetail mcro:facebookmusicgenmedium-ModelDetail .

mcro:facebookopt125m a mcro:Model ;
    mcro:hasCitation mcro:facebookopt125m-Citation ;
    mcro:hasIntendedUseCase mcro:facebookopt125m-UseCase ;
    mcro:hasLimitation mcro:facebookopt125m-Limitation ;
    mcro:hasModelArchitecture mcro:facebookopt125m-Arch ;
    mcro:hasTrainingData mcro:facebookopt125m-TrainingData .

mcro:facebookwav2vec2base960h a mcro:Model ;
    mcro:hasDataset mcro:facebookwav2vec2base960h-Dataset ;
    mcro:hasModelDetail mcro:facebookwav2vec2base960h-ModelDetail ;
    mcro:hasUseCase mcro:facebookwav2vec2base960h-UseCase .

mcro:faceparsing a mcro:Model ;
    mcro:hasConsideration mcro:faceparsing-Consideration ;
    mcro:hasModelDetail mcro:faceparsing-ModelDetail ;
    mcro:hasUseCase mcro:faceparsing-UseCase .

mcro:fasttextlanguageidentification a mcro:Model ;
    mcro:hasDataset mcro:fasttextlanguageidentification-Dataset ;
    mcro:hasModelArchitecture mcro:fasttextlanguageidentification-Architecture ;
    mcro:hasModelDetail mcro:fasttextlanguageidentification-ModelDetail ;
    mcro:hasUseCase mcro:fasttextlanguageidentification-UseCase .

mcro:flairnerenglishlarge a mcro:Model ;
    mcro:hasCitation mcro:flairnerenglishlarge-Citation ;
    mcro:hasModelArchitecture mcro:flairnerenglishlarge-ModelArchitecture ;
    mcro:hasPerformanceMetric mcro:flairnerenglishlarge-PerformanceMetric .

mcro:flairnerfrench a mcro:Model ;
    mcro:hasCitation mcro:flairnerfrench-Citation ;
    mcro:hasModelArchitecture mcro:flairnerfrench-ModelArchitecture ;
    mcro:hasModelDetail mcro:flairnerfrench-ModelDetail ;
    mcro:hasPerformanceMetric mcro:flairnerfrench-PerformanceMetric ;
    mcro:hasUseCase mcro:flairnerfrench-UseCase .

mcro:flan-t5-base a mcro:Model ;
    mcro:hasCitation mcro:flan-t5-base-Citation ;
    mcro:hasConsideration mcro:flan-t5-base-Consideration ;
    mcro:hasEvaluationData mcro:flan-t5-base-EvaluationData ;
    mcro:hasModelArchitecture mcro:flan-t5-base-ModelArchitecture ;
    mcro:hasModelDetail mcro:flan-t5-base-ModelDetail ;
    mcro:hasTrainingData mcro:flan-t5-base-TrainingData ;
    mcro:hasUseCase mcro:flan-t5-base-UseCase .

mcro:flan-t5-large a mcro:Model ;
    mcro:hasCitation mcro:flan-t5-large-Citation ;
    mcro:hasConsideration mcro:flan-t5-large-Consideration ;
    mcro:hasEvaluationData mcro:flan-t5-large-EvaluationData ;
    mcro:hasModelArchitecture mcro:flan-t5-large-ModelArchitecture ;
    mcro:hasModelDetail mcro:flan-t5-large-ModelDetail ;
    mcro:hasTrainingData mcro:flan-t5-large-TrainingData ;
    mcro:hasUseCase mcro:flan-t5-large-UseCase .

mcro:fnetbasemodel a mcro:Model ;
    mcro:hasModelArchitecture mcro:fnetbasemodel-ModelArchitecture ;
    mcro:hasModelDetail mcro:fnetbasemodel-ModelDetail ;
    mcro:hasModelParameter mcro:fnetbasemodel-ModelParameter ;
    mcro:hasQuantativeAnalysis mcro:fnetbasemodel-QuantativeAnalysis ;
    mcro:hasTrainingData mcro:fnetbasemodel-TrainingData ;
    mcro:hasUseCase mcro:fnetbasemodel-UseCase .

mcro:gemma2 a mcro:Model ;
    mcro:hasDataset mcro:gemma2-Dataset ;
    mcro:hasIntendedUseCase mcro:gemma2-UseCase ;
    mcro:hasLicense mcro:gemma2-License ;
    mcro:hasModelArchitecture mcro:gemma2-Architecture ;
    mcro:hasModelDetail mcro:gemma2-ModelDetail .

mcro:gemma2b a mcro:Model ;
    mcro:hasConsideration mcro:gemma2b-Consideration ;
    mcro:hasDataset mcro:gemma2b-Dataset ;
    mcro:hasModelDetail mcro:gemma2b-ModelDetail ;
    mcro:hasModelParameter mcro:gemma2b-ModelParameter ;
    mcro:hasQuantativeAnalysis mcro:gemma2b-QuantativeAnalysis .

mcro:gemma3 a mcro:Model ;
    mcro:hasCitation mcro:gemma3-Citation ;
    mcro:hasDataset mcro:gemma3-Dataset ;
    mcro:hasModelArchitecture mcro:gemma3-Architecture ;
    mcro:hasModelDetail mcro:gemma3-ModelDetail ;
    mcro:hasUseCase mcro:gemma3-UseCase .

mcro:gemma3model a mcro:Model ;
    mcro:hasModelDetail mcro:gemma3model-ModelDetail .

mcro:germanbert a mcro:Model ;
    mcro:hasDataset mcro:germanbert-Dataset ;
    mcro:hasModelArchitecture mcro:germanbert-ModelArchitecture ;
    mcro:hasModelDetail mcro:germanbert-ModelDetail ;
    mcro:hasOwner mcro:germanbert-Owner .

mcro:germansentimentbert a mcro:Model ;
    mcro:hasCitation mcro:germansentimentbert-Citation ;
    mcro:hasDataset mcro:germansentimentbert-Dataset ;
    mcro:hasModelArchitecture mcro:germansentimentbert-ModelArchitecture .

mcro:googleelectrabasediscriminator a mcro:Model ;
    mcro:hasCitation mcro:googleelectrabasediscriminator-Citation ;
    mcro:hasModelArchitecture mcro:googleelectrabasediscriminator-ModelArchitecture ;
    mcro:hasUseCase mcro:googleelectrabasediscriminator-UseCase .

mcro:googleelectrasmalldiscriminator a mcro:Model ;
    mcro:hasCitation mcro:googleelectrasmalldiscriminator-Citation ;
    mcro:hasModelArchitecture mcro:googleelectrasmalldiscriminator-Architecture ;
    mcro:hasModelDetail mcro:googleelectrasmalldiscriminator-ModelDetail ;
    mcro:hasUseCase mcro:googleelectrasmalldiscriminator-UseCase .

mcro:googleflant5small a mcro:Model ;
    mcro:hasModelDetail mcro:googleflant5small-ModelDetail ;
    mcro:hasUseCase mcro:googleflant5small-UseCase .

mcro:googlegemma31bit a mcro:Model ;
    mcro:hasCitation mcro:googlegemma31bit-Citation ;
    mcro:hasDataset mcro:googlegemma31bit-Dataset ;
    mcro:hasModelArchitecture mcro:googlegemma31bit-Arch ;
    mcro:hasUseCase mcro:googlegemma31bit-UseCase .

mcro:googlest5v11 a mcro:Model ;
    mcro:hasCitation mcro:googlest5v11-Citation ;
    mcro:hasDataset mcro:googlest5v11-Dataset ;
    mcro:hasModelArchitecture mcro:googlest5v11-Architecture ;
    mcro:hasUseCase mcro:googlest5v11-UseCase .

mcro:gpt2 a mcro:Model ;
    mcro:hasCitation mcro:gpt2-Citation ;
    mcro:hasConsideration mcro:gpt2-Consideration ;
    mcro:hasEvaluationResults mcro:gpt2-EvaluationResults ;
    mcro:hasIntendedUseCase mcro:gpt2-IntendedUseCase ;
    mcro:hasModelDetail mcro:gpt2-ModelDetail ;
    mcro:hasTrainingData mcro:gpt2-TrainingData ;
    mcro:hasTrainingProcedure mcro:gpt2-TrainingProcedure .

mcro:gpt2medium a mcro:Model ;
    mcro:hasConsideration mcro:gpt2medium-Consideration ;
    mcro:hasEvaluationData mcro:gpt2medium-EvaluationData ;
    mcro:hasModelDetail mcro:gpt2medium-ModelDetail ;
    mcro:hasTrainingData mcro:gpt2medium-TrainingData ;
    mcro:hasUseCase mcro:gpt2medium-UseCase .

mcro:granitetimeseriesttmr1 a mcro:Model ;
    mcro:hasDataset mcro:granitetimeseriesttmr1-DatasetInformationSection ;
    mcro:hasModelDetail mcro:granitetimeseriesttmr1-ModelDetail ;
    mcro:hasTrainingData mcro:granitetimeseriesttmr1-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:granitetimeseriesttmr1-UseCaseInformationSection .

mcro:granitetimeseriesttmr2model a mcro:Model ;
    mcro:hasCitationInformation mcro:granitetimeseriesttmr2model-CitationInformation ;
    mcro:hasModelDetail mcro:granitetimeseriesttmr2model-ModelDetail ;
    mcro:hasTrainingDataInformation mcro:granitetimeseriesttmr2model-TrainingDataInformation ;
    mcro:hasUseCaseInformation mcro:granitetimeseriesttmr2model-UseCaseInformation .

mcro:graphcodebert a mcro:Model ;
    mcro:hasCitation mcro:graphcodebert-CitationInformationSection ;
    mcro:hasDataset mcro:graphcodebert-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:graphcodebert-ModelArchitectureInformationSection .

mcro:gte-base a mcro:Model ;
    mcro:hasCitation mcro:gte-base-CitationInformationSection ;
    mcro:hasLimitation mcro:gte-base-LimitationInformationSection ;
    mcro:hasModelArchitecture mcro:gte-base-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:gte-base-UseCaseInformationSection .

mcro:gte-large a mcro:Model ;
    mcro:hasLimitation mcro:gte-large-Limitation ;
    mcro:hasModelDetail mcro:gte-large-ModelDetail ;
    mcro:hasUseCase mcro:gte-large-UseCase .

mcro:gte-small a mcro:Model ;
    mcro:hasCitation mcro:gte-small-CitationInformationSection ;
    mcro:hasIntendedUseCase mcro:gte-small-UseCaseInformationSection ;
    mcro:hasLimitation mcro:gte-small-LimitationInformationSection ;
    mcro:hasModelArchitecture mcro:gte-small-ModelArchitectureInformationSection .

mcro:gtelargeenv15 a mcro:Model ;
    mcro:hasCitation mcro:gtelargeenv15-Citation ;
    mcro:hasModelArchitecture mcro:gtelargeenv15-ModelArchitecture ;
    mcro:hasUseCase mcro:gtelargeenv15-UseCase .

mcro:hallucinationevaluationmodel a mcro:Model ;
    mcro:hasCitation mcro:hallucinationevaluationmodel-CitationInformationSection ;
    mcro:hasDataset mcro:hallucinationevaluationmodel-DatasetInformationSection ;
    mcro:hasIntendedUseCase mcro:hallucinationevaluationmodel-UseCaseInformationSection ;
    mcro:hasLicense mcro:hallucinationevaluationmodel-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:hallucinationevaluationmodel-ModelArchitectureInformationSection .

mcro:hubertbase a mcro:Model ;
    mcro:hasCitation mcro:hubertbase-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:hubertbase-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:hubertbase-UseCaseInformationSection .

mcro:hubertsiuzdaksnac24khz a mcro:Model ;
    mcro:hasModelArchitecture mcro:hubertsiuzdaksnac24khz-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:hubertsiuzdaksnac24khz-UseCaseInformationSection .

mcro:huggingquantsMetaLlama318BInstructAWQINT4 a mcro:Model ;
    mcro:hasModelDetail mcro:huggingquantsMetaLlama318BInstructAWQINT4-ModelDetail .

mcro:huggingquantsMetaLlama318BInstructGPTQINT4 a mcro:Model ;
    mcro:hasModelDetail mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelDetail .

mcro:ibmgranitegranite318binstruct a mcro:Model ;
    mcro:hasDataset mcro:ibmgranitegranite318binstruct-Dataset ;
    mcro:hasModelDetail mcro:ibmgranitegranite318binstruct-ModelDetail ;
    mcro:hasUseCase mcro:ibmgranitegranite318binstruct-UseCase .

mcro:ibmresearchMoLFormerXLboth10pct a mcro:Model ;
    mcro:hasModelDetail mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail .

mcro:indobenchmarkindobertbasep1 a mcro:Model ;
    mcro:hasCitation mcro:indobenchmarkindobertbasep1-Citation ;
    mcro:hasModelArchitecture mcro:indobenchmarkindobertbasep1-ModelArchitecture .

mcro:indonesianrobertabaseposptagger a mcro:Model ;
    mcro:hasDataset mcro:indonesianrobertabaseposptagger-Dataset ;
    mcro:hasModelArchitecture mcro:indonesianrobertabaseposptagger-ModelArchitecture ;
    mcro:hasModelDetail mcro:indonesianrobertabaseposptagger-ModelDetail ;
    mcro:hasUseCase mcro:indonesianrobertabaseposptagger-UseCase .

mcro:infoxlm a mcro:Model ;
    mcro:hasCitation mcro:infoxlm-Citation .

mcro:intfloate5mistral7binstruct a mcro:Model ;
    mcro:hasCitation mcro:intfloate5mistral7binstruct-Citation ;
    mcro:hasLimitation mcro:intfloate5mistral7binstruct-Limitation ;
    mcro:hasModelArchitecture mcro:intfloate5mistral7binstruct-ModelArchitecture ;
    mcro:hasUseCase mcro:intfloate5mistral7binstruct-UseCase .

mcro:intfloatmultilinguale5base a mcro:Model ;
    mcro:hasBenchmarkResults mcro:intfloatmultilinguale5base-BenchmarkResultsSection ;
    mcro:hasCitation mcro:intfloatmultilinguale5base-CitationInformationSection ;
    mcro:hasLimitation mcro:intfloatmultilinguale5base-LimitationInformationSection ;
    mcro:hasModelArchitecture mcro:intfloatmultilinguale5base-ModelArchitectureInformationSection ;
    mcro:hasTrainingDataInformation mcro:intfloatmultilinguale5base-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:intfloatmultilinguale5base-UseCaseInformationSection .

mcro:intfloatmultilinguale5largeinstruct a mcro:Model ;
    mcro:hasCitation mcro:intfloatmultilinguale5largeinstruct-Citation ;
    mcro:hasLimitation mcro:intfloatmultilinguale5largeinstruct-Limitation ;
    mcro:hasModelArchitecture mcro:intfloatmultilinguale5largeinstruct-Architecture ;
    mcro:hasTrainingData mcro:intfloatmultilinguale5largeinstruct-TrainingData ;
    mcro:hasUseCase mcro:intfloatmultilinguale5largeinstruct-UseCase .

mcro:intfloatmultilinguale5small a mcro:Model ;
    mcro:hasCitation mcro:intfloatmultilinguale5small-Citation ;
    mcro:hasLimitation mcro:intfloatmultilinguale5small-Limitation ;
    mcro:hasModelArchitecture mcro:intfloatmultilinguale5small-Architecture ;
    mcro:hasTrainingData mcro:intfloatmultilinguale5small-TrainingData ;
    mcro:hasUseCase mcro:intfloatmultilinguale5small-UseCase .

mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli a mcro:Model ;
    mcro:hasDataset mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-DatasetInformationSection ;
    mcro:hasLimitation mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-LimitationInformationSection ;
    mcro:hasPerformanceMetric mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-PerformanceMetricInformationSection ;
    mcro:hasUseCase mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-UseCaseInformationSection .

mcro:jhartmannemotionenglishdistilrobertabase a mcro:Model ;
    mcro:hasModelDetail mcro:jhartmannemotionenglishdistilrobertabase-ModelDetail .

mcro:jinaaijinaembeddingsv3 a mcro:Model ;
    mcro:hasCitation mcro:jinaaijinaembeddingsv3-CitationInformationSection ;
    mcro:hasLicense mcro:jinaaijinaembeddingsv3-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:jinaaijinaembeddingsv3-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:jinaaijinaembeddingsv3-UseCaseInformationSection .

mcro:jinaaijinarerankerv2basemultilingual a mcro:Model ;
    mcro:hasLicense mcro:jinaaijinarerankerv2basemultilingual-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:jinaaijinarerankerv2basemultilingual-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:jinaaijinarerankerv2basemultilingual-UseCaseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53arabic a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Citation ;
    mcro:hasDataset mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset,
        mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset-2 ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53arabic-ModelArchitecture ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2largexlsr53arabic-UseCase .

mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-ModelArchitecture ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase .

mcro:jonatasgrosmanwav2vec2largexlsr53dutch a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Citation ;
    mcro:hasDataset mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset,
        mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2 ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2largexlsr53dutch-UseCase .

mcro:jonatasgrosmanwav2vec2largexlsr53english a mcro:Model ;
    mcro:hasModelDetail mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelDetail ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2largexlsr53english-UseCaseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53hungarian a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-CitationInformationSection ;
    mcro:hasDataset mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-UseCaseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53japanese a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53japanese-Citation ;
    mcro:hasIntendedUseCase mcro:jonatasgrosmanwav2vec2largexlsr53japanese-UseCase ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53japanese-ModelArchitecture .

mcro:jonatasgrosmanwav2vec2largexlsr53persian a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53persian-Citation ;
    mcro:hasDataset mcro:jonatasgrosmanwav2vec2largexlsr53persian-Dataset ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53persian-ModelArchitecture ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2largexlsr53persian-UseCase .

mcro:jonatasgrosmanwav2vec2largexlsr53polish a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53polish-Citation ;
    mcro:hasDataset mcro:jonatasgrosmanwav2vec2largexlsr53polish-Dataset ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelArchitecture ;
    mcro:hasModelDetail mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelDetail ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2largexlsr53polish-UseCase .

mcro:jonatasgrosmanwav2vec2largexlsr53portuguese a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-CitationInformationSection ;
    mcro:hasDataset mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-UseCaseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53russian a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53russian-Citation ;
    mcro:hasIntendedUseCase mcro:jonatasgrosmanwav2vec2largexlsr53russian-UseCase ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53russian-ModelArchitecture .

mcro:jonatasgrosmanwav2vec2xlsr1bportuguese a mcro:Model ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Citation ;
    mcro:hasDataset mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Dataset ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-ModelArchitecture ;
    mcro:hasUseCase mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-UseCase .

mcro:keremberkeyolov5nlicenseplate a mcro:Model ;
    mcro:hasModelArchitecture mcro:keremberkeyolov5nlicenseplate-ModelArchitecture ;
    mcro:hasUseCase mcro:keremberkeyolov5nlicenseplate-UseCase .

mcro:khawhitemangaocrbase a mcro:Model ;
    mcro:hasModelArchitecture mcro:khawhitemangaocrbase-ModelArchitecture ;
    mcro:hasUseCase mcro:khawhitemangaocrbase-UseCase .

mcro:kluerobertabase a mcro:Model ;
    mcro:hasCitation mcro:kluerobertabase-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:kluerobertabase-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:kluerobertabase-UseCaseInformationSection .

mcro:ko-sroberta-multitask a mcro:Model ;
    mcro:hasCitation mcro:ko-sroberta-multitask-Citation ;
    mcro:hasModelArchitecture mcro:ko-sroberta-multitask-ModelArchitecture .

mcro:kobert a mcro:Model ;
    mcro:hasReference mcro:kobert-ReferenceInformationSection ;
    mcro:hasUseCase mcro:kobert-UseCaseInformationSection .

mcro:kokoro82M a mcro:Model ;
    mcro:hasLicense mcro:kokoro82M-License ;
    mcro:hasModelArchitecture mcro:kokoro82M-Architecture ;
    mcro:hasTrainingData mcro:kokoro82M-TrainingData ;
    mcro:hasUseCase mcro:kokoro82M-UseCase .

mcro:kresnikwav2vec2largexlsrkorean a mcro:Model ;
    mcro:hasDataset mcro:kresnikwav2vec2largexlsrkorean-DatasetInformationSection ;
    mcro:hasPerformanceMetric mcro:kresnikwav2vec2largexlsrkorean-PerformanceMetricInformationSection ;
    mcro:hasUseCase mcro:kresnikwav2vec2largexlsrkorean-UseCaseInformationSection .

mcro:layoutlmbaseuncased a mcro:Model ;
    mcro:hasIntendedUseCase mcro:layoutlmbaseuncased-UseCase ;
    mcro:hasModelDetail mcro:layoutlmbaseuncased-ModelDetail ;
    mcro:hasTrainingData mcro:layoutlmbaseuncased-TrainingData .

mcro:layoutlmv3 a mcro:Model ;
    mcro:hasCitation mcro:layoutlmv3-CitationInformationSection ;
    mcro:hasLicense mcro:layoutlmv3-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:layoutlmv3-ModelArchitectureInformationSection .

mcro:legalbertTheMuppetsStraightOutOfLawSchool a mcro:Model ;
    mcro:hasCitation mcro:legalbertTheMuppetsStraightOutOfLawSchool-CitationInformationSection ;
    mcro:hasDataset mcro:legalbertTheMuppetsStraightOutOfLawSchool-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:legalbertTheMuppetsStraightOutOfLawSchool-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:legalbertTheMuppetsStraightOutOfLawSchool-UseCaseInformationSection .

mcro:lengyue233contentvecbest a mcro:Model ;
    mcro:hasModelArchitecture mcro:lengyue233contentvecbest-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:lengyue233contentvecbest-UseCaseInformationSection .

mcro:lftwr4target a mcro:Model ;
    mcro:hasCitation mcro:lftwr4target-CitationInformationSection .

mcro:llama2 a mcro:Model ;
    mcro:hasCitation mcro:llama2-Citation ;
    mcro:hasModelArchitecture mcro:llama2-ModelArchitecture ;
    mcro:hasModelDetail mcro:llama2-ModelDetail ;
    mcro:hasTrainingData mcro:llama2-TrainingData ;
    mcro:hasUseCase mcro:llama2-UseCase ;
    mcro:hasUseCaseInformation mcro:llama2-UseCase .

mcro:llama32Collection a mcro:Model ;
    mcro:hasLicense mcro:llama32Collection-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:llama32Collection-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:llama32Collection-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:llama32Collection-UseCaseInformationSection .

mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms a mcro:Model ;
    mcro:hasLicense mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-License ;
    mcro:hasModelArchitecture mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-Architecture ;
    mcro:hasTrainingData mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-TrainingData ;
    mcro:hasUseCase mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-UseCase .

mcro:llama32CollectionOfMultilingualLlLMs a mcro:Model ;
    mcro:hasLicense mcro:llama32CollectionOfMultilingualLlLMs-License ;
    mcro:hasModelArchitecture mcro:llama32CollectionOfMultilingualLlLMs-Architecture ;
    mcro:hasTrainingData mcro:llama32CollectionOfMultilingualLlLMs-TrainingData ;
    mcro:hasUseCase mcro:llama32CollectionOfMultilingualLlLMs-UseCase .

mcro:llama4Scout17B16EInstruct a mcro:Model ;
    mcro:hasIntendedUseCase mcro:llama4Scout17B16EInstruct-IntendedUseCase ;
    mcro:hasModelDetail mcro:llama4Scout17B16EInstruct-ModelDetail ;
    mcro:hasTrainingData mcro:llama4Scout17B16EInstruct-TrainingData .

mcro:llamaGuard38B a mcro:Model ;
    mcro:hasCitation mcro:llamaGuard38B-Citation ;
    mcro:hasLimitation mcro:llamaGuard38B-Limitation ;
    mcro:hasModelDetails mcro:llamaGuard38B-ModelDetail ;
    mcro:hasUseCase mcro:llamaGuard38B-UseCase .

mcro:llavahfllava157bhf a mcro:Model ;
    mcro:hasModelDetail mcro:llavahfllava157bhf-ModelDetail .

mcro:llavahfllavaonevisionqwen205bovhf a mcro:Model ;
    mcro:hasModelDetail mcro:llavahfllavaonevisionqwen205bovhf-ModelDetail ;
    mcro:hasUseCase mcro:llavahfllavaonevisionqwen205bovhf-UseCase .

mcro:llavahfllavav16mistral7bhf a mcro:Model ;
    mcro:hasCitation mcro:llavahfllavav16mistral7bhf-Citation,
        mcro:llavahfllavav16mistral7bhf-Citation2 ;
    mcro:hasModelArchitecture mcro:llavahfllavav16mistral7bhf-ModelArchitecture ;
    mcro:hasUseCase mcro:llavahfllavav16mistral7bhf-UseCase .

mcro:llavamodelcard a mcro:Model ;
    mcro:hasEvaluationData mcro:llavamodelcard-EvaluationData ;
    mcro:hasLicense mcro:llavamodelcard-License ;
    mcro:hasModelDetail mcro:llavamodelcard-ModelDetail ;
    mcro:hasTrainingData mcro:llavamodelcard-TrainingData ;
    mcro:hasUseCase mcro:llavamodelcard-UseCase .

mcro:lmmslabLLaVAVideo7BQwen2 a mcro:Model ;
    mcro:hasCitation mcro:lmmslabLLaVAVideo7BQwen2-Citation ;
    mcro:hasLicense mcro:lmmslabLLaVAVideo7BQwen2-License ;
    mcro:hasLimitation mcro:lmmslabLLaVAVideo7BQwen2-Limitation ;
    mcro:hasModelArchitecture mcro:lmmslabLLaVAVideo7BQwen2-Architecture ;
    mcro:hasModelDetail mcro:lmmslabLLaVAVideo7BQwen2-ModelDetail ;
    mcro:hasTrainingData mcro:lmmslabLLaVAVideo7BQwen2-TrainingData ;
    mcro:hasUseCase mcro:lmmslabLLaVAVideo7BQwen2-UseCase .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit a mcro:Model ;
    mcro:hasModelDetail mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelDetail .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit a mcro:Model ;
    mcro:hasModelDetail mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail .

mcro:longformerbase4096 a mcro:Model ;
    mcro:hasCitation mcro:longformerbase4096-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:longformerbase4096-ModelArchitectureInformationSection .

mcro:madebyollinsdxlvaefp16fix a mcro:Model ;
    mcro:hasModelDetail mcro:madebyollinsdxlvaefp16fix-ModelDetail .

mcro:mask2former a mcro:Model ;
    mcro:hasCitation mcro:mask2former-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:mask2former-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:mask2former-UseCaseInformationSection .

mcro:mbartlarge50manytomanymmt a mcro:Model ;
    mcro:hasCitation mcro:mbartlarge50manytomanymmt-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:mbartlarge50manytomanymmt-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:mbartlarge50manytomanymmt-UseCaseInformationSection .

mcro:metaLlama3 a mcro:Model ;
    mcro:hasCitation mcro:metaLlama3-Citation ;
    mcro:hasDataset mcro:metaLlama3-Dataset ;
    mcro:hasModelDetail mcro:metaLlama3-ModelDetail ;
    mcro:hasUseCase mcro:metaLlama3-UseCase .

mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm a mcro:Model ;
    mcro:hasDataset mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-Dataset ;
    mcro:hasIntendedUseCase mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-IntendedUse ;
    mcro:hasModelDetail mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelDetail .

mcro:metaLlama31Instruct a mcro:Model ;
    mcro:hasDataset mcro:metaLlama31Instruct-Dataset ;
    mcro:hasIntendedUseCase mcro:metaLlama31Instruct-IntendedUseCase ;
    mcro:hasLicense mcro:metaLlama31Instruct-License ;
    mcro:hasModelArchitecture mcro:metaLlama31Instruct-ModelArchitecture ;
    mcro:hasModelDetail mcro:metaLlama31Instruct-ModelDetail ;
    mcro:hasOutOfScopeUseCase mcro:metaLlama31Instruct-OutOfScopeUseCase ;
    mcro:hasTrainingData mcro:metaLlama31Instruct-TrainingData .

mcro:metaLlama32Collection a mcro:Model ;
    mcro:hasDataset mcro:metaLlama32Collection-DatasetInformationSection ;
    mcro:hasModelDetail mcro:metaLlama32Collection-ModelDetailSection ;
    mcro:hasUseCase mcro:metaLlama32Collection-UseCaseInformationSection .

mcro:metallamaLlama31 a mcro:Model ;
    mcro:hasModelDetail mcro:metallamaLlama31-ModelDetail ;
    mcro:hasUseCase mcro:metallamaLlama31-UseCase .

mcro:metallamaLlama3211BVisionInstruct a mcro:Model ;
    mcro:hasIntendedUseCaseInformation mcro:metallamaLlama3211BVisionInstruct-IntendedUse ;
    mcro:hasLicense mcro:metallamaLlama3211BVisionInstruct-License ;
    mcro:hasModelArchitectureInformation mcro:metallamaLlama3211BVisionInstruct-ModelArchitecture ;
    mcro:hasOutOfScopeUseCaseInformation mcro:metallamaLlama3211BVisionInstruct-OutOfScope ;
    mcro:hasTrainingDataInformation mcro:metallamaLlama3211BVisionInstruct-TrainingData .

mcro:metallamaLlama323BInstruct a mcro:Model ;
    mcro:hasDataset mcro:metallamaLlama323BInstruct-Dataset ;
    mcro:hasIntendedUseCase mcro:metallamaLlama323BInstruct-IntendedUseCase ;
    mcro:hasModelDetail mcro:metallamaLlama323BInstruct-ModelDetail .

mcro:metallamaLlama3370BInstruct a mcro:Model ;
    mcro:hasModelDetail mcro:metallamaLlama3370BInstruct-ModelDetail ;
    mcro:hasTrainingData mcro:metallamaLlama3370BInstruct-TrainingData ;
    mcro:hasUseCase mcro:metallamaLlama3370BInstruct-UseCase .

mcro:metallamaMetaLlama38B a mcro:Model ;
    mcro:hasBenchmark mcro:metallamaMetaLlama38B-Benchmark ;
    mcro:hasConsideration mcro:metallamaMetaLlama38B-Consideration ;
    mcro:hasModelDetail mcro:metallamaMetaLlama38B-ModelDetail ;
    mcro:hasTrainingData mcro:metallamaMetaLlama38B-TrainingData ;
    mcro:hasUseCase mcro:metallamaMetaLlama38B-UseCase .

mcro:microsoftFlorence2base a mcro:Model ;
    mcro:hasCitation mcro:Florence-2Citation ;
    mcro:hasDataset mcro:FLD-5B ;
    mcro:hasModelArchitecture mcro:Florence-2Architecture ;
    mcro:hasModelDetail mcro:microsoftFlorence2base-ModelDetail ;
    mcro:hasUseCase mcro:VisionTasksUseCase .

mcro:microsoftFlorence2large a mcro:Model ;
    mcro:hasCitation mcro:microsoftFlorence2large-Citation ;
    mcro:hasDataset mcro:microsoftFlorence2large-Dataset ;
    mcro:hasModelArchitecture mcro:microsoftFlorence2large-ModelArchitecture ;
    mcro:hasUseCase mcro:microsoftFlorence2large-UseCase .

mcro:microsoftPhi35miniinstruct a mcro:Model ;
    mcro:hasLicense mcro:microsoftPhi35miniinstruct-License ;
    mcro:hasModelDetail mcro:microsoftPhi35miniinstruct-ModelDetail ;
    mcro:hasModelParameter mcro:microsoftPhi35miniinstruct-ModelParameter .

mcro:microsoftPhi35visioninstruct a mcro:Model ;
    mcro:hasLicense mcro:microsoftPhi35visioninstruct-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:microsoftPhi35visioninstruct-ModelArchitectureInformationSection ;
    mcro:hasModelDetail mcro:microsoftPhi35visioninstruct-ModelDetailSection ;
    mcro:hasTrainingData mcro:microsoftPhi35visioninstruct-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:microsoftPhi35visioninstruct-UseCaseInformationSection .

mcro:microsoftPhi3mini128kinstruct a mcro:Model ;
    mcro:hasDataset mcro:microsoftPhi3mini128kinstruct-Dataset ;
    mcro:hasModelDetail mcro:microsoftPhi3mini128kinstruct-ModelDetail ;
    mcro:hasUseCase mcro:microsoftPhi3mini128kinstruct-UseCase .

mcro:microsoftPhi3mini4kinstruct a mcro:Model ;
    mcro:hasBenchmark mcro:microsoftPhi3mini4kinstruct-Benchmark ;
    mcro:hasLicense mcro:microsoftPhi3mini4kinstruct-License ;
    mcro:hasModelDetail mcro:microsoftPhi3mini4kinstruct-ModelDetail ;
    mcro:hasTrainingData mcro:microsoftPhi3mini4kinstruct-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:microsoftPhi3mini4kinstruct-UseCaseInformationSection .

mcro:microsoftPhi4multimodalinstruct a mcro:Model ;
    mcro:hasIntendedUseCase mcro:microsoftPhi4multimodalinstruct-IntendedUseCase ;
    mcro:hasModelDetail mcro:microsoftPhi4multimodalinstruct-ModelDetail .

mcro:microsoftbeitlargepatch16224 a mcro:Model ;
    mcro:hasIntendedUseCase mcro:microsoftbeitlargepatch16224-IntendedUseCase ;
    mcro:hasModelDetail mcro:microsoftbeitlargepatch16224-ModelDetail ;
    mcro:hasTrainingData mcro:microsoftbeitlargepatch16224-TrainingData .

mcro:microsoftcodebertbasemlm a mcro:Model ;
    mcro:hasCitation mcro:microsoftcodebertbasemlm-Citation ;
    mcro:hasDataset mcro:microsoftcodebertbasemlm-Dataset ;
    mcro:hasModelArchitecture mcro:microsoftcodebertbasemlm-Architecture ;
    mcro:hasUseCase mcro:microsoftcodebertbasemlm-UseCase .

mcro:microsoftdebertalargemnli a mcro:Model ;
    mcro:hasCitation mcro:microsoftdebertalargemnli-Citation ;
    mcro:hasModelArchitecture mcro:microsoftdebertalargemnli-ModelArchitecture .

mcro:microsoftdebertav3base a mcro:Model ;
    mcro:hasCitation mcro:microsoftdebertav3base-Citation,
        mcro:microsoftdebertav3base-Citation2 ;
    mcro:hasModelArchitecture mcro:microsoftdebertav3base-ModelArchitecture .

mcro:microsoftdebertav3large a mcro:Model ;
    mcro:hasCitation mcro:microsoftdebertav3large-Citation ;
    mcro:hasDataset mcro:microsoftdebertav3large-Dataset ;
    mcro:hasModelArchitecture mcro:microsoftdebertav3large-ModelArchitecture ;
    mcro:hasUseCase mcro:microsoftdebertav3large-UseCase .

mcro:microsoftdebertaxlargemnli a mcro:Model ;
    mcro:hasCitation mcro:microsoftdebertaxlargemnli-Citation ;
    mcro:hasModelArchitecture mcro:microsoftdebertaxlargemnli-ModelArchitecture ;
    mcro:hasUseCase mcro:microsoftdebertaxlargemnli-UseCase .

mcro:microsoftmdebertav3base a mcro:Model ;
    mcro:hasCitation mcro:microsoftmdebertav3base-Citation ;
    mcro:hasDataset mcro:microsoftmdebertav3base-Dataset ;
    mcro:hasModelArchitecture mcro:microsoftmdebertav3base-ModelArchitecture ;
    mcro:hasUseCase mcro:microsoftmdebertav3base-UseCase .

mcro:microsoftphi2 a mcro:Model ;
    mcro:hasLimitationInformation mcro:microsoftphi2-Limitation ;
    mcro:hasModelDetail mcro:microsoftphi2-ModelDetail ;
    mcro:hasTrainingDataInformation mcro:microsoftphi2-Training ;
    mcro:hasUseCaseInformation mcro:microsoftphi2-UseCase .

mcro:microsoftwavlmbaseplussv a mcro:Model ;
    mcro:hasCitation mcro:microsoftwavlmbaseplussv-CitationInformationSection ;
    mcro:hasDataset mcro:microsoftwavlmbaseplussv-DatasetInformationSection ;
    mcro:hasLicense mcro:microsoftwavlmbaseplussv-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:microsoftwavlmbaseplussv-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:microsoftwavlmbaseplussv-UseCaseInformationSection .

mcro:mimi a mcro:Model ;
    mcro:hasCitation mcro:mimi-Citation ;
    mcro:hasConsideration mcro:mimi-Consideration ;
    mcro:hasIntendedUseCase mcro:mimi-IntendedUseCase ;
    mcro:hasModelDetail mcro:mimi-ModelDetail ;
    mcro:hasOutOfScopeUseCase mcro:mimi-OutOfScopeUseCase ;
    mcro:hasTrainingData mcro:mimi-TrainingData ;
    mcro:hasUseCase mcro:mimi-UseCase .

mcro:mistralaiMistral7BInstructv02 a mcro:Model ;
    mcro:hasCitation mcro:mistralaiMistral7BInstructv02-Citation ;
    mcro:hasLicense mcro:mistralaiMistral7BInstructv02-License ;
    mcro:hasLimitation mcro:mistralaiMistral7BInstructv02-Limitation ;
    mcro:hasModelDetail mcro:mistralaiMistral7BInstructv02-ModelDetail ;
    mcro:hasOwner mcro:mistralaiMistral7BInstructv02-Owner ;
    mcro:hasUseCase mcro:mistralaiMistral7BInstructv02-UseCase .

mcro:mistralaiMistral7BInstructv03 a mcro:Model ;
    mcro:hasLimitation mcro:mistralaiMistral7BInstructv03-Limitation ;
    mcro:hasModelDetail mcro:mistralaiMistral7BInstructv03-ModelDetail ;
    mcro:hasOwnerInformation mcro:mistralaiMistral7BInstructv03-OwnerInformation .

mcro:mistralaiMistral7Bv01 a mcro:Model ;
    mcro:hasConsideration mcro:mistralaiMistral7Bv01-Consideration ;
    mcro:hasModelArchitecture mcro:mistralaiMistral7Bv01-ModelArchitecture .

mcro:mistralaiMixtral8x7BInstructv01 a mcro:Model ;
    mcro:hasConsideration mcro:mistralaiMixtral8x7BInstructv01-ConsiderationSection ;
    mcro:hasLimitation mcro:mistralaiMixtral8x7BInstructv01-LimitationSection ;
    mcro:hasModelDetail mcro:mistralaiMixtral8x7BInstructv01-ModelDetailSection ;
    mcro:hasUseCase mcro:mistralaiMixtral8x7BInstructv01-UseCaseSection .

mcro:mixedbreadaimxbaiembedlargev1 a mcro:Model ;
    mcro:hasCitation mcro:mixedbreadaimxbaiembedlargev1-CitationInformationSection ;
    mcro:hasLicense mcro:mixedbreadaimxbaiembedlargev1-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:mixedbreadaimxbaiembedlargev1-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:mixedbreadaimxbaiembedlargev1-UseCaseInformationSection .

mcro:mlxcommunitygemma312bitqat4bit a mcro:Model ;
    mcro:hasModelDetail mcro:mlxcommunitygemma312bitqat4bit-ModelDetail ;
    mcro:hasUseCase mcro:mlxcommunitygemma312bitqat4bit-UseCase .

mcro:mmlwretrievalrobertalarge a mcro:Model ;
    mcro:hasCitation mcro:mmlwretrievalrobertalarge-CitationInformationSection ;
    mcro:hasDataset mcro:mmlwretrievalrobertalarge-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:mmlwretrievalrobertalarge-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:mmlwretrievalrobertalarge-UseCaseInformationSection .

mcro:mobilebertuncased a mcro:Model ;
    mcro:hasModelArchitecture mcro:mobilebertuncased-ModelArchitecture .

mcro:mobilenetv3small100lambin1k a mcro:Model ;
    mcro:hasDataset mcro:mobilenetv3small100lambin1k-Dataset ;
    mcro:hasModelDetail mcro:mobilenetv3small100lambin1k-ModelDetail ;
    mcro:hasUseCase mcro:mobilenetv3small100lambin1k-UseCase,
        mcro:mobilenetv3small100lambin1k-UseCase2,
        mcro:mobilenetv3small100lambin1k-UseCase3 .

mcro:mobilevitsmall a mcro:Model ;
    mcro:hasCitation mcro:mobilevitsmall-Citation ;
    mcro:hasModelDetail mcro:mobilevitsmall-ModelDetail ;
    mcro:hasTrainingData mcro:mobilevitsmall-TrainingData ;
    mcro:hasUseCase mcro:mobilevitsmall-UseCase .

mcro:modelcard123 a mcro:Model ;
    mcro:hasCitation mcro:modelcard123-Citation ;
    mcro:hasDataset mcro:modelcard123-Dataset ;
    mcro:hasLicense mcro:modelcard123-License ;
    mcro:hasModelArchitecture mcro:modelcard123-ModelArchitecture ;
    mcro:hasUseCase mcro:modelcard123-UseCase .

mcro:modelid a mcro:Model ;
    mcro:hasEvaluation mcro:modelid-Evaluation ;
    mcro:hasModelArchitecture mcro:modelid-ModelArchitecture ;
    mcro:hasModelDetail mcro:modelid-ModelDetail ;
    mcro:hasTrainingData mcro:modelid-TrainingData ;
    mcro:hasUseCase mcro:modelid-UseCase .

mcro:moondream2 a mcro:Model ;
    mcro:hasUseCase mcro:moondream2-UseCase ;
    mcro:hasVersion mcro:moondream2-Version .

mcro:mradermacherDeepSeekV2LiteGGUF a mcro:Model ;
    mcro:hasLicense mcro:mradermacherDeepSeekV2LiteGGUF-License ;
    mcro:hasQuantativeAnalysis mcro:mradermacherDeepSeekV2LiteGGUF-QuantativeAnalysis .

mcro:multilinguale5large a mcro:Model ;
    mcro:hasCitation mcro:multilinguale5large-Citation ;
    mcro:hasDataset mcro:multilinguale5large-Dataset,
        mcro:multilinguale5large-TrainingDetails ;
    mcro:hasLimitation mcro:multilinguale5large-Limitation ;
    mcro:hasModelArchitecture mcro:multilinguale5large-ModelArchitecture ;
    mcro:hasUseCase mcro:multilinguale5large-UseCase .

mcro:multiqaminiLML6cosv1 a mcro:Model ;
    mcro:hasIntendedUseCase mcro:multiqaminiLML6cosv1-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:multiqaminiLML6cosv1-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:multiqaminiLML6cosv1-TrainingDataInformationSection .

mcro:mxbai-rerank-xsmall-v1 a mcro:Model ;
    mcro:hasCitation mcro:mxbai-rerank-xsmall-v1-CitationInformationSection ;
    mcro:hasDataset mcro:mxbai-rerank-xsmall-v1-DatasetInformationSection ;
    mcro:hasLicense mcro:mxbai-rerank-xsmall-v1-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:mxbai-rerank-xsmall-v1-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:mxbai-rerank-xsmall-v1-UseCaseInformationSection .

mcro:myshellaiMeloTTS a mcro:Model ;
    mcro:hasCitation mcro:myshellaiMeloTTS-Citation ;
    mcro:hasLicense mcro:myshellaiMeloTTS-License ;
    mcro:hasModelArchitecture mcro:myshellaiMeloTTS-ModelArchitecture ;
    mcro:hasUseCase mcro:myshellaiMeloTTS-UseCase .

mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B a mcro:Model ;
    mcro:hasConsideration mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Consideration ;
    mcro:hasDataset mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Dataset ;
    mcro:hasModelDetail mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-QuantativeAnalysis ;
    mcro:hasUseCase mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-UseCase .

mcro:nguyenvulebinhwav2vec2basevi a mcro:Model ;
    mcro:hasDataset mcro:nguyenvulebinhwav2vec2basevi-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:nguyenvulebinhwav2vec2basevi-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:nguyenvulebinhwav2vec2basevi-UseCaseInformationSection .

mcro:nlpconnectvitgpt2imagecaptioning a mcro:Model ;
    mcro:hasModelArchitecture mcro:nlpconnectvitgpt2imagecaptioning-ModelArchitecture ;
    mcro:hasModelDetail mcro:nlpconnectvitgpt2imagecaptioning-ModelDetail .

mcro:nomicainomicembedtextv1 a mcro:Model ;
    mcro:hasCitation mcro:nomicainomicembedtextv1-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:nomicainomicembedtextv1-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:nomicainomicembedtextv1-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:nomicainomicembedtextv1-UseCaseInformationSection .

mcro:nomicainomicembedtextv15 a mcro:Model ;
    mcro:hasCitation mcro:nomicainomicembedtextv15-Citation ;
    mcro:hasModelArchitecture mcro:nomicainomicembedtextv15-Arch ;
    mcro:hasUseCase mcro:nomicainomicembedtextv15-UseCase .

mcro:nvidiatitanetlargeenus a mcro:Model ;
    mcro:hasCitation mcro:nvidiatitanetlargeenus-CitationInformationSection ;
    mcro:hasDataset mcro:nvidiatitanetlargeenus-DatasetInformationSection ;
    mcro:hasLicense mcro:nvidiatitanetlargeenus-LicenseInformationSection ;
    mcro:hasLimitation mcro:nvidiatitanetlargeenus-LimitationInformationSection ;
    mcro:hasModelArchitecture mcro:nvidiatitanetlargeenus-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:nvidiatitanetlargeenus-UseCaseInformationSection .

mcro:obideidrobertai2b2 a mcro:Model ;
    mcro:hasDataset mcro:obideidrobertai2b2-Dataset ;
    mcro:hasModelDetail mcro:obideidrobertai2b2-ModelDetail ;
    mcro:hasModelParameter mcro:obideidrobertai2b2-ModelParameter ;
    mcro:hasTrainingData mcro:obideidrobertai2b2-Training ;
    mcro:hasUseCase mcro:obideidrobertai2b2-UseCase .

mcro:ocr-equation-images-and-text-to-latex a mcro:Model ;
    mcro:hasCitation mcro:ocr-equation-images-and-text-to-latex-Citation .

mcro:oliverguhrfullstoppunctuationmultilanglarge a mcro:Model ;
    mcro:hasConsideration mcro:oliverguhrfullstoppunctuationmultilanglarge-Consideration ;
    mcro:hasDataset mcro:oliverguhrfullstoppunctuationmultilanglarge-Dataset,
        mcro:oliverguhrfullstoppunctuationmultilanglarge-DatasetInformationSection ;
    mcro:hasIntendedUseCase mcro:oliverguhrfullstoppunctuationmultilanglarge-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:oliverguhrfullstoppunctuationmultilanglarge-ModelArchitecture ;
    mcro:hasQuantativeAnalysis mcro:oliverguhrfullstoppunctuationmultilanglarge-QuantativeAnalysis .

mcro:openaiclip a mcro:Model ;
    mcro:hasConsideration mcro:openaiclip-Consideration ;
    mcro:hasDataset mcro:openaiclip-Dataset ;
    mcro:hasLimitation mcro:openaiclip-Limitation ;
    mcro:hasModelDetail mcro:openaiclip-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:openaiclip-QuantativeAnalysis ;
    mcro:hasUseCase mcro:openaiclip-UseCase .

mcro:openaiwhisperlargev3turbo a mcro:Model ;
    mcro:hasIntendedUseCase mcro:openaiwhisperlargev3turbo-UseCase ;
    mcro:hasModelDetail mcro:openaiwhisperlargev3turbo-ModelDetail .

mcro:openbmbMiniCPMo26 a mcro:Model ;
    mcro:hasCitation mcro:openbmbMiniCPMo26-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:openbmbMiniCPMo26-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:openbmbMiniCPMo26-UseCaseInformationSection .

mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill a mcro:Model ;
    mcro:hasCitation mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-Citation ;
    mcro:hasDataset mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection ;
    mcro:hasLicense mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-License ;
    mcro:hasModelArchitecture mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-UseCaseInformationSection .

mcro:openvla7b a mcro:Model ;
    mcro:hasCitation mcro:openvla7b-Citation ;
    mcro:hasModelDetail mcro:openvla7b-ModelDetail ;
    mcro:hasUseCase mcro:openvla7b-UseCase .

mcro:opusmtenfr a mcro:Model ;
    mcro:hasDataset mcro:opusmtenfr-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:opusmtenfr-ModelArchitectureInformationSection .

mcro:opusmtnlen a mcro:Model ;
    mcro:hasDataset mcro:opusmtnlen-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:opusmtnlen-ModelArchitectureInformationSection .

mcro:opusmtruen a mcro:Model ;
    mcro:hasArchitecture mcro:opusmtruen-Architecture ;
    mcro:hasEvaluationData mcro:opusmtruen-EvaluationData ;
    mcro:hasModelDetail mcro:opusmtruen-ModelDetail ;
    mcro:hasTrainingData mcro:opusmtruen-TrainingData ;
    mcro:hasUseCase mcro:opusmtruen-UseCase .

mcro:owlv2 a mcro:Model ;
    mcro:hasDataset mcro:owlv2-Dataset ;
    mcro:hasModelArchitecture mcro:owlv2-ModelArchitecture ;
    mcro:hasModelDetail mcro:owlv2-ModelDetail,
        mcro:owlv2-ModelDetails ;
    mcro:hasUseCase mcro:owlv2-UseCase .

mcro:owlvitbasepatch32 a mcro:Model ;
    mcro:hasDataset mcro:owlvitbasepatch32-Dataset ;
    mcro:hasModelDetail mcro:owlvitbasepatch32-ModelDetail ;
    mcro:hasUseCase mcro:owlvitbasepatch32-UseCase .

mcro:parakeet_rnnt_06b a mcro:Model ;
    mcro:hasDataset mcro:parakeet_rnnt_06b-Dataset ;
    mcro:hasLicense mcro:parakeet_rnnt_06b-License ;
    mcro:hasModelArchitecture mcro:parakeet_rnnt_06b-ModelArchitecture ;
    mcro:hasReference mcro:parakeet_rnnt_06b-Reference ;
    mcro:hasUseCase mcro:parakeet_rnnt_06b-UseCase .

mcro:parakeettdt06bv2 a mcro:Model ;
    mcro:hasDatasetInformation mcro:parakeettdt06bv2-Dataset ;
    mcro:hasEvaluationDataInformation mcro:parakeettdt06bv2-EvaluationDataset ;
    mcro:hasLicense mcro:parakeettdt06bv2-License ;
    mcro:hasModelArchitectureInformation mcro:parakeettdt06bv2-Architecture ;
    mcro:hasPerformanceMetricInformation mcro:parakeettdt06bv2-Performance ;
    mcro:hasReferenceInformation mcro:parakeettdt06bv2-Reference ;
    mcro:hasTrainingDataInformation mcro:parakeettdt06bv2-TrainingData ;
    mcro:hasUseCaseInformation mcro:parakeettdt06bv2-UseCase .

mcro:patrickjohncyhfashionclip a mcro:Model ;
    mcro:hasDataset mcro:patrickjohncyhfashionclip-Dataset ;
    mcro:hasModelDetail mcro:patrickjohncyhfashionclip-ModelDetail ;
    mcro:hasUseCase mcro:patrickjohncyhfashionclip-UseCase .

mcro:petalsteamStableBeluga2 a mcro:Model ;
    mcro:hasCitation mcro:petalsteamStableBeluga2-Citation,
        mcro:petalsteamStableBeluga2-Citation2,
        mcro:petalsteamStableBeluga2-Citation3 ;
    mcro:hasConsideration mcro:petalsteamStableBeluga2-Consideration ;
    mcro:hasDataset mcro:petalsteamStableBeluga2-Dataset ;
    mcro:hasModelDetail mcro:petalsteamStableBeluga2-ModelDetail,
        mcro:petalsteamStableBeluga2-ModelDetail2 ;
    mcro:hasModelParameter mcro:petalsteamStableBeluga2-ModelParameter ;
    mcro:hasUseCase mcro:petalsteamStableBeluga2-UseCase .

mcro:phi4 a mcro:Model ;
    mcro:hasCitation mcro:phi4-Citation ;
    mcro:hasDataset mcro:phi4-Dataset ;
    mcro:hasModelDetail mcro:phi4-ModelDetail ;
    mcro:hasUseCase mcro:phi4-UseCase .

mcro:phobertpretrainedlanguagemodels a mcro:Model ;
    mcro:hasCitation mcro:phobertpretrainedlanguagemodels-Citation ;
    mcro:hasModelArchitecture mcro:phobertpretrainedlanguagemodels-ModelArchitecture .

mcro:playgroundaiplaygroundv251024pxaesthetic a mcro:Model ;
    mcro:hasCitation mcro:playgroundaiplaygroundv251024pxaesthetic-Citation ;
    mcro:hasLicense mcro:playgroundaiplaygroundv251024pxaesthetic-License ;
    mcro:hasModelDetail mcro:playgroundaiplaygroundv251024pxaesthetic-ModelDetail ;
    mcro:hasUseCase mcro:playgroundaiplaygroundv251024pxaesthetic-UseCase .

mcro:potion-base-8M a mcro:Model ;
    mcro:hasCitation mcro:potion-base-8M-Citation ;
    mcro:hasModelArchitecture mcro:potion-base-8M-ModelArchitecture ;
    mcro:hasUseCase mcro:potion-base-8M-UseCase .

mcro:prajjwal1bertmedium a mcro:Model ;
    mcro:hasCitation mcro:prajjwal1bertmedium-Citation ;
    mcro:hasIntendedUseCase mcro:prajjwal1bertmedium-UseCase ;
    mcro:hasModelArchitecture mcro:prajjwal1bertmedium-Architecture .

mcro:prajjwal1berttiny a mcro:Model ;
    mcro:hasCitation mcro:prajjwal1berttiny-Citation ;
    mcro:hasModelArchitecture mcro:prajjwal1berttiny-ModelArchitecture .

mcro:prithividaparrotparaphraseronT5 a mcro:Model ;
    mcro:hasLicense mcro:prithividaparrotparaphraseronT5-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:prithividaparrotparaphraseronT5-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:prithividaparrotparaphraseronT5-UseCaseInformationSection .

mcro:pyannotesegmentation30 a mcro:Model ;
    mcro:hasDataset mcro:pyannotesegmentation30-Dataset ;
    mcro:hasModelDetail mcro:pyannotesegmentation30-ModelDetail .

mcro:pyannotespeakerdiarization30 a mcro:Model ;
    mcro:hasBenchmark mcro:pyannotespeakerdiarization30-Benchmark ;
    mcro:hasCitation mcro:pyannotespeakerdiarization30-Citation ;
    mcro:hasConsideration mcro:pyannotespeakerdiarization30-Consideration ;
    mcro:hasDataset mcro:pyannotespeakerdiarization30-Dataset ;
    mcro:hasModelDetail mcro:pyannotespeakerdiarization30-ModelDetail ;
    mcro:hasUseCase mcro:pyannotespeakerdiarization30-UseCase .

mcro:pyannotespeakerdiarization31 a mcro:Model ;
    mcro:hasCitation mcro:pyannotespeakerdiarization31-CitationInformationSection ;
    mcro:hasDataset mcro:pyannotespeakerdiarization31-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:pyannotespeakerdiarization31-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:pyannotespeakerdiarization31-UseCaseInformationSection .

mcro:pyannotewespeakervoxcelebresnet34LM a mcro:Model ;
    mcro:hasCitation mcro:pyannotewespeakervoxcelebresnet34LM-Citation ;
    mcro:hasLicense mcro:pyannotewespeakervoxcelebresnet34LM-License ;
    mcro:hasModelArchitecture mcro:pyannotewespeakervoxcelebresnet34LM-ModelArchitecture ;
    mcro:hasUseCase mcro:pyannotewespeakervoxcelebresnet34LM-UseCase .

mcro:rbhatia46financialragmatryoshka a mcro:Model ;
    mcro:hasCitation mcro:rbhatia46financialragmatryoshka-Citation ;
    mcro:hasModelDetail mcro:rbhatia46financialragmatryoshka-ModelDetail ;
    mcro:hasUseCase mcro:rbhatia46financialragmatryoshka-UseCase .

mcro:resnet18a1in1k a mcro:Model ;
    mcro:hasDataset mcro:resnet18a1in1k-Dataset ;
    mcro:hasModelDetail mcro:resnet18a1in1k-ModelDetail ;
    mcro:hasUseCase mcro:resnet18a1in1k-UseCase .

mcro:resnet50.a1_in1k a mcro:Model ;
    mcro:hasDataset mcro:resnet50.a1_in1k-Dataset ;
    mcro:hasModelArchitecture mcro:resnet50.a1_in1k-Arch ;
    mcro:hasModelDetail mcro:resnet50.a1_in1k-ModelDetail ;
    mcro:hasUseCase mcro:resnet50.a1_in1k-UseCase .

mcro:resnet50v1.5 a mcro:Model ;
    mcro:hasModelDetail mcro:resnet50v1.5-ModelDetail ;
    mcro:hasUseCase mcro:resnet50v1.5-UseCase .

mcro:rinnajapanesecloobvitb16 a mcro:Model ;
    mcro:hasCitation mcro:rinnajapanesecloobvitb16-Citation1,
        mcro:rinnajapanesecloobvitb16-Citation2 ;
    mcro:hasDataset mcro:rinnajapanesecloobvitb16-Dataset ;
    mcro:hasLicense mcro:rinnajapanesecloobvitb16-License ;
    mcro:hasModelArchitecture mcro:rinnajapanesecloobvitb16-ModelArchitecture .

mcro:robertBase a mcro:Model ;
    mcro:hasDataset mcro:robertBase-Dataset ;
    mcro:hasModelDetail mcro:robertBase-ModelDetail ;
    mcro:hasUseCase mcro:robertBase-UseCase .

mcro:roberta-baseforextractiveqa a mcro:Model ;
    mcro:hasDataset mcro:roberta-baseforextractiveqa-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:roberta-baseforextractiveqa-ModelArchitectureInformationSection ;
    mcro:hasModelDetail mcro:roberta-baseforextractiveqa-ModelDetailSection ;
    mcro:hasUseCase mcro:roberta-baseforextractiveqa-UseCaseInformationSection .

mcro:rorshark-vit-base a mcro:Model ;
    mcro:hasDataset mcro:rorshark-vit-base-Dataset ;
    mcro:hasModelDetail mcro:rorshark-vit-base-ModelDetail ;
    mcro:hasUseCase mcro:rorshark-vit-base-UseCase .

mcro:rubertbasecased a mcro:Model ;
    mcro:hasCitation mcro:rubertbasecased-Citation ;
    mcro:hasModelArchitecture mcro:rubertbasecased-ModelArchitecture .

mcro:rubertbasecasedsentimentrusentiment a mcro:Model ;
    mcro:hasCitation mcro:rubertbasecasedsentimentrusentiment-Citation ;
    mcro:hasDataset mcro:rubertbasecasedsentimentrusentiment-Dataset ;
    mcro:hasIntendedUseCase mcro:rubertbasecasedsentimentrusentiment-UseCase .

mcro:ruri-small-v2 a mcro:Model ;
    mcro:hasCitation mcro:ruri-small-v2-ReferenceInformationSection ;
    mcro:hasDataset mcro:ruri-small-v2-DatasetInformationSection ;
    mcro:hasLicense mcro:ruri-small-v2-LicenseInformationSection ;
    mcro:hasModelDetail mcro:ruri-small-v2-ModelDetailSection ;
    mcro:hasUseCase mcro:ruri-small-v2-UseCaseInformationSection .

mcro:sarvamaisarvamm a mcro:Model ;
    mcro:hasModelArchitecture mcro:sarvamaisarvamm-ModelArchitecture ;
    mcro:hasModelDetail mcro:sarvamaisarvamm-ModelDetail ;
    mcro:hasUseCase mcro:sarvamaisarvamm-UseCase .

mcro:sat-3l-sm a mcro:Model ;
    mcro:hasCitation mcro:sat-3l-sm-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sat-3l-sm-ModelArchitectureInformationSection .

mcro:scb10xtyphoon21gemma312b a mcro:Model ;
    mcro:hasIntendedUseCase mcro:scb10xtyphoon21gemma312b-IntendedUseCase ;
    mcro:hasModelDetail mcro:scb10xtyphoon21gemma312b-ModelDetail .

mcro:scibert a mcro:Model ;
    mcro:hasCitation mcro:scibert-Citation ;
    mcro:hasModelArchitecture mcro:scibert-ModelArchitecture .

mcro:sdxl10base a mcro:Model ;
    mcro:hasLimitation mcro:sdxl10base-Limitation ;
    mcro:hasModelDetail mcro:sdxl10base-ModelDetail ;
    mcro:hasUseCase mcro:sdxl10base-UseCase .

mcro:sdxl10refinermodel a mcro:Model ;
    mcro:hasLimitation mcro:sdxl10refinermodel-Limitation ;
    mcro:hasModelDetail mcro:sdxl10refinermodel-ModelDetail ;
    mcro:hasUseCase mcro:sdxl10refinermodel-UseCase .

mcro:sdxlinpainting01 a mcro:Model ;
    mcro:hasConsideration mcro:sdxlinpainting01-Bias ;
    mcro:hasIntendedUseCase mcro:sdxlinpainting01-intendeduse ;
    mcro:hasLimitation mcro:sdxlinpainting01-Limitation ;
    mcro:hasModelArchitecture mcro:sdxlinpainting01-Arch ;
    mcro:hasModelDetail mcro:sdxlinpainting01-ModelDetail ;
    mcro:hasOutOfScopeUseCase mcro:sdxlinpainting01-outofscopeuse ;
    mcro:hasUseCase mcro:sdxlinpainting01-UseCase .

mcro:sdxlturbo a mcro:Model ;
    mcro:hasLimitation mcro:sdxlturbo-Limitation ;
    mcro:hasModelDetail mcro:sdxlturbo-ModelDetail ;
    mcro:hasUseCase mcro:sdxlturbo-UseCase .

mcro:sdxlvae a mcro:Model ;
    mcro:hasDataset mcro:sdxlvae-DatasetInformationSection ;
    mcro:hasEvaluation mcro:sdxlvae-EvaluationInformationSection ;
    mcro:hasModelArchitecture mcro:sdxlvae-ModelArchitectureInformationSection .

mcro:segformerb0finetunedade20k a mcro:Model ;
    mcro:hasCitation mcro:segformerb0finetunedade20k-Citation ;
    mcro:hasIntendedUseCase mcro:segformerb0finetunedade20k-IntendedUseCase ;
    mcro:hasLicense mcro:segformerb0finetunedade20k-License ;
    mcro:hasModelArchitecture mcro:segformerb0finetunedade20k-ModelArchitecture .

mcro:segformerb1finetunedade20k a mcro:Model ;
    mcro:hasCitation mcro:segformerb1finetunedade20k-Citation ;
    mcro:hasIntendedUseCase mcro:segformerb1finetunedade20k-IntendedUseCase ;
    mcro:hasModelDetail mcro:segformerb1finetunedade20k-ModelDetail .

mcro:segformerb2finetunedforclothessegmentation a mcro:Model ;
    mcro:hasCitation mcro:segformerb2finetunedforclothessegmentation-CitationInformationSection ;
    mcro:hasDataset mcro:segformerb2finetunedforclothessegmentation-DatasetInformationSection ;
    mcro:hasLicense mcro:segformerb2finetunedforclothessegmentation-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:segformerb2finetunedforclothessegmentation-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:segformerb2finetunedforclothessegmentation-UseCaseInformationSection .

mcro:segformerb5finetunedade20k a mcro:Model ;
    mcro:hasCitationInformationSection mcro:segformerb5finetunedade20k-CitationInformationSection ;
    mcro:hasLicenseInformationSection mcro:segformerb5finetunedade20k-LicenseInformationSection ;
    mcro:hasModelArchitectureInformationSection mcro:segformerb5finetunedade20k-ModelArchitectureInformationSection ;
    mcro:hasUseCaseInformationSection mcro:segformerb5finetunedade20k-UseCaseInformationSection .

mcro:sentencetransformersLaBSE a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersLaBSE-Citation ;
    mcro:hasModelArchitecture mcro:sentencetransformersLaBSE-ModelArchitecture ;
    mcro:hasUseCase mcro:sentencetransformersLaBSE-UseCase .

mcro:sentencetransformersbertbasenlimeantokens a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersbertbasenlimeantokens-Citation ;
    mcro:hasModelArchitecture mcro:sentencetransformersbertbasenlimeantokens-ModelArchitecture ;
    mcro:hasUseCase mcro:sentencetransformersbertbasenlimeantokens-UseCase .

mcro:sentencetransformersdistilbertbasenlimeantokens a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersdistilbertbasenlimeantokens-CitationInformationSection ;
    mcro:hasIntendedUseCase mcro:sentencetransformersdistilbertbasenlimeantokens-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersdistilbertbasenlimeantokens-ModelArchitectureInformationSection .

mcro:sentencetransformersdistilusebasemultilingualcasedv1 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersdistilusebasemultilingualcasedv1-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersdistilusebasemultilingualcasedv1-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersdistilusebasemultilingualcasedv1-UseCaseInformationSection .

mcro:sentencetransformersdistilusebasemultilingualcasedv2 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersdistilusebasemultilingualcasedv2-Citation ;
    mcro:hasModelArchitecture mcro:sentencetransformersdistilusebasemultilingualcasedv2-ModelArchitecture ;
    mcro:hasUseCase mcro:sentencetransformersdistilusebasemultilingualcasedv2-UseCase .

mcro:sentencetransformersmsmarcodistilbertbasev4 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersmsmarcodistilbertbasev4-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersmsmarcodistilbertbasev4-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersmsmarcodistilbertbasev4-UseCaseInformationSection .

mcro:sentencetransformersmultiqampnetbasedotv1 a mcro:Model ;
    mcro:hasIntendedUseCase mcro:sentencetransformersmultiqampnetbasedotv1-UseCaseInformationSection ;
    mcro:hasModelDetail mcro:sentencetransformersmultiqampnetbasedotv1-ModelDetailSection .

mcro:sentencetransformersparaphraseMiniLML3v2 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersparaphraseMiniLML3v2-Citation ;
    mcro:hasModelArchitecture mcro:sentencetransformersparaphraseMiniLML3v2-ModelArchitecture ;
    mcro:hasUseCase mcro:sentencetransformersparaphraseMiniLML3v2-UseCase .

mcro:sentencetransformersparaphraseMiniLML6v2 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersparaphraseMiniLML6v2-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersparaphraseMiniLML6v2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersparaphraseMiniLML6v2-UseCaseInformationSection .

mcro:sentencetransformersparaphrasempnetbasev2 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersparaphrasempnetbasev2-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersparaphrasempnetbasev2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersparaphrasempnetbasev2-UseCaseInformationSection .

mcro:sentencetransformersparaphrasemultilingualMiniLML12v2 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-UseCaseInformationSection .

mcro:sentencetransformersparaphrasemultilingualmpnetbasev2 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-UseCaseInformationSection .

mcro:sentencetransformersparaphrasexlmrmultilingualv1 a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersparaphrasexlmrmultilingualv1-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersparaphrasexlmrmultilingualv1-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersparaphrasexlmrmultilingualv1-UseCaseInformationSection .

mcro:sentencetransformersrobertabasenlimeantokens a mcro:Model ;
    mcro:hasCitationInformation mcro:sentencetransformersrobertabasenlimeantokens-CitationInformationSection ;
    mcro:hasModelArchitectureInformation mcro:sentencetransformersrobertabasenlimeantokens-ModelArchitectureInformationSection ;
    mcro:hasUseCaseInformation mcro:sentencetransformersrobertabasenlimeantokens-UseCaseInformationSection .

mcro:sentencetransformersstsbrobertabase a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersstsbrobertabase-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersstsbrobertabase-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersstsbrobertabase-UseCaseInformationSection .

mcro:sentencetransformersstsbxlmrmultilingual a mcro:Model ;
    mcro:hasCitation mcro:sentencetransformersstsbxlmrmultilingual-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersstsbxlmrmultilingual-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:sentencetransformersstsbxlmrmultilingual-UseCaseInformationSection .

mcro:siglip2so400mpatch16naflex a mcro:Model ;
    mcro:hasCitation mcro:siglip2so400mpatch16naflex-CitationInformationSection ;
    mcro:hasTrainingData mcro:siglip2so400mpatch16naflex-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:siglip2so400mpatch16naflex-UseCaseInformationSection .

mcro:siglipso400mpatch14384 a mcro:Model ;
    mcro:hasIntendedUseCase mcro:siglipso400mpatch14384-IntendedUseCase ;
    mcro:hasModelDetail mcro:siglipso400mpatch14384-ModelDetail ;
    mcro:hasTrainingData mcro:siglipso400mpatch14384-TrainingData .

mcro:snunlpKRSBERTV40KklueNLIaugSTS a mcro:Model ;
    mcro:hasModelDetail mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail ;
    mcro:hasUseCase mcro:snunlpKRSBERTV40KklueNLIaugSTS-UseCase .

mcro:spa-eng a mcro:Model ;
    mcro:hasDataset mcro:spa-eng-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:spa-eng-ModelArchitectureInformationSection .

mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP a mcro:Model ;
    mcro:hasCitation mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-CitationInformationSection ;
    mcro:hasIntendedUseCase mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-UseCaseInformationSection ;
    mcro:hasLicense mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-ModelArchitectureInformationSection .

mcro:speechbrainspkrececapavoxceleb a mcro:Model ;
    mcro:hasLicense mcro:speechbrainspkrececapavoxceleb-License ;
    mcro:hasModelDetail mcro:speechbrainspkrececapavoxceleb-ModelDetail ;
    mcro:hasUseCase mcro:speechbrainspkrececapavoxceleb-UseCase .

mcro:spladecocondenserensembledistil a mcro:Model ;
    mcro:hasCitation mcro:spladecocondenserensembledistil-Citation .

mcro:stabilityaisdturbo a mcro:Model ;
    mcro:hasLimitation mcro:stabilityaisdturbo-Limitation ;
    mcro:hasModelDetail mcro:stabilityaisdturbo-ModelDetail ;
    mcro:hasOutOfScopeUse mcro:stabilityaisdturbo-OutOfScopeUse ;
    mcro:hasUseCase mcro:stabilityaisdturbo-UseCase .

mcro:stabilityaistablediffusion35medium a mcro:Model ;
    mcro:hasConsideration mcro:stabilityaistablediffusion35medium-Consideration ;
    mcro:hasModelDetail mcro:stabilityaistablediffusion35medium-ModelDetail ;
    mcro:hasOutofScopeUseCase mcro:stabilityaistablediffusion35medium-OutofScopeUseCase ;
    mcro:hasUseCase mcro:stabilityaistablediffusion35medium-UseCase .

mcro:stablediffusioninpaintingmodelcard a mcro:Model ;
    mcro:hasDataset mcro:stablediffusioninpaintingmodelcard-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:stablediffusioninpaintingmodelcard-ModelArchitectureInformationSection ;
    mcro:hasModelDetail mcro:stablediffusioninpaintingmodelcard-ModelDetailSection ;
    mcro:hasUseCase mcro:stablediffusioninpaintingmodelcard-UseCaseInformationSection .

mcro:stablediffusionv15modelcard a mcro:Model ;
    mcro:hasConsideration mcro:stablediffusionv15modelcard-Consideration ;
    mcro:hasModelDetail mcro:stablediffusionv15modelcard-ModelDetail ;
    mcro:hasUseCase mcro:stablediffusionv15modelcard-UseCase .

mcro:stablediffusionv2-1 a mcro:Model ;
    mcro:hasDataset mcro:stablediffusionv2-1-Dataset ;
    mcro:hasModelDetail mcro:stablediffusionv2-1-ModelDetail ;
    mcro:hasUseCase mcro:stablediffusionv2-1-UseCase .

mcro:surya a mcro:Model ;
    mcro:hasModelArchitecture mcro:surya-ModelArchitecture .

mcro:swin_transformer_v2_tiny_sized_model a mcro:Model ;
    mcro:hasDataset mcro:swin_transformer_v2_tiny_sized_model-Dataset ;
    mcro:hasIntendedUseCase mcro:swin_transformer_v2_tiny_sized_model-UseCase ;
    mcro:hasModelDetail mcro:swin_transformer_v2_tiny_sized_model-ModelDetail .

mcro:t53b a mcro:Model ;
    mcro:hasDataset mcro:t53b-Dataset ;
    mcro:hasModelDetail mcro:t53b-ModelDetail ;
    mcro:hasUseCase mcro:t53b-UseCase .

mcro:t5base a mcro:Model ;
    mcro:hasConsideration mcro:t5base-Consideration ;
    mcro:hasModelDetail mcro:t5base-ModelDetail ;
    mcro:hasReference mcro:t5base-Reference ;
    mcro:hasTrainingData mcro:t5base-TrainingData ;
    mcro:hasUseCase mcro:t5base-UseCase .

mcro:t5large a mcro:Model ;
    mcro:hasConsideration mcro:t5large-Consideration ;
    mcro:hasDataset mcro:t5large-Dataset ;
    mcro:hasModelDetail mcro:t5large-ModelDetail ;
    mcro:hasModelParameter mcro:t5large-ModelParameter ;
    mcro:hasQuantativeAnalysis mcro:t5large-QuantativeAnalysis ;
    mcro:hasUseCase mcro:t5large-UseCase .

mcro:t5small a mcro:Model ;
    mcro:hasEvaluationData mcro:t5small-EvaluationData ;
    mcro:hasModelDetail mcro:t5small-ModelDetail ;
    mcro:hasTrainingData mcro:t5small-TrainingData ;
    mcro:hasUseCase mcro:t5small-UseCase .

mcro:tabletransformerfinetunedfortabledetection a mcro:Model ;
    mcro:hasCitation mcro:tabletransformerfinetunedfortabledetection-CitationInformationSection ;
    mcro:hasDataset mcro:tabletransformerfinetunedfortabledetection-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:tabletransformerfinetunedfortabledetection-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:tabletransformerfinetunedfortabledetection-UseCaseInformationSection .

mcro:tabletransformerfinetunedfortablestructurerecognition a mcro:Model ;
    mcro:hasModelArchitecture mcro:tabletransformerfinetunedfortablestructurerecognition-ModelArchitecture ;
    mcro:hasUseCase mcro:tabletransformerfinetunedfortablestructurerecognition-UseCase .

mcro:tabletransformerpretrainedfortablestructurerecognition a mcro:Model ;
    mcro:hasCitation mcro:tabletransformerpretrainedfortablestructurerecognition-Citation ;
    mcro:hasIntendedUseCase mcro:tabletransformerpretrainedfortablestructurerecognition-UseCase ;
    mcro:hasModelArchitecture mcro:tabletransformerpretrainedfortablestructurerecognition-ModelArchitecture .

mcro:testModel a mcro:Model ;
    mcro:hasArchitecture mcro:testModel-ModelArchitectureInformationSection ;
    mcro:hasCitation mcro:testModel-CitationInformationSection ;
    mcro:hasDataset mcro:testModel-DatasetInformationSection ;
    mcro:hasLicense mcro:testModel-LicenseInformationSection ;
    mcro:hasUseCase mcro:testModel-UseCaseInformationSection .

mcro:testmodel a mcro:Model ;
    mcro:hasCitation mcro:testmodel-CitationInformationSection ;
    mcro:hasDataset mcro:testmodel-DatasetInformationSection ;
    mcro:hasLicense mcro:testmodel-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:testmodel-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:testmodel-UseCaseInformationSection .

mcro:text-detection-model-for-surya a mcro:Model ;
    mcro:hasModelArchitecture mcro:text-detection-model-for-surya-ModelArchitecture .

mcro:tiiuaefalconrw1b a mcro:Model ;
    mcro:hasCitation mcro:tiiuaefalconrw1b-Citation ;
    mcro:hasDataset mcro:tiiuaefalconrw1b-Dataset ;
    mcro:hasLicense mcro:tiiuaefalconrw1b-License ;
    mcro:hasModelArchitecture mcro:tiiuaefalconrw1b-ModelArchitecture ;
    mcro:hasUseCase mcro:tiiuaefalconrw1b-UseCase .

mcro:timmViTB16SigLIPi18n256 a mcro:Model ;
    mcro:hasModelDetail mcro:timmViTB16SigLIPi18n256-ModelDetail ;
    mcro:hasUseCase mcro:timmViTB16SigLIPi18n256-UseCase .

mcro:toxigen a mcro:Model ;
    mcro:hasCitation mcro:toxigen-Citation ;
    mcro:hasReference mcro:toxigen-Reference ;
    mcro:hasUseCase mcro:toxigen-UseCase .

mcro:trlinternaltestingtinyLlamaForCausalLM32 a mcro:Model ;
    mcro:hasModelDetail mcro:trlinternaltestingtinyLlamaForCausalLM32-ModelDetail .

mcro:trlinternaltestingtinyQwen2ForCausalLM25 a mcro:Model ;
    mcro:hasModelDetail mcro:trlinternaltestingtinyQwen2ForCausalLM25-ModelDetail .

mcro:trlinternaltestingtinyT5ForConditionalGeneration a mcro:Model ;
    mcro:hasModelDetail mcro:trlinternaltestingtinyT5ForConditionalGeneration-ModelDetail .

mcro:trocrbasesroie a mcro:Model ;
    mcro:hasCitation mcro:trocrbasesroie-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:trocrbasesroie-ModelArchitectureInformationSection ;
    mcro:hasUseCase mcro:trocrbasesroie-UseCaseInformationSection .

mcro:trocrsmalliam a mcro:Model ;
    mcro:hasCitation mcro:trocrsmalliam-Citation ;
    mcro:hasIntendedUseCase mcro:trocrsmalliam-UseCase ;
    mcro:hasModelArchitecture mcro:trocrsmalliam-ModelArchitecture .

mcro:trpakovvitfaceexpression a mcro:Model ;
    mcro:hasDataset mcro:trpakovvitfaceexpression-Dataset ;
    mcro:hasLimitation mcro:trpakovvitfaceexpression-Limitation,
        mcro:trpakovvitfaceexpression-LimitationGeneralization ;
    mcro:hasModelArchitecture mcro:trpakovvitfaceexpression-ModelArchitecture ;
    mcro:hasUseCase mcro:trpakovvitfaceexpression-UseCase .

mcro:tsmatzxlmrobertanerjapanese a mcro:Model ;
    mcro:hasModelArchitecture mcro:tsmatzxlmrobertanerjapanese-ModelArchitectureInformationSection ;
    mcro:hasModelParameter mcro:tsmatzxlmrobertanerjapanese-ModelParameterSection ;
    mcro:hasTrainingData mcro:tsmatzxlmrobertanerjapanese-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:tsmatzxlmrobertanerjapanese-UseCaseInformationSection .

mcro:twitterrobertabasesentiment a mcro:Model ;
    mcro:hasCitation mcro:twitterrobertabasesentiment-Citation ;
    mcro:hasModelArchitecture mcro:twitterrobertabasesentiment-ModelArchitecture ;
    mcro:hasUseCase mcro:twitterrobertabasesentiment-UseCase .

mcro:twitterxlmrobertabasesentiment a mcro:Model ;
    mcro:hasModelDetail mcro:twitterxlmrobertabasesentiment-ModelDetailSection ;
    mcro:hasUseCase mcro:twitterxlmrobertabasesentiment-UseCaseInformationSection .

mcro:unik3d a mcro:Model ;
    mcro:hasReference mcro:unik3d-ReferenceInformationSection .

mcro:unslothDeepSeekR10528Qwen38BGGUF a mcro:Model ;
    mcro:hasCitation mcro:unslothDeepSeekR10528Qwen38BGGUF-CitationInformationSection ;
    mcro:hasIntendedUseCase mcro:unslothDeepSeekR10528Qwen38BGGUF-UseCaseInformationSection ;
    mcro:hasLicense mcro:unslothDeepSeekR10528Qwen38BGGUF-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:unslothDeepSeekR10528Qwen38BGGUF-ModelArchitectureInformationSection .

mcro:unslothQwen2505BInstructbnb4bit a mcro:Model ;
    mcro:hasCitation mcro:unslothQwen2505BInstructbnb4bit-Citation ;
    mcro:hasIntendedUseCase mcro:unslothQwen2505BInstructbnb4bit-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:unslothQwen2505BInstructbnb4bit-ModelArchitecture .

mcro:upskyybgem3korean a mcro:Model ;
    mcro:hasCitation mcro:upskyybgem3korean-CitationInformationSection ;
    mcro:hasModelDetails mcro:upskyybgem3korean-ModelDetailSection ;
    mcro:hasUseCase mcro:upskyybgem3korean-UseCaseInformationSection .

mcro:variousmodels a mcro:Model ;
    mcro:hasUseCase mcro:variousmodels-UseCaseInformationSection .

mcro:vikplayoutsegmenter a mcro:Model ;
    mcro:hasModelArchitecture mcro:vikplayoutsegmenter-ModelArchitecture ;
    mcro:hasUseCase mcro:vikplayoutsegmenter-UseCase .

mcro:visiontransformerbase-sizedmodel a mcro:Model ;
    mcro:hasCitation mcro:visiontransformerbase-sizedmodel-Citation ;
    mcro:hasIntendedUseCase mcro:visiontransformerbase-sizedmodel-UseCase ;
    mcro:hasModelArchitecture mcro:visiontransformerbase-sizedmodel-ModelArchitecture ;
    mcro:hasTrainingData mcro:visiontransformerbase-sizedmodel-TrainingData .

mcro:visiontransformerbase-sizedmodel-hybrid a mcro:Model ;
    mcro:hasCitation mcro:visiontransformerbase-sizedmodel-hybrid-Citation ;
    mcro:hasEvaluationResults mcro:visiontransformerbase-sizedmodel-hybrid-EvaluationResults ;
    mcro:hasIntendedUseCase mcro:visiontransformerbase-sizedmodel-hybrid-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:visiontransformerbase-sizedmodel-hybrid-ModelArchitecture ;
    mcro:hasModelDetail mcro:visiontransformerbase-sizedmodel-hybrid-ModelDetail ;
    mcro:hasTrainingData mcro:visiontransformerbase-sizedmodel-hybrid-TrainingData ;
    mcro:hasTrainingProcedure mcro:visiontransformerbase-sizedmodel-hybrid-TrainingProcedure .

mcro:visiontransformerbasesizedmodel a mcro:Model ;
    mcro:hasCitation mcro:visiontransformerbasesizedmodel-CitationInformationSection,
        mcro:visiontransformerbasesizedmodel-CitationInformationSection1,
        mcro:visiontransformerbasesizedmodel-CitationInformationSection2,
        mcro:visiontransformerbasesizedmodel-CitationInformationSection3 ;
    mcro:hasIntendedUseCase mcro:visiontransformerbasesizedmodel-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:visiontransformerbasesizedmodel-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:visiontransformerbasesizedmodel-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:visiontransformerbasesizedmodel-UseCaseInformationSection .

mcro:vit-age-classifier a mcro:Model ;
    mcro:hasModelArchitecture mcro:vit-age-classifier-ModelArchitecture .

mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k a mcro:Model ;
    mcro:hasModelDetail mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail ;
    mcro:hasUseCase mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-UseCase .

mcro:vitbasepatch32clip448laion2bftin12kin1k a mcro:Model ;
    mcro:hasModelDetail mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelDetail ;
    mcro:hasUseCase mcro:vitbasepatch32clip448laion2bftin12kin1k-UseCase .

mcro:vitbasepatch8224augreg2in21kftin1k a mcro:Model ;
    mcro:hasModelDetail mcro:vitbasepatch8224augreg2in21kftin1k-ModelDetail ;
    mcro:hasUseCase mcro:vitbasepatch8224augreg2in21kftin1k-UseCase .

mcro:vitlargepatch14reg4dinov2lvd142m a mcro:Model ;
    mcro:hasModelDetail mcro:vitlargepatch14reg4dinov2lvd142m-ModelDetail ;
    mcro:hasUseCase mcro:vitlargepatch14reg4dinov2lvd142m-UseCase .

mcro:vitmatte-model a mcro:Model ;
    mcro:hasModelDetail mcro:vitmatte-model-ModelDetail ;
    mcro:hasUseCase mcro:vitmatte-model-UseCaseInformationSection .

mcro:vocos a mcro:Model ;
    mcro:hasCitation mcro:vocos-CitationInformationSection ;
    mcro:hasLicense mcro:vocos-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:vocos-ModelArchitectureInformationSection .

mcro:wav2vec2-baseforemotionrecognition a mcro:Model ;
    mcro:hasCitation mcro:wav2vec2-baseforemotionrecognition-CitationInformationSection ;
    mcro:hasDataset mcro:wav2vec2-baseforemotionrecognition-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:wav2vec2-baseforemotionrecognition-ModelArchitectureInformationSection .

mcro:wav2vec2base a mcro:Model ;
    mcro:hasCitation mcro:wav2vec2base-Citation ;
    mcro:hasModelArchitecture mcro:wav2vec2base-ModelArchitecture ;
    mcro:hasUseCase mcro:wav2vec2base-UseCase .

mcro:wav2vec2largerobustftlibri960h a mcro:Model ;
    mcro:hasCitation mcro:wav2vec2largerobustftlibri960h-Citation ;
    mcro:hasDataset mcro:wav2vec2largerobustftlibri960h-Dataset ;
    mcro:hasModelArchitecture mcro:wav2vec2largerobustftlibri960h-Architecture ;
    mcro:hasUseCase mcro:wav2vec2largerobustftlibri960h-UseCase .

mcro:wav2vec2largexlsr53 a mcro:Model ;
    mcro:hasModelDetail mcro:wav2vec2largexlsr53-ModelDetail ;
    mcro:hasUseCase mcro:wav2vec2largexlsr53-UseCase .

mcro:wav2vec2largexlsr53espeakcvft a mcro:Model ;
    mcro:hasCitation mcro:wav2vec2largexlsr53espeakcvft-Citation ;
    mcro:hasDataset mcro:wav2vec2largexlsr53espeakcvft-Dataset ;
    mcro:hasModelArchitecture mcro:wav2vec2largexlsr53espeakcvft-ModelArchitecture ;
    mcro:hasUseCase mcro:wav2vec2largexlsr53espeakcvft-UseCase .

mcro:wav2vec2largexlsrhindi a mcro:Model ;
    mcro:hasDataset mcro:wav2vec2largexlsrhindi-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:wav2vec2largexlsrhindi-ModelArchitectureInformationSection ;
    mcro:hasQuantativeAnalysis mcro:wav2vec2largexlsrhindi-QuantativeAnalysisSection ;
    mcro:hasUseCase mcro:wav2vec2largexlsrhindi-UseCaseInformationSection .

mcro:wav2vec2xlsr300mcv7turkish a mcro:Model ;
    mcro:hasModelDetail mcro:wav2vec2xlsr300mcv7turkish-ModelDetail ;
    mcro:hasModelParameter mcro:wav2vec2xlsr300mcv7turkish-ModelParameter .

mcro:wav2vec2xlsr300mhebrew a mcro:Model ;
    mcro:hasIntendedUseCase mcro:wav2vec2xlsr300mhebrew-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:wav2vec2xlsr300mhebrew-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:wav2vec2xlsr300mhebrew-TrainingDataInformationSection .

mcro:wavlmbaseplus a mcro:Model ;
    mcro:hasDataset mcro:wavlmbaseplus-Dataset ;
    mcro:hasModelArchitecture mcro:wavlmbaseplus-ModelArchitecture ;
    mcro:hasModelDetail mcro:wavlmbaseplus-ModelDetail ;
    mcro:hasUseCase mcro:wavlmbaseplus-UseCase .

mcro:wavlmlarge a mcro:Model ;
    mcro:hasDataset mcro:wavlmlarge-Dataset ;
    mcro:hasModelDetail mcro:wavlmlarge-ModelDetail ;
    mcro:hasUseCase mcro:wavlmlarge-UseCase .

mcro:we_log_statistics a mcro:Model ;
    prov1:hasTextValue "We log statistics to see if any envs are breaking" .

mcro:whisper a mcro:Model ;
    mcro:hasCitation mcro:whisper-Citation ;
    mcro:hasConsideration mcro:whisper-Consideration ;
    mcro:hasDataset mcro:whisper-Dataset,
        mcro:whisper-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:whisper-ModelArchitecture ;
    mcro:hasModelDetail mcro:whisper-ModelDetail ;
    mcro:hasModelParameter mcro:whisper-ModelParameter ;
    mcro:hasPerformance mcro:whisper-Performance ;
    mcro:hasQuantativeAnalysis mcro:whisper-QuantativeAnalysisSection ;
    mcro:hasTrainingData mcro:whisper-TrainingData ;
    mcro:hasUseCase mcro:whisper-UseCase,
        mcro:whisper-UseCaseInformationSection .

mcro:whisper-baseen a mcro:Model ;
    mcro:hasIntendedUseCase mcro:whisper-baseen-IntendedUseCase ;
    mcro:hasModelDetail mcro:whisper-baseen-ModelDetail ;
    mcro:hasPerformanceLimitations mcro:whisper-baseen-PerformanceLimitations ;
    mcro:hasTrainingData mcro:whisper-baseen-TrainingData .

mcro:whisperbase a mcro:Model ;
    mcro:hasIntendedUseCase mcro:whisperbase-IntendedUseCase ;
    mcro:hasModelDetail mcro:whisperbase-ModelDetail ;
    mcro:hasTrainingData mcro:whisperbase-TrainingData .

mcro:whisperkit a mcro:Model ;
    mcro:hasModelArchitecture mcro:whisperkit-ModelArchitecture .

mcro:whisperlargev3 a mcro:Model ;
    mcro:hasModelDetail mcro:whisperlargev3-ModelDetail ;
    mcro:hasTrainingData mcro:whisperlargev3-TrainingData ;
    mcro:hasUseCase mcro:whisperlargev3-UseCase .

mcro:xclipbasesizedmodel a mcro:Model ;
    mcro:hasCitation mcro:xclipbasesizedmodel-CitationInformationSection ;
    mcro:hasIntendedUseCase mcro:xclipbasesizedmodel-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:xclipbasesizedmodel-ModelArchitectureInformationSection ;
    mcro:hasQuantativeAnalysis mcro:xclipbasesizedmodel-QuantativeAnalysisSection ;
    mcro:hasTrainingData mcro:xclipbasesizedmodel-TrainingDataInformationSection .

mcro:xlmrobertabaselanguagedetection a mcro:Model ;
    mcro:hasIntendedUseCase mcro:xlmrobertabaselanguagedetection-IntendedUseCase ;
    mcro:hasModelDetail mcro:xlmrobertabaselanguagedetection-ModelDetail ;
    mcro:hasQuantativeAnalysis mcro:xlmrobertabaselanguagedetection-QuantativeAnalysis ;
    mcro:hasTrainingData mcro:xlmrobertabaselanguagedetection-TrainingData .

mcro:xtunerllavallama38bv11transformers a mcro:Model ;
    mcro:hasCitation mcro:xtunerllavallama38bv11transformers-Citation ;
    mcro:hasModelDetail mcro:xtunerllavallama38bv11transformers-ModelDetail .

mcro:yiyanghkustfinberttone a mcro:Model ;
    mcro:hasCitation mcro:yiyanghkustfinberttone-Citation ;
    mcro:hasModelArchitecture mcro:yiyanghkustfinberttone-ModelArchitecture ;
    mcro:hasUseCase mcro:yiyanghkustfinberttone-UseCase .

mcro:yuvalkirstainPickScorev1 a mcro:Model ;
    mcro:hasModelDetails mcro:yuvalkirstainPickScorev1-ModelDetail ;
    mcro:hasTrainingData mcro:yuvalkirstainPickScorev1-TrainingData ;
    mcro:hasUseCase mcro:yuvalkirstainPickScorev1-UseCase .

mcro:AlibabaNLPgteQwen27Binstruct-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}""" .

mcro:AlibabaNLPgteQwen27Binstruct-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT Licensed inference server" .

mcro:AlibabaNLPgteQwen27Binstruct-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. This training leverages both weakly supervised and supervised data, ensuring the model's applicability across numerous languages and a wide array of downstream tasks.",
        "Instruction tuning, applied solely on the query side for streamlined efficiency",
        "Integration of bidirectional attention mechanisms, enriching its contextual understanding." .

mcro:AlibabaNLPgteQwen27Binstruct-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "General Text Embedding" .

mcro:AlibabaNLPgtebaseenv15-Citation a mcro:CitationInformationSection .

mcro:AlibabaNLPgtebaseenv15-Consideration a mcro:ConsiderationInformationSection .

mcro:AlibabaNLPgtebaseenv15-Dataset a mcro:DatasetInformationSection ;
    mcro:hasTrainingData mcro:AlibabaNLPgtebaseenv15-TrainingData .

mcro:AlibabaNLPgtebaseenv15-License a mcro:LicenseInformationSection .

mcro:AlibabaNLPgtebaseenv15-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:AlibabaNLPgtebaseenv15-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:AlibabaNLPgtebaseenv15-Citation ;
    mcro:hasLicense mcro:AlibabaNLPgtebaseenv15-License .

mcro:AlibabaNLPgtebaseenv15-ModelParameter a mcro:ModelParameterSection .

mcro:AlibabaNLPgtebaseenv15-QuantativeAnalysis a mcro:QuantativeAnalysisSection .

mcro:AlibabaNLPgtebaseenv15-TrainingData a mcro:TrainingDataInformationSection .

mcro:AlibabaNLPgtebaseenv15-UseCase a mcro:UseCaseInformationSection .

mcro:AlibabaNLPgtemultilingualbase-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "encoder-only transformers architecture" .

mcro:AlibabaNLPgtemultilingualbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{zhang2024mgte,
  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},
  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={1393--1412},
  year={2024}
}""" .

mcro:AlibabaNLPgtemultilingualbase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "multilingual retrieval, cross-lingual retrieval, long text retrieval, and general text representation" .

mcro:AudioSpectrogramTransformerfinetunedonAudioSet-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "It was introduced in the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al. and first released in [this repository](https://github.com/YuanGongND/ast)." .

mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Audio Spectrogram Transformer",
        "The Audio Spectrogram Transformer is equivalent to [ViT](https://huggingface.co/docs/transformers/model_doc/vit), but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks." .

mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue "The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks." .

mcro:AudioSpectrogramTransformerfinetunedonAudioSet-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for classifying audio into one of the AudioSet classes.",
        "You can use the raw model for classifying audio into one of the AudioSet classes. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info." .

mcro:Auralis-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{auralis2024,
  author = {AstraMind AI},
  title = {Auralis: High-Performance Text-to-Speech Engine},
  year = {2024},
  url = {https://huggingface.co/AstraMindAI/auralis}
}""" .

mcro:Auralis-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:Auralis-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Based on [Coqui XTTS-v2](https://huggingface.co/coqui/XTTS-v2)" .

mcro:Auralis-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:Auralis-Citation ;
    mcro:hasLicense mcro:Auralis-License ;
    mcro:hasModelArchitecture mcro:Auralis-ModelArchitecture .

mcro:Auralis-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Text-to-Speech (TTS) generation for real-world applications, including books, dialogues, and multilingual tasks." .

mcro:BAAIbgebaseen-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:BAAIbgebaseen-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The [masive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released" .

mcro:BAAIbgebaseen-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge." .

mcro:BAAIbgebaseen-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning." .

mcro:BAAIbgebaseen-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification,  clustering, or semantic search.
And it also can be used in vector databases for LLMs.""" .

mcro:BAAIbgelargeen-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:BAAIbgelargeen-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge." .

mcro:BAAIbgem3-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bge-m3,
      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, 
      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2402.03216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:BAAIbgem3-Dataset a mcro:DatasetInformationSection .

mcro:BAAIbgem3-License a mcro:LicenseInformationSection .

mcro:BAAIbgem3-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:BAAIbgem3-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:BAAIbgem3-Citation .

mcro:BAAIbgem3-UseCase a mcro:UseCaseInformationSection .

mcro:BAAIbgereRankerBase-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:BAAIbgereRankerBase-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:BAAIbgereRankerBase-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:BAAIbgereRankerBase-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:BAAIbgereRankerBase-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:BAAIbgererankerlarge-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:BAAIbgererankerlarge-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "multilingual pair data" .

mcro:BAAIbgererankerlarge-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:BAAIbgererankerlarge-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "cross-encoder" .

mcro:BAAIbgererankerlarge-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "re-rank top-k documents" .

mcro:BAAIbgererankerv2m3-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{li2023making,
      title={Making Large Language Models A Better Foundation For Dense Retrieval}, 
      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},
      year={2023},
      eprint={2312.15503},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chen2024bge,
      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, 
      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2402.03216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:BAAIbgererankerv2m3-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "reranker" .

mcro:BAAIbgererankerv2m3-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """reranker uses question and document as input and directly output similarity instead of embedding.
You can get a relevance score by inputting query and passage to the reranker.
And the score can be mapped to a float value in [0,1] by sigmoid function.""" .

mcro:BAAIbgesmallen-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:BAAIbgesmallen-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "MTEB, C-MTEB" .

mcro:BAAIbgesmallen-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:BAAIbgesmallen-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer" .

mcro:BAAIbgesmallen-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Retrieval-augmented LLMs" .

mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2106-08254,
  author    = {Hangbo Bao and
               Li Dong and
               Furu Wei},
  title     = {BEiT: {BERT} Pre-Training of Image Transformers},
  journal   = {CoRR},
  volume    = {abs/2106.08254},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.08254},
  archivePrefix = {arXiv},
  eprint    = {2106.08254},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}""" .

mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like)." .

mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection1,
        mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection2 ;
    mcro:hasModelArchitecture mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelArchitectureInformationSection .

mcro:BEiTbasesizedmodelfinetunedonImageNet22k-TrainingDataInformationSection a mcro:TrainingDataInformationSection .

mcro:BEiTbasesizedmodelfinetunedonImageNet22k-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=microsoft/beit) to look for
fined-tuned versions on a task that interests you.""" .

mcro:BRIA_Background_Removal_v14-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """RMBG v1.4 is developed on the [IS-Net](https://github.com/xuebinqin/DIS) enhanced with our unique training scheme and proprietary dataset. 
These modifications significantly improve the models accuracy and effectiveness in diverse image-processing scenarios.""" .

mcro:BRIA_Background_Removal_v14-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """Bria-RMBG model was trained with over 12,000 high-quality, high-resolution, manually labeled (pixel-wise accuracy), fully licensed images.
Our benchmark included balanced gender, balanced ethnicity, and people with different types of disabilities.
For clarity, we provide our data distribution according to different categories, demonstrating our models versatility.""" .

mcro:BRIA_Background_Removal_v14-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue """The model is released under a Creative Commons license for non-commercial use.
  - Commercial use is subject to a commercial agreement with BRIA. To purchase a commercial license simply click """ .

mcro:BRIA_Background_Removal_v14-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """designed to effectively separate foreground from background in a range of
categories and image types. This model has been trained on a carefully selected dataset, which includes:
general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale.""" .

mcro:Bertbasemultilingualcasednerhrl-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "NER" .

mcro:Bertbasemultilingualcasednerhrl-Limitation a mcro:LimitationInformationSection .

mcro:Bertbasemultilingualcasednerhrl-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "bert-base-multilingual-cased" .

mcro:Bertbasemultilingualcasednerhrl-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ANERcorp",
        "Europeana Newspapers",
        "Italian I-CAB",
        "Latvian NER",
        "MSRA",
        "Paramopama + Second Harem",
        "conll 2002",
        "conll 2003" .

mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{zheng2024birefnet,
  title={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},
  author={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},
  journal={CAAI Artificial Intelligence Research},
  volume = {3},
  pages = {9150038},
  year={2024}
}""" .

mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "DIS-TR and validated on DIS-TEs and DIS-VD" .

mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT" .

mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:BiomedParse-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """Zhao, T., Gu, Y., Yang, J. et al. A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nat Methods 22, 166176 (2025). https://doi.org/10.1038/s41592-024-02499-w

@article{zhao2025foundation,
  title={A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities},
  author={Zhao, Theodore and Gu, Yu and Yang, Jianwei and Usuyama, Naoto and Lee, Ho Hin and Kiblawi, Sid and Naumann, Tristan and Gao, Jianfeng and Crabtree, Angela and Abel, Jacob and others},
  journal={Nature methods},
  volume={22},
  number={1},
  pages={166--176},
  year={2025},
  publisher={Nature Publishing Group US New York}
}""" .

mcro:BiomedParse-DataSpecificationSection a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue """- The model expect 2D 8-bit RGB or grayscale images by default, with pixel values ranging from 0 to 255 and resolution 1024*1024.
- The model outputs pixel probabilities in the same shape as the input image. We convert the floating point probabilities to 8-bit grayscale outputs. The probability threshold for segmentation mask is 0.5, which corresponds to 127.5 in 8-bit grayscale output.
- The model takes in text prompts for segmentation and doesn't have a fixed number of targets to handle. However, to ensure quality performance, we recommend the following tasks based on evaluation results. However, as we only evaluated the model on the test split of BiomedParseData, there is no guarantee for the same performance on external datasets even for the same task, due to variation in device, preprocessing, resolution and other distribution shifts. For best performance, we recommend finetuning on your specific tasks.
  - CT:
    - abdomen: adrenal gland, aorta, bladder, duodenum, esophagus, gallbladder, kidney, kidney cyst, kidney tumor, left adrenal gland, left kidney, liver, pancreas, postcava, right adrenal gland, right kidney, spleen, stomach, tumor
    - colon: tumor
    - liver: liver, tumor
    - lung: COVID-19 infection, nodule
    - pelvis: uterus
  - MRI-FLAIR: brain: edema, lower-grade glioma, tumor, tumor core, whole tumor
  - MRI-T1-Gd: brain: enhancing tumor, tumor core
  - MRI-T2: prostate: prostate peripheral zone, prostate transitional zone,
  - MRI:
    - abdomen: aorta, esophagus, gallbladder, kidney, left kidney, liver, pancreas, postcava, right kidney, spleen, stomach
    - brain: anterior hippocampus, posterior hippocampus
    - heart: left heart atrium, left heart ventricle, myocardium, right heart ventricle
    - prostate: prostate
  - OCT: retinal: edema
  - X-Ray: chest: COVID-19 infection, left lung, lung, lung opacity, right lung, viral pneumonia
  - Dermoscopy: skin: lesion, melanoma
  - Endoscope: colon: neoplastic polyp, non-neoplastic polyp, polyp
  - Fundus: retinal: optic cup, optic disc,
  - Pathology:
    - bladder: neoplastic cells
    - breast: epithelial cells, neoplastic cells
    - cervix: neoplastic cells
    - colon: glandular structure, neoplastic cells
    - esophagus: neoplastic cells
    - kidney: neoplastic cells
    - liver: epithelial cells, neoplastic cells
    - ovarian: epithelial cells, neoplastic cells
    - prostate: neoplastic cells skin: neoplastic cells
    - stomach: neoplastic cells
    - testis: epithelial cells
    - thyroid: epithelial cells, neoplastic cells
    - uterus: neoplastic cells
  - Ultrasound:
    - breast: benign tumor, malignant tumor, tumor
    - heart: left heart atrium, left heart ventricle
    - transperineal: fetal head, public symphysis""" .

mcro:BiomedParse-EthicalConsiderationSection a mcro:EthicalConsiderationSection ;
    prov1:hasTextValue """Microsoft believes Responsible AI is a shared responsibility and we have identified six principles and practices to help organizations address risks, innovate, and create value: fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant use case and addresses unforeseen product misuse.

While testing the model with images and/or text, ensure that the data is PHI free and that there are no patient information or information that can be tracked to a patient identity.

The model is not designed for the following use cases:

- Use by clinicians to inform clinical decision-making, as a diagnostic tool or as a medical device - Although MedImageParse is highly accurate in parsing biomedical data, it is not desgined or intended to be deployed in clinical settings as-is not is it for use in the diagnosis, cure, mitigation, treatment, or prevention of disease or other conditions (including to support clinical decision-making), or as a substitute of professional medical advice, diagnosis, treatment, or clinical judgment of a healthcare professional.

- Scenarios without consent for data -Any scenario that uses health data for a purpose for which consent was not obtained.

- Use outside of health scenarios - Any scenario that uses non-medical related image and/or serving purposes outside of the healthcare domain.

Please see Microsoft's Responsible AI Principles and approach available at https://www.microsoft.com/en-us/ai/principles-and-approach/""" .

mcro:BiomedParse-ModelDetailSection a mcro:ModelDetailSection ;
    prov1:hasTextValue "Basic information about the model that includes licensing information, owner information, the architecture of the model (algorthim employed), references (cited papers), and versioning information." .

mcro:BiomedParse-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "\"This section should allow readers to quickly grasp what the model should and should not be used for, and why it was created. It can also help frame the statistical analysis presented in the rest of the card, including a short description of the user(s), use-case(s), and context(s) for which the model was originally developed.\"" .

mcro:BridgeTowerbridgetowerlargeitmmlmitc-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{xu2022bridge,
  title={BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning},
  author={Xu, Xiao and Wu, Chenfei and Rosenman, Shachar and Lal, Vasudev and Che, Wanxiang and Duan, Nan},
  journal={arXiv preprint arXiv:2206.08657},
  year={2022}
}""" .

mcro:BridgeTowerbridgetowerlargeitmmlmitc-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance on various downstream vision-language tasks. In particular, on the VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs. Notably, when further scaling the model, BridgeTower achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets." .

mcro:BridgeTowerbridgetowerlargeitmmlmitc-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """The BridgeTower model was pretrained on four public image-caption datasets:
- [Conceptual Captions (CC3M)](https://ai.google.com/research/ConceptualCaptions/)
- [Conceptual 12M (CC12M)](https://github.com/google-research-datasets/conceptual-12m)
- [SBU Captions](https://www.cs.rice.edu/~vo9/sbucaptions/)
- [MSCOCO Captions](https://arxiv.org/pdf/1504.00325.pdf)
- [Visual Genome](https://visualgenome.org/)
  
The total number of unique images in the combined data is around 14M.""" .

mcro:BridgeTowerbridgetowerlargeitmmlmitc-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Here is how to use this model to perform contrastive learning between image and text pairs:

from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
import requests
from PIL import Image
import torch

image_urls = [
    "https://farm4.staticflickr.com/3395/3428278415_81c3e27f15_z.jpg",
  "http://images.cocodataset.org/val2017/000000039769.jpg"]
texts = [
    "two dogs in a car",
    "two cats sleeping on a couch"]
images = [Image.open(requests.get(url, stream=True).raw) for url in image_urls]

processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-large-itm-mlm")
model = BridgeTowerForContrastiveLearning.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")

inputs = processor(images, texts, padding=True, return_tensors="pt")
outputs = model(**inputs)

inputs = processor(images, texts[::-1], padding=True, return_tensors="pt")
outputs_swapped = model(**inputs)

print('Loss', outputs.loss.item())
# Loss 0.00191505195107311
print('Loss with swapped images', outputs_swapped.loss.item())
# Loss with swapped images 2.1259872913360596 


Here is how to use this model to perform image and text matching

from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
import requests
from PIL import Image

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = ["An image of two cats chilling on a couch", "A football player scoring a goal"]

processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-gaudi")
model = BridgeTowerForImageAndTextRetrieval.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-gaudi")

# forward pass
scores = dict()
for text in texts:
    # prepare inputs
    encoding = processor(image, text, return_tensors="pt")
    outputs = model(**encoding)
    scores[text] = outputs.logits[0,1].item()


Here is how to use this model to perform masked language modeling:

from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000360943.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
text = "a <mask> looking out of the window"

processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-gaudi")
model = BridgeTowerForMaskedLM.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-gaudi")

# prepare inputs
encoding = processor(image, text, return_tensors="pt")

# forward pass
outputs = model(**encoding)

results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())

print(results)
#.a cat looking out of the window.""" .

mcro:CLIPSegModel-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Lddecke et al. and first released in [this repository](https://github.com/timojl/clipseg)." .

mcro:CLIPSegModel-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model is intended for zero-shot and one-shot image segmentation." .

mcro:CLIPViTB32LAION2B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "OpenAI CLIP paper",
        "OpenCLIP software" .

mcro:CLIPViTB32LAION2B-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "LAION-2B" .

mcro:CLIPViTB32LAION2B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CLIP ViT-B/32" .

mcro:CLIPViTB32LAION2B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:CLIPViTB32LAION2B-Citation ;
    mcro:hasDataset mcro:CLIPViTB32LAION2B-Dataset ;
    mcro:hasModelArchitecture mcro:CLIPViTB32LAION2B-ModelArchitecture .

mcro:CLIPViTB32LAION2B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "research output for research communities" .

mcro:CLIPViTH14LAION2B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{schuhmann2022laionb,
  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
  author={Christoph Schuhmann and
          Romain Beaumont and
          Richard Vencu and
          Cade W Gordon and
          Ross Wightman and
          Mehdi Cherti and
          Theo Coombes and
          Aarush Katta and
          Clayton Mullis and
          Mitchell Wortsman and
          Patrick Schramowski and
          Srivatsa R Kundurthy and
          Katherine Crowson and
          Ludwig Schmidt and
          Robert Kaczmarczyk and
          Jenia Jitsev},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
  url={https://openreview.net/forum?id=M3Y74vmsMcY}
}

@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={ICML},
  year={2021}
}

@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}""" .

mcro:CLIPViTH14LAION2B-Evaluation a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "The model achieves a 78.0 zero-shot top-1 accuracy on ImageNet-1k." .

mcro:CLIPViTH14LAION2B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CLIP ViT-H/14" .

mcro:CLIPViTH14LAION2B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitectureInformationSection mcro:CLIPViTH14LAION2B-ModelArchitecture ;
    prov1:hasTextValue "A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip)." .

mcro:CLIPViTH14LAION2B-TrainingDetails a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)." .

mcro:CLIPViTH14LAION2B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model." .

mcro:CLIPViTbigG14LAION2B-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip)." .

mcro:CLIPViTbigG14LAION2B-Citation1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{schuhmann2022laionb,
  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
  author={Christoph Schuhmann and
          Romain Beaumont and
          Richard Vencu and
          Cade W Gordon and
          Ross Wightman and
          Mehdi Cherti and
          Theo Coombes and
          Aarush Katta and
          Clayton Mullis and
          Mitchell Wortsman and
          Patrick Schramowski and
          Srivatsa R Kundurthy and
          Katherine Crowson and
          Ludwig Schmidt and
          Robert Kaczmarczyk and
          Jenia Jitsev},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
  url={https://openreview.net/forum?id=M3Y74vmsMcY}
}""" .

mcro:CLIPViTbigG14LAION2B-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={ICML},
  year={2021}
}""" .

mcro:CLIPViTbigG14LAION2B-Citation3 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}""" .

mcro:CLIPViTbigG14LAION2B-Citation4 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{cherti2022reproducible,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  journal={arXiv preprint arXiv:2212.07143},
  year={2022}
}""" .

mcro:CLIPViTbigG14LAION2B-Evaluation a mcro:QuantativeAnalysisSection .

mcro:CLIPViTbigG14LAION2B-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT" .

mcro:CLIPViTbigG14LAION2B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:CLIPViTbigG14LAION2B-Citation1,
        mcro:CLIPViTbigG14LAION2B-Citation2,
        mcro:CLIPViTbigG14LAION2B-Citation3,
        mcro:CLIPViTbigG14LAION2B-Citation4 ;
    mcro:hasLicense mcro:CLIPViTbigG14LAION2B-License .

mcro:CLIPViTbigG14LAION2B-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue """This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/). 
Fine-tuning was also partially done on LAION-A, a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and phash deduplicated.

**IMPORTANT NOTE:** The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a safe subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress.""" .

mcro:CLIPViTbigG14LAION2B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """As per the original [OpenAI CLIP model card](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/model-card.md), this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. 

The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset.""" .

mcro:ChatGLM2-6B-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "GLM" .

mcro:ChatGLM2-6B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}""" .

mcro:ChatGLM2-6B-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "1.4T " .

mcro:ChatGLM2-6B-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache-2.0" .

mcro:ChatGLM2-6B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "" .

mcro:CryptoBERT-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "For academic reference, cite the following paper: https://ieeexplore.ieee.org/document/10223689" .

mcro:CryptoBERT-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "CryptoBERT's sentiment classification head was fine-tuned on a balanced dataset of 2M labelled StockTwits posts, sampled from [ElKulako/stocktwits-crypto](https://huggingface.co/datasets/ElKulako/stocktwits-crypto)." .

mcro:CryptoBERT-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CryptoBERT is a pre-trained NLP model to analyse the language and sentiments of cryptocurrency-related social media posts and messages. It was built by further training the [vinai's bertweet-base](https://huggingface.co/vinai/bertweet-base) language model on the cryptocurrency domain, using a corpus of over 3.2M unique cryptocurrency-related social media posts. (A research paper with more details will follow soon.)" .

mcro:CryptoBERT-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "CryptoBERT is a pre-trained NLP model to analyse the language and sentiments of cryptocurrency-related social media posts and messages." .

mcro:DFN5BCLIPViTH14378-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{fang2023data,
  title={Data Filtering Networks},
  author={Fang, Alex and Jose, Albin Madappally and Jain, Amit and Schmidt, Ludwig and Toshev, Alexander and Shankar, Vaishaal},
  journal={arXiv preprint arXiv:2309.17425},
  year={2023}
}
""" .

mcro:DFN5BCLIPViTH14378-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "DFN-5b" .

mcro:DFN5BCLIPViTH14378-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Contrastive Image-Text, Zero-Shot Image Classification." .

mcro:DFN5BCLIPViTH14378-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:DFN5BCLIPViTH14378-Citation ;
    mcro:hasDataset mcro:DFN5BCLIPViTH14378-Dataset ;
    mcro:hasModelArchitecture mcro:DFN5BCLIPViTH14378-ModelArchitecture .

mcro:DFN5BCLIPViTH14378-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This section details whether the model was developed with general or specific tasks in mind (e.g., plant recognition worldwide or in the Pacific Northwest). The use cases may be as broadly or narrowly defined as the developers intend. For example, if the model was built simply to label images, then this task should be indicated as the primary intended use case." .

mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}""" .

mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "DeBERTa: Decoding-enhanced BERT with Disentangled Attention improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder" .

mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Fine-tuning on NLU tasks" .

mcro:Distilbertbasemultilingualcasednerhrl-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ANERcorp",
        "Europeana Newspapers",
        "Italian I-CAB",
        "Latvian NER",
        "MSRA",
        "Paramopama + Second Harem",
        "conll 2002",
        "conll 2003" .

mcro:Distilbertbasemultilingualcasednerhrl-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "distilbert-base-multilingual-cased" .

mcro:Distilbertbasemultilingualcasednerhrl-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:Distilbertbasemultilingualcasednerhrl-ModelArchitecture .

mcro:Distilbertbasemultilingualcasednerhrl-UseCase a mcro:UseCaseInformationSection .

mcro:Dreamshaper8inpainting-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "runwayml/stable-diffusion-inpainting" .

mcro:Dreamshaper8inpainting-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Stable Diffusion Inpainting model" .

mcro:Dreamshaper8inpainting-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasDataset mcro:Dreamshaper8inpainting-Dataset ;
    mcro:hasModelArchitecture mcro:Dreamshaper8inpainting-ModelArchitecture .

mcro:Dreamshaper8inpainting-UseCase a mcro:UseCaseInformationSection .

mcro:ESM2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "accompanying paper" .

mcro:ESM2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "state-of-the-art protein model trained on a masked language modelling objective" .

mcro:ESM2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "suitable for fine-tuning on a wide range of tasks that take protein sequences as input" .

mcro:ESMFold-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "For details on the model architecture and training, please refer to the [accompanying paper](https://www.science.org/doi/10.1126/science.ade2574)." .

mcro:ESMFold-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ESMFold is a state-of-the-art end-to-end protein folding model based on an ESM-2 backbone. It does not require any lookup or MSA step, and therefore does not require any external databases to be present in order to make predictions. As a result, inference time is very significantly faster than AlphaFold2." .

mcro:ESMFold-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "If you're interested in using ESMFold in practice, please check out the associated [tutorial notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb)." .

mcro:EleutherAIpythia70mdeduped-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-based Language Model" .

mcro:EleutherAIpythia70mdeduped-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "https://arxiv.org/pdf/2304.01373.pdf" .

mcro:EleutherAIpythia70mdeduped-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:EleutherAIpythia70mdeduped-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Never rely on Pythia-70M-deduped to produce factually accurate output." .

mcro:EleutherAIpythia70mdeduped-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:EleutherAIpythia70mdeduped-Architecture ;
    mcro:hasLicense mcro:EleutherAIpythia70mdeduped-License ;
    mcro:hasUseCase mcro:EleutherAIpythia70mdeduped-UseCase .

mcro:EleutherAIpythia70mdeduped-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "the Pile after the dataset has been globally deduplicated" .

mcro:EleutherAIpythia70mdeduped-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "research on the behavior, functionality, and limitations of large language models" .

mcro:EmergentMethodsglinermediumnewsv21-Citation a mcro:CitationInformationSection .

mcro:EmergentMethodsglinermediumnewsv21-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:EmergentMethodsglinermediumnewsv21-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "microsoft/deberta" .

mcro:EmergentMethodsglinermediumnewsv21-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:EmergentMethodsglinermediumnewsv21-License ;
    mcro:hasModelArchitecture mcro:EmergentMethodsglinermediumnewsv21-ModelArchitecture .

mcro:EmergentMethodsglinermediumnewsv21-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "AskNews-NER-v0" .

mcro:EmergentMethodsglinermediumnewsv21-UseCase a mcro:UseCaseInformationSection .

mcro:FLD-5B a mcro:DatasetInformationSection ;
    prov1:hasTextValue "FLD-5B dataset, containing 5.4 billion annotations across 126 million images" .

mcro:FacebookAIxlmrobertabase-Citation a mcro:CitationInformationSection .

mcro:FacebookAIxlmrobertabase-Limitation a mcro:LimitationInformationSection .

mcro:FacebookAIxlmrobertabase-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:FacebookAIxlmrobertabase-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:FacebookAIxlmrobertabase-Citation ;
    mcro:hasModelArchitecture mcro:FacebookAIxlmrobertabase-ModelArchitecture .

mcro:FacebookAIxlmrobertabase-UseCase a mcro:UseCaseInformationSection .

mcro:FacebookAIxlmrobertalarge-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-1911-02116,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  eprinttype = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:FacebookAIxlmrobertalarge-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers" .

mcro:FacebookAIxlmrobertalarge-ModelDetail a mcro:ModelDetailSection .

mcro:FacebookAIxlmrobertalarge-UseCase a mcro:UseCaseInformationSection .

mcro:Falconsainsfwimagedetection-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru. 2019. Model Cards for Model Reporting. In FAT* 19: Conference on Fairness, Accountability, and Transparency, January 2931, 2019, Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3287560. 3287596" .

mcro:Falconsainsfwimagedetection-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "unknown" .

mcro:Falconsainsfwimagedetection-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Fine-Tuned Vision Transformer (ViT)" .

mcro:Falconsainsfwimagedetection-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:Falconsainsfwimagedetection-Citation ;
    mcro:hasLicense mcro:Falconsainsfwimagedetection-License ;
    mcro:hasModelArchitecture mcro:Falconsainsfwimagedetection-ModelArchitecture .

mcro:Falconsainsfwimagedetection-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue """- [Hugging Face Model Hub](https://huggingface.co/models)
- [Vision Transformer (ViT) Paper](https://arxiv.org/abs/2010.11929)
- [ImageNet-21k Dataset](http://www.image-net.org/)""" .

mcro:Falconsainsfwimagedetection-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The model's training data includes a proprietary dataset comprising approximately 80,000 images. This dataset encompasses a significant amount of variability and consists of two distinct classes: \"normal\" and \"nsfw.\" The training process on this data aimed to equip the model with the ability to distinguish between safe and explicit content effectively." .

mcro:Falconsainsfwimagedetection-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """- **NSFW Image Classification**: The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications.
- **Specialized Task Fine-Tuning**: While the model is adept at NSFW image classification, its performance may vary when applied to other tasks.
- Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.""" .

mcro:FinBERT-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "[Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014)" .

mcro:FinBERT-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue "[FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063)" .

mcro:FinBERT-Citation3 a mcro:CitationInformationSection ;
    prov1:hasTextValue "[blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium." .

mcro:FinBERT-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT language model" .

mcro:FinBERT-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:FinBERT-Citation,
        mcro:FinBERT-Citation2,
        mcro:FinBERT-Citation3 .

mcro:FinBERT-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "analyze sentiment of financial text" .

mcro:FinetunedT5SmallTextSummarization-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "a variety of documents and their corresponding human-generated summaries" .

mcro:FinetunedT5SmallTextSummarization-License a mcro:LicenseInformationSection .

mcro:FinetunedT5SmallTextSummarization-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "T5 transformer model" .

mcro:FinetunedT5SmallTextSummarization-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:FinetunedT5SmallTextSummarization-License ;
    mcro:hasModelArchitecture mcro:FinetunedT5SmallTextSummarization-ModelArchitecture .

mcro:FinetunedT5SmallTextSummarization-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Text Summarization" .

mcro:FlagEmbedding-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:FlagEmbedding-CitationInformationSection a mcro:CitationInformationSection .

mcro:FlagEmbedding-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning." .

mcro:FlagEmbedding-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.",
        "MIT License" .

mcro:FlagEmbedding-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:FlagEmbedding-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning.",
        "retromae and contrastive learning" .

mcro:FlagEmbedding-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """FlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:

- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)
- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)
- **Dense Retrieval**: [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)
- **Reranker Model**: [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)
- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)""",
        "Retrieval-augmented LLMs" .

mcro:Florence-2Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "sequence-to-sequence architecture" .

mcro:Florence-2Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{xiao2023florence,
  title={Florence-2: Advancing a unified representation for a variety of vision tasks},
  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},
  journal={arXiv preprint arXiv:2311.06242},
  year={2023}
}""" .

mcro:FsoftAICpiiphi-ConsiderationInformationSection a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue """* Not guaranteed to detect all forms of PII in every context.
* May return false positives or omit contextually relevant information.""" .

mcro:FsoftAICpiiphi-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model is intended for PII detection in text documents to support tasks such as data anonymization, compliance, and security auditing." .

mcro:GIT-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer decoder conditioned on both CLIP image tokens and text tokens" .

mcro:GIT-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:GIT-ModelArchitecture .

mcro:GIT-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "10 million image-text pairs" .

mcro:GIT-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "image and video captioning",
        "image classification",
        "visual question answering (VQA) on images and videos" .

mcro:HelsinkiNLPopusmtzhen-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Translation" .

mcro:HelsinkiNLPopusmtzhen-Citation a mcro:CitationInformationSection .

mcro:HelsinkiNLPopusmtzhen-EvaluationData a mcro:EvaluationDataInformationSection .

mcro:HelsinkiNLPopusmtzhen-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CC-BY-4.0" .

mcro:HelsinkiNLPopusmtzhen-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:HelsinkiNLPopusmtzhen-Citation ;
    mcro:hasLicense mcro:HelsinkiNLPopusmtzhen-License .

mcro:HelsinkiNLPopusmtzhen-TrainingData a mcro:TrainingDataInformationSection .

mcro:HelsinkiNLPopusmtzhen-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "translation and text-to-text generation" .

mcro:HuggingFaceH4zephyr7bbeta-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{tunstall2023zephyr,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clmentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
      eprint={2310.16944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}""" .

mcro:HuggingFaceH4zephyr7bbeta-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "UltraChat" .

mcro:HuggingFaceH4zephyr7bbeta-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "the model can be used for chat" .

mcro:HuggingFaceH4zephyr7bbeta-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT" .

mcro:HuggingFaceH4zephyr7bbeta-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets." .

mcro:HuggingFaceM4idefics28b-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{laurencon2023obelics,
      title={OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},
      author={Hugo Laurenon and Lucile Saulnier and Lo Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M. Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},
      year={2023},
      eprint={2306.16527},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{laurenon2024matters,
      title={What matters when building vision-language models?},
      author={Hugo Laurenon and Lo Tronchon and Matthieu Cord and Victor Sanh},
      year={2024},
      eprint={2405.02246},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:HuggingFaceM4idefics28b-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "OBELICS" .

mcro:HuggingFaceM4idefics28b-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:HuggingFaceM4idefics28b-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Multi-modal model (image+text)" .

mcro:HuggingFaceM4idefics28b-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "`idefics2-8b-base` and `idefics2-8b` can be used to perform inference on multimodal (image + text) tasks in which the input is composed of a text query along with one (or multiple) image(s). Text and images can be arbitrarily interleaved. That includes image captioning, visual question answering, etc." .

mcro:IDEAResearchgroundingdinobase-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang" .

mcro:IDEAResearchgroundingdinobase-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "extends a closed-set object detection model with a text encoder, enabling open-set object detection" .

mcro:IDEAResearchgroundingdinobase-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "zero-shot object detection (the task of detecting things in an image out-of-the-box without labeled data)" .

mcro:IDEAResearchgroundingdinotiny-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{liu2023grounding,
      title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}, 
      author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},
      year={2023},
      eprint={2303.05499},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:IDEAResearchgroundingdinotiny-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection." .

mcro:IDEAResearchgroundingdinotiny-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for zero-shot object detection (the task of detecting things in an image out-of-the-box without labeled data)." .

mcro:IP-Adapter-FaceID-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "AS InsightFace pretrained models are available for non-commercial research purposes, IP-Adapter-FaceID models are released exclusively for research purposes and is not intended for commercial use." .

mcro:IP-Adapter-FaceID-LimitationInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue """- The models do not achieve perfect photorealism and ID consistency.
- The generalization of the models is limited due to limitations of the training data, base model and face recognition model.""" .

mcro:IP-Adapter-FaceID-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:IP-Adapter-FaceID-LicenseInformationSection ;
    mcro:hasLimitation mcro:IP-Adapter-FaceID-LimitationInformationSection .

mcro:IP-Adapter-FaceID-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "IP-Adapter-FaceID can generate various style images conditioned on a face with only text prompts." .

mcro:Inteldptlarge-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2103-13413,
  author    = {Ren{\\'{e}} Ranftl and
               Alexey Bochkovskiy and
               Vladlen Koltun},
  title     = {Vision Transformers for Dense Prediction},
  journal   = {CoRR},
  volume    = {abs/2103.13413},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.13413},
  eprinttype = {arXiv},
  eprint    = {2103.13413},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13413.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:Inteldptlarge-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "Multiple datasets compiled together" .

mcro:Inteldptlarge-EthicalConsideration a mcro:EthicalConsiderationSection ;
    prov1:hasTextValue "The training data come from multiple image datasets compiled together." .

mcro:Inteldptlarge-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:Inteldptlarge-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. 
It was introduced in the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ranftl et al. (2021) and first released in [this repository](https://github.com/isl-org/DPT). 
DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.
![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dpt_architecture.jpg)

The model card has been written in combination by the Hugging Face team and Intel.""" .

mcro:Inteldptlarge-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue """Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the
protocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. ([Ranftl et al., 2021](https://arxiv.org/abs/2103.13413))""" .

mcro:Inteldptlarge-Tradeoff a mcro:Trade-offInformationSection ;
    prov1:hasTextValue "Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. There are no additional caveats or recommendations for this model." .

mcro:Inteldptlarge-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The dataset is called MIX 6, and contains around 1.4M images. The model was initialized with ImageNet-pretrained weights." .

mcro:Inteldptlarge-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for zero-shot monocular depth estimation. See the [model hub](https://huggingface.co/models?search=dpt) to look for fine-tuned versions on a task that interests you." .

mcro:InternVL22B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{chen2024expanding,
  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{gao2024mini,
  title={Mini-internvl: A flexible-transfer pocket multimodal model with 5\\% parameters and 90\\% performance},
  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},
  journal={arXiv preprint arXiv:2410.16261},
  year={2024}
}
@article{chen2024far,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}
@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}""" .

mcro:InternVL22B-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:InternVL22B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "InternViT-300M-448px, an MLP projector, and internlm2-chat-1_8b" .

mcro:InternVL22B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:InternVL22B-ModelArchitecture .

mcro:InternVL378B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{chen2024expanding,
  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{wang2024mpo,
  title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization},
  author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2411.10442},
  year={2024}
}
@article{chen2024far,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}
@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}""" .

mcro:InternVL378B-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:InternVL378B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "InternVL3 retains the same model architecture as InternVL 2.5 and its predecessors, InternVL 1.5 and 2.0, following the \"ViT-MLP-LLM\" paradigm. In this new version, we integrate a newly incrementally pre-trained InternViT with various pre-trained LLMs, including InternLM 3 and Qwen 2.5, using a randomly initialized MLP projector." .

mcro:InternVL378B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "an advanced multimodal large language model (MLLM) series that demonstrates superior overall performance" .

mcro:JackFramllama68m-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{miao2023specinfer,
      title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification}, 
      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},
      year={2023},
      eprint={2305.09781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:JackFramllama68m-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:JackFramllama68m-Citation ;
    prov1:hasTextValue """This is a LLaMA-like model with only 68M parameters trained on Wikipedia and part of the C4-en and C4-realnewslike datasets.

No evaluation has been conducted yet, so use it with care.

The model is mainly developed as a base Small Speculative Model in the [SpecInfer](https://arxiv.org/abs/2305.09781) paper.""" .

mcro:LTXVideo-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-video and image-to-video generation model" .

mcro:LTXVideo-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "As a statistical model this checkpoint might amplify existing societal biases.",
        "Prompt following is heavily influenced by the prompting-style.",
        "The model may fail to generate videos that matches the prompts perfectly.",
        "This model is not intended or able to provide factual information." .

mcro:LTXVideo-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "license" .

mcro:LTXVideo-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Prompts should be in English. The more elaborate the better. Good prompt looks like `The turquoise waves crash against the dark, jagged rocks of the shore, sending white foam spraying into the air. The scene is dominated by the stark contrast between the bright blue water and the dark, almost black rocks. The water is a clear, turquoise color, and the waves are capped with white foam. The rocks are dark and jagged, and they are covered in patches of green moss. The shore is lined with lush green vegetation, including trees and bushes. In the background, there are rolling hills covered in dense forest. The sky is cloudy, and the light is dim.`",
        "The model works best on resolutions under 720 x 1280 and number of frames below 257.",
        "The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames." .

mcro:LTXVideo-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:LTXVideo-Architecture ;
    mcro:hasLicense mcro:LTXVideo-License ;
    mcro:hasUseCase mcro:LTXVideo-UseCase .

mcro:LTXVideo-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "text-to-video as well as image+text-to-video usecases" .

mcro:Llama160m-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{miao2023specinfer,
      title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification}, 
      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},
      year={2023},
      eprint={2305.09781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:Llama160m-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This is a LLaMA-like model with only 160M parameters trained on Wikipedia and part of the C4-en and C4-realnewslike datasets." .

mcro:Llama160m-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model is mainly developed as a base Small Speculative Model in the [SpecInfer](https://arxiv.org/abs/2305.09781) paper." .

mcro:Llama3370BInstructAWQ-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "AWQ 4-bit quantized version" .

mcro:Llama3370BInstructAWQ-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "`meta-llama/Llama-3.3-70B-Instruct`" .

mcro:M2M100418M-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{fan2020englishcentric,
      title={Beyond English-Centric Multilingual Machine Translation}, 
      author={Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Edouard Grave and Michael Auli and Armand Joulin},
      year={2020},
      eprint={2010.11125},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
""" .

mcro:M2M100418M-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation." .

mcro:M2M100418M-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The model that can directly translate between the 9,900 directions of 100 languages.
To translate into a target language, the target language id is forced as the first generated token.
To force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.""" .

mcro:MERTv195M-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{li2023mert,
      title={MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}, 
      author={Yizhi Li and Ruibin Yuan and Ge Zhang and Yinghao Ma and Xingran Chen and Hanzhi Yin and Chenghua Lin and Anton Ragni and Emmanouil Benetos and Norbert Gyenge and Roger Dannenberg and Ruibo Liu and Wenhu Chen and Gus Xia and Yemin Shi and Wenhao Huang and Yike Guo and Jie Fu},
      year={2023},
      eprint={2306.00107},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}""" .

mcro:MERTv195M-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "music generation" .

mcro:MERTv195M-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer Layer-Dimension" .

mcro:MahmoudAshrafmms300m1130forcedaligner-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "conversion from torchaudio to HF Transformers for the MMS-300M checkpoint trained on forced alignment dataset" .

mcro:Marqo-FashionSigLIP-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:Marqo-FashionSigLIP-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:Marqo-FashionSigLIP-ModelArchitecture ;
    mcro:hasUseCase mcro:Marqo-FashionSigLIP-UseCase .

mcro:Marqo-FashionSigLIP-UseCase a mcro:UseCaseInformationSection .

mcro:MaziyarPanahiMistral7BInstructv03GGUF-Description a obo:IAO_0000314 ;
    prov1:hasTextValue "[MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF) contains GGUF format model files for [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)." .

mcro:MaziyarPanahiPhi35miniinstructGGUF-License a mcro:LicenseInformationSection .

mcro:MaziyarPanahiPhi35miniinstructGGUF-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:MaziyarPanahiPhi35miniinstructGGUF-License .

mcro:MaziyarPanahigemma31bitGGUF-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "[MaziyarPanahi/gemma-3-1b-it-GGUF](https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF) contains GGUF format model files for [google/gemma-3-1b-it](https://huggingface.co/google/gemma-3-1b-it)." .

mcro:MaziyarPanahigemma31bitGGUF-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "None" .

mcro:MaziyarPanahigemma31bitGGUF-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "GGUF" .

mcro:ModelCardReport-CitationInformationSection a mcro:CitationInformationSection .

mcro:ModelCardReport-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:ModelCardReport-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:ModelCardReport-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:ModelCardReport-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Architecture a mcro:ModelArchitectureInformationSection .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. Less Annotating, More Classifying  Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI. Preprint, June. Open Science Framework. https://osf.io/74b8k." .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasLimitation mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Limitation .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-License a mcro:LicenseInformationSection .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Limitation a mcro:LimitationInformationSection .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Architecture ;
    mcro:hasCitation mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Citation ;
    mcro:hasLicense mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-License .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelParameter a mcro:ModelParameterSection ;
    mcro:hasTrainingData mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-TrainingData .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-TrainingData a mcro:TrainingDataInformationSection .

mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-UseCase a mcro:UseCaseInformationSection .

mcro:MyAwesomeModel-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CNN" .

mcro:MyAwesomeModel-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Reference Paper" .

mcro:MyAwesomeModel-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet" .

mcro:MyAwesomeModel-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "mit" .

mcro:MyAwesomeModel-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Image Classification" .

mcro:NLLB200-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "- Information about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper." .

mcro:NLLB200-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "- Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022",
        "Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022" .

mcro:NLLB200-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).",
        " In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety)." .

mcro:NLLB200-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """- Datasets: Flores-200 dataset is described in Section 4
- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200
- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The
SentencePiece model is released along with NLLB-200.""" .

mcro:NLLB200-EvalData a mcro:EvaluationDataInformationSection ;
    prov1:hasTextValue """- Datasets: Flores-200 dataset is described in Section 4
- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200
- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The
SentencePiece model is released along with NLLB-200.""" .

mcro:NLLB200-IntendedUse a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """- Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.
- Primary intended users: Primary users are researchers and machine translation research community.
- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations.""" .

mcro:NLLB200-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CC-BY-NC" .

mcro:NLLB200-Metrics a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue " Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations." .

mcro:NLLB200-PerfMetric a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue " Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations." .

mcro:NLLB200-Tradeoff a mcro:TradeoffInformationSection ;
    prov1:hasTextValue "Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.",
        " Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments." .

mcro:NLLB200-TrainData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue " We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2." .

mcro:NLLB200-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2." .

mcro:NLLB200-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.
- Primary intended users: Primary users are researchers and machine translation research community.
- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations.""" .

mcro:NepaliBERT-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """S. Pudasaini, S. Shakya, A. Tamang, S. Adhikari, S. Thapa and S. Lamichhane, "NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus," 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), Kirtipur, Nepal, 2023, pp. 325-330, doi: 10.1109/I-SMAC58438.2023.10290690.

@INPROCEEDINGS{10290690,
  author={Pudasaini, Shushanta and Shakya, Subarna and Tamang, Aakash and Adhikari, Sajjan and Thapa, Sunil and Lamichhane, Sagar},
  booktitle={2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus}, 
  year={2023},
  volume={},
  number={},
  pages={325-330},
  doi={10.1109/I-SMAC58438.2023.10290690}}""" .

mcro:NepaliBERT-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """This model is a fine-tuned version of [Bert Base Uncased](https://huggingface.co/bert-base-uncased) on dataset composed of different news scrapped from nepali news portals comprising of 4.6 GB of textual data.
THe training corpus is developed using 85467 news scrapped from different job portals. This is a preliminary dataset 
for the experimentation. THe corpus size is about 4.3 GB of textual data. Similary evaluation data contains few news articles about 12 mb of textual data.
Trained on about 4.6 GB of Nepali text corpus collected from various sources
These data were collected from nepali news site, OSCAR nepali corpus""" .

mcro:NepaliBERT-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Pretraining done on bert base architecture." .

mcro:NepaliBERT-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """This transformer model can be used for any NLP tasks related to Devenagari language. At the time of training, this is the state of the art model developed 
for Devanagari dataset. Intrinsic evaluation with Perplexity of 8.56 achieves this state of the art whereas extrinsit evaluation done on sentiment analysis of Nepali tweets outperformed other existing 
masked language models on Nepali dataset.""" .

mcro:NusaBertnerv13-DatasetInformation a mcro:DatasetInformationSection .

mcro:NusaBertnerv13-IntendedUseCase a mcro:UseCaseInformationSection .

mcro:NusaBertnerv13-ModelArchitectureInformation a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ModernBERT architecture" .

mcro:NusaBertnerv13-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasDatasetInformation mcro:NusaBertnerv13-DatasetInformation ;
    mcro:hasIntendedUseCase mcro:NusaBertnerv13-IntendedUseCase ;
    mcro:hasModelArchitectureInformation mcro:NusaBertnerv13-ModelArchitectureInformation .

mcro:OrengutengLlama38BLexiUncensored-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "META LLAMA 3 COMMUNITY LICENSE AGREEMENT" .

mcro:OrengutengLlama38BLexiUncensored-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama-3-8b-Instruct" .

mcro:OrengutengLlama38BLexiUncensored-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue """|             Metric              |Value|
|---------------------------------|----:|
|Avg.                             |66.18|
|AI2 Reasoning Challenge (25-Shot)|59.56|
|HellaSwag (10-Shot)              |77.88|
|MMLU (5-Shot)                    |67.68|
|TruthfulQA (0-shot)              |47.72|
|Winogrande (5-shot)              |75.85|
|GSM8k (5-shot)                   |68.39|""" .

mcro:OrengutengLlama38BLexiUncensored-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Lexi is uncensored, which makes the model compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones.

You are responsible for any content you create using this model. Please use it responsibly.""" .

mcro:ProtGPT2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "UniRef50 (version 2021_04)" .

mcro:ProtGPT2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ProtGPT2 is based on the GPT2 Transformer architecture and contains 36 layers with a model dimensionality of 1280, totalling 738 million parameters." .

mcro:ProtGPT2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasDataset mcro:ProtGPT2-Dataset ;
    mcro:hasModelArchitecture mcro:ProtGPT2-ModelArchitecture ;
    prov1:hasTextValue """ProtGPT2 is a decoder-only transformer model pre-trained on the protein space, database UniRef50 (version 2021_04). The pre-training was done on the raw sequences without FASTA headers. Details of training and datasets can be found here: https://huggingface.co/datasets/nferruz/UR50_2021_04

ProtGPT2 was trained in a self-supervised fashion, i.e., the raw sequence data was used during training without including the annotation of sequences. In particular, ProtGPT2 was trained using a causal modelling objective, in which the model is trained to predict the next token (or, in this case, oligomer) in the sequence.
 By doing so, the model learns an internal representation of proteins and is able to <em>speak</em> the protein language.""" .

mcro:ProtGPT2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "ProtGPT2 can be used for de novo protein design and engineering." .

mcro:Protbertmodel-CitationInformationSection a mcro:CitationInformationSection .

mcro:Protbertmodel-EvaluationDataInformationSection a mcro:EvaluationDataInformationSection .

mcro:Protbertmodel-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:Protbertmodel-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:Protbertmodel-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:Protbertmodel-CitationInformationSection ;
    mcro:hasLicense mcro:Protbertmodel-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:Protbertmodel-ModelArchitectureInformationSection .

mcro:Protbertmodel-TrainingDataInformationSection a mcro:TrainingDataInformationSection .

mcro:Protbertmodel-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:QdrantallminiLML6v2withattentions-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model is intended to be used for [BM42 searches](https://qdrant.tech/articles/bm42/)." .

mcro:Qwen1505BChat-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}""" .

mcro:Qwen1505BChat-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data." .

mcro:Qwen1505BChat-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue "Qwen1.5 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes. For the beta version, temporarily we did not include GQA (except for 32B) and the mixture of SWA and full attention." .

mcro:Qwen2.5-1.5B-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:Qwen2.5-1.5B-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:Qwen2.5-1.5B-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:Qwen2.5-VL-32B-Instruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}""" .

mcro:Qwen2.5-VL-32B-Instruct-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:Qwen2.5-VL-32B-Instruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:Qwen2.5-VL-32B-Instruct-Citation ;
    mcro:hasModelArchitecture mcro:Qwen2.5-VL-32B-Instruct-ModelArchitecture .

mcro:Qwen2.5-VL-32B-Instruct-UseCase a mcro:UseCaseInformationSection .

mcro:Qwen2505B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:Qwen2505B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:Qwen2505B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "We do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model." .

mcro:Qwen2505BInstruct-ArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:Qwen2505BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:Qwen2505BInstruct-Dataset a mcro:DatasetInformationSection .

mcro:Qwen2505BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:Qwen2505BInstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:Qwen2505BInstruct-Citation ;
    mcro:hasModelArchitecture mcro:Qwen2505BInstruct-ModelArchitecture .

mcro:Qwen2505BInstruct-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:Qwen2505BInstruct-ArchitectureInformationSection ;
    prov1:hasTextValue """This repo contains an **unmodified version** of the instruction-tuned 0.5B Qwen2.5 model, which has the following features:
- Type: Causal Language Models
- Training Stage: Pretraining & Post-training
- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings
- Number of Parameters: 0.49B
- Number of Paramaters (Non-Embedding): 0.36B
- Number of Layers: 24
- Number of Attention Heads (GQA): 14 for Q and 2 for KV
- Context Length: Full 32,768 tokens and generation 8192 tokens""" .

mcro:Qwen2505BInstruct-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training." .

mcro:Qwen2514BInstruct-Citation a mcro:CitationInformationSection .

mcro:Qwen2514BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias" .

mcro:Qwen2514BInstruct-UseCase a mcro:UseCaseInformationSection .

mcro:Qwen2515BInstruct-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:Qwen2515BInstruct-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training." .

mcro:Qwen2532BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:Qwen2532BInstruct-License a mcro:LicenseInformationSection .

mcro:Qwen2532BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias" .

mcro:Qwen2532BInstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:Qwen2532BInstruct-Citation ;
    mcro:hasLicense mcro:Qwen2532BInstruct-License ;
    mcro:hasModelArchitecture mcro:Qwen2532BInstruct-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen2532BInstruct-UseCase .

mcro:Qwen2532BInstruct-UseCase a mcro:UseCaseInformationSection .

mcro:Qwen253BInstruct-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:Qwen253BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:Qwen253BInstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Causal Language Models" .

mcro:Qwen257B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:Qwen257B-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Causal Language Models" .

mcro:Qwen257B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias" .

mcro:Qwen257BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}
""" .

mcro:Qwen257BInstruct-CitationInformationSection a mcro:CitationInformationSection .

mcro:Qwen257BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias" .

mcro:Qwen257BInstruct-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias" .

mcro:Qwen257BInstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "instruction-tuned 7B Qwen2.5 model" .

mcro:Qwen257BInstruct-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:Qwen257BInstruct1M-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5-1m,
    title = {Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens},
    url = {https://qwenlm.github.io/blog/qwen2.5-1m/},
    author = {Qwen Team},
    month = {January},
    year = {2025}
}

@article{qwen2.5,
      title={Qwen2.5-1M Technical Report},
      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},
      journal={arXiv preprint arXiv:2501.15383},
      year={2025}
}""" .

mcro:Qwen257BInstruct1M-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias" .

mcro:Qwen257BInstruct1M-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Causal Language Models" .

mcro:Qwen25Coder7BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{hui2024qwen2,
      title={Qwen2. 5-Coder Technical Report},
      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
      journal={arXiv preprint arXiv:2409.12186},
      year={2024}
}
@article{qwen2,
      title={Qwen2 Technical Report},
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:Qwen25Coder7BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias" .

mcro:Qwen25Coder7BInstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Code Agents" .

mcro:Qwen25Omni-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{Qwen2.5-Omni,
  title={Qwen2.5-Omni Technical Report},
  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},
  journal={arXiv preprint arXiv:2503.20215},
  year={2025}
}""" .

mcro:Qwen25Omni-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Thinker-Talker architecture" .

mcro:Qwen25Omni-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner" .

mcro:Qwen2VL2BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "@article{Qwen-VL",
        "@article{Qwen2VL" .

mcro:Qwen2VL2BInstruct-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Constraints in Individuals and Intellectual Property (IP)",
        "Data timeliness",
        "Insufficient Counting Accuracy",
        "Lack of Audio Support",
        "Limited Capacity for Complex Instruction",
        "Weak Spatial Reasoning Skills" .

mcro:Qwen2VL2BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "* Multimodal Rotary Position Embedding (M-ROPE)",
        "* Naive Dynamic Resolution" .

mcro:Qwen2VL2BInstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Agent that can operate your mobiles, robots, etc.",
        "Multilingual Support",
        "SoTA understanding of images of various resolution & ratio",
        "Understanding videos of 20min+" .

mcro:Qwen3-14B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Causal Language Models" .

mcro:Qwen3-14B-UseCase a mcro:UseCaseInformationSection .

mcro:Qwen3-32B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Qwen Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}""" .

mcro:Qwen3-32B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Causal Language Models" .

mcro:Qwen3-32B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "general-purpose dialogue" .

mcro:Qwen3-4B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Causal Language Models" .

mcro:Qwen3-4B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:Qwen3-4B-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen3-4B-UseCase .

mcro:Qwen3-4B-UseCase a mcro:UseCaseInformationSection .

mcro:Qwen3-8B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Qwen Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}""" .

mcro:Qwen3-8B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Causal Language Models" .

mcro:Qwen3-8B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:Qwen3-8B-ModelArchitecture ;
    mcro:hasUseCase mcro:Qwen3-8B-UseCase .

mcro:Qwen3-8B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "general-purpose dialogue" .

mcro:Qwen306B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Qwen Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}""" .

mcro:Qwen306B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "0.44B",
        "0.6B",
        "16 for Q and 8 for KV",
        "28",
        "32,768",
        "Causal Language Models",
        "Pretraining & Post-training" .

mcro:Qwen306B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "complex logical reasoning, math, and coding",
        "creative writing",
        "efficient, general-purpose dialogue",
        "instruction following",
        "multi-turn dialogues",
        "multilingual instruction following",
        "precise integration with external tools in both thinking and unthinking modes",
        "role-playing",
        "translation" .

mcro:Qwen317B-Citation a mcro:CitationInformationSection .

mcro:Qwen317B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Causal Language Models" .

mcro:Qwen317B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:Qwen317B-ModelArchitecture .

mcro:Qwen317B-UseCase a mcro:UseCaseInformationSection .

mcro:QwenQwen2505BInstruct-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinze He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:QwenQwen2505BInstruct-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:QwenQwen2505BInstruct-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:QwenQwen2505BInstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:QwenQwen2505BInstruct-CitationInformationSection ;
    mcro:hasLicense mcro:QwenQwen2505BInstruct-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:QwenQwen2505BInstruct-ModelArchitectureInformationSection .

mcro:QwenQwen2505BInstruct-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:QwenQwen2515BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:QwenQwen2515BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:QwenQwen2515BInstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:QwenQwen2515BInstruct-Citation ;
    mcro:hasModelArchitecture mcro:QwenQwen2515BInstruct-ModelArchitecture .

mcro:QwenQwen2515BInstruct-UseCase a mcro:UseCaseInformationSection .

mcro:QwenQwen25Coder7BInstructGPTQInt4-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{hui2024qwen2,
      title={Qwen2. 5-Coder Technical Report},
      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
      journal={arXiv preprint arXiv:2409.12186},
      year={2024}
}
@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:QwenQwen25Coder7BInstructGPTQInt4-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias" .

mcro:QwenQwen25Coder7BInstructGPTQInt4-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Code Agents" .

mcro:QwenQwen25VL3BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "@misc{qwen2.5-VL," .

mcro:QwenQwen25VL3BInstruct-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue "@article{Qwen2VL," .

mcro:QwenQwen25VL3BInstruct-Citation3 a mcro:CitationInformationSection ;
    prov1:hasTextValue "@article{Qwen-VL," .

mcro:QwenQwen25VL3BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "* Dynamic Resolution and Frame Rate Training for Video Understanding",
        "* Streamlined and Efficient Vision Encoder" .

mcro:QwenQwen25VL3BInstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "computer use",
        "phone use",
        "reasoning",
        "tool use",
        "visual agent" .

mcro:QwenQwen25VL7BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "@misc{qwen2.5-VL," .

mcro:QwenQwen25VL7BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "* Dynamic Resolution and Frame Rate Training for Video Understanding" .

mcro:QwenQwen25VL7BInstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images." .

mcro:QwenQwen2VL7BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{Qwen2VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}""" .

mcro:QwenQwen2VL7BInstruct-License a mcro:LicenseInformationSection .

mcro:QwenQwen2VL7BInstruct-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Constraints in Individuals and Intellectual Property (IP): The model's capacity to recognize specific individuals or IPs is limited, potentially failing to comprehensively cover all well-known personalities or brands.",
        "Data timeliness: Our image dataset is updated until June 2023, and information subsequent to this date may not be covered.",
        "Insufficient Counting Accuracy: Particularly in complex scenes, the accuracy of object counting is not high, necessitating further improvements.",
        "Lack of Audio Support: The current model does not comprehend audio information within videos.",
        "Limited Capacity for Complex Instruction: When faced with intricate multi-step instructions, the model's understanding and execution capabilities require enhancement.",
        "Weak Spatial Reasoning Skills: Especially in 3D spaces, the model's inference of object positional relationships is inadequate, making it difficult to precisely judge the relative positions of objects." .

mcro:QwenQwen2VL7BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Multimodal Rotary Position Embedding (M-ROPE)",
        "Naive Dynamic Resolution" .

mcro:QwenQwen2VL7BInstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:QwenQwen2VL7BInstruct-ModelArchitecture .

mcro:QwenQwen2VL7BInstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Agent that can operate your mobiles, robots, etc.",
        "Multilingual Support",
        "SoTA understanding of images of various resolution & ratio",
        "Understanding videos of 20min+" .

mcro:RoBERTaBaseModel-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:RoBERTaBaseModel-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers model" .

mcro:RoBERTaBaseModel-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "BookCorpus",
        "CC-News",
        "English Wikipedia",
        "OpenWebText",
        "Stories" .

mcro:RoBERTaBaseModel-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "fine-tuned on a downstream task",
        "masked language modeling" .

mcro:RoBERTaLargeModel-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:RoBERTaLargeModel-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task." .

mcro:RoBERTaLargeModel-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion." .

mcro:RoBERTaLargeModel-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """The RoBERTa model was pretrained on the reunion of five datasets:
- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;
- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;
- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news
  articles crawled between September 2016 and February 2019.
- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to
  train GPT-2,
- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the
  story-like style of Winograd schemas.""" .

mcro:RobertaLargeMnli-Citation a mcro:CitationInformationSection .

mcro:RobertaLargeMnli-EvaluationData a mcro:DatasetInformationSection .

mcro:RobertaLargeMnli-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT" .

mcro:RobertaLargeMnli-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-based language model" .

mcro:RobertaLargeMnli-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:RobertaLargeMnli-License ;
    mcro:hasModelArchitecture mcro:RobertaLargeMnli-ModelArchitecture .

mcro:RobertaLargeMnli-TrainingData a mcro:DatasetInformationSection .

mcro:RobertaLargeMnli-UseCase a mcro:UseCaseInformationSection .

mcro:SWividF5TTS-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching" .

mcro:Salesforceblip2opt27b-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue """BLIP2-OPT uses off-the-shelf OPT as the language model. It inherits the same risks and limitations as mentioned in Meta's model card.

> Like other large language models for which the diversity (or lack thereof) of training
> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms
> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and
> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern
> large language models.
>
BLIP2 is fine-tuned on image-text datasets (e.g. [LAION](https://laion.ai/blog/laion-400-open-dataset/) ) collected from the internet.  As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.

BLIP2 has not been tested in real world applications. It should not be directly deployed in any applications. Researchers should first carefully assess the safety and fairness of the model in relation to the specific context theyre being deployed within.""" .

mcro:Salesforceblip2opt27b-EthicalConsideration a mcro:EthicalConsiderationSection ;
    prov1:hasTextValue "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peoples lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP." .

mcro:Salesforceblip2opt27b-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters).
It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository.

Disclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.

## Model description

BLIP-2 consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model.

The authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen
while training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of "query tokens" to query embeddings,
which bridge the gap between the embedding space of the image encoder and the large language model.

The goal for the model is simply to predict the next text token, giving the query embeddings and the previous text.

This allows the model to be used for tasks like:

- image captioning
- visual question answering (VQA)
- chat-like conversations by feeding the image and the previous conversation as prompt to the model""" .

mcro:Salesforceblip2opt27b-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:Salesforceblip2opt27b-ModelArchitecture ;
    mcro:hasUseCase mcro:Salesforceblip2opt27b-UseCase .

mcro:Salesforceblip2opt27b-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for conditional text generation given an image and optional text. See the model hub to look for
fine-tuned versions on a task that interests you.""" .

mcro:Salesforceblipimagecaptioningbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{https://doi.org/10.48550/arxiv.2201.12086,
  doi = {10.48550/ARXIV.2201.12086},
  
  url = {https://arxiv.org/abs/2201.12086},
  
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}""" .

mcro:Salesforceblipimagecaptioningbase-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peoples lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP." .

mcro:Salesforceblipimagecaptioningbase-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "image captioning pretrained on COCO dataset" .

mcro:Salesforceblipimagecaptioningbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "base architecture (with ViT base backbone)" .

mcro:Salesforceblipimagecaptioningbase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use this model for conditional and un-conditional image captioning" .

mcro:Salesforceblipimagecaptioninglarge-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{https://doi.org/10.48550/arxiv.2201.12086,
  doi = {10.48550/ARXIV.2201.12086},
  
  url = {https://arxiv.org/abs/2201.12086},
  
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}""" .

mcro:Salesforceblipimagecaptioninglarge-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peoples lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP." .

mcro:Salesforceblipimagecaptioninglarge-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "COCO dataset" .

mcro:Salesforceblipimagecaptioninglarge-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ViT large backbone" .

mcro:Salesforceblipimagecaptioninglarge-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "image captioning" .

mcro:Salesforceblipvqabase-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BLIP trained on visual question answering- base architecture (with ViT base backbone)." .

mcro:Salesforceblipvqabase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{https://doi.org/10.48550/arxiv.2201.12086,
  doi = {10.48550/ARXIV.2201.12086},
  
  url = {https://arxiv.org/abs/2201.12086},
  
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}""" .

mcro:Salesforceblipvqabase-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peoples lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP." .

mcro:Salesforceblipvqabase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use this model for conditional and un-conditional image captioning" .

mcro:Salesforceblipvqacapfiltlarge-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "large architecture (with ViT large backbone)" .

mcro:Salesforceblipvqacapfiltlarge-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{https://doi.org/10.48550/arxiv.2201.12086,
  doi = {10.48550/ARXIV.2201.12086},
  
  url = {https://arxiv.org/abs/2201.12086},
  
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}""" .

mcro:Salesforceblipvqacapfiltlarge-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasEthicalConsideration mcro:Salesforceblipvqacapfiltlarge-EthicalConsideration .

mcro:Salesforceblipvqacapfiltlarge-EthicalConsideration a mcro:EthicalConsiderationSection ;
    prov1:hasTextValue "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peoples lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP." .

mcro:Salesforceblipvqacapfiltlarge-License a mcro:LicenseInformationSection .

mcro:Salesforceblipvqacapfiltlarge-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:Salesforceblipvqacapfiltlarge-Arch ;
    mcro:hasCitation mcro:Salesforceblipvqacapfiltlarge-Citation ;
    mcro:hasLicense mcro:Salesforceblipvqacapfiltlarge-License .

mcro:Salesforceblipvqacapfiltlarge-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "conditional and un-conditional image captioning" .

mcro:Salesforcecodet5basemultisum-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """[CodeT5-base](https://huggingface.co/Salesforce/codet5-base) model fine-tuned on CodeSearchNet data in a multi-lingual training setting (
Ruby/JavaScript/Go/Python/Java/PHP) for code summarization.""" .

mcro:Salesforcecodet5basemultisum-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{
    wang2021codet5,
    title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, 
    author={Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi},
    booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021},
    year={2021},
}""" .

mcro:Salesforcecodet5basemultisum-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """We employ the filtered version of CodeSearchNet data [[Husain et al., 2019](https://arxiv.org/abs/1909.09436)]
from [CodeXGLUE](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text) benchmark for fine-tuning on
code summarization. The data is tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer. One can
prepare text (or code) for the model using RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base).""" .

mcro:Salesforcecodet5basemultisum-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "for code summarization" .

mcro:Segformerb0finetunedade512512-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Image segmentation" .

mcro:SmolDocling256Mpreview-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{nassar2025smoldoclingultracompactvisionlanguagemodel,
      title={SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion}, 
      author={Ahmed Nassar and Andres Marafioti and Matteo Omenetti and Maksym Lysak and Nikolaos Livathinos and Christoph Auer and Lucas Morin and Rafael Teixeira de Lima and Yusik Kim and A. Said Gurbuz and Michele Dolfi and Miquel Farr and Peter W. J. Staar},
      year={2025},
      eprint={2503.11576},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.11576}, 
}""" .

mcro:SmolDocling256Mpreview-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:SmolDocling256Mpreview-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)" .

mcro:SmolDocling256Mpreview-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "multimodal Image-Text-to-Text model designed for efficient document conversion" .

mcro:SmolLM2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{allal2025smollm2smolgoesbig,
      title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model}, 
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martn Blzquez and Guilherme Penedo and Lewis Tunstall and Andrs Marafioti and Hynek Kydlek and Agustn Piqueres Lajarn and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Clmentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},
      year={2025},
      eprint={2502.02737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.02737}, 
}""" .

mcro:SmolLM2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "FineWeb-Edu, DCLM, The Stack" .

mcro:SmolLM2-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:SmolLM2-Limitations a mcro:LimitationInformationSection .

mcro:SmolLM2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer decoder" .

mcro:SmolLM2-Training a mcro:TrainingDataInformationSection .

mcro:SmolLM2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "text rewriting, summarization and function calling" .

mcro:Snowflakesnowflakearcticembedm-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Arctic is licensed under the [Apache-2](https://www.apache.org/licenses/LICENSE-2.0). The released models can be used for commercial purposes free of charge." .

mcro:Snowflakesnowflakearcticembedm-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance. First, the models are trained with large batches of query-document pairs where negatives are derived in-batchpretraining leverages about 400m samples of a mix of public datasets and proprietary web search data. Following pretraining models are further optimized with long training on a smaller dataset (about 1m samples) of triplets of query, positive document, and negative document derived from hard harmful mining. Mining of the negatives and data curation is crucial to retrieval accuracy." .

mcro:Snowflakesnowflakearcticembedm-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:Snowflakesnowflakearcticembedm-ModelArchitecture ;
    prov1:hasTextValue "snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance." .

mcro:Snowflakesnowflakearcticembedm-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance." .

mcro:Snowflakesnowflakearcticembedxs-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance." .

mcro:Snowflakesnowflakearcticembedxs-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The `snowflake-arctic-embedding` models achieve **state-of-the-art performance on the MTEB/BEIR leaderboard** for each of their size variants." .

mcro:Snowflakesnowflakearcticembedxs-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Arctic is licensed under the [Apache-2](https://www.apache.org/licenses/LICENSE-2.0). The released models can be used for commercial purposes free of charge." .

mcro:Snowflakesnowflakearcticembedxs-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance." .

mcro:Snowflakesnowflakearcticembedxs-ModelDetail a mcro:ModelDetailSection .

mcro:Stablediffusionsafetychecker-BiasRiskLimitationConsideration a mcro:ConsiderationInformationSection .

mcro:Stablediffusionsafetychecker-Citation a mcro:CitationInformationSection .

mcro:Stablediffusionsafetychecker-EnvironmentalImpact a mcro:ConsiderationInformationSection .

mcro:Stablediffusionsafetychecker-Evaluation a mcro:QuantativeAnalysisSection .

mcro:Stablediffusionsafetychecker-License a mcro:LicenseInformationSection .

mcro:Stablediffusionsafetychecker-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:Stablediffusionsafetychecker-License .

mcro:Stablediffusionsafetychecker-TechnicalSpecification a mcro:ModelParameterSection .

mcro:Stablediffusionsafetychecker-TrainingDetail a mcro:ModelParameterSection .

mcro:Stablediffusionsafetychecker-UseCase a mcro:UseCaseInformationSection .

mcro:Stablediffusionv14ModelCard-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@InProceedings{Rombach_2022_CVPR,
 author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\"orn},
 title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 month     = {June},
 year      = {2022},
 pages     = {10684-10695}
    }
""" .

mcro:Stablediffusionv14ModelCard-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "LAION-2B (en) and subsets thereof" .

mcro:Stablediffusionv14ModelCard-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The CreativeML OpenRAIL M license" .

mcro:Stablediffusionv14ModelCard-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-image generation model" .

mcro:Stablediffusionv14ModelCard-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:Stablediffusionv14ModelCard-Citation ;
    mcro:hasLicense mcro:Stablediffusionv14ModelCard-License ;
    mcro:hasModelArchitecture mcro:Stablediffusionv14ModelCard-ModelArchitecture .

mcro:Stablediffusionv14ModelCard-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model is intended for research purposes only." .

mcro:StanfordAIMIstanforddeidentifierbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{10.1093/jamia/ocac219,
    author = {Chambon, Pierre J and Wu, Christopher and Steinkamp, Jackson M and Adleberg, Jason and Cook, Tessa S and Langlotz, Curtis P},
    title = "{Automated deidentification of radiology reports combining transformer and hide in plain sight rule-based methods}",
    journal = {Journal of the American Medical Informatics Association},
    year = {2022},
    month = {11},
    abstract = "{To develop an automated deidentification pipeline for radiology reports that detect protected health information (PHI) entities and replaces them with realistic surrogates hiding in plain sight.In this retrospective study, 999 chest X-ray and CT reports collected between November 2019 and November 2020 were annotated for PHI at the token level and combined with 3001 X-rays and 2193 medical notes previously labeled, forming a large multi-institutional and cross-domain dataset of 6193 documents. Two radiology test sets, from a known and a new institution, as well as i2b2 2006 and 2014 test sets, served as an evaluation set to estimate model performance and to compare it with previously released deidentification tools. Several PHI detection models were developed based on different training datasets, fine-tuning approaches and data augmentation techniques, and a synthetic PHI generation algorithm. These models were compared using metrics such as precision, recall and F1 score, as well as paired samples Wilcoxon tests.Our best PHI detection model achieves 97.9 F1 score on radiology reports from a known institution, 99.6 from a new institution, 99.5 on i2b2 2006, and 98.9 on i2b2 2014. On reports from a known institution, it achieves 99.1 recall of detecting the core of each PHI span.Our model outperforms all deidentifiers it was compared to on all test sets as well as human labelers on i2b2 2014 data. It enables accurate and automatic deidentification of radiology reports.A transformer-based deidentification pipeline can achieve state-of-the-art performance for deidentifying radiology reports and other medical documents.}",
    issn = {1527-974X},
    doi = {10.1093/jamia/ocac219},
    url = {https://doi.org/10.1093/jamia/ocac219},
    note = {ocac219},
    eprint = {https://academic.oup.com/jamia/advance-article-pdf/doi/10.1093/jamia/ocac219/47220191/ocac219.pdf},
}""" .

mcro:StanfordAIMIstanforddeidentifierbase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production." .

mcro:StellaEn400Mv5-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{zhang2025jasperstelladistillationsota,
      title={Jasper and Stella: distillation of SOTA embedding models}, 
      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},
      year={2025},
      eprint={2412.19048},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2412.19048}, 
}""" .

mcro:StellaEn400Mv5-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The models are trained based on `Alibaba-NLP/gte-large-en-v1.5` and `Alibaba-NLP/gte-Qwen2-1.5B-instruct`." .

mcro:StellaEn400Mv5-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "retrieve task and semantic textual similarity task" .

mcro:Supabasegtesmall-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens." .

mcro:Supabasegtesmall-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT framework" .

mcro:Supabasegtesmall-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """General Text Embeddings (GTE) model.

The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including [GTE-large](https://huggingface.co/thenlper/gte-large), [GTE-base](https://huggingface.co/thenlper/gte-base), and [GTE-small](https://huggingface.co/thenlper/gte-small). The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including **information retrieval**, **semantic textual similarity**, **text reranking**, etc.""" .

mcro:Supabasegtesmall-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "information retrieval",
        "semantic textual similarity",
        "text reranking" .

mcro:Systranfasterwhisperbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CTranslate2" .

mcro:Systranfasterwhispersmall-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "openai/whisper-small" .

mcro:T5basesummarizationclaimextractor-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{scire-etal-2024-fenice,
    title = "{FENICE}: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction",
    author = "Scir{\\`e}, Alessandro and Ghonim, Karim and Navigli, Roberto",
    editor = "Ku, Lun-Wei  and Martins, Andre and Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.841",
    pages = "14148--14161",
}""" .

mcro:T5basesummarizationclaimextractor-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "N/A" .

mcro:T5basesummarizationclaimextractor-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "T5" .

mcro:T5basesummarizationclaimextractor-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:T5basesummarizationclaimextractor-License .

mcro:T5basesummarizationclaimextractor-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Extraction of atomic claims from summaries.
Serve as a component in pipelines for factuality evaluation of summaries.""" .

mcro:TRELLISImageLarge-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Structured 3D Latents for Scalable and Versatile 3D Generation" .

mcro:TRELLISImageLarge-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "TRELLIS",
        "large 3D genetive model" .

mcro:TahaDouajidetrdoctabledetection-Citation a mcro:CitationInformationSection .

mcro:TahaDouajidetrdoctabledetection-Consideration a mcro:ConsiderationInformationSection .

mcro:TahaDouajidetrdoctabledetection-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "More information needed" .

mcro:TahaDouajidetrdoctabledetection-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:TahaDouajidetrdoctabledetection-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:TahaDouajidetrdoctabledetection-Citation ;
    mcro:hasLicense mcro:TahaDouajidetrdoctabledetection-License ;
    mcro:hasModelArchitecture mcro:TahaDouajidetrdoctabledetection-ModelArchitecture .

mcro:TahaDouajidetrdoctabledetection-OutOfScopeUseCase a mcro:OutOfScopeUseCaseSectionInformation .

mcro:TahaDouajidetrdoctabledetection-PrimaryIntendedUseCase a mcro:PrimaryIntendedUseCaseInformationSection .

mcro:TahaDouajidetrdoctabledetection-TrainingData a mcro:TrainingDataInformationSection .

mcro:TahaDouajidetrdoctabledetection-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasOutOfScopeUseCase mcro:TahaDouajidetrdoctabledetection-OutOfScopeUseCase ;
    mcro:hasPrimaryIntendedUseCase mcro:TahaDouajidetrdoctabledetection-PrimaryIntendedUseCase .

mcro:TaiyiCLIPRoberta102MChinese-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CLIP (Roberta)" .

mcro:TaiyiCLIPRoberta102MChinese-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{fengshenbang,
  author    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},
  title     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},
  journal   = {CoRR},
  volume    = {abs/2209.02970},
  year      = {2022}
}""" .

mcro:TaiyiCLIPRoberta102MChinese-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{Fengshenbang-LM,
  title={Fengshenbang-LM},
  author={IDEA-CCNL},
  year={2021},
  howpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},
}""" .

mcro:TaiyiCLIPRoberta102MChinese-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Noah-Wukong",
        "Zero" .

mcro:TaiyiCLIPRoberta102MChinese-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:TaiyiCLIPRoberta102MChinese-Architecture ;
    mcro:hasCitation mcro:TaiyiCLIPRoberta102MChinese-Citation,
        mcro:TaiyiCLIPRoberta102MChinese-Citation2 .

mcro:TaiyiCLIPRoberta102MChinese-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue " Multimodal" .

mcro:TheBlokeMistral7BInstructv01GGUF-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """This instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:
- Grouped-Query Attention
- Sliding-Window Attention
- Byte-fallback BPE tokenizer""" .

mcro:TheBlokephi2GGUF-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Microsoft's Phi 2" .

mcro:TheBlokephi2GGUF-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4." .

mcro:TheBlokephi2GGUF-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The model is licensed under the [microsoft-research-license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE)." .

mcro:TheBlokephi2GGUF-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "a Transformer-based model with next-word prediction objective" .

mcro:TheBlokephi2GGUF-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Phi-2 is intended for research purposes only. Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format." .

mcro:TinyLlama11BChatv10-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "UltraChat",
        "UltraFeedback" .

mcro:TinyLlama11BChatv10-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 2" .

mcro:TinyLlama11BChatv10-ModelDetail a mcro:ModelDetailSection .

mcro:TinyLlama11BChatv10-UseCase a mcro:UseCaseInformationSection .

mcro:UFNLPgatortronbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien C, Compas C, Martin C, Costa AB, Flores MG, Zhang Y, Magoc T, Harle CA, Lipori G, Mitchell DA, Hogan WR, Shenkman EA, Bian J, Wu Y. A large language model for electronic health records. Npj Digit Med. Nature Publishing Group; . 2022 Dec 26;5(1):19. https://www.nature.com/articles/s41746-022-00742-2",
        "Yang X, Lyu T, Li Q, Lee C-Y, Bian J, Hogan WR, Wu Y. A study of deep learning methods for de-identification of clinical notes in cross-institute settings. BMC Med Inform Decis Mak. 2020 Dec 5;19(5):232. https://www.ncbi.nlm.nih.gov/pubmed/31801524." .

mcro:UFNLPgatortronbase-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "0.5B words of de-identified clinical notes from MIMIC-III",
        "2.5B words from WikiText",
        "6.1B words from PubMed CC0",
        "82B words of de-identified clinical notes from the University of Florida Health System" .

mcro:UFNLPgatortronbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT architecure" .

mcro:USERbgem3-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "sentence-transformer" .

mcro:USERbgem3-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepvk2024user,
    title={USER: Universal Sentence Encoder for Russian},
    author={Malashenko, Boris and  Zemerov, Anton and Spirin, Egor},
    url={https://huggingface.co/datasets/deepvk/USER-base},
    publisher={Hugging Face}
    year={2024},
}""" .

mcro:USERbgem3-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "We did not thoroughly evaluate the model's ability for sparse and multi-vec encoding." .

mcro:USERbgem3-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:USERbgem3-Architecture ;
    mcro:hasCitation mcro:USERbgem3-Citation .

mcro:USERbgem3-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "AllNLI",
        "DSumRu",
        "Fialka-v1",
        "Gazeta",
        "Gsm8k-ru",
        "Lenta",
        "MIRACL",
        "MedNLI",
        "Mlsum",
        "Mr-TyDi",
        "Panorama",
        "PravoIsrael",
        "RCB",
        "RussianKeywords",
        "SummDialogNews",
        "Tapaco",
        "Terra",
        "Xlsum",
        "deepvk/ru-HNP",
        "deepvk/ru-WANLI" .

mcro:USERbgem3-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:ViTSO400M14SigLIP384-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Contrastive Image-Text, Zero-Shot Image Classification" .

mcro:ViTSO400M14SigLIP384-Citation1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  journal={arXiv preprint arXiv:2303.15343},
  year={2023}
}""" .

mcro:ViTSO400M14SigLIP384-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{big_vision,
  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},
  title = {Big Vision},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\\url{https://github.com/google-research/big_vision}}
}""" .

mcro:ViTSO400M14SigLIP384-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "WebLI" .

mcro:ViTSO400M14SigLIP384-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:ViTSO400M14SigLIP384-Citation1,
        mcro:ViTSO400M14SigLIP384-Citation2 ;
    mcro:hasDataset mcro:ViTSO400M14SigLIP384-Dataset .

mcro:ViTSO400M14SigLIP384-UseCase a mcro:UseCaseInformationSection .

mcro:VisionTasksUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "captioning, object detection, and segmentation" .

mcro:VitTinyVitSmall-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "vit-tiny and vit-small" .

mcro:WDViTLargeTaggerV3-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """Last image id: 7220105
Trained on Danbooru images with IDs modulo 0000-0899.
Validated on images with IDs modulo 0950-0999.
Images with less than 10 general tags were filtered out.
Tags with less than 600 images were filtered out.""" .

mcro:WDViTLargeTaggerV3-PerformanceMetric a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "`v1.0: P=R: threshold = 0.2606, F1 = 0.4674`" .

mcro:WhereIsAIUAELargeV1-Citation a mcro:CitationInformationSection .

mcro:WhereIsAIUAELargeV1-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT" .

mcro:WhereIsAIUAELargeV1-UseCase a mcro:UseCaseInformationSection .

mcro:WhisperbaseenmodelforCTranslate2-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:Whisperlargev2modelforCTranslate2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "openai/whisper-large-v2" .

mcro:Whisperlargev2modelforCTranslate2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper." .

mcro:Whisperlargev3model-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Whisper large-v3" .

mcro:Whisperlargev3model-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:Whisperlargev3model-ModelArchitecture .

mcro:Whispertiny-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasUseCase mcro:Whispertiny-UseCase .

mcro:Whispertiny-UseCase a mcro:UseCaseInformationSection .

mcro:XTTS-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "[Coqui Public Model License](https://coqui.ai/cpml)" .

mcro:XTTS-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip." .

mcro:XenovaallMiniLML6v2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:XenovaallMiniLML6v2-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:Xenovabgebaseenv15-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:Xenovabgebaseenv15-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:Xlmrobertalargenerspanish-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "CoNLL-2002" .

mcro:Xlmrobertalargenerspanish-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "XLM-Roberta-large" .

mcro:Xlmrobertalargenerspanish-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasDataset mcro:Xlmrobertalargenerspanish-Dataset ;
    mcro:hasModelArchitecture mcro:Xlmrobertalargenerspanish-ModelArchitecture .

mcro:Xlmrobertalargenerspanish-Performance a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "F1-score of 89.17" .

mcro:XuhuiToxDectrobertalarge-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{zhou-etal-2020-debiasing,
  title = {Challenges in Automated Debiasing for Toxic Language Detection},
  author = {Zhou, Xuhui and Sap, Maarten and Swayamdipta, Swabha and Choi, Yejin and Smith, Noah A.},
  booktitle = {EACL},
  abbr = {EACL},
  html = {https://www.aclweb.org/anthology/2021.eacl-main.274.pdf},
  code = {https://github.com/XuhuiZhou/Toxic_Debias},
  year = {2021},
  bibtex_show = {true},
  selected = {true}
}""" .

mcro:XuhuiToxDectrobertalarge-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "tweets" .

mcro:XuhuiToxDectrobertalarge-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "toxic language detection" .

mcro:XuhuiToxDectrobertalarge-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Roberta-large" .

mcro:YOLOWorldMirror-Documentation a obo:IAO_0000314 ;
    prov1:hasTextValue "https://docs.ultralytics.com/models/yolo-world/#available-models-supported-tasks-and-operating-modes" .

mcro:YOLOWorldMirror-ModelDetail a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "model weights for ultralytics yolo models" .

mcro:YOLOv8DetectionModel-ConsiderationInformationSection a mcro:ConsiderationInformationSection .

mcro:YOLOv8DetectionModel-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:YOLOv8DetectionModel-PerformanceMetricInformationSection a mcro:PerformanceMetricInformationSection .

mcro:YOLOv8DetectionModel-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:Yehorw2vxlsruk-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc {
	smoliakov_2025,
	author       = { {Smoliakov} },
	title        = { w2v-xls-r-uk (Revision 55b6dc0) },
	year         = 2025,
	url          = { https://huggingface.co/Yehor/w2v-xls-r-uk },
	doi          = { 10.57967/hf/4556 },
	publisher    = { Hugging Face }
}""" .

mcro:ai4bharatindictrans2indicen1B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{gala2023indictrans,
title={IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},
author={Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=vfT4YuzAYA},
note={}
}""" .

mcro:ai4bharatindictrans2indicen1B-UseCase a mcro:UseCaseInformationSection .

mcro:aiforeversbertlargenluru-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT large model (uncased) for Sentence Embeddings in Russian language." .

mcro:aiforeversbertlargenluru-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "compute sentence embeddings" .

mcro:alakxendermmsttsdivfinetunedmdm01-IntendedUseCase a mcro:UseCaseInformationSection .

mcro:alakxendermmsttsdivfinetunedmdm01-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MMS-TTS (VITS)" .

mcro:alakxendermmsttsdivfinetunedmdm01-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasIntendedUseCase mcro:alakxendermmsttsdivfinetunedmdm01-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:alakxendermmsttsdivfinetunedmdm01-ModelArchitecture .

mcro:albertbasev2-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ALBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion." .

mcro:albertbasev2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-1909-11942,
  author    = {Zhenzhong Lan and
               Mingda Chen and
               Sebastian Goodman and
               Kevin Gimpel and
               Piyush Sharma and
               Radu Soricut},
  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
               Representations},
  journal   = {CoRR},
  volume    = {abs/1909.11942},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.11942},
  archivePrefix = {arXiv},
  eprint    = {1909.11942},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:albertbasev2-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task." .

mcro:albertbasev2-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions" .

mcro:albertbasev2-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """The ALBERT model was pretrained on BookCorpus, a dataset consisting of 11,038
unpublished books and English Wikipedia (excluding lists, tables and
headers).""" .

mcro:all-distilroberta-v1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """We used the pretrained [`distilroberta-base`](https://huggingface.co/distilroberta-base) model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.""" .

mcro:all-distilroberta-v1-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.


| Dataset                                                  | Paper                                    | Number of training tuples  |
|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|
| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |
| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |
| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |
| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |
| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |
| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |
| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |
| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |
| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |
| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|
| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |
| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |
| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |
| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |
| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |
| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | 
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |
| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |
| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |
| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |
| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |
| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |
| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |
| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |
| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |
| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |
| **Total** | | **1,124,818,467** |""" .

mcro:all-distilroberta-v1-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.

By default, input text longer than 128 word pieces is truncated.""" .

mcro:allMiniLML12v2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:allMiniLML12v2-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.""" .

mcro:allMiniLML12v2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.

By default, input text longer than 256 word pieces is truncated.""" .

mcro:allMiniLML6v2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:allMiniLML6v2-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.""" .

mcro:allMiniLML6v2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Our model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.""" .

mcro:allmpnetbasev2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.""" .

mcro:allmpnetbasev2-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.""" .

mcro:allmpnetbasev2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.""" .

mcro:allrobertalargev1-BackgroundInformationSection a obo:IAO_0000314 ;
    prov1:hasTextValue """The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised 
contrastive learning objective. We used the pretrained [`roberta-large`](https://huggingface.co/roberta-large) model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.

We developped this model during the 
[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), 
organized by Hugging Face. We developped this model as part of the project:
[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.""" .

mcro:allrobertalargev1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """We used the pretrained [`roberta-large`](https://huggingface.co/roberta-large) model and fine-tuned in on a 
1B sentence pairs dataset.""" .

mcro:allrobertalargev1-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.""" .

mcro:allrobertalargev1-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.""" .

mcro:answerdotaiModernBERTbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{modernbert,
      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, 
      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavi and Orion Weller and Oskar Hallstrm and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
      year={2024},
      eprint={2412.13663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13663}, 
}""" .

mcro:answerdotaiModernBERTbase-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "We release the ModernBERT model architectures, model weights, training codebase under the Apache 2.0 license." .

mcro:answerdotaiModernBERTbase-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "ModernBERTs training data is primarily English and code, so performance may be lower for other languages. While it can handle long sequences efficiently, using the full 8,192 tokens window may be slower than short-context inference. Like any large language model, ModernBERT may produce representations that reflect biases present in its training data. Verify critical or sensitive outputs before relying on them." .

mcro:answerdotaiModernBERTbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """- Architecture: Encoder-only, Pre-Norm Transformer with GeGLU activations.
- Sequence Length: Pre-trained up to 1,024 tokens, then extended to 8,192 tokens.
- Data: 2 trillion tokens of English text and code.
- Optimizer: StableAdamW with trapezoidal LR scheduling and 1-sqrt decay.
- Hardware: Trained on 8x H100 GPUs.

See the paper for more details.""" .

mcro:answerdotaiModernBERTbase-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:answerdotaiModernBERTbase-Citation ;
    mcro:hasLicense mcro:answerdotaiModernBERTbase-License .

mcro:answerdotaiModernBERTbase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "ModernBERTs native long context length makes it ideal for tasks that require processing long documents, such as retrieval, classification, and semantic search within large corpora. The model was trained on a large corpus of text and code, making it suitable for a wide range of downstream tasks, including code retrieval and hybrid (text + code) semantic search." .

mcro:appleOpenELM11BInstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{mehtaOpenELMEfficientLanguage2024,
	title = {{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}},
	shorttitle = {{OpenELM}},
	url = {https://arxiv.org/abs/2404.14619v1},
	language = {en},
	urldate = {2024-04-24},
	journal = {arXiv.org},
	author = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},
	month = apr,
	year = {2024},
}

@inproceedings{mehta2022cvnets, 
     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, 
     title = {CVNets: High Performance Library for Computer Vision}, 
     year = {2022}, 
     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, 
     series = {MM '22} 
}""" .

mcro:appleOpenELM11BInstruct-Dataset a mcro:DatasetInformationSection .

mcro:appleOpenELM11BInstruct-License a mcro:LicenseInformationSection .

mcro:appleOpenELM11BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:appleOpenELM11BInstruct-UseCase a mcro:UseCaseInformationSection .

mcro:arabertv1andv2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{antoun2020arabert,
  title={AraBERT: Transformer-based Model for Arabic Language Understanding},
  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  booktitle={LREC 2020 Workshop Language Resources and Evaluation Conference 11--16 May 2020},
  pages={9}
}""" .

mcro:arabertv1andv2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """OSCAR unshuffled and filtered.
- [Arabic Wikipedia dump](https://archive.org/details/arwiki-20190201) from 2020/09/01
- [The 1.5B words Arabic Corpus](https://www.semanticscholar.org/paper/1.5-billion-words-Arabic-Corpus-El-Khair/f3eeef4afb81223df96575adadf808fe7fe440b4)
- [The OSIAN Corpus](https://www.aclweb.org/anthology/W19-4619)
- Assafir news articles.""" .

mcro:arabertv1andv2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "based on Google's BERT architechture" .

mcro:arabertv1andv2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Arabic Language Understanding" .

mcro:atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "A tiny version of https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" .

mcro:audeeringwav2vec2largerobust12ftemotionmspdim-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "doi:10.5281/zenodo.6221127" .

mcro:audeeringwav2vec2largerobust12ftemotionmspdim-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "MSP-Podcast" .

mcro:audeeringwav2vec2largerobust12ftemotionmspdim-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "for research purpose only" .

mcro:audeeringwav2vec2largerobust12ftemotionmspdim-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Wav2Vec2-Large-Robust" .

mcro:audeeringwav2vec2largerobust12ftemotionmspdim-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "dimensional speech emotion recognition" .

mcro:autogluonchronosboltbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{ansari2024chronos,
    title={Chronos: Learning the Language of Time Series},
    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=gerNCVqqtR}
}""" .

mcro:autogluonchronosboltbase-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This project is licensed under the Apache-2.0 License." .

mcro:autogluonchronosboltbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683)" .

mcro:autogluonchronosboltbase-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:autogluonchronosboltbase-Citation ;
    mcro:hasLicense mcro:autogluonchronosboltbase-License .

mcro:autogluonchronosboltbase-ModelParameter a mcro:ModelParameterSection .

mcro:autogluonchronosboltbase-QuantativeAnalysis a mcro:QuantativeAnalysisSection .

mcro:autogluonchronosboltbase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting." .

mcro:avsolatorioGISTsmallEmbeddingv0-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{solatorio2024gistembed,
    title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},
    author={Aivin V. Solatorio},
    journal={arXiv preprint arXiv:2402.16829},
    year={2024},
    URL={https://arxiv.org/abs/2402.16829}
    eprint={2402.16829},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}""" .

mcro:avsolatorioGISTsmallEmbeddingv0-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """The dataset used is a compilation of the MEDI and MTEB Classification training datasets. Third-party datasets may be subject to additional terms and conditions under their associated licenses. A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:

- Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets)
- Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb

The dataset contains a `task_type` key, which can be used to select only the mteb classification tasks (prefixed with `mteb_`).

The **MEDI Dataset** is published in the following paper: [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741).

The MTEB Benchmark results of the GIST embedding model, compared with the base model, suggest that the fine-tuning dataset has perturbed the model considerably, which resulted in significant improvements in certain tasks while adversely degrading performance in some.

The retrieval performance for the TRECCOVID task is of note. The fine-tuning dataset does not contain significant knowledge about COVID-19, which could have caused the observed performance degradation. We found some evidence, detailed in the paper, that thematic coverage of the fine-tuning data can affect downstream performance.""" .

mcro:avsolatorioGISTsmallEmbeddingv0-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """The model is fine-tuned on top of the [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) using the [MEDI dataset](https://github.com/xlang-ai/instructor-embedding.git) augmented with mined triplets from the [MTEB Classification](https://huggingface.co/mteb) training dataset (excluding data from the Amazon Polarity Classification task).

The model does not require any instruction for generating embeddings. This means that queries for retrieval tasks can be directly encoded without crafting instructions.

Technical paper: [GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning](https://arxiv.org/abs/2402.16829)""" .

mcro:banglat5_banglaparaphrase-CitationInformationSection a mcro:CitationInformationSection .

mcro:banglat5_banglaparaphrase-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:banglat5_banglaparaphrase-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:banglat5_banglaparaphrase-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:bartbase-CitationInformationSection a mcro:CitationInformationSection .

mcro:bartbase-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:bartbase-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:bartbase-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:bartbase-CitationInformationSection ;
    mcro:hasLicense mcro:bartbase-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:bartbase-ModelArchitectureInformationSection .

mcro:bartbase-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:bartlargemnli-Citation a mcro:CitationInformationSection .

mcro:bartlargemnli-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:bartlargemnli-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:bartlargemnli-Citation ;
    mcro:hasModelArchitecture mcro:bartlargemnli-ModelArchitecture .

mcro:bartlargemnli-UseCase a mcro:UseCaseInformationSection .

mcro:bartlargesizedmodelfinetunedoncnndailymail-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-1910-13461,
  author    = {Mike Lewis and
               Yinhan Liu and
               Naman Goyal and
               Marjan Ghazvininejad and
               Abdelrahman Mohamed and
               Omer Levy and
               Veselin Stoyanov and
               Luke Zettlemoyer},
  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
               Generation, Translation, and Comprehension},
  journal   = {CoRR},
  volume    = {abs/1910.13461},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.13461},
  eprinttype = {arXiv},
  eprint    = {1910.13461},
  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:bartlargesizedmodelfinetunedoncnndailymail-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use this model for text summarization." .

mcro:bartlargesizedmodelfinetunedoncnndailymail-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text." .

mcro:bert-base-uncased-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CNN" .

mcro:bert-base-uncased-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "citation text" .

mcro:bert-base-uncased-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet" .

mcro:bert-base-uncased-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "mit" .

mcro:bert-base-uncased-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Intended use case" .

mcro:bertMiniatures-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "standard BERT recipe (including model architecture and training objective)" .

mcro:bertMiniatures-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{turc2019,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962v2 },
  year={2019}
}""" .

mcro:bertMiniatures-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher." .

mcro:bertbaseNER-Citation a mcro:CitationInformationSection .

mcro:bertbaseNER-EvalResults a mcro:QuantativeAnalysisSection .

mcro:bertbaseNER-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Named Entity Recognition" .

mcro:bertbaseNER-Limitation a mcro:LimitationInformationSection .

mcro:bertbaseNER-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT model" .

mcro:bertbaseNER-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:bertbaseNER-Citation ;
    mcro:hasIntendedUseCase mcro:bertbaseNER-IntendedUseCase ;
    mcro:hasLimitation mcro:bertbaseNER-Limitation ;
    mcro:hasModelArchitecture mcro:bertbaseNER-ModelArchitecture ;
    mcro:hasTrainingData mcro:bertbaseNER-TrainingData .

mcro:bertbaseNER-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "CoNLL-2003 Named Entity Recognition" .

mcro:bertbasechinese-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "[BERT](https://arxiv.org/abs/1810.04805)" .

mcro:bertbasechinese-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue """**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).""" .

mcro:bertbasechinese-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "[More Information needed]" .

mcro:bertbasechinese-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper).

- **Developed by:** HuggingFace team
- **Model Type:** Fill-Mask
- **Language(s):** Chinese
- **License:** [More Information needed]
- **Parent Model:** See the [BERT base uncased model](https://huggingface.co/bert-base-uncased) for more information about the BERT base model.""" .

mcro:bertbasechinese-ModelParameter a mcro:ModelParameterSection ;
    prov1:hasTextValue """* **type_vocab_size:** 2
* **vocab_size:** 21128
* **num_hidden_layers:** 12""" .

mcro:bertbasechinese-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "[More Information Needed]" .

mcro:bertbasechinese-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "[More Information Needed]" .

mcro:bertbasechinese-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model can be used for masked language modeling" .

mcro:bertbasemodelcased-Architecture a mcro:ModelArchitectureInformationSection .

mcro:bertbasemodelcased-Citation a mcro:CitationInformationSection .

mcro:bertbasemodelcased-Consideration a mcro:ConsiderationInformationSection .

mcro:bertbasemodelcased-EvaluationData a mcro:EvaluationDataInformationSection .

mcro:bertbasemodelcased-License a mcro:LicenseInformationSection .

mcro:bertbasemodelcased-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:bertbasemodelcased-Architecture ;
    mcro:hasCitation mcro:bertbasemodelcased-Citation ;
    mcro:hasLicense mcro:bertbasemodelcased-License .

mcro:bertbasemodelcased-TrainingData a mcro:TrainingDataInformationSection .

mcro:bertbasemodelcased-UseCase a mcro:UseCaseInformationSection .

mcro:bertbasemultilingualuncasedsentiment-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Here is the number of product reviews we used for finetuning the model:" .

mcro:bertbasemultilingualuncasedsentiment-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasDataset mcro:bertbasemultilingualuncasedsentiment-Dataset ;
    mcro:hasQuantativeAnalysis mcro:bertbasemultilingualuncasedsentiment-QuantativeAnalysis ;
    mcro:hasUseCase mcro:bertbasemultilingualuncasedsentiment-UseCase .

mcro:bertbasemultilingualuncasedsentiment-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "The fine-tuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:" .

mcro:bertbasemultilingualuncasedsentiment-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above or for further finetuning on related sentiment analysis tasks." .

mcro:bertbaseuncased-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:bertbaseuncased-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue """Even if the training data used for this model could be characterized as fairly neutral, this model can have biased
predictions:""" .

mcro:bertbaseuncased-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion." .

mcro:bertbaseuncased-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:bertbaseuncased-Citation ;
    mcro:hasModelArchitecture mcro:bertbaseuncased-ModelArchitecture .

mcro:bertbaseuncased-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038
unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and
headers).""" .

mcro:bertbaseuncased-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task.""" .

mcro:bertlargemodeluncased-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion." .

mcro:bertlargemodeluncased-Citation a mcro:CitationInformationSection .

mcro:bertlargemodeluncased-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:bertlargemodeluncased-Citation ;
    mcro:hasModelArchitecture mcro:bertlargemodeluncased-Arch .

mcro:bertlargemodeluncased-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038
unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and
headers).""" .

mcro:bertlargemodeluncased-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task." .

mcro:bertmultilingualbasemodelcased-Citation a mcro:CitationInformationSection .

mcro:bertmultilingualbasemodelcased-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:bertmultilingualbasemodelcased-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:bertmultilingualbasemodelcased-Citation .

mcro:bertmultilingualbasemodelcased-TrainingData a mcro:TrainingDataInformationSection .

mcro:bertmultilingualbasemodelcased-UseCase a mcro:UseCaseInformationSection .

mcro:bertmultilingualbasemodeluncased-Citation a mcro:CitationInformationSection .

mcro:bertmultilingualbasemodeluncased-Dataset a mcro:DatasetInformationSection .

mcro:bertmultilingualbasemodeluncased-License a mcro:LicenseInformationSection .

mcro:bertmultilingualbasemodeluncased-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:bertmultilingualbasemodeluncased-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:bertmultilingualbasemodeluncased-Citation ;
    mcro:hasLicense mcro:bertmultilingualbasemodeluncased-License .

mcro:bertmultilingualbasemodeluncased-TrainingData a mcro:TrainingDataInformationSection .

mcro:bertmultilingualbasemodeluncased-UseCase a mcro:UseCaseInformationSection .

mcro:bertsmalluncased-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{turc2019,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962v2 },
  year={2019}
}""" .

mcro:bertsmalluncased-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Mini BERT models from https://arxiv.org/abs/1908.08962 that the HF team didn't convert. The original [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py) is used.

See the original Google repo: [google-research/bert](https://github.com/google-research/bert)

Note: it's not clear if these checkpoints have undergone knowledge distillation.""" .

mcro:bertsmalluncased-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "See other BERT model cards e.g. https://huggingface.co/bert-base-uncased" .

mcro:bertwithflashattention-ModelParameterSection a mcro:ModelParameterSection ;
    prov1:hasTextValue """- `use_flash_attn`: If `True`, always use flash attention. If `None`, use flash attention when GPU is available. If `False`, never use flash attention (works on CPU).
- `window_size`: Size (left and right) of the local attention window. If `(-1, -1)`, use global attention
- `dense_seq_output`: If true, we only need to pass the hidden states for the masked out token (around 15%) to the classifier heads. I set this to true for pretraining.
- `fused_mlp`: Whether to use fused-dense. Useful to reduce VRAM in combination with activation checkpointing
- `mlp_checkpoint_lvl`: One of `{0, 1, 2}`. Increasing this increases the amount of activation checkpointing within the MLP. Keep this at 0 for pretraining and use gradient accumulation instead. For embedding training, increase this as much as needed.
- `last_layer_subset`: If true, we only need the compute the last layer for a subset of tokens. I left this to false.
- `use_qk_norm`: Whether or not to use QK-normalization
- `num_loras`: Number of LoRAs to use when initializing a `BertLoRA` model. Has no effect on other models.""" .

mcro:bge-m3-onnx-o4-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """- [x] Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.
- [x] Multi-Linguality: It can support more than **100** working languages.
- [x] Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to **8192** tokens.""" .

mcro:bigsciencebloomz560m-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}""" .

mcro:bigsciencebloomz560m-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Same as [bloom-560m](https://huggingface.co/bigscience/bloom-560m), also refer to the `config.json` file" .

mcro:bigsciencebloomz560m-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "We recommend using the model to perform tasks expressed in natural language. For example, given the prompt \"*Translate to English: Je taime.*\", the model will most likely answer \"*I love you.*\"." .

mcro:bigvganv222khz80band256x-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon" .

mcro:bigvganv222khz80band256x-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Large-scale Compilation" .

mcro:bigvganv222khz80band256x-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BigVGAN" .

mcro:bigvganv222khz80band256x-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "universal neural vocoder" .

mcro:bigvganv244khz128band512x-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon" .

mcro:bigvganv244khz128band512x-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Large-scale Compilation" .

mcro:bigvganv244khz128band512x-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BigVGAN" .

mcro:bigvganv244khz128band512x-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Universal Neural Vocoder" .

mcro:blackforestlabsFLUX1Filldev-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Generated outputs can be used for personal, scientific, and commercial purposes as described in the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)." .

mcro:blackforestlabsFLUX1Filldev-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.
- There may be slight-color shifts in areas that are not filled in
- Filling in complex textures may produce lines at the edges of the filled-area.""" .

mcro:blackforestlabsFLUX1Filldev-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """`FLUX.1 Fill [dev]` is a 12 billion parameter rectified flow transformer capable of filling areas in existing images based on a text description.
For more information, please read our [blog post](https://blackforestlabs.ai/flux-1-tools/).""" .

mcro:blackforestlabsFLUX1Filldev-OutofScopeUseCase a mcro:OutofScopeUseCaseSectionInformation .

mcro:blackforestlabsFLUX1Filldev-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The model and its derivatives may not be used

- In any way that violates any applicable national, federal, state, local or international law or regulation.
- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.
- To generate or disseminate verifiably false information and/or content with the purpose of harming others.
- To generate or disseminate personal identifiable information that can be used to harm an individual.
- To harass, abuse, threat""" .

mcro:blackforestlabsFLUX1dev-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions." .

mcro:blackforestlabsFLUX1dev-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This model falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)." .

mcro:blackforestlabsFLUX1dev-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate output that matches the prompts.
- Prompt following is heavily influenced by the prompting-style.""" .

mcro:blackforestlabsFLUX1dev-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The model and its derivatives may not be used

- In any way that violates any applicable national, federal, state, local or international law or regulation.
- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.
- To generate or disseminate verifiably false information and/or content with the purpose of harming others.
- To generate or disseminate personal identifiable information that can be used to harm an individual.
- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.
- To create non-consensual nudity or illegal pornographic content.
- For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.
- Generating or facilitating large-scale disinformation campaigns.""" .

mcro:blackforestlabsFLUX1schnell-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "apache-2.0" .

mcro:blackforestlabsFLUX1schnell-LimitationInformationSection a mcro:LimitationInformationSection .

mcro:blackforestlabsFLUX1schnell-OutOfScopeUse a mcro:OutofScopeUseCaseSectionInformation .

mcro:blackforestlabsFLUX1schnell-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:byt5small-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "ByT5: Towards a token-free future with pre-trained byte-to-byte models" .

mcro:byt5small-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "mC4" .

mcro:byt5small-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "the architecture of MT5" .

mcro:byt5small-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:byt5small-Citation ;
    mcro:hasModelArchitecture mcro:byt5small-ModelArchitecture .

mcro:byt5small-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "ByT5 works especially well on noisy text data" .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Architecture a mcro:ModelArchitectureInformationSection .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Citation a mcro:CitationInformationSection .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "UMLS" .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Citation ;
    mcro:hasModelArchitecture mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Architecture .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-UseCase a mcro:UseCaseInformationSection .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{liu-etal-2021-self,
    title = "Self-Alignment Pretraining for Biomedical Entity Representations",
    author = "Liu, Fangyu  and
      Shareghi, Ehsan  and
      Meng, Zaiqiao  and
      Basaldella, Marco  and
      Collier, Nigel",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.334",
    pages = "4228--4238",
    abstract = "Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.",
}
""" .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "UMLS" .

mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SapBERT by [Liu et al. (2020)](https://arxiv.org/pdf/2010.11784.pdf). Trained with [UMLS](https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html) 2020AA (English only), using [microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) as the base model. Please use the mean-pooling of the output as the representation." .

mcro:canary1bflash-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """[1] [Training and Inference Efficiency of Encoder-Decoder Speech Models](https://arxiv.org/abs/2503.05931)

[2] [Less is More: Accurate Speech Recognition & Translation without Web-Scale Data](https://www.isca-archive.org/interspeech_2024/puvvada24_interspeech.pdf)

[3] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389701)

[4] [Attention is All You Need](https://arxiv.org/abs/1706.03762)

[5] [Unified Model for Code-Switching Speech Recognition and Language Identification Based on Concatenated Tokenizer](https://aclanthology.org/2023.calcs-1.7.pdf)

[6] [Google Sentencepiece Tokenizer](https://github.com/google/sentencepiece)

[7] [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo)

[8] [EMMeTT: Efficient Multimodal Machine Translation Training](https://arxiv.org/abs/2409.13523)

[9] [Towards Measuring Fairness in AI: the Casual Conversations Dataset](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9634168)""" .

mcro:canary1bflash-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """The canary-1b-flash model is trained on a total of 85K hrs of speech data. It consists of 31K hrs of public data, 20K hrs collected by [Suno](https://suno.ai/), and 34K hrs of in-house data. 
The datasets below include conversations, videos from the web and audiobook recordings.

**Data Collection Method:**
* Human <br>

**Labeling Method:**
* Hybrid: Human, Automated <br>

The constituents of public data are as follows. 

#### English (25.5k hours)
- Librispeech 960 hours
- Fisher Corpus
- Switchboard-1 Dataset
- WSJ-0 and WSJ-1
- National Speech Corpus (Part 1, Part 6)
- VCTK
- VoxPopuli (EN)
- Europarl-ASR (EN)
- Multilingual Librispeech (MLS EN) - 2,000 hour subset
- Mozilla Common Voice (v7.0)
- People's Speech - 12,000 hour subset
- Mozilla Common Voice (v11.0)  - 1,474 hour subset

#### German (2.5k hours)
- Mozilla Common Voice (v12.0)  - 800 hour subset
- Multilingual Librispeech (MLS DE) - 1,500 hour subset
- VoxPopuli (DE) - 200 hr subset

#### Spanish (1.4k hours)
- Mozilla Common Voice (v12.0)  - 395 hour subset
- Multilingual Librispeech (MLS ES) - 780 hour subset
- VoxPopuli (ES) - 108 hour subset
- Fisher  - 141 hour subset

#### French (1.8k hours)
- Mozilla Common Voice (v12.0)  - 708 hour subset
- Multilingual Librispeech (MLS FR) - 926 hour subset
- VoxPopuli (FR) - 165 hour subset""" .

mcro:canary1bflash-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "canary-1b-flash is released under the CC-BY-4.0 license. By using this model, you are agreeing to the terms and conditions of the license." .

mcro:canary1bflash-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Canary is an encoder-decoder model with FastConformer [3] Encoder and Transformer Decoder [4]. With audio features extracted from the encoder, task tokens such as <target language>, <task>, <toggle timestamps> and <toggle PnC> are fed into the Transformer Decoder to trigger the text generation process. Canary uses a concatenated tokenizer [5] from individual SentencePiece [6] tokenizers of each language, which makes it easy to scale up to more languages. The canary-1b-flash model has 32 encoder layers and 4 decoder layers, leading to a total of 883M parameters. For more details about the architecture, please refer to [1]." .

mcro:canary1bflash-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "None" .

mcro:chronos-t5-base-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{ansari2024chronos,
    title={Chronos: Learning the Language of Time Series},
    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=gerNCVqqtR}
}""" .

mcro:chronos-t5-base-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This project is licensed under the Apache-2.0 License." .

mcro:chronos-t5-base-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters." .

mcro:chronos-t5-base-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Chronos is a family of **pretrained time series forecasting models** based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes." .

mcro:chronos-t5-small-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{ansari2024chronos,
    title={Chronos: Learning the Language of Time Series},
    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=gerNCVqqtR}
}""" .

mcro:chronos-t5-small-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This project is licensed under the Apache-2.0 License." .

mcro:chronos-t5-small-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters." .

mcro:chronos-t5-small-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Chronos is a family of **pretrained time series forecasting models** based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes." .

mcro:chronos-t5-tiny-ArchitectureSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters." .

mcro:chronos-t5-tiny-CitationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """If you find Chronos models useful for your research, please consider citing the associated [paper](https://arxiv.org/abs/2403.07815):

@article{ansari2024chronos,
    title={Chronos: Learning the Language of Time Series},
    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=gerNCVqqtR}
}""" .

mcro:chronos-t5-tiny-LicenseSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This project is licensed under the Apache-2.0 License." .

mcro:chronos-t5-tiny-UseCaseSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """To perform inference with Chronos models, install the package in the GitHub [companion repo](https://github.com/amazon-science/chronos-forecasting) by running:

pip install git+https://github.com/amazon-science/chronos-forecasting.git

A minimal example showing how to perform inference using Chronos models:

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from chronos import ChronosPipeline

pipeline = ChronosPipeline.from_pretrained(
  "amazon/chronos-t5-tiny",
  device_map="cuda",
  torch_dtype=torch.bfloat16,
)

df = pd.read_csv("https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv")

# context must be either a 1D tensor, a list of 1D tensors,
# or a left-padded 2D tensor with batch as the first dimension
context = torch.tensor(df["#Passengers"])
prediction_length = 12
forecast = pipeline.predict(context, prediction_length)  # shape [num_series, num_samples, prediction_length]

# visualize the forecast
forecast_index = range(len(df), len(df) + prediction_length)
low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)

plt.figure(figsize=(8, 4))
plt.plot(df["#Passengers"], color="royalblue", label="historical data")
plt.plot(forecast_index, median, color="tomato", label="median forecast")
plt.fill_between(forecast_index, low, high, color="tomato", alpha=0.3, label="80% prediction interval")
plt.legend()
plt.grid()
plt.show()""" .

mcro:chronosboltbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{ansari2024chronos,
    title={Chronos: Learning the Language of Time Series},
    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=gerNCVqqtR}
}""" .

mcro:chronosboltbase-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache-2.0 License" .

mcro:chronosboltbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "T5 encoder-decoder architecture" .

mcro:chronosboltbase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "time series forecasting" .

mcro:chronosboltmini-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{ansari2024chronos,
    title={Chronos: Learning the Language of Time Series},
    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=gerNCVqqtR}
}""" .

mcro:chronosboltmini-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future stepsa method known as direct multi-step forecasting." .

mcro:chronosboltmini-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This project is licensed under the Apache-2.0 License." .

mcro:chronosboltmini-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations." .

mcro:chronosboltmini-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting." .

mcro:chronosboltsmall-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{ansari2024chronos,
    title={Chronos: Learning the Language of Time Series},
    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=gerNCVqqtR}
}""" .

mcro:chronosboltsmall-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "nearly 100 billion time series observations" .

mcro:chronosboltsmall-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache-2.0 License" .

mcro:chronosboltsmall-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "T5 encoder-decoder architecture" .

mcro:chronosboltsmall-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "zero-shot forecasting" .

mcro:chronosbolttiny-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{ansari2024chronos,
    title={Chronos: Learning the Language of Time Series},
    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=gerNCVqqtR}
}""" .

mcro:chronosbolttiny-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "It chunks the historical time series context into patches of multiple observations" .

mcro:chronosbolttiny-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache-2.0 License",
        "This project is licensed under the Apache-2.0 License." .

mcro:chronosbolttiny-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations.",
        "T5 encoder-decoder architecture" .

mcro:chronosbolttiny-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting.",
        "zero-shot forecasting" .

mcro:clip-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """- [Blog Post](https://openai.com/blog/clip/)
- [CLIP Paper](https://arxiv.org/abs/2103.00020)""" .

mcro:clip-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasEthicalConsideration mcro:clip-EthicalConsideration ;
    mcro:hasLimitation mcro:clip-Limitation .

mcro:clip-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users." .

mcro:clip-EthicalConsideration a mcro:EthicalConsiderationSection ;
    prov1:hasTextValue """We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).

We also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with Middle Eastern having the highest accuracy (98.4%) and White having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.""" .

mcro:clip-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance." .

mcro:clip-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.

The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.""" .

mcro:clip-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:clip-Citation ;
    mcro:hasModelArchitecture mcro:clip-ModelArchitecture ;
    prov1:hasTextValue "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context theyre being deployed within." .

mcro:clip-OutOfScopeUseCase a mcro:OutOfScopeUseCaseSectionInformation ;
    prov1:hasTextValue """**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIPs performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.

Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.

Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.""" .

mcro:clip-PrimaryIntendedUseCase a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue """The primary intended users of these models are AI researchers.

We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.""" .

mcro:clip-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue """We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:

- Food101
- CIFAR10   
- CIFAR100   
- Birdsnap
- SUN397
- Stanford Cars
- FGVC Aircraft
- VOC2007
- DTD
- Oxford-IIIT Pet dataset
- Caltech101
- Flowers102
- MNIST   
- SVHN 
- IIIT5K   
- Hateful Memes   
- SST-2
- UCF101
- Kinetics700
- Country211
- CLEVR Counting
- KITTI Distance
- STL-10
- RareAct
- Flickr30
- MSCOCO
- ImageNet
- ImageNet-A
- ImageNet-R
- ImageNet Sketch
- ObjectNet (ImageNet Overlap)
- Youtube-BB
- ImageNet-Vid""" .

mcro:clip-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasOutOfScopeUseCase mcro:clip-OutOfScopeUseCase ;
    mcro:hasPrimaryIntendedUseCase mcro:clip-PrimaryIntendedUseCase ;
    prov1:hasTextValue "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis." .

mcro:clip-vit-large-patch14-336-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "More information needed" .

mcro:clip-vit-large-patch14-336-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "More information needed" .

mcro:clip-vit-large-patch14-336-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "More information needed" .

mcro:clipvitlargepatch14-Citation a mcro:CitationInformationSection .

mcro:clipvitlargepatch14-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasEthicalConsideration mcro:clipvitlargepatch14-EthicalConsideration ;
    mcro:hasLimitation mcro:clipvitlargepatch14-Limitation .

mcro:clipvitlargepatch14-Dataset a mcro:DatasetInformationSection ;
    mcro:hasEvaluationDataInformation mcro:clipvitlargepatch14-EvaluationDataInformation ;
    mcro:hasSensitiveDataInformation mcro:clipvitlargepatch14-SensitiveDataInformation ;
    mcro:hasTrainingDataInformation mcro:clipvitlargepatch14-TrainingDataInformation .

mcro:clipvitlargepatch14-EthicalConsideration a mcro:EthicalConsiderationSection ;
    prov1:hasTextValue "bias and fairness" .

mcro:clipvitlargepatch14-EvaluationDataInformation a mcro:EvaluationDataInformationSection .

mcro:clipvitlargepatch14-License a mcro:LicenseInformationSection .

mcro:clipvitlargepatch14-Limitation a mcro:LimitationInformationSection .

mcro:clipvitlargepatch14-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ViT-L/14 Transformer" .

mcro:clipvitlargepatch14-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:clipvitlargepatch14-Citation ;
    mcro:hasLicense mcro:clipvitlargepatch14-License ;
    mcro:hasModelArchitecture mcro:clipvitlargepatch14-ModelArchitecture .

mcro:clipvitlargepatch14-OutOfScopeUseCase a mcro:OutOfScopeUseCaseSectionInformation ;
    prov1:hasTextValue "Any deployed use case of the model - whether commercial or not - is currently out of scope." .

mcro:clipvitlargepatch14-PerformanceMetric a mcro:PerformanceMetricInformationSection .

mcro:clipvitlargepatch14-PrimaryIntendedUseCase a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue "research" .

mcro:clipvitlargepatch14-QuantitativeAnalysis a mcro:QuantativeAnalysisSection ;
    mcro:hasPerformanceMetric mcro:clipvitlargepatch14-PerformanceMetric .

mcro:clipvitlargepatch14-SensitiveDataInformation a mcro:SensitiveDataInformationSection .

mcro:clipvitlargepatch14-TrainingDataInformation a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "publicly available image-caption data" .

mcro:clipvitlargepatch14-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasOutOfScopeUseCase mcro:clipvitlargepatch14-OutOfScopeUseCase ;
    mcro:hasPrimaryIntendedUseCase mcro:clipvitlargepatch14-PrimaryIntendedUseCase .

mcro:codebertbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{feng2020codebert,
    title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
    author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
    year={2020},
    eprint={2002.08155},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}""" .

mcro:codebertbase-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The model is trained on bi-modal data (documents & code) of [CodeSearchNet](https://github.com/github/CodeSearchNet)" .

mcro:codebertbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model is initialized with Roberta-base and trained with MLM+RTD objective (cf. the paper)." .

mcro:codebertbase-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "Please see [the official repository](https://github.com/microsoft/CodeBERT) for scripts that support \"code search\" and \"code-to-document generation\"." .

mcro:codebertbase-Reference2 a mcro:ReferenceInformationSection ;
    prov1:hasTextValue """1. [CodeBERT trained with Masked LM objective](https://huggingface.co/microsoft/codebert-base-mlm) (suitable for code completion)
2.  [Hugging Face's CodeBERTa](https://huggingface.co/huggingface/CodeBERTa-small-v1) (small size, 6 layers)""" .

mcro:cognitivecomputationsdolphin291yi1534b-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Yi-1.5-34B" .

mcro:cognitivecomputationsdolphin291yi1534b-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "apache 2.0 license" .

mcro:cognitivecomputationsdolphin291yi1534b-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue """/workspace/datasets/dolphin-2.9/dolphin201-sharegpt2.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/dolphin-coder-translate-sharegpt2.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/dolphin-coder-codegen-sharegpt2.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/m-a-p_Code-Feedback-sharegpt-unfiltered.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/m-a-p_CodeFeedback-Filtered-Instruction-sharegpt-unfiltered.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/not_samantha_norefusals.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/Orca-Math-resort-unfiltered.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/agent_instruct_react_unfiltered.jsonl
    type: sharegpt  
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/toolbench_instruct_j1s1_3k_unfiltered.jsonl
    type: sharegpt  
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/toolbench_negative_unfiltered.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/toolbench_react_10p_unfiltered.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/toolbench_tflan_cot_30p_unfiltered.jsonl
    type: sharegpt
    conversation: chatml
  - path: /workspace/datasets/dolphin-2.9/openhermes200k_unfiltered.jsonl
    type: sharegpt 
    conversation: chatml""" .

mcro:cognitivecomputationsdolphin291yi1534b-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling" .

mcro:cointegratedruberttiny2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "a small Russian BERT-based encoder with high-quality sentence embeddings" .

mcro:cointegratedruberttiny2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task." .

mcro:colbertv2-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """* [**ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT**](https://arxiv.org/abs/2004.12832) (SIGIR'20).
* [**Relevance-guided Supervision for OpenQA with ColBERT**](https://arxiv.org/abs/2007.00814) (TACL'21).
* [**Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval**](https://arxiv.org/abs/2101.00436) (NeurIPS'21).
* [**ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction**](https://arxiv.org/abs/2112.01488) (NAACL'22).
* [**PLAID: An Efficient Engine for Late Interaction Retrieval**](https://arxiv.org/abs/2205.09707) (CIKM'22).""" .

mcro:colbertv2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ColBERT is a _fast_ and _accurate_ retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds." .

mcro:colbertv2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Using ColBERT on a dataset typically involves the following steps." .

mcro:conjunctsditre15-Citation a mcro:CitationInformationSection .

mcro:conjunctsditre15-Consideration a mcro:ConsiderationInformationSection .

mcro:conjunctsditre15-EvaluationData a mcro:EvaluationDataInformationSection .

mcro:conjunctsditre15-ModelDetail a mcro:ModelDetailSection .

mcro:conjunctsditre15-TechnicalSpecification a obo:IAO_0000314 .

mcro:conjunctsditre15-TrainingData a mcro:TrainingDataInformationSection .

mcro:conjunctsditre15-UseCase a mcro:UseCaseInformationSection .

mcro:convnextv2_nano.fcmae_ft_in22kin1k-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{Woo2023ConvNeXtV2,
  title={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},
  author={Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon and Saining Xie},
  year={2023},
  journal={arXiv preprint arXiv:2301.00808},
}""" .

mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}
}""" .

mcro:convnextv2_nano.fcmae_ft_in22kin1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:convnextv2_nano.fcmae_ft_in22kin1k-Architecture ;
    mcro:hasCitation mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation1,
        mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation2 ;
    mcro:hasDataset mcro:convnextv2_nano.fcmae_ft_in22kin1k-Dataset .

mcro:convnextv2_nano.fcmae_ft_in22kin1k-UseCase a mcro:UseCaseInformationSection .

mcro:crossencodermsmarcoMiniLML12v2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task." .

mcro:crossencodermsmarcoMiniLML12v2-Performance a mcro:PerformanceMetricInformationSection .

mcro:crossencodermsmarcoMiniLML12v2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)" .

mcro:crossencodermsmarcoMiniLML4v2-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task." .

mcro:crossencodermsmarcoMiniLML4v2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)" .

mcro:crossencodermsmarcoMiniLML6v2-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Cross-Encoder for MS Marco" .

mcro:crossencodermsmarcoMiniLML6v2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "This model was trained on the MS Marco Passage Ranking task." .

mcro:crossencodermsmarcoMiniLML6v2-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset." .

mcro:crossencodermsmarcoMiniLML6v2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order." .

mcro:crossencodermsmarcoTinyBERTL2v2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "MS Marco Passage Ranking" .

mcro:crossencodermsmarcoTinyBERTL2v2-ModelDetail a mcro:ModelDetailSection .

mcro:crossencodermsmarcoTinyBERTL2v2-ModelParameter a mcro:ModelParameterSection .

mcro:crossencodermsmarcoTinyBERTL2v2-QuantativeAnalysis a mcro:QuantativeAnalysisSection .

mcro:crossencodermsmarcoTinyBERTL2v2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Information Retrieval" .

mcro:crossencodernlidebertav3base-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The model was trained on the [SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral." .

mcro:crossencodernlidebertav3base-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model was trained using [SentenceTransformers](https://sbert.net) [Cross-Encoder](https://www.sbert.net/examples/applications/cross-encoder/README.html) class. This model is based on [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base)" .

mcro:crossencodernlidebertav3base-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasDataset mcro:crossencodernlidebertav3base-Dataset ;
    mcro:hasModelArchitecture mcro:crossencodernlidebertav3base-ModelArchitecture .

mcro:crossencodernlidebertav3base-PerformanceMetric1 a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "- Accuracy on SNLI-test dataset: 92.38" .

mcro:crossencodernlidebertav3base-PerformanceMetric2 a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "- Accuracy on  MNLI mismatched set: 90.04" .

mcro:crossencodernlidebertav3base-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    mcro:hasPerformanceMetric mcro:crossencodernlidebertav3base-PerformanceMetric1,
        mcro:crossencodernlidebertav3base-PerformanceMetric2 .

mcro:crossencodernlidebertav3base-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model can also be used for zero-shot-classification:" .

mcro:czechwav2vec2xlsr300mcs250-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "common_voice 8.0" .

mcro:czechwav2vec2xlsr300mcs250-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "wav2vec2-xls-r-300m" .

mcro:czechwav2vec2xlsr300mcs250-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasDataset mcro:czechwav2vec2xlsr300mcs250-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:czechwav2vec2xlsr300mcs250-ModelArchitectureInformationSection .

mcro:deberta-v3-base-prompt-injection-v2-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "deberta-v3-base" .

mcro:deberta-v3-base-prompt-injection-v2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deberta-v3-base-prompt-injection-v2,
  author = {ProtectAI.com},
  title = {Fine-Tuned DeBERTa-v3-base for Prompt Injection Detection},
  year = {2024},
  publisher = {HuggingFace},
  url = {https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2},
}""" .

mcro:deberta-v3-base-prompt-injection-v2-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache License 2.0" .

mcro:deberta-v3-base-prompt-injection-v2-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """`deberta-v3-base-prompt-injection-v2` is highly accurate in identifying prompt injections in English.
It does not detect jailbreak attacks or handle non-English prompts, which may limit its applicability in diverse linguistic environments or against advanced adversarial techniques.

Additionally, we do not recommend using this scanner for system prompts, as it produces false-positives.""" .

mcro:deberta-v3-base-prompt-injection-v2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:deberta-v3-base-prompt-injection-v2-Architecture ;
    mcro:hasCitation mcro:deberta-v3-base-prompt-injection-v2-Citation ;
    mcro:hasLicense mcro:deberta-v3-base-prompt-injection-v2-License ;
    prov1:hasTextValue "This model is a fine-tuned version of [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) specifically developed to detect and classify prompt injection attacks which can manipulate language models into producing unintended outputs." .

mcro:deberta-v3-base-prompt-injection-v2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model classifies inputs into benign (`0`) and injection-detected (`1`)." .

mcro:debertaV3Small-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}""",
        """@misc{he2021debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:debertaV3Small-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "160GB data" .

mcro:debertaV3Small-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "44M backbone parameters",
        "6 layers",
        "98M parameters in the Embedding layer",
        "hidden size of 768",
        "vocabulary containing 128K tokens" .

mcro:debertav3basetasksourcenli-Arch a mcro:ModelArchitectureInformationSection .

mcro:debertav3basetasksourcenli-Citation a mcro:CitationInformationSection .

mcro:debertav3basetasksourcenli-License a mcro:LicenseInformationSection .

mcro:debertav3basetasksourcenli-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:debertav3basetasksourcenli-License .

mcro:debertav3basetasksourcenli-Owner a mcro:OwnerInformationSection ;
    prov1:hasTextValue "damien.sileo@inria.fr" .

mcro:debertav3basetasksourcenli-UseCase a mcro:UseCaseInformationSection .

mcro:deepseekaiDeepSeekR1-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}""" .

mcro:deepseekaiDeepSeekR1-CitationInformationSection a mcro:CitationInformationSection .

mcro:deepseekaiDeepSeekR1-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:deepseekaiDeepSeekR1-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:deepseekaiDeepSeekR1-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:deepseekaiDeepSeekR1-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "DeepSeek-V3",
        "MoE" .

mcro:deepseekaiDeepSeekR1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MoE" .

mcro:deepseekaiDeepSeekR1-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:deepseekaiDeepSeekR10528-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}""" .

mcro:deepseekaiDeepSeekR10528-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:deepseekaiDeepSeekR10528-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528." .

mcro:deepseekaiDeepSeekR10528-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic." .

mcro:deepseekaiDeepSeekR1DistillLlama8B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}""" .

mcro:deepseekaiDeepSeekR1DistillLlama8B-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Qwen2.5 and Llama3" .

mcro:deepseekaiDeepSeekR1DistillLlama8B-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:deepseekaiDeepSeekR1DistillLlama8B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "DeepSeek-R1" .

mcro:deepseekaiDeepSeekR1DistillLlama8B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Reasoning" .

mcro:deepseekaiDeepSeekR1DistillQwen14B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}""" .

mcro:deepseekaiDeepSeekR1DistillQwen14B-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Qwen2.5 series" .

mcro:deepseekaiDeepSeekR1DistillQwen14B-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT" .

mcro:deepseekaiDeepSeekR1DistillQwen14B-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MoE" .

mcro:deepseekaiDeepSeekR1DistillQwen14B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "reasoning" .

mcro:deepseekaiDeepSeekR1DistillQwen7B-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1." .

mcro:deepseekaiDeepSeekR1DistillQwen7B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}""" .

mcro:deepseekaiDeepSeekR1DistillQwen7B-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:deepseekaiDeepSeekR1DistillQwen7B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"" .

mcro:deepseekaiDeepSeekV3-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Innovative Load Balancing Strategy and Training Objective

- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. 
    It can also be used for speculative decoding for inference acceleration.""" .

mcro:deepseekaiDeepSeekV3-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}""" .

mcro:deepseekaiDeepSeekV3-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """Pre-Training: Towards Ultimate Training Efficiency

- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  
- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  
  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  
- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""" .

mcro:deepseekaiDeepSeekV3-Dataset2 a mcro:DatasetInformationSection ;
    prov1:hasTextValue """Post-Training: Knowledge Distillation from DeepSeek-R1

-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.""" .

mcro:deepseekaiDeepSeekV3-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use." .

mcro:deepseekaiDeepSeekV3-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:deepseekaiDeepSeekV3-Architecture ;
    mcro:hasCitation mcro:deepseekaiDeepSeekV3-Citation ;
    mcro:hasLicense mcro:deepseekaiDeepSeekV3-License .

mcro:deepseekaiDeepSeekV30324-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3." .

mcro:deepseekaiDeepSeekV30324-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}""" .

mcro:deepseekaiDeepSeekV30324-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This repository and the model weights are licensed under the [MIT License](LICENSE)." .

mcro:deepseekaiDeepSeekV30324-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model supports features such as function calling, JSON output, and FIM completion." .

mcro:depthanythingDepthAnythingV2Smallhf-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{yang2024depth,
      title={Depth Anything V2}, 
      author={Lihe Yang and Bingyi Kang and Zilong Huang and Zhen Zhao and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},
      year={2024},
      eprint={2406.09414},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}""" .

mcro:depthanythingDepthAnythingV2Smallhf-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Depth Anything V2 leverages the [DPT](https://huggingface.co/docs/transformers/model_doc/dpt) architecture with a [DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2) backbone." .

mcro:depthanythingDepthAnythingV2Smallhf-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for tasks like zero-shot depth estimation. See the [model hub](https://huggingface.co/models?search=depth-anything) to look for
other versions on a task that interests you.""" .

mcro:detoxify-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{Detoxify,
  title={Detoxify},
  author={Hanu, Laura and {Unitary team}},
  howpublished={Github. https://github.com/unitaryai/detoxify},
  year={2020}
}""" .

mcro:detoxify-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Toxic comment classification" .

mcro:detoxify-DatasetInformationSection_Multilingual a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Multilingual toxic comment classification" .

mcro:detoxify-DatasetInformationSection_Unintended_Bias a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Unintended Bias in Toxic comments" .

mcro:detoxify-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "mit" .

mcro:detoxify-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformers" .

mcro:detoxify-ModelArchitectureInformationSection_Pytorch_Lightning a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Pytorch Lightning" .

mcro:detoxify-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "research purposes" .

mcro:detoxify-UseCaseInformationSection_content_moderators a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "aid content moderators in flagging out harmful content quicker" .

mcro:detoxify-UseCaseInformationSection_fine_tuning a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "fine-tuning" .

mcro:detrresnet50-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2005-12872,
  author    = {Nicolas Carion and
               Francisco Massa and
               Gabriel Synnaeve and
               Nicolas Usunier and
               Alexander Kirillov and
               Sergey Zagoruyko},
  title     = {End-to-End Object Detection with Transformers},
  journal   = {CoRR},
  volume    = {abs/2005.12872},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.12872},
  archivePrefix = {arXiv},
  eprint    = {2005.12872},
  timestamp = {Thu, 28 May 2020 17:38:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:detrresnet50-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100." .

mcro:detrresnet50-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:detrresnet50-Citation ;
    mcro:hasModelArchitecture mcro:detrresnet50-ModelArchitecture .

mcro:detrresnet50-PerformanceMetric a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "This model achieves an AP (average precision) of **42.0** on COCO 2017 validation." .

mcro:detrresnet50-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    mcro:hasPerformanceMetric mcro:detrresnet50-PerformanceMetric .

mcro:detrresnet50-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The DETR model was trained on [COCO 2017 object detection](https://cocodataset.org/#download), a dataset consisting of 118k/5k annotated images for training/validation respectively." .

mcro:detrresnet50-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for object detection. See the [model hub](https://huggingface.co/models?search=facebook/detr) to look for all available DETR models." .

mcro:dima806fairfaceageimagedetection-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Detects age group with about 59% accuracy based on an image." .

mcro:distilbartcnn126-ModelParameterSection a mcro:ModelParameterSection ;
    prov1:hasTextValue "This checkpoint should be loaded into `BartForConditionalGeneration.from_pretrained`. See the [BART docs](https://huggingface.co/transformers/model_doc/bart.html?#transformers.BartForConditionalGeneration) for more information." .

mcro:distilbartcnn126-QuantitativeAnalysisSection a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue """| Model Name                 |   MM Params |   Inference Time (MS) |   Speedup |   Rouge 2 |   Rouge-L |
|:---------------------------|------------:|----------------------:|----------:|----------:|----------:|
| distilbart-xsum-12-1       |         222 |                    90 |      2.54 |     18.31 |     33.37 |
| distilbart-xsum-6-6        |         230 |                   132 |      1.73 |     20.92 |     35.73 |
| distilbart-xsum-12-3       |         255 |                   106 |      2.16 |     21.37 |     36.39 |
| distilbart-xsum-9-6        |         268 |                   136 |      1.68 |     21.72 |     36.61 |
| bart-large-xsum (baseline) |         406 |                   229 |      1    |     21.85 |     36.50 |
| distilbart-xsum-12-6       |         306 |                   137 |      1.68 |     22.12 |     36.99 |
| bart-large-cnn (baseline)  |         406 |                   381 |      1    |     21.06 |     30.63 |
| distilbart-12-3-cnn        |         255 |                   214 |      1.78 |     20.57 |     30.00 |
| distilbart-12-6-cnn        |         306 |                   307 |      1.24 |     21.26 |     30.59 |
| distilbart-6-6-cnn         |         230 |                   182 |      2.09 |     20.17 |     29.70 |""" .

mcro:distilbertbasemodeluncased-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}""" .

mcro:distilbertbasemodeluncased-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for
fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.""" .

mcro:distilbertbasemodeluncased-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a
self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,
with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic
process to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained
with three objectives:

- Distillation loss: the model was trained to return the same probabilities as the BERT base model.
- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a
  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the
  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that
  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future
  tokens. It allows the model to learn a bidirectional representation of the sentence.
- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base
  model.

This way, the model learns the same inner representation of the English language than its teacher model, while being
faster for inference or downstream tasks.""" .

mcro:distilbertbasemodeluncased-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:distilbertbasemodeluncased-ModelArchitecture .

mcro:distilbertbasemodeluncased-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset
consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)
(excluding lists, tables and headers).""" .

mcro:distilbertbasemultilingualcased-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}""" .

mcro:distilbertbasemultilingualcased-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue """Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.

## Recommendations

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.""" .

mcro:distilbertbasemultilingualcased-EnvironmentalImpact a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue """Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** More information needed
- **Hours used:** More information needed
- **Cloud Provider:** More information needed
- **Compute Region:** More information needed
- **Carbon Emitted:** More information needed""" .

mcro:distilbertbasemultilingualcased-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:distilbertbasemultilingualcased-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-based language model" .

mcro:distilbertbasemultilingualcased-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:distilbertbasemultilingualcased-Citation ;
    mcro:hasLicense mcro:distilbertbasemultilingualcased-License ;
    mcro:hasModelArchitecture mcro:distilbertbasemultilingualcased-ModelArchitecture .

mcro:distilbertbasemultilingualcased-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue """The model developers report the following accuracy results for DistilmBERT (see [GitHub Repo](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)): 

> Here are the results on the test sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):

| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |
| :---:                        | :---:   | :---:   | :---:   | :---:  | :---:   | :---:|
| mBERT base cased (computed)  | 82.1    | 74.6    | 69.1    | 72.3   | 66.4    | 58.5 |
| mBERT base uncased (reported)| 81.4    | 74.3    | 63.8    | 70.5   | 62.1    | 58.3 |
| DistilmBERT                  | 78.2    | 69.1    | 64.0    | 66.3   | 59.1    | 54.7 |""" .

mcro:distilbertbasemultilingualcased-TrainingParameter a mcro:ModelParameterSection ;
    prov1:hasTextValue """- The model was pretrained with the supervision of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the concatenation of Wikipedia in 104 different languages
- The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters.
- Further information about the training procedure and data is included in the [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) model card.""" .

mcro:distilbertbasemultilingualcased-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you.

Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.""" .

mcro:distilbertbasemultilingualcasedsentimentsstudent-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "distilbert-base-multilingual-cased" .

mcro:distilbertbasemultilingualcasedsentimentsstudent-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "sentiment analysis" .

mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache-2.0" .

mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Text Classification" .

mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-License ;
    mcro:hasModelArchitecture mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelArchitecture .

mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Stanford Sentiment Treebank" .

mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "topic classification" .

mcro:distilgpt2-Architecture a mcro:ModelArchitectureInformationSection .

mcro:distilgpt2-Citation a mcro:CitationInformationSection .

mcro:distilgpt2-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:distilgpt2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:distilgpt2-Architecture ;
    mcro:hasCitation mcro:distilgpt2-Citation ;
    mcro:hasLicense mcro:distilgpt2-License .

mcro:distilgpt2-TrainingData a mcro:DatasetInformationSection .

mcro:distilgpt2-UseCase a mcro:UseCaseInformationSection .

mcro:distilrobertabase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}""" .

mcro:distilrobertabase-Consideration a mcro:ConsiderationInformationSection .

mcro:distilrobertabase-Dataset a mcro:DatasetInformationSection .

mcro:distilrobertabase-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:distilrobertabase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-based language model" .

mcro:distilrobertabase-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:distilrobertabase-Citation ;
    mcro:hasLicense mcro:distilrobertabase-License .

mcro:distilrobertabase-ModelParameter a mcro:ModelParameterSection .

mcro:distilrobertabase-QuantativeAnalysis a mcro:QuantativeAnalysisSection .

mcro:distilrobertabase-UseCase a mcro:UseCaseInformationSection .

mcro:distilwhisperdistillargev3-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "encoder-decoder architecture" .

mcro:distilwhisperdistillargev3-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{gandhi2023distilwhisper,
      title={Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling}, 
      author={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},
      year={2023},
      eprint={2311.00430},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:distilwhisperdistillargev3-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "22,000 hours of audio data" .

mcro:distilwhisperdistillargev3-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT license" .

mcro:distilwhisperdistillargev3-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "drop-in replacement for Whisper large-v3 on English speech recognition" .

mcro:doclingmodels-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "DocLayNet" .

mcro:doclingmodels-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "RT-DETR" .

mcro:doclingmodels-ModelDetail a mcro:ModelDetailSection .

mcro:doclingmodels-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue """@techreport{Docling,
  author = {Deep Search Team},
  month = {8},
  title = {{Docling Technical Report}},
  url={https://arxiv.org/abs/2408.09869},
  eprint={2408.09869},
  doi = "10.48550/arXiv.2408.09869",
  version = {1.0.0},
  year = {2024}
}

@article{doclaynet2022,
  title = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis},  
  doi = {10.1145/3534678.353904},
  url = {https://arxiv.org/abs/2206.01062},
  author = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J},
  year = {2022}
}

@InProceedings{TableFormer2022,
    author    = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
    title     = {TableFormer: Table Structure Understanding With Transformers},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {4614-4623},
    doi = {https://doi.org/10.1109/CVPR52688.2022.00457}
}
""" .

mcro:doclingmodels-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "identify the structure of the table, starting from an image of a table" .

mcro:e5-base-v2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """[Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/pdf/2212.03533.pdf).
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022""" .

mcro:e5-base-v2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model has 12 layers and the embedding size is 768." .

mcro:e5largev2-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{wang2022text,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}""" .

mcro:e5largev2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model has 24 layers and the embedding size is 1024." .

mcro:e5largev2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.

import torch.nn.functional as F

from torch import Tensor
from transformers import AutoTokenizer, AutoModel


def average_pool(last_hidden_states: Tensor,
                 attention_mask: Tensor) -> Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]


# Each input text should start with "query: " or "passage: ".
# For tasks other than retrieval, you can simply use the "query: " prefix.
input_texts = ['query: how much protein should a female eat',
               'query: summit define',
               "passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.",
               "passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments."]

tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')
model = AutoModel.from_pretrained('intfloat/e5-large-v2')

# Tokenize the input texts
batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')

outputs = model(**batch_dict)
embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])

# normalize embeddings
embeddings = F.normalize(embeddings, p=2, dim=1)
scores = (embeddings[:2] @ embeddings[2:].T) * 100
print(scores.tolist())
""" .

mcro:e5smallv2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{wang2022text,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}""" .

mcro:e5smallv2-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "This model only works for English texts. Long texts will be truncated to at most 512 tokens." .

mcro:e5smallv2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model has 12 layers and the embedding size is 384." .

mcro:e5smallv2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Use \\\"query: \\\" and \\\"passage: \\\" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\\n\\n- Use \\\"query: \\\" prefix for symmetric tasks such as semantic similarity, paraphrase retrieval.\\n\\n- Use \\\"query: \\\" prefix if you want to use embeddings as features, such as linear probing classification, clustering." .

mcro:edgenextsmallusiin1k-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:edgenextsmallusiin1k-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{Maaz2022EdgeNeXt,
  title={EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications},
    author={Muhammad Maaz and Abdelrahman Shaker and Hisham Cholakkal and Salman Khan and Syed Waqas Zamir and Rao Muhammad Anwer and Fahad Shahbaz Khan},
  booktitle={International Workshop on Computational Aspects of Deep Learning at 17th European Conference on Computer Vision (CADL2022)},
  year={2022},
  organization={Springer}
}
""" .

mcro:edgenextsmallusiin1k-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{https://doi.org/10.48550/arxiv.2204.03475,
  doi = {10.48550/ARXIV.2204.03475},  
  url = {https://arxiv.org/abs/2204.03475},  
  author = {Ridnik, Tal and Lawen, Hussam and Ben-Baruch, Emanuel and Noy, Asaf},  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results},  
  publisher = {arXiv},  
  year = {2022},  
}
""" .

mcro:edgenextsmallusiin1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:edgenextsmallusiin1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:edgenextsmallusiin1k-Citation,
        mcro:edgenextsmallusiin1k-Citation2 ;
    mcro:hasDataset mcro:edgenextsmallusiin1k-Dataset ;
    mcro:hasModelArchitecture mcro:edgenextsmallusiin1k-Arch .

mcro:edgenextsmallusiin1k-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Image Classification" .

mcro:efficientnet_b0.ra_in1k-Citation1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}""" .

mcro:efficientnet_b0.ra_in1k-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}
}""" .

mcro:efficientnet_b0.ra_in1k-Citation3 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{wightman2021resnet,
  title={ResNet strikes back: An improved training procedure in timm},
  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},
  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}
}""" .

mcro:efficientnet_b0.ra_in1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:efficientnet_b0.ra_in1k-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:efficientnet_b0.ra_in1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:efficientnet_b0.ra_in1k-Citation1,
        mcro:efficientnet_b0.ra_in1k-Citation2,
        mcro:efficientnet_b0.ra_in1k-Citation3 ;
    mcro:hasDataset mcro:efficientnet_b0.ra_in1k-Dataset ;
    mcro:hasModelArchitecture mcro:efficientnet_b0.ra_in1k-ModelArchitecture .

mcro:efficientnet_b0.ra_in1k-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Feature Map Extraction",
        "Image Classification",
        "Image Embeddings" .

mcro:efficientnet_b3.ra2_in1k-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "EfficientNet" .

mcro:efficientnet_b3.ra2_in1k-Citation1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}""" .

mcro:efficientnet_b3.ra2_in1k-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}
}""" .

mcro:efficientnet_b3.ra2_in1k-Citation3 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{wightman2021resnet,
  title={ResNet strikes back: An improved training procedure in timm},
  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},
  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}
}""" .

mcro:efficientnet_b3.ra2_in1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:efficientnet_b3.ra2_in1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:efficientnet_b3.ra2_in1k-Citation1,
        mcro:efficientnet_b3.ra2_in1k-Citation2,
        mcro:efficientnet_b3.ra2_in1k-Citation3 ;
    mcro:hasDataset mcro:efficientnet_b3.ra2_in1k-Dataset ;
    mcro:hasModelArchitecture mcro:efficientnet_b3.ra2_in1k-Arch .

mcro:efficientnet_b3.ra2_in1k-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:emilyalsentzerBioClinicalBERT-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "[Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) (NAACL Clinical NLP Workshop 2019)" .

mcro:emilyalsentzerBioClinicalBERT-DatasetInfo a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. For more details on MIMIC, see [here](https://mimic.physionet.org/). All notes from the `NOTEEVENTS` table were included (~880M words)." .

mcro:emilyalsentzerBioClinicalBERT-ModelArch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "initialized from [BioBERT](https://arxiv.org/abs/1901.08746) & trained on all MIMIC notes" .

mcro:emilyalsentzerBioClinicalBERT-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model card describes the Bio+Clinical BERT model, which was initialized from [BioBERT](https://arxiv.org/abs/1901.08746) & trained on all MIMIC notes." .

mcro:emimodel-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{podell2023sdxl,
      title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis}, 
      author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas Mller and Joe Penna and Robin Rombach},
      year={2023},
      eprint={2307.01952},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:emimodel-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CreativeML Open RAIL++-M License" .

mcro:emimodel-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Latent Diffusion Model" .

mcro:emimodel-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:emimodel-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:emimodel-ModelArchitectureInformationSection .

mcro:emimodel-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "" .

mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SentenceTransformer" .

mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "NLI",
        "STS-b" .

mcro:emrecanbertbaseturkishcasedmeannlistsbtr-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. The model was trained on Turkish machine translated versions of [NLI](https://huggingface.co/datasets/nli_tr) and [STS-b](https://huggingface.co/datasets/emrecan/stsb-mt-turkish) datasets, using example [training scripts]( https://github.com/UKPLab/sentence-transformers/tree/master/examples/training) from sentence-transformers GitHub repository." .

mcro:emrecanbertbaseturkishcasedmeannlistsbtr-TrainingData a mcro:TrainingDataInformationSection .

mcro:emrecanbertbaseturkishcasedmeannlistsbtr-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "tasks like clustering or semantic search" .

mcro:eng-zho-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "pre-processing: normalization + SentencePiece (spm32k,spm32k)",
        "source group: English ",
        "target group: Chinese " .

mcro:eng-zho-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformer" .

mcro:eng-zho-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "source language(s): eng",
        "target language(s): cjy_Hans cjy_Hant cmn cmn_Hans cmn_Hant gan lzh lzh_Hans nan wuu yue yue_Hans yue_Hant" .

mcro:englishnerinflairfastmodel-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{akbik2018coling,
  title={Contextual String Embeddings for Sequence Labeling},
  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},
  pages     = {1638--1649},
  year      = {2018}
}""" .

mcro:englishnerinflairfastmodel-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Flair embeddings and LSTM-CRF" .

mcro:englishnerinflairfastmodel-PerformanceMetric a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "92,92" .

mcro:englishnerinflairfastmodel-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "English NER" .

mcro:example_model-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "example citation" .

mcro:example_model-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet" .

mcro:example_model-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "mit" .

mcro:example_model-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CNN" .

mcro:example_model-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "example use case" .

mcro:facebookcontriever-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "This model has been trained without supervision following the approach described in [Towards Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118). The associated GitHub repository is available here https://github.com/facebookresearch/contriever." .

mcro:facebookcontriever-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Using the model directly available in HuggingFace transformers requires to add a mean pooling operation to obtain a sentence embedding." .

mcro:facebookdinov2large-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """misc{oquab2023dinov2,
      title={DINOv2: Learning Robust Visual Features without Supervision}, 
      author={Maxime Oquab and Timothe Darcet and Tho Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
      year={2023},
      eprint={2304.07193},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:facebookdinov2large-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "feature extraction" .

mcro:facebookdinov2large-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Vision Transformer (ViT)" .

mcro:facebookdinov2large-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasIntendedUseCase mcro:facebookdinov2large-IntendedUseCase ;
    mcro:hasModelArchitecture mcro:facebookdinov2large-ModelArchitecture .

mcro:facebookencodec24khz-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{dfossez2022high,
      title={High Fidelity Neural Audio Compression}, 
      author={Alexandre Dfossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},
      year={2022},
      eprint={2210.13438},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}""" .

mcro:facebookencodec24khz-DatasetInformationSection a mcro:DatasetInformationSection ;
    mcro:hasTrainingData mcro:facebookencodec24khz-TrainingDataInformationSection ;
    prov1:hasTextValue "AudioSet",
        "Common Voice",
        "DNS Challenge 4",
        "FSD50K",
        "Jamendo dataset" .

mcro:facebookencodec24khz-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:facebookencodec24khz-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Audio Codec" .

mcro:facebookencodec24khz-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:facebookencodec24khz-ModelArchitectureInformationSection ;
    mcro:hasCitation mcro:facebookencodec24khz-CitationInformationSection ;
    mcro:hasLicense mcro:facebookencodec24khz-LicenseInformationSection .

mcro:facebookencodec24khz-TrainingDataInformationSection a mcro:TrainingDataInformationSection .

mcro:facebookencodec24khz-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:facebookesm2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2" .

mcro:facebookesm2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "state-of-the-art protein model trained on a masked language modelling objective" .

mcro:facebookesm2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "suitable for fine-tuning on a wide range of tasks that take protein sequences as input" .

mcro:facebookesm2t363BUR50D-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "For detailed information on the model architecture and training data, please refer to the [accompanying paper](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2)." .

mcro:facebookesm2t363BUR50D-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ESM-2 is a state-of-the-art protein model trained on a masked language modelling objective." .

mcro:facebookesm2t363BUR50D-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It is suitable for fine-tuning on a wide range of tasks that take protein sequences as input." .

mcro:facebookhubertlargels960ft-Citation a mcro:CitationInformationSection .

mcro:facebookhubertlargels960ft-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:facebookhubertlargels960ft-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:facebookhubertlargels960ft-Citation ;
    mcro:hasModelArchitecture mcro:facebookhubertlargels960ft-ModelArchitecture ;
    mcro:hasUseCase mcro:facebookhubertlargels960ft-UseCase .

mcro:facebookhubertlargels960ft-UseCase a mcro:UseCaseInformationSection .

mcro:facebookmask2formerswintinycocoinstance-Architecture a mcro:ModelArchitectureInformationSection .

mcro:facebookmask2formerswintinycocoinstance-Citation a mcro:CitationInformationSection .

mcro:facebookmask2formerswintinycocoinstance-License a mcro:LicenseInformationSection .

mcro:facebookmask2formerswintinycocoinstance-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:facebookmask2formerswintinycocoinstance-Citation ;
    mcro:hasModelArchitecture mcro:facebookmask2formerswintinycocoinstance-Architecture .

mcro:facebookmask2formerswintinycocoinstance-UseCase a mcro:UseCaseInformationSection .

mcro:facebookmmslid256-Citation a mcro:CitationInformationSection .

mcro:facebookmmslid256-Dataset a mcro:DatasetInformationSection .

mcro:facebookmmslid256-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CC-BY-NC 4.0 license" .

mcro:facebookmmslid256-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Wav2Vec2 architecture" .

mcro:facebookmmslid256-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:facebookmmslid256-Citation ;
    mcro:hasLicense mcro:facebookmmslid256-License ;
    mcro:hasModelArchitecture mcro:facebookmmslid256-ModelArchitecture .

mcro:facebookmmslid256-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech language identification (LID)" .

mcro:facebookmusicgenmedium-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{copet2023simple,
      title={Simple and Controllable Music Generation}, 
      author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre Dfossez},
      year={2023},
      eprint={2306.05284},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}""" .

mcro:facebookmusicgenmedium-IntendedUseCase a mcro:UseCaseInformationSection ;
    mcro:hasOutOfScopeUseCase mcro:facebookmusicgenmedium-OutOfScopeUseCase ;
    mcro:hasPrimaryIntendedUseCase mcro:facebookmusicgenmedium-PrimaryIntendedUseCase ;
    mcro:hasPrimaryIntendedUser mcro:facebookmusicgenmedium-PrimaryIntendedUser .

mcro:facebookmusicgenmedium-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Code is released under MIT, model weights are released under CC-BY-NC 4.0." .

mcro:facebookmusicgenmedium-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MusicGen consists of an EnCodec model for audio tokenization, an auto-regressive language model based on the transformer architecture for music modeling. The model comes in different sizes: 300M, 1.5B and 3.3B parameters ; and two variants: a model trained for text-to-music generation task and a model trained for melody-guided music generation." .

mcro:facebookmusicgenmedium-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:facebookmusicgenmedium-Citation ;
    mcro:hasLicense mcro:facebookmusicgenmedium-License ;
    mcro:hasModelArchitecture mcro:facebookmusicgenmedium-ModelArchitecture .

mcro:facebookmusicgenmedium-OutOfScopeUseCase a mcro:OutOfScopeUseCaseSectionInformation ;
    prov1:hasTextValue "The model should not be used on downstream applications without further risk evaluation and mitigation. The model should not be used to intentionally create or disseminate music pieces that create hostile or alienating environments for people. This includes generating music that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes." .

mcro:facebookmusicgenmedium-PrimaryIntendedUseCase a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue """The primary use of MusicGen is research on AI-based music generation, including:

- Research efforts, such as probing and better understanding the limitations of generative models to further improve the state of science
- Generation of music guided by text or melody to understand current abilities of generative AI models by machine learning amateurs""" .

mcro:facebookmusicgenmedium-PrimaryIntendedUser a mcro:PrimaryIntendedUserInformationSection ;
    prov1:hasTextValue "The primary intended users of the model are researchers in audio, machine learning and artificial intelligence, as well as amateur seeking to better understand those models." .

mcro:facebookopt125m-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """OPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.
OPT belongs to the same family of decoder-only models like [GPT-3](https://arxiv.org/abs/2005.14165). As such, it was pretrained using the self-supervised causal language modedling objective.""" .

mcro:facebookopt125m-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:facebookopt125m-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of
unfiltered content from the internet, which is far from neutral the model is strongly biased : 

> Like other large language models for which the diversity (or lack thereof) of training
> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms
> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and
> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern
> large language models. 

This bias will also affect all fine-tuned versions of this model.""" .

mcro:facebookopt125m-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """The Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents: 

  - BookCorpus, which consists of more than 10K unpublished books,
  - CC-Stories, which contains a subset of CommonCrawl data filtered to match the
story-like style of Winograd schemas,
  - The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included. 
  - Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in
Roller et al. (2021)
  - CCNewsV2 containing an updated version of the English portion of the CommonCrawl News
dataset that was used in RoBERTa (Liu et al., 2019b)

The final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally
to each datasets size in the pretraining corpus. 

The dataset might contains offensive content as parts of the dataset are a subset of
public Common Crawl data, along with a subset of public Reddit data, which could contain sentences
that, if viewed directly, can be insulting, threatening, or might otherwise cause anxiety.""" .

mcro:facebookopt125m-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.
In addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling). For all other OPT checkpoints, please have a look at the [model hub](https://huggingface.co/models?filter=opt).""" .

mcro:facebookwav2vec2base960h-Citation a mcro:CitationInformationSection .

mcro:facebookwav2vec2base960h-Dataset a mcro:DatasetInformationSection .

mcro:facebookwav2vec2base960h-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:facebookwav2vec2base960h-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:facebookwav2vec2base960h-Citation ;
    mcro:hasModelArchitecture mcro:facebookwav2vec2base960h-ModelArchitecture .

mcro:facebookwav2vec2base960h-UseCase a mcro:UseCaseInformationSection .

mcro:faceparsing-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-based semantic segmentation image model" .

mcro:faceparsing-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "While the capabilities of computer vision models are impressive, they can also reinforce or exacerbate social biases. The [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ) dataset used for fine-tuning is large but not necessarily perfectly diverse or representative. Also, they are images of.... just celebrities." .

mcro:faceparsing-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "non-commercial research and educational purposes" .

mcro:faceparsing-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:faceparsing-License ;
    mcro:hasModelArchitecture mcro:faceparsing-Architecture .

mcro:faceparsing-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Semantic segmentation model fine-tuned from nvidia/mit-b5 with CelebAMask-HQ for face parsing" .

mcro:fasttextlanguageidentification-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "fastText is a library for efficient learning of word representations and sentence classification." .

mcro:fasttextlanguageidentification-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Please cite [1] if using this code for learning word representations or [2] if using for text classification." .

mcro:fasttextlanguageidentification-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Pre-trained word vectors for 157 languages were trained on [Common Crawl](http://commoncrawl.org/) and [Wikipedia](https://www.wikipedia.org/) using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish." .

mcro:fasttextlanguageidentification-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The language identification model is distributed under the [*Creative Commons Attribution-NonCommercial 4.0 International Public License*](https://creativecommons.org/licenses/by-nc/4.0/)." .

mcro:fasttextlanguageidentification-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:fasttextlanguageidentification-Citation ;
    mcro:hasLicense mcro:fasttextlanguageidentification-License .

mcro:fasttextlanguageidentification-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use pre-trained word vectors for text classification or language identification. See the [tutorials](https://fasttext.cc/docs/en/supervised-tutorial.html) and [resources](https://fasttext.cc/docs/en/english-vectors.html) on its official website to look for tasks that interest you." .

mcro:flairnerenglishlarge-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{schweter2020flert,
    title={FLERT: Document-Level Features for Named Entity Recognition},
    author={Stefan Schweter and Alan Akbik},
    year={2020},
    eprint={2011.06993},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}""" .

mcro:flairnerenglishlarge-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Based on document-level XLM-R embeddings and FLERT" .

mcro:flairnerenglishlarge-PerformanceMetric a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "94,36 (corrected CoNLL-03)" .

mcro:flairnerfrench-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{akbik2018coling,
  title={Contextual String Embeddings for Sequence Labeling},
  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},
  pages     = {1638--1649},
  year      = {2018}
}""" .

mcro:flairnerfrench-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Flair embeddings",
        "LSTM-CRF" .

mcro:flairnerfrench-ModelDetail a mcro:ModelDetailSection .

mcro:flairnerfrench-PerformanceMetric a mcro:PerformanceMetricInformationSection ;
    mcro:0000051 <WikiNER> ;
    prov1:hasTextValue "90,61" .

mcro:flairnerfrench-UseCase a mcro:UseCaseInformationSection .

mcro:flan-t5-base-Citation a mcro:CitationInformationSection .

mcro:flan-t5-base-Consideration a mcro:ConsiderationInformationSection .

mcro:flan-t5-base-EvaluationData a mcro:EvaluationDataInformationSection .

mcro:flan-t5-base-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:flan-t5-base-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:flan-t5-base-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:flan-t5-base-License .

mcro:flan-t5-base-TrainingData a mcro:TrainingDataInformationSection .

mcro:flan-t5-base-UseCase a mcro:UseCaseInformationSection .

mcro:flan-t5-large-Citation a mcro:CitationInformationSection .

mcro:flan-t5-large-Consideration a mcro:ConsiderationInformationSection .

mcro:flan-t5-large-EvaluationData a mcro:EvaluationDataInformationSection .

mcro:flan-t5-large-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:flan-t5-large-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:flan-t5-large-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:flan-t5-large-License .

mcro:flan-t5-large-TrainingData a mcro:TrainingDataInformationSection .

mcro:flan-t5-large-UseCase a mcro:UseCaseInformationSection .

mcro:fnetbasemodel-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2105-03824,
  author    = {James Lee{-}Thorp and
               Joshua Ainslie and
               Ilya Eckstein and
               Santiago Onta{\\~{n}}{\\'{o}}n},
  title     = {FNet: Mixing Tokens with Fourier Transforms},
  journal   = {CoRR},
  volume    = {abs/2105.03824},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.03824},
  archivePrefix = {arXiv},
  eprint    = {2105.03824},
  timestamp = {Fri, 14 May 2021 12:13:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-03824.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:fnetbasemodel-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "FNet is a transformers model with attention replaced with fourier transforms." .

mcro:fnetbasemodel-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:fnetbasemodel-Citation .

mcro:fnetbasemodel-ModelParameter a mcro:ModelParameterSection ;
    prov1:hasTextValue "The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000." .

mcro:fnetbasemodel-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "FNet-base was fine-tuned and evaluated on the validation data of the [GLUE benchamrk](https://huggingface.co/datasets/glue)." .

mcro:fnetbasemodel-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The FNet model was pretrained on [C4](https://huggingface.co/datasets/c4), a cleaned version of the Common Crawl dataset." .

mcro:fnetbasemodel-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task.""" .

mcro:gemma2-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Gemma is a family of lightweight, state-of-the-art open models from Google,
built from the same research and technology used to create the Gemini models.""" .

mcro:gemma2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{gemma_2024,
    title={Gemma},
    url={https://www.kaggle.com/m/3301},
    DOI={10.34740/KAGGLE/M/3301},
    publisher={Kaggle},
    author={Gemma Team},
    year={2024}
}""" .

mcro:gemma2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """These models were trained on a dataset of text data that includes a wide variety
of sources.""" .

mcro:gemma2-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Terms of Use: [Terms][terms]" .

mcro:gemma2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:gemma2-Citation .

mcro:gemma2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Open Large Language Models (LLMs) have a wide range of applications across
various industries and domains.""" .

mcro:gemma2b-Citation a mcro:CitationInformationSection .

mcro:gemma2b-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasLimitation mcro:gemma2b-Limitation ;
    mcro:hasUseCase mcro:gemma2b-UseCase .

mcro:gemma2b-Dataset a mcro:DatasetInformationSection ;
    mcro:hasTrainingData mcro:gemma2b-TrainingData .

mcro:gemma2b-EvaluationData a mcro:EvaluationDataInformationSection .

mcro:gemma2b-License a mcro:LicenseInformationSection .

mcro:gemma2b-Limitation a mcro:LimitationInformationSection .

mcro:gemma2b-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:gemma2b-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:gemma2b-Citation ;
    mcro:hasLicense mcro:gemma2b-License ;
    mcro:hasModelArchitecture mcro:gemma2b-ModelArchitecture .

mcro:gemma2b-ModelParameter a mcro:ModelParameterSection .

mcro:gemma2b-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    mcro:hasEvaluationData mcro:gemma2b-EvaluationData .

mcro:gemma2b-TrainingData a mcro:TrainingDataInformationSection .

mcro:gemma2b-UseCase a mcro:UseCaseInformationSection .

mcro:gemma3-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,
TPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant
computational power. TPUs, designed specifically for matrix operations common in
machine learning, offer several advantages in this domain:

-   Performance: TPUs are specifically designed to handle the massive
    computations involved in training VLMs. They can speed up training
    considerably compared to CPUs.
-   Memory: TPUs often come with large amounts of high-bandwidth memory,
    allowing for the handling of large models and batch sizes during training.
    This can lead to better model quality.
-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable
    solution for handling the growing complexity of large foundation models.
    You can distribute training across multiple TPU devices for faster and more
    efficient processing.
-   Cost-effectiveness: In many scenarios, TPUs can provide a more
    cost-effective solution for training large models compared to CPU-based
    infrastructure, especially when considering the time and resources saved
    due to faster training.
-   These advantages are aligned with
    [Google's commitments to operate sustainably][sustainability].""",
        """Training was done using [JAX][jax] and [ML Pathways][ml-pathways].

JAX allows researchers to take advantage of the latest generation of hardware,
including TPUs, for faster and more efficient training of large models. ML
Pathways is Google's latest effort to build artificially intelligent systems
capable of generalizing across multiple tasks. This is specially suitable for
foundation models, including large language models like these ones.

Together, JAX and ML Pathways are used as described in the
[paper about the Gemini family of models][gemini-2-paper]; *"the 'single
controller' programming model of Jax and Pathways allows a single Python
process to orchestrate the entire training run, dramatically simplifying the
development workflow."*""" .

mcro:gemma3-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,
TPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant
computational power. TPUs, designed specifically for matrix operations common in
machine learning, offer several advantages in this domain:

-   Performance: TPUs are specifically designed to handle the massive
    computations involved in training VLMs. They can speed up training
    considerably compared to CPUs.
-   Memory: TPUs often come with large amounts of high-bandwidth memory,
    allowing for the handling of large models and batch sizes during training.
    This can lead to better model quality.
-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable
    solution for handling the growing complexity of large foundation models.
    You can distribute training across multiple TPU devices for faster and more
    efficient processing.
-   Cost-effectiveness: In many scenarios, TPUs can provide a more
    cost-effective solution for training large models compared to CPU-based
    infrastructure, especially when considering the time and resources saved
    due to faster training.
-   These advantages are aligned with
    [Google's commitments to operate sustainably][sustainability].

### Software

Training was done using [JAX][jax] and [ML Pathways][ml-pathways].

JAX allows researchers to take advantage of the latest generation of hardware,
including TPUs, for faster and more efficient training of large models. ML
Pathways is Google's latest effort to build artificially intelligent systems
capable of generalizing across multiple tasks. This is specially suitable for
foundation models, including large language models like these ones.

Together, JAX and ML Pathways are used as described in the
[paper about the Gemini family of models][gemini-2-paper]; *"the 'single
controller' programming model of Jax and Pathways allows a single Python
process to orchestrate the entire training run, dramatically simplifying the
development workflow."*""" .

mcro:gemma3-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:gemma3-Citation ;
    mcro:hasDataset mcro:gemma3-Dataset ;
    mcro:hasModelArchitecture mcro:gemma3-ModelArchitecture ;
    mcro:hasUseCase mcro:gemma3-UseCase .

mcro:gemma3model-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,
TPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant
computational power. TPUs, designed specifically for matrix operations common in
machine learning, offer several advantages in this domain:

-   Performance: TPUs are specifically designed to handle the massive
    computations involved in training VLMs. They can speed up training
    considerably compared to CPUs.
-   Memory: TPUs often come with large amounts of high-bandwidth memory,
    allowing for the handling of large models and batch sizes during training.
    This can lead to better model quality.
-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable
    solution for handling the growing complexity of large foundation models.
    You can distribute training across multiple TPU devices for faster and more
    efficient processing.
-   Cost-effectiveness: In many scenarios, TPUs can provide a more
    cost-effective solution for training large models compared to CPU-based
    infrastructure, especially when considering the time and resources saved
    due to faster training.
-   These advantages are aligned with
    [Google's commitments to operate sustainably][sustainability].

### Software

Training was done using [JAX][jax] and [ML Pathways][ml-pathways].

JAX allows researchers to take advantage of the latest generation of hardware,
including TPUs, for faster and more efficient training of large models. ML
Pathways is Google's latest effort to build artificially intelligent systems
capable of generalizing across multiple tasks. This is specially suitable for
foundation models, including large language models like these ones.

Together, JAX and ML Pathways are used as described in the
[paper about the Gemini family of models][gemini-2-paper]; *"the 'single
controller' programming model of Jax and Pathways allows a single Python
process to orchestrate the entire training run, dramatically simplifying the
development workflow."*""" .

mcro:gemma3model-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{gemma_2025,
    title={Gemma 3},
    url={https://goo.gle/Gemma3Report},
    publisher={Kaggle},
    author={Gemma Team},
    year={2025}
}""" .

mcro:gemma3model-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """These models were trained on a dataset of text data that includes a wide variety
of sources. The 27B model was trained with 14 trillion tokens, the 12B model was
trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and
1B with 2 trillion tokens. Here are the key components:

-   Web Documents: A diverse collection of web text ensures the model is
    exposed to a broad range of linguistic styles, topics, and vocabulary. The
    training dataset includes content in over 140 languages.
-   Code: Exposing the model to code helps it to learn the syntax and
    patterns of programming languages, which improves its ability to generate
    code and understand code-related questions.
-   Mathematics: Training on mathematical text helps the model learn logical
    reasoning, symbolic representation, and to address mathematical queries.
-   Images: A wide range of images enables the model to perform image
    analysis and visual data extraction tasks.

The combination of these diverse data sources is crucial for training a powerful
multimodal model that can handle a wide variety of different tasks and data
formats.""" .

mcro:gemma3model-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:gemma3model-Architecture ;
    mcro:hasCitation mcro:gemma3model-Citation ;
    mcro:hasDataset mcro:gemma3model-Dataset ;
    mcro:hasIntendedUseCase mcro:gemma3model-UseCase .

mcro:gemma3model-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Content Creation and Communication
    -   Text Generation: These models can be used to generate creative text
        formats such as poems, scripts, code, marketing copy, and email drafts.
    -   Chatbots and Conversational AI: Power conversational interfaces
        for customer service, virtual assistants, or interactive applications.
    -   Text Summarization: Generate concise summaries of a text corpus,
        research papers, or reports.
    -   Image Data Extraction: These models can be used to extract,
        interpret, and summarize visual data for text communications.
-   Research and Education
    -   Natural Language Processing (NLP) and VLM Research: These
        models can serve as a foundation for researchers to experiment with VLM
        and NLP techniques, develop algorithms, and contribute to the
        advancement of the field.
    -   Language Learning Tools: Support interactive language learning
        experiences, aiding in grammar correction or providing writing practice.
    -   Knowledge Exploration: Assist researchers in exploring large
        bodies of text by generating summaries or answering questions about
        specific topics.""" .

mcro:germanbert-Dataset a mcro:DatasetInformationSection .

mcro:germanbert-License a mcro:LicenseInformationSection .

mcro:germanbert-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:germanbert-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:germanbert-License .

mcro:germanbert-Owner a mcro:OwnerInformationSection .

mcro:germansentimentbert-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@InProceedings{guhr-EtAl:2020:LREC,
  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  Bhme, Hans Joachim},
  title     = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},
  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},
  month          = {May},
  year           = {2020},
  address        = {Marseille, France},
  publisher      = {European Language Resources Association},
  pages     = {1620--1625},
  url       = {https://www.aclweb.org/anthology/2020.lrec-1.202}
}""" .

mcro:germansentimentbert-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "1.834 million German-language samples" .

mcro:germansentimentbert-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Googles Bert architecture" .

mcro:googleelectrabasediscriminator-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "For a detailed description and experimental results, please refer to our paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB)." .

mcro:googleelectrabasediscriminator-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "**ELECTRA** is a new method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish \"real\" input tokens vs \"fake\" input tokens generated by another neural network, similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset." .

mcro:googleelectrabasediscriminator-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/))." .

mcro:googleelectrasmalldiscriminator-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformer networks" .

mcro:googleelectrasmalldiscriminator-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators" .

mcro:googleelectrasmalldiscriminator-ModelDetail a mcro:ModelDetailSection .

mcro:googleelectrasmalldiscriminator-UseCase a mcro:UseCaseInformationSection .

mcro:googleflant5small-Citation a mcro:CitationInformationSection .

mcro:googleflant5small-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:googleflant5small-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:googleflant5small-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:googleflant5small-Citation ;
    mcro:hasLicense mcro:googleflant5small-License ;
    mcro:hasModelArchitecture mcro:googleflant5small-ModelArchitecture .

mcro:googleflant5small-UseCase a mcro:UseCaseInformationSection .

mcro:googlegemma31bit-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Gemma is a family of lightweight, state-of-the-art open models from Google,
built from the same research and technology used to create the Gemini models.
Gemma 3 models are multimodal, handling text and image input and generating text
output, with open weights for both pre-trained variants and instruction-tuned
variants. Gemma 3 has a large, 128K context window, multilingual support in over
140 languages, and is available in more sizes than previous versions. Gemma 3
models are well-suited for a variety of text generation and image understanding
tasks, including question answering, summarization, and reasoning. Their
relatively small size makes it possible to deploy them in environments with
limited resources such as laptops, desktops or your own cloud infrastructure,
democratizing access to state of the art AI models and helping foster innovation
for everyone.""" .

mcro:googlegemma31bit-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{gemma_2025,
    title={Gemma 3},
    url={https://goo.gle/Gemma3Report},
    publisher={Kaggle},
    author={Gemma Team},
    year={2025}
}""" .

mcro:googlegemma31bit-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """These models were trained on a dataset of text data that includes a wide variety
of sources. The 27B model was trained with 14 trillion tokens, the 12B model was
trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and
1B with 2 trillion tokens. Here are the key components:

-   Web Documents: A diverse collection of web text ensures the model is
    exposed to a broad range of linguistic styles, topics, and vocabulary. The
    training dataset includes content in over 140 languages.
-   Code: Exposing the model to code helps it to learn the syntax and
    patterns of programming languages, which improves its ability to generate
    code and understand code-related questions.
-   Mathematics: Training on mathematical text helps the model learn logical
    reasoning, symbolic representation, and to address mathematical queries.
-   Images: A wide range of images enables the model to perform image
    analysis and visual data extraction tasks.

The combination of these diverse data sources is crucial for training a powerful
multimodal model that can handle a wide variety of different tasks and data
formats.""" .

mcro:googlegemma31bit-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Open vision-language models (VLMs) models have a wide range of applications
across various industries and domains. The following list of potential uses is
not comprehensive. The purpose of this list is to provide contextual information
about the possible use-cases that the model creators considered as part of model
training and development.

-   Content Creation and Communication
    -   Text Generation: These models can be used to generate creative text
        formats such as poems, scripts, code, marketing copy, and email drafts.
    -   Chatbots and Conversational AI: Power conversational interfaces
        for customer service, virtual assistants, or interactive applications.
    -   Text Summarization: Generate concise summaries of a text corpus,
        research papers, or reports.
    -   Image Data Extraction: These models can be used to extract,
        interpret, and summarize visual data for text communications.
-   Research and Education
    -   Natural Language Processing (NLP) and VLM Research: These
        models can serve as a foundation for researchers to experiment with VLM
        and NLP techniques, develop algorithms, and contribute to the
        advancement of the field.
    -   Language Learning Tools: Support interactive language learning
        experiences, aiding in grammar correction or providing writing practice.
    -   Knowledge Exploration: Assist researchers in exploring large
        bodies of text by generating summaries or answering questions about
        specific topics.""" .

mcro:googlest5v11-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "GEGLU activation in feed-forward hidden layer, rather than ReLU",
        "no parameter sharing between embedding and classifier layer" .

mcro:googlest5v11-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" .

mcro:googlest5v11-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Pre-trained on C4 only without mixing in the downstream tasks." .

mcro:googlest5v11-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task" .

mcro:gpt2-Citation a mcro:CitationInformationSection .

mcro:gpt2-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasLimitation mcro:gpt2-Limitation .

mcro:gpt2-EvaluationResults a mcro:QuantativeAnalysisSection .

mcro:gpt2-IntendedUseCase a mcro:UseCaseInformationSection .

mcro:gpt2-Limitation a mcro:LimitationInformationSection .

mcro:gpt2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers model" .

mcro:gpt2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:gpt2-ModelArchitecture .

mcro:gpt2-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "all the web pages from outbound links on Reddit which received at least 3 karma" .

mcro:gpt2-TrainingProcedure a mcro:ModelParameterSection .

mcro:gpt2medium-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-based language model" .

mcro:gpt2medium-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}""" .

mcro:gpt2medium-Consideration a mcro:ConsiderationInformationSection .

mcro:gpt2medium-EvaluationData a mcro:DatasetInformationSection .

mcro:gpt2medium-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Modified MIT License" .

mcro:gpt2medium-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:gpt2medium-Architecture ;
    mcro:hasCitation mcro:gpt2medium-Citation ;
    mcro:hasLicense mcro:gpt2medium-License .

mcro:gpt2medium-TrainingData a mcro:DatasetInformationSection .

mcro:gpt2medium-UseCase a mcro:UseCaseInformationSection .

mcro:granitetimeseriesttmr1-CitationInformationSection a mcro:CitationInformationSection .

mcro:granitetimeseriesttmr1-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:granitetimeseriesttmr1-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:granitetimeseriesttmr1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:granitetimeseriesttmr1-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:granitetimeseriesttmr1-CitationInformationSection ;
    mcro:hasLicense mcro:granitetimeseriesttmr1-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:granitetimeseriesttmr1-ModelArchitectureInformationSection .

mcro:granitetimeseriesttmr1-TrainingDataInformationSection a mcro:TrainingDataInformationSection .

mcro:granitetimeseriesttmr1-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:granitetimeseriesttmr2model-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{ekambaram2024tinytimemixersttms,
      title={Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series},
      author={Vijay Ekambaram and Arindam Jati and Pankaj Dayama and Sumanta Mukherjee and Nam H. Nguyen and Wesley M. Gifford and Chandra Reddy and Jayant Kalagnanam},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS 2024)},
      year={2024},
}""" .

mcro:granitetimeseriesttmr2model-CitationInformation a mcro:CitationInformationSection .

mcro:granitetimeseriesttmr2model-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:granitetimeseriesttmr2model-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitationInformation mcro:granitetimeseriesttmr2model-Citation ;
    mcro:hasModelArchitectureInformation mcro:granitetimeseriesttmr2model-ModelArchitecture .

mcro:granitetimeseriesttmr2model-TrainingDataInformation a mcro:TrainingDataInformationSection .

mcro:granitetimeseriesttmr2model-UseCaseInformation a mcro:UseCaseInformationSection .

mcro:graphcodebert-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "More details can be found in the [paper](https://arxiv.org/abs/2009.08366) by Guo et. al." .

mcro:graphcodebert-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages." .

mcro:graphcodebert-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512." .

mcro:gte-base-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}""" .

mcro:gte-base-LimitationInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens." .

mcro:gte-base-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT framework" .

mcro:gte-base-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "information retrieval",
        "semantic textual similarity",
        "text reranking" .

mcro:gte-large-Citation a mcro:CitationInformationSection .

mcro:gte-large-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens." .

mcro:gte-large-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:gte-large-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:gte-large-Citation ;
    mcro:hasModelArchitecture mcro:gte-large-ModelArchitecture .

mcro:gte-large-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "information retrieval",
        "semantic textual similarity",
        "text reranking" .

mcro:gte-small-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}""" .

mcro:gte-small-LimitationInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens." .

mcro:gte-small-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT framework" .

mcro:gte-small-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "information retrieval",
        "semantic textual similarity",
        "text reranking" .

mcro:gtelargeenv15-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """mGTE: Generalized Long-Context Text Representation and Reranking
Models for Multilingual Text Retrieval""" .

mcro:gtelargeenv15-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformer++" .

mcro:gtelargeenv15-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Text Embeddings" .

mcro:hallucinationevaluationmodel-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc {hhem-2.1-open,
	author       = {Forrest Bao and Miaoran Li and Rogger Luo and Ofer Mendelevitch},
	title        = {{HHEM-2.1-Open}},
	year         = 2024,
	url          = { https://huggingface.co/vectara/hallucination_evaluation_model },
	doi          = { 10.57967/hf/3240 },
	publisher    = { Hugging Face }
}""" .

mcro:hallucinationevaluationmodel-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "AggreFact",
        "RAGTruth" .

mcro:hallucinationevaluationmodel-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:hallucinationevaluationmodel-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "T5-base" .

mcro:hallucinationevaluationmodel-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "detecting hallucinations in LLMs" .

mcro:hubertbase-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Paper" .

mcro:hubertbase-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz." .

mcro:hubertbase-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "See [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`." .

mcro:hubertsiuzdaksnac24khz-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SNAC encodes audio into hierarchical tokens similarly to SoundStream, EnCodec, and DAC" .

mcro:hubertsiuzdaksnac24khz-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech synthesis" .

mcro:huggingquantsMetaLlama318BInstructAWQINT4-Architecture a mcro:ModelArchitectureInformationSection .

mcro:huggingquantsMetaLlama318BInstructAWQINT4-License a mcro:LicenseInformationSection .

mcro:huggingquantsMetaLlama318BInstructAWQINT4-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:huggingquantsMetaLlama318BInstructAWQINT4-Architecture ;
    mcro:hasLicense mcro:huggingquantsMetaLlama318BInstructAWQINT4-License ;
    mcro:hasUseCase mcro:huggingquantsMetaLlama318BInstructAWQINT4-UseCase .

mcro:huggingquantsMetaLlama318BInstructAWQINT4-UseCase a mcro:UseCaseInformationSection .

mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Meta Llama 3.1" .

mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelArchitecture ;
    mcro:hasUseCase mcro:huggingquantsMetaLlama318BInstructGPTQINT4-UseCase .

mcro:huggingquantsMetaLlama318BInstructGPTQINT4-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "multilingual dialogue use cases" .

mcro:ibmgranitegranite318binstruct-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Granite-3.1-8B-Instruct is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings." .

mcro:ibmgranitegranite318binstruct-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Overall, our SFT data is largely comprised of three key sources: (1) publicly available datasets with permissive license, (2) internal synthetic data targeting specific capabilities including long-context tasks, and (3) very small amounts of human-curated data. A detailed attribution of datasets can be found in the [Granite 3.0 Technical Report](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf), [Granite 3.1 Technical Report (coming soon)](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d), and [Accompanying Author List](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/author-ack.pdf)." .

mcro:ibmgranitegranite318binstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:ibmgranitegranite318binstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:ibmgranitegranite318binstruct-Architecture ;
    mcro:hasLicense mcro:ibmgranitegranite318binstruct-License .

mcro:ibmgranitegranite318binstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model is designed to respond to general instructions and can be used to build AI assistants for multiple domains, including business applications." .

mcro:ibmresearchMoLFormerXLboth10pct-Citation1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{10.1038/s42256-022-00580-7,
  year = {2022},
  title = {{Large-scale chemical language representations capture molecular structure and   properties}},
  author = {Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and   Mroueh, Youssef and Das, Payel},
  journal = {Nature Machine Intelligence},
  doi = {10.1038/s42256-022-00580-7},
  pages = {1256--1264},
  number = {12},
  volume = {4}
}""" .

mcro:ibmresearchMoLFormerXLboth10pct-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{https://doi.org/10.48550/arxiv.2106.09553,
  doi = {10.48550/ARXIV.2106.09553},
  url = {https://arxiv.org/abs/2106.09553},
  author = {Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and Mroueh, Youssef and Das, Payel},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Biomolecules (q-bio.BM), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  title = {Large-Scale Chemical Language Representations Capture Molecular Structure and Properties},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}""" .

mcro:ibmresearchMoLFormerXLboth10pct-Dataset a mcro:DatasetInformationSection ;
    mcro:hasTrainingData mcro:ibmresearchMoLFormerXLboth10pct-TrainingData ;
    prov1:hasTextValue "We trained MoLFormer-XL on a combination of molecules from the ZINC15 and PubChem datasets. This repository contains the version trained on 10% ZINC + 10% PubChem." .

mcro:ibmresearchMoLFormerXLboth10pct-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the model for masked language modeling, but it is mainly intended to be used as a feature extractor or to be fine-tuned for a prediction task. The \"frozen\" model embeddings may be used for similarity measurements, visualization, or training predictor models. The model may also be fine-tuned for sequence classification tasks (e.g., solubility, toxicity, etc.)." .

mcro:ibmresearchMoLFormerXLboth10pct-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "This model is not intended for molecule generation. It is also not tested for molecules larger than ~200 atoms (i.e., macromolecules). Furthermore, using invalid or noncanonical SMILES may result in worse performance." .

mcro:ibmresearchMoLFormerXLboth10pct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MoLFormer is a large-scale chemical language model designed with the intention of learning a model trained on small molecules which are represented as SMILES strings. MoLFormer leverges masked language modeling and employs a linear attention Transformer combined with rotary embeddings." .

mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:ibmresearchMoLFormerXLboth10pct-Citation1,
        mcro:ibmresearchMoLFormerXLboth10pct-Citation2 ;
    mcro:hasDataset mcro:ibmresearchMoLFormerXLboth10pct-Dataset ;
    mcro:hasIntendedUseCase mcro:ibmresearchMoLFormerXLboth10pct-IntendedUseCase ;
    mcro:hasLimitation mcro:ibmresearchMoLFormerXLboth10pct-Limitation ;
    mcro:hasModelArchitecture mcro:ibmresearchMoLFormerXLboth10pct-ModelArchitecture .

mcro:ibmresearchMoLFormerXLboth10pct-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Molecules were canonicalized with RDKit prior to training and isomeric information was removed. Also, molecules longer than 202 tokens were dropped." .

mcro:indobenchmarkindobertbasep1-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{wilie2020indonlu,
  title={IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding},
  author={Bryan Wilie and Karissa Vincentio and Genta Indra Winata and Samuel Cahyawijaya and X. Li and Zhi Yuan Lim and S. Soleman and R. Mahendra and Pascale Fung and Syafri Bahar and A. Purwarianti},
  booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
  year={2020}
}""" .

mcro:indobenchmarkindobertbasep1-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT model" .

mcro:indonesianrobertabaseposptagger-Dataset a mcro:DatasetInformationSection .

mcro:indonesianrobertabaseposptagger-License a mcro:LicenseInformationSection .

mcro:indonesianrobertabaseposptagger-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:indonesianrobertabaseposptagger-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:indonesianrobertabaseposptagger-License .

mcro:indonesianrobertabaseposptagger-UseCase a mcro:UseCaseInformationSection .

mcro:infoxlm-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{chi-etal-2021-infoxlm,
  title = "{I}nfo{XLM}: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training",
  author={Chi, Zewen and Dong, Li and Wei, Furu and Yang, Nan and Singhal, Saksham and Wang, Wenhui and Song, Xia and Mao, Xian-Ling and Huang, Heyan and Zhou, Ming},
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jun,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.naacl-main.280",
  doi = "10.18653/v1/2021.naacl-main.280",
  pages = "3576--3588",}""" .

mcro:intfloate5mistral7binstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{wang2023improving,
  title={Improving Text Embeddings with Large Language Models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}

@article{wang2022text,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}""" .

mcro:intfloate5mistral7binstruct-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """Using this model for inputs longer than 4096 tokens is not recommended.

This model's multilingual capability is still inferior to [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) for some cases.""" .

mcro:intfloate5mistral7binstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model has 32 layers and the embedding size is 4096." .

mcro:intfloate5mistral7binstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.

This model is initialized from [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)
and fine-tuned on a mixture of multilingual datasets.
As a result, it has some multilingual capability.
However, since Mistral-7B-v0.1 is mainly trained on English data, we recommend using this model for English only.
For multilingual use cases, please refer to [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large).""" .

mcro:intfloatmultilinguale5base-BenchmarkResultsSection a mcro:QuantativeAnalysisSection .

mcro:intfloatmultilinguale5base-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{wang2024multilingual,
  title={Multilingual E5 Text Embeddings: A Technical Report},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2402.05672},
  year={2024}
}""" .

mcro:intfloatmultilinguale5base-LimitationInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Long texts will be truncated to at most 512 tokens." .

mcro:intfloatmultilinguale5base-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model has 12 layers and the embedding size is 768." .

mcro:intfloatmultilinguale5base-TrainingDataInformationSection a mcro:TrainingDataInformationSection .

mcro:intfloatmultilinguale5base-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:intfloatmultilinguale5largeinstruct-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model has 24 layers and the embedding size is 1024." .

mcro:intfloatmultilinguale5largeinstruct-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """[Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/pdf/2402.05672).
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv 2024""" .

mcro:intfloatmultilinguale5largeinstruct-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Long texts will be truncated to at most 512 tokens." .

mcro:intfloatmultilinguale5largeinstruct-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """**Initialization**: [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)

**First stage**: contrastive pre-training with 1 billion weakly supervised text pairs.

**Second stage**: fine-tuning on datasets from the [E5-mistral](https://arxiv.org/abs/2401.00368) paper.""" .

mcro:intfloatmultilinguale5largeinstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Below are examples to encode queries and passages from the MS-MARCO passage ranking dataset." .

mcro:intfloatmultilinguale5small-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model has 12 layers and the embedding size is 384." .

mcro:intfloatmultilinguale5small-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Multilingual E5 Text Embeddings: A Technical Report" .

mcro:intfloatmultilinguale5small-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Long texts will be truncated to at most 512 tokens." .

mcro:intfloatmultilinguale5small-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "xP3" .

mcro:intfloatmultilinguale5small-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Use \"query: \" prefix for symmetric tasks such as semantic similarity, bitext mining, paraphrase retrieval." .

mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "This model was created by fine-tuning the `facebook/wav2vec2-large-robust-ft-libri-960h` checkpoint on the [libritts](https://research.google/tools/datasets/libri-tts/) and [voxpopuli](https://github.com/facebookresearch/voxpopuli) datasets with a new vocabulary that includes punctuation." .

mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-LimitationInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Since the model was fine-tuned on clean audio, it is not well-suited for noisy audio like CommonVoice (though I may upload a checkpoint for that soon too). It still does pretty good, though." .

mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-PerformanceMetricInformationSection a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "The model gets a respectable WER of 4.45% on the librispeech validation set. The baseline, `facebook/wav2vec2-large-robust-ft-libri-960h`, got 4.3%." .

mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody." .

mcro:jhartmannemotionenglishdistilrobertabase-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "DistilRoBERTa-base" .

mcro:jhartmannemotionenglishdistilrobertabase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{hartmann2022emotionenglish,
  author={Hartmann, Jochen},
  title={Emotion English DistilRoBERTa-base},
  year={2022},
  howpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},
}""" .

mcro:jhartmannemotionenglishdistilrobertabase-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:jhartmannemotionenglishdistilrobertabase-Citation ;
    mcro:hasModelArchitecture mcro:jhartmannemotionenglishdistilrobertabase-Architecture ;
    mcro:hasReference mcro:jhartmannemotionenglishdistilrobertabase-Reference ;
    mcro:hasUseCase mcro:jhartmannemotionenglishdistilrobertabase-UseCase .

mcro:jhartmannemotionenglishdistilrobertabase-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "Jochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022." .

mcro:jhartmannemotionenglishdistilrobertabase-UseCase a mcro:UseCaseInformationSection .

mcro:jinaaijinaembeddingsv3-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,
      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, 
      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael Gnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},
      year={2024},
      eprint={2409.10173},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.10173}, 
}""" .

mcro:jinaaijinaembeddingsv3-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "`jina-embeddings-v3` is listed on AWS & Azure. If you need to use it beyond those platforms or on-premises within your company, note that the models is licensed under CC BY-NC 4.0. For commercial usage inquiries, feel free to [contact us](https://jina.ai/contact-sales/)." .

mcro:jinaaijinaembeddingsv3-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Jina-XLM-RoBERTa architecture" .

mcro:jinaaijinaembeddingsv3-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """`jina-embeddings-v3` is a **multilingual multi-task text embedding model** designed for a variety of NLP applications.
Based on the [Jina-XLM-RoBERTa architecture](https://huggingface.co/jinaai/xlm-roberta-flash-implementation), 
this model supports Rotary Position Embeddings to handle long input sequences up to **8192 tokens**.
Additionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently.""" .

mcro:jinaaijinarerankerv2basemultilingual-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "_This model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please [contact us](https://jina.ai/contact-sales) for any further clarifications._" .

mcro:jinaaijinarerankerv2basemultilingual-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformer-based model" .

mcro:jinaaijinarerankerv2basemultilingual-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The Jina Reranker v2 (`jina-reranker-v2-base-multilingual`) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy.

Compared with the state-of-the-art reranker models, including the previous released `jina-reranker-v1-base-en`, the **Jina Reranker v2** model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks.

The `jina-reranker-v2-base-multilingual` model is capable of handling long texts with a context length of up to `1024` tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately.

The model is also equipped with a flash attention mechanism, which significantly improves the model's performance.""" .

mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{grosman2021xlsr53-large-arabic,
  title={Fine-tuned {XLSR}-53 large model for speech recognition in {A}rabic},
  author={Grosman, Jonatas},
  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-arabic}},
  year={2021}
}""" .

mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Common Voice 6.1" .

mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset-2 a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Arabic Speech Corpus" .

mcro:jonatasgrosmanwav2vec2largexlsr53arabic-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Fine-tuned XLSR-53 large model" .

mcro:jonatasgrosmanwav2vec2largexlsr53arabic-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech recognition in Arabic" .

mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{grosman2021xlsr53-large-chinese,
  title={Fine-tuned {XLSR}-53 large model for speech recognition in {C}hinese},
  author={Grosman, Jonatas},
  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn}},
  year={2021}
}""" .

mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Chinese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [ST-CMDS](http://www.openslr.org/38/).
When using this model, make sure that your speech input is sampled at 16kHz.""" .

mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech recognition in Chinese" .

mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{grosman2021xlsr53-large-dutch,
  title={Fine-tuned {XLSR}-53 large model for speech recognition in {D}utch},
  author={Grosman, Jonatas},
  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-dutch}},
  year={2021}
}""" .

mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Common Voice 6.1" .

mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2 a mcro:DatasetInformationSection ;
    prov1:hasTextValue "CSS10" .

mcro:jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)" .

mcro:jonatasgrosmanwav2vec2largexlsr53dutch-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech recognition in Dutch" .

mcro:jonatasgrosmanwav2vec2largexlsr53english-Citation a mcro:CitationInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53english-Dataset a mcro:DatasetInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:jonatasgrosmanwav2vec2largexlsr53english-Citation ;
    mcro:hasDataset mcro:jonatasgrosmanwav2vec2largexlsr53english-Dataset ;
    mcro:hasModelArchitecture mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelArchitecture .

mcro:jonatasgrosmanwav2vec2largexlsr53english-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-CitationInformationSection a mcro:CitationInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53japanese-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{grosman2021xlsr53-large-japanese,
  title={Fine-tuned {XLSR}-53 large model for speech recognition in {J}apanese},
  author={Grosman, Jonatas},
  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese}},
  year={2021}
}""" .

mcro:jonatasgrosmanwav2vec2largexlsr53japanese-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Japanese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [JSUT](https://sites.google.com/site/shinnosuketakamichi/publication/jsut)." .

mcro:jonatasgrosmanwav2vec2largexlsr53japanese-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech recognition in Japanese" .

mcro:jonatasgrosmanwav2vec2largexlsr53persian-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{grosman2021xlsr53-large-persian,
  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ersian},
  author={Grosman, Jonatas},
  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-persian}},
  year={2021}
}""" .

mcro:jonatasgrosmanwav2vec2largexlsr53persian-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Common Voice 6.1" .

mcro:jonatasgrosmanwav2vec2largexlsr53persian-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)" .

mcro:jonatasgrosmanwav2vec2largexlsr53persian-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech recognition in Persian" .

mcro:jonatasgrosmanwav2vec2largexlsr53polish-Citation a mcro:CitationInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53polish-Dataset a mcro:DatasetInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53polish-License a mcro:LicenseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:jonatasgrosmanwav2vec2largexlsr53polish-License .

mcro:jonatasgrosmanwav2vec2largexlsr53polish-UseCase a mcro:UseCaseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{grosman2021xlsr53-large-portuguese,
  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ortuguese},
  author={Grosman, Jonatas},
  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese}},
  year={2021}
}""" .

mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "[Common Voice 6.1](https://huggingface.co/datasets/common_voice)" .

mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)" .

mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:jonatasgrosmanwav2vec2largexlsr53russian-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{grosman2021xlsr53-large-russian,
  title={Fine-tuned {XLSR}-53 large model for speech recognition in {R}ussian},
  author={Grosman, Jonatas},
  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian}},
  year={2021}
}""" .

mcro:jonatasgrosmanwav2vec2largexlsr53russian-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Russian using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [CSS10](https://github.com/Kyubyong/css10)." .

mcro:jonatasgrosmanwav2vec2largexlsr53russian-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech recognition in Russian" .

mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{grosman2021xlsr-1b-portuguese,
  title={Fine-tuned {XLS-R} 1{B} model for speech recognition in {P}ortuguese},
  author={Grosman, Jonatas},
  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-xls-r-1b-portuguese}},
  year={2022}
}""" .

mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "train and validation splits of [Common Voice 8.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_8_0), [CORAA](https://github.com/nilc-nlp/CORAA), [Multilingual TEDx](http://www.openslr.org/100), and [Multilingual LibriSpeech](https://www.openslr.org/94/)." .

mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Fine-tuned [facebook/wav2vec2-xls-r-1b](https://huggingface.co/facebook/wav2vec2-xls-r-1b) on Portuguese" .

mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech recognition in Portuguese" .

mcro:keremberkeyolov5nlicenseplate-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:keremberkeyolov5nlicenseplate-UseCase a mcro:UseCaseInformationSection .

mcro:khawhitemangaocrbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Vision Encoder Decoder framework" .

mcro:khawhitemangaocrbase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "general purpose printed Japanese OCR" .

mcro:kluerobertabase-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{park2021klue,
      title={KLUE: Korean Language Understanding Evaluation},
      author={Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},
      year={2021},
      eprint={2105.09680},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:kluerobertabase-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Pretrained RoBERTa Model on Korean Language" .

mcro:kluerobertabase-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "_NOTE:_ Use `BertTokenizer` instead of RobertaTokenizer. (`AutoTokenizer` will load `BertTokenizer`)" .

mcro:ko-sroberta-multitask-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """- Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). Kornli and korsts: New benchmark datasets for korean natural language understanding. arXiv
preprint arXiv:2004.03289
- Reimers, Nils and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. ArXiv abs/1908.10084 (2019)
- Reimers, Nils and Iryna Gurevych. Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. EMNLP (2020).""" .

mcro:ko-sroberta-multitask-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SentenceTransformer(  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: RobertaModel   (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False}))" .

mcro:kobert-ReferenceInformationSection a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "- https://github.com/SKTBrain/KoBERT" .

mcro:kobert-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "If you want to import KoBERT tokenizer with `AutoTokenizer`, you should give `trust_remote_code=True`." .

mcro:kokoro82M-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "open-weight TTS model with 82 million parameters" .

mcro:kokoro82M-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache-licensed weights" .

mcro:kokoro82M-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "permissive/non-copyrighted audio data and IPA phoneme labels" .

mcro:kokoro82M-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "can be deployed anywhere from production environments to personal projects" .

mcro:kresnikwav2vec2largexlsrkorean-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "kresnik/zeroth_korean" .

mcro:kresnikwav2vec2largexlsrkorean-PerformanceMetricInformationSection a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "CER: 1.78%",
        "WER: 4.74%" .

mcro:kresnikwav2vec2largexlsrkorean-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:layoutlmbaseuncased-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{xu2019layoutlm,
    title={LayoutLM: Pre-training of Text and Layout for Document Image Understanding},
    author={Yiheng Xu and Minghao Li and Lei Cui and Shaohan Huang and Furu Wei and Ming Zhou},
    year={2019},
    eprint={1912.13318},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}""" .

mcro:layoutlmbaseuncased-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "12-layer, 768-hidden, 12-heads, 113M parameters" .

mcro:layoutlmbaseuncased-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:layoutlmbaseuncased-Citation ;
    mcro:hasModelArchitecture mcro:layoutlmbaseuncased-ModelArchitecture .

mcro:layoutlmbaseuncased-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "IIT-CDIP Test Collection 1.0" .

mcro:layoutlmbaseuncased-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "document image understanding and information extraction tasks, such as form understanding and receipt understanding" .

mcro:layoutlmv3-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{huang2022layoutlmv3,
  author={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},
  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  year={2022}
}""" .

mcro:layoutlmv3-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue """The content of this project itself is licensed under the [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/).
Portions of the source code are based on the [transformers](https://github.com/huggingface/transformers) project.
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)""" .

mcro:layoutlmv3-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis." .

mcro:legalbertTheMuppetsStraightOutOfLawSchool-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{chalkidis-etal-2020-legal,
    title = "{LEGAL}-{BERT}: The Muppets straight out of Law School",
    author = "Chalkidis, Ilias  and
      Fergadiotis, Manos  and
      Malakasiotis, Prodromos  and
      Aletras, Nikolaos  and
      Androutsopoulos, Ion",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.findings-emnlp.261",
    pages = "2898--2904"
}""" .

mcro:legalbertTheMuppetsStraightOutOfLawSchool-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """* 116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.
    
* 61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk).
    
* 19,867 cases from the European Court of Justice (ECJ), also available from EURLEX.
    
* 12,554 cases from HUDOC, the repository of the European Court of Human Rights (ECHR) (http://hudoc.echr.coe.int/eng).
    
* 164,141 cases from various courts across the USA, hosted in the Case Law Access Project portal (https://case.law).
    
* 76,366 US contracts from EDGAR, the database of US Securities and Exchange Commission (SECOM) (https://www.sec.gov/edgar.shtml).""" .

mcro:legalbertTheMuppetsStraightOutOfLawSchool-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """* We trained BERT using the official code provided in Google BERT's GitHub repository (https://github.com/google-research/bert).
* We released a model similar to the English BERT-BASE model (12-layer, 768-hidden, 12-heads, 110M parameters).
* We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4.
* We were able to use a single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc), while also utilizing [GCP research credits](https://edu.google.com/programs/credits/research). Huge thanks to both Google programs for supporting us!
* Part of LEGAL-BERT is a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.""" .

mcro:legalbertTheMuppetsStraightOutOfLawSchool-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available." .

mcro:lengyue233contentvecbest-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:lengyue233contentvecbest-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:lftwr4target-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{vidgen2021lftw,
  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},
  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},
  booktitle={ACL},
  year={2021}
}""" .

mcro:llama2-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)" .

mcro:llama2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:llama2-Citation ;
    mcro:hasLicense mcro:llama2-License ;
    mcro:hasModelArchitecture mcro:llama2-ModelArchitecture .

mcro:llama2-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data." .

mcro:llama32Collection-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)." .

mcro:llama32Collection-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:llama32Collection-TrainingDataInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)." .

mcro:llama32Collection-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources." .

mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)." .

mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)." .

mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources." .

mcro:llama32CollectionOfMultilingualLlLMs-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:llama32CollectionOfMultilingualLlLMs-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)." .

mcro:llama32CollectionOfMultilingualLlLMs-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)." .

mcro:llama32CollectionOfMultilingualLlLMs-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources." .

mcro:llama4Scout17B16EInstruct-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases." .

mcro:llama4Scout17B16EInstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "A custom commercial license, the Llama 4 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE)" .

mcro:llama4Scout17B16EInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality." .

mcro:llama4Scout17B16EInstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:llama4Scout17B16EInstruct-License ;
    mcro:hasModelArchitecture mcro:llama4Scout17B16EInstruct-ModelArchitecture .

mcro:llama4Scout17B16EInstruct-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Metas products and services. This includes publicly shared posts from Instagram and Facebook and peoples interactions with Meta AI." .

mcro:llamaGuard38B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{dubey2024llama3herdmodels,
  title =         {The Llama 3 Herd of Models},
  author =        {Llama Team, AI @ Meta},
  year =          {2024},
  eprint =        {2407.21783},
  archivePrefix = {arXiv},
  primaryClass =  {cs.AI},
  url =           {https://arxiv.org/abs/2407.21783}
}""" .

mcro:llamaGuard38B-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """There are some limitations associated with Llama Guard 3. First, Llama Guard 3 itself is an LLM fine-tuned on Llama 3.1. Thus, its performance (e.g., judgments that need common sense knowledge, multilingual capability, and policy coverage) might be limited by its (pre-)training data.

Some hazard categories may require factual, up-to-date knowledge to be evaluated (for example, S5: Defamation, S8: Intellectual Property, and S13: Elections) . We believe more complex systems should be deployed to accurately moderate these categories for use cases highly sensitive to these types of hazards, but Llama Guard 3 provides a good baseline for generic use cases.

Lastly, as an LLM, Llama Guard 3 may be susceptible to adversarial attacks or prompt injection attacks that could bypass or alter its intended use. Please feel free to [report](https://github.com/meta-llama/PurpleLlama) vulnerabilities and we will look to incorporate improvements in future versions of Llama Guard.""" .

mcro:llamaGuard38B-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM  it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.

Llama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.

Below is a response classification example for Llama Guard 3.


  
  


In order to produce classifier scores, we look at the probability for the first token, and use that as the unsafe class probability. We can then apply score thresholding to make binary decisions.""" .

mcro:llamaGuard38B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "As outlined in the Llama 3 paper, Llama Guard 3 provides industry leading system-level safety performance and is recommended to be deployed along with Llama 3.1. Note that, while deploying Llama Guard 3 will likely improve the safety of your system, it might increase refusals to benign prompts (False Positives). Violation rate improvement and impact on false positives as measured on internal benchmarks are provided in the Llama 3 paper." .

mcro:llavahfllava157bhf-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue """Llama 2 is licensed under the LLAMA 2 Community License,
Copyright (c) Meta Platforms, Inc. All Rights Reserved.""" .

mcro:llavahfllava157bhf-ModelArchitectureInformation a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.""" .

mcro:llavahfllava157bhf-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:llavahfllava157bhf-License ;
    mcro:hasModelArchitectureInformation mcro:llavahfllava157bhf-ModelArchitectureInformation .

mcro:llavahfllavaonevisionqwen205bovhf-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SO400M + Qwen2" .

mcro:llavahfllavaonevisionqwen205bovhf-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{li2024llavaonevisioneasyvisualtask,
      title={LLaVA-OneVision: Easy Visual Task Transfer}, 
      author={Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
      year={2024},
      eprint={2408.03326},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.03326}, 
}""" .

mcro:llavahfllavaonevisionqwen205bovhf-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:llavahfllavaonevisionqwen205bovhf-Architecture ;
    mcro:hasCitation mcro:llavahfllavaonevisionqwen205bovhf-Citation .

mcro:llavahfllavaonevisionqwen205bovhf-UseCase a mcro:UseCaseInformationSection .

mcro:llavahfllavav16mistral7bhf-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "The LLaVA-NeXT model was proposed in [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://llava-vl.github.io/blog/2024-01-30-llava-next/) by Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee. LLaVa-NeXT (also called LLaVa-1.6) improves upon [LLaVa-1.5](https://huggingface.co/transformers/main/model_doc/llava.html) by increasing the input image resolution and training on an improved visual instruction tuning dataset to improve OCR and common sense reasoning." .

mcro:llavahfllavav16mistral7bhf-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{liu2023improved,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
      year={2023},
      eprint={2310.03744},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:llavahfllavav16mistral7bhf-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "LLaVa-NeXT, leveraging [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) as LLM" .

mcro:llavahfllavav16mistral7bhf-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for tasks like image captioning, visual question answering, multimodal chatbot use cases. See the [model hub](https://huggingface.co/models?search=llava-hf) to look for
other versions on a task that interests you.""" .

mcro:llavamodelcard-EvaluationData a mcro:EvaluationDataInformationSection ;
    prov1:hasTextValue "A collection of 12 benchmarks, including 5 academic VQA benchmarks and 7 recent benchmarks specifically proposed for instruction-following LMMs." .

mcro:llavamodelcard-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue """Llama 2 is licensed under the LLAMA 2 Community License,
Copyright (c) Meta Platforms, Inc. All Rights Reserved.""",
        "https://github.com/haotian-liu/LLaVA/issues" .

mcro:llavamodelcard-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.""",
        "LLaVA-v1.5-7B was trained in September 2023.",
        "https://llava-vl.github.io/" .

mcro:llavamodelcard-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """- 558K filtered image-text pairs from LAION/CC/SBU, captioned by BLIP.
- 158K GPT-generated multimodal instruction-following data.
- 450K academic-task-oriented VQA data mixture.
- 40K ShareGPT data.""" .

mcro:llavamodelcard-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence.",
        "The primary use of LLaVA is research on large multimodal models and chatbots." .

mcro:lmmslabLLaVAVideo7BQwen2-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SO400M + Qwen2" .

mcro:lmmslabLLaVAVideo7BQwen2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{zhang2024videoinstructiontuningsynthetic,
    title={Video Instruction Tuning With Synthetic Data}, 
    author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
    year={2024},
    eprint={2410.02713},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2410.02713}, 
}""" .

mcro:lmmslabLLaVAVideo7BQwen2-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "None" .

mcro:lmmslabLLaVAVideo7BQwen2-Limitation a mcro:LimitationInformationSection .

mcro:lmmslabLLaVAVideo7BQwen2-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """The LLaVA-Video models are 7/72B parameter models trained on LLaVA-Video-178K and LLaVA-OneVision Dataset, based on Qwen2 language model with a context window of 32K tokens.

This model support at most 64 frames.""" .

mcro:lmmslabLLaVAVideo7BQwen2-TrainingData a mcro:ModelParameterSection ;
    prov1:hasTextValue """- **Architecture:** SO400M + Qwen2
- **Initialized Model:** lmms-lab/llava-onevision-qwen2-7b-si
- **Data:** A mixture of 1.6M single-image/multi-image/video data, 1 epoch, full model
- **Precision:** bfloat16""" .

mcro:lmmslabLLaVAVideo7BQwen2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model was trained on LLaVA-Video-178K and LLaVA-OneVision Dataset, having the ability to interact with images, multi-image and videos, but specific to videos." .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "4-bit quantized version of DeepSeek-R1-0528-Qwen3-8B using MLX, optimized for Apple Silicon." .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelArchitecture .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Citation a mcro:CitationInformationSection .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Disclaimer a obo:IAO_0000314 ;
    prov1:hasTextValue "LM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model.  You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models.  LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio." .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Citation ;
    mcro:hasDisclaimer mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Disclaimer ;
    mcro:hasModelArchitecture mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelArchitecture ;
    mcro:hasTechnicalDetail mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-TechnicalDetail .

mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-TechnicalDetail a obo:IAO_0000314 ;
    prov1:hasTextValue "8-bit quantized version of DeepSeek-R1-0528-Qwen3-8B using MLX, optimized for Apple Silicon." .

mcro:longformerbase4096-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={arXiv:2004.05150},
  year={2020},
}""" .

mcro:longformerbase4096-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Longformer is a transformer model for long documents. `longformer-base-4096` is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. 
 
Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.""" .

mcro:madebyollinsdxlvaefp16fix-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue "SDXL-VAE-FP16-Fix is the [SDXL VAE](https://huggingface.co/stabilityai/sdxl-vae)*, but modified to run in fp16 precision without generating NaNs." .

mcro:mask2former-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation
](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/).""" .

mcro:mask2former-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone)." .

mcro:mask2former-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use this particular checkpoint for panoptic segmentation. See the [model hub](https://huggingface.co/models?search=mask2former) to look for other
fined-tuned versions on a task that interests you.""" .

mcro:mbartlarge50manytomanymmt-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{tang2020multilingual,
    title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},
    author={Yuqing Tang and Chau Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},
    year={2020},
    eprint={2008.00401},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}""" .

mcro:mbartlarge50manytomanymmt-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "mBART-50 many to many multilingual machine translation" .

mcro:mbartlarge50manytomanymmt-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "multilingual machine translation" .

mcro:metaLlama3-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{llama3modelcard,

  title={Llama 3 Model Card},

  author={AI@Meta},

  year={2024},

  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}

}""" .

mcro:metaLlama3-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data." .

mcro:metaLlama3-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)" .

mcro:metaLlama3-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:metaLlama3-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:metaLlama3-License ;
    mcro:hasModelArchitecture mcro:metaLlama3-ModelArchitecture .

mcro:metaLlama3-OutOfScopeUseCase a mcro:OutOfScopeUseCaseSectionInformation ;
    prov1:hasTextValue "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**." .

mcro:metaLlama3-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasOutOfScopeUseCase mcro:metaLlama3-OutOfScopeUseCase ;
    prov1:hasTextValue "Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks." .

mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. 

**Data Freshness:** The pretraining data has a cutoff of December 2023.""" .

mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-IntendedUse a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases." .

mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)" .

mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-License ;
    mcro:hasModelArchitecture mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelArchitecture .

mcro:metaLlama31Instruct-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples." .

mcro:metaLlama31Instruct-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases." .

mcro:metaLlama31Instruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:metaLlama31Instruct-License ;
    mcro:hasModelArchitecture mcro:metaLlama31Instruct-ModelArchitecture .

mcro:metaLlama31Instruct-OutOfScopeUseCase a mcro:OutofScopeUseCaseSectionInformation ;
    prov1:hasTextValue "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**." .

mcro:metaLlama31Instruct-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples." .

mcro:metaLlama32Collection-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)." .

mcro:metaLlama32Collection-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)." .

mcro:metaLlama32Collection-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:metaLlama32Collection-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:metaLlama32Collection-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:metaLlama32Collection-ModelArchitectureInformationSection .

mcro:metaLlama32Collection-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources." .

mcro:metallamaLlama31-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)" .

mcro:metallamaLlama31-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:metallamaLlama31-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:metallamaLlama31-License ;
    mcro:hasModelArchitecture mcro:metallamaLlama31-ModelArchitecture .

mcro:metallamaLlama31-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases." .

mcro:metallamaLlama3211BVisionInstruct-IntendedUse a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Llama 3.2-Vision is intended for commercial and research use. Instruction tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pretrained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2-Visions ability to take images and text as inputs, additional use cases could include:

1. Visual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it.   
2. Document Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image.  
3. Image Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story.  
4. Image-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words.  
5. Visual Grounding: Visual grounding is like connecting the dots between what we see and say. Its about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions.""" .

mcro:metallamaLlama3211BVisionInstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)." .

mcro:metallamaLlama3211BVisionInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM." .

mcro:metallamaLlama3211BVisionInstruct-OutOfScope a mcro:OutOfScopeUseCaseSectionInformation ;
    prov1:hasTextValue "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card." .

mcro:metallamaLlama3211BVisionInstruct-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.

**Data Freshness:** The pretraining data has a cutoff of December 2023.""" .

mcro:metallamaLlama323BInstruct-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)." .

mcro:metallamaLlama323BInstruct-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources." .

mcro:metallamaLlama323BInstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)." .

mcro:metallamaLlama323BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:metallamaLlama323BInstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:metallamaLlama323BInstruct-License ;
    mcro:hasModelArchitecture mcro:metallamaLlama323BInstruct-ModelArchitecture .

mcro:metallamaLlama3370BInstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "A custom commercial license, the Llama 3.3 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE)" .

mcro:metallamaLlama3370BInstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:metallamaLlama3370BInstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:metallamaLlama3370BInstruct-License ;
    mcro:hasModelArchitecture mcro:metallamaLlama3370BInstruct-ModelArchitecture .

mcro:metallamaLlama3370BInstruct-OutOfScopeUseCase a mcro:OutOfScopeUseCaseSectionInformation ;
    prov1:hasTextValue "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card" .

mcro:metallamaLlama3370BInstruct-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples." .

mcro:metallamaLlama3370BInstruct-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasOutOfScopeUseCase mcro:metallamaLlama3370BInstruct-OutOfScopeUseCase ;
    prov1:hasTextValue "Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases." .

mcro:metallamaMetaLlama38B-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:metallamaMetaLlama38B-Benchmark a mcro:QuantativeAnalysisSection .

mcro:metallamaMetaLlama38B-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}""" .

mcro:metallamaMetaLlama38B-Consideration a mcro:ConsiderationInformationSection .

mcro:metallamaMetaLlama38B-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)" .

mcro:metallamaMetaLlama38B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:metallamaMetaLlama38B-Architecture ;
    mcro:hasCitation mcro:metallamaMetaLlama38B-Citation ;
    mcro:hasLicense mcro:metallamaMetaLlama38B-License .

mcro:metallamaMetaLlama38B-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data." .

mcro:metallamaMetaLlama38B-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks." .

mcro:microsoftFlorence2base-License a mcro:LicenseInformationSection .

mcro:microsoftFlorence2base-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:microsoftFlorence2base-License .

mcro:microsoftFlorence2large-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{xiao2023florence,
  title={Florence-2: Advancing a unified representation for a variety of vision tasks},
  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},
  journal={arXiv preprint arXiv:2311.06242},
  year={2023}
}""" .

mcro:microsoftFlorence2large-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "FLD-5B dataset, containing 5.4 billion annotations across 126 million images" .

mcro:microsoftFlorence2large-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks. Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model." .

mcro:microsoftFlorence2large-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model is capable of performing different tasks through changing the prompts. Tasks include Caption, Detailed Caption, More Detailed Caption, Caption to Phrase Grounding, Object Detection, Dense Region Caption, Region proposal, OCR, OCR with Region, Output confidence score with Object Detection" .

mcro:microsoftPhi35miniinstruct-Dataset a mcro:DatasetInformationSection .

mcro:microsoftPhi35miniinstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The model is licensed under the [MIT license](./LICENSE)." .

mcro:microsoftPhi35miniinstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "**Architecture:** Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini." .

mcro:microsoftPhi35miniinstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasUseCase mcro:microsoftPhi35miniinstruct-UseCase .

mcro:microsoftPhi35miniinstruct-ModelParameter a mcro:ModelParameterSection ;
    mcro:hasDataset mcro:microsoftPhi35miniinstruct-Dataset ;
    mcro:hasModelArchitecture mcro:microsoftPhi35miniinstruct-ModelArchitecture .

mcro:microsoftPhi35miniinstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:

1) Memory/compute constrained environments
2) Latency bound scenarios
3) Strong reasoning (especially code, math and logic)

Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.""" .

mcro:microsoftPhi35visioninstruct-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The model is licensed under the [MIT license](./LICENSE)." .

mcro:microsoftPhi35visioninstruct-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """**Architecture:** Phi-3.5-vision has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.<br>
**Inputs:** Text and Image. Its best suited for prompts using the chat format.<br>
**Context length:** 128K tokens<br>
**GPUs:** 256 A100-80G<br>
**Training time:** 6 days<br>
**Training data:** 500B tokens (vision tokens + text tokens)<br>
**Outputs:** Generated text in response to the input<br>
**Dates:** Trained between July and August 2024<br>
**Status:** This is a static model trained on an offline text dataset with cutoff date March 15, 2024. Future versions of the tuned models may be released as we improve models.<br>
**Release date:** August 2024<br>""" .

mcro:microsoftPhi35visioninstruct-ModelDetailSection a mcro:ModelDetailSection ;
    prov1:hasTextValue """Phi-3.5-vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.

 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>
 [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>
 [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>
 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>
 [Try It](https://aka.ms/try-phi3.5vision) <br>

**Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)""" .

mcro:microsoftPhi35visioninstruct-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """Our training data includes a wide variety of sources, and is a combination of 
1) publicly available documents filtered rigorously for quality, selected high-quality educational data and code;
2) selected high-quality image-text interleave data;
3) newly created synthetic, textbook-like data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides, newly created multi-image and video data, e.g., short video clips/pair of two similar images;
4) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.

The data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).""" .

mcro:microsoftPhi35visioninstruct-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require:

1) Memory/compute constrained environments
2) Latency bound scenarios
3) General image understanding
4) Optical character recognition
5) Chart and table understanding
6) Multiple image comparison
7) Multi-image or video clip summarization

Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.

### Use Case Considerations

Our models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.

***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.***""" .

mcro:microsoftPhi3mini128kinstruct-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Our training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of 1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) Newly created synthetic, textbook-like data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness." .

mcro:microsoftPhi3mini128kinstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The model is licensed under the MIT license." .

mcro:microsoftPhi3mini128kinstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Phi-3 Mini-128K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines." .

mcro:microsoftPhi3mini128kinstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:microsoftPhi3mini128kinstruct-License ;
    mcro:hasModelArchitecture mcro:microsoftPhi3mini128kinstruct-ModelArchitecture .

mcro:microsoftPhi3mini128kinstruct-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The model is intended for commercial and research use in English. The model provides uses for applications which require:

1) Memory/compute constrained environments
2) Latency bound scenarios
3) Strong reasoning (especially code, math and logic)""" .

mcro:microsoftPhi3mini4kinstruct-Benchmark a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue """We report the results under completion format for Phi-3-Mini-4K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT3.5-Turbo-1106.

All the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.

As is now standard, we use few-shot prompts to evaluate the models, at temperature 0. 
The prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.
More specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.

The number of kshot examples is listed per-benchmark. 

| Category | Benchmark | Phi-3-Mini-4K-Ins | Gemma-7B | Mistral-7b | Mixtral-8x7b | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |
|:----------|:-----------|:-------------------|:----------|:------------|:--------------|:----------------|:-------------------|
| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.0 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |
| | MMLU <br>5-shot | 70.9 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |
| | BigBench Hard CoT<br>3-shot| 73.5 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |
| Language Understanding | ANLI <br>7-shot | 53.6 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |
| | HellaSwag <br>5-shot| 75.3 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |
| Reasoning | ARC Challenge <br>10-shot | 86.3 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |
| | BoolQ <br>0-shot | 78.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |
| | MedQA <br>2-shot| 56.5 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |
| | OpenBookQA <br>10-shot| 82.2 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |
| | PIQA <br>5-shot| 83.5 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |
| | GPQA <br>0-shot| 30.6 | 2.9 | 15 | 6.9 | 32.4 | 30.8 |
| | Social IQA <br>5-shot| 77.6 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |
| | TruthfulQA (MC2) <br>10-shot| 64.7 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |
| | WinoGrande <br>5-shot| 71.6 | 55.6 | 54.2 | 62 | 65 | 68.8 |
| Factual Knowledge | TriviaQA <br>5-shot| 61.4 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |
| Math | GSM8K CoT <br>8-shot| 85.7 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |
| Code Generation | HumanEval <br>0-shot| 57.3 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |
| | MBPP <br>3-shot| 69.8 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |
| **Average** | | **67.6** | **56.0** | **56.4** | **64.4** | **65.5** | **70.4** |


We take a closer look at different categories across 100 public benchmark datasets at the table below: 

| Category | Phi-3-Mini-4K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |
|:----------|:------------------------|:----------|:------------|:--------------|:---------------------|:---------------|
| Popular aggregated benchmark | 61.1 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |
| Reasoning | 70.8 | 60.3 | 62.8 | 68.1 | 69.6 | 71.8 |
| Language understanding | 60.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |
| Code generation | 60.7 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |
| Math | 50.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |
| Factual knowledge | 38.4 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |
| Multilingual | 56.7 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |
| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |


Overall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine. """ .

mcro:microsoftPhi3mini4kinstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-mini-4k/resolve/main/LICENSE)." .

mcro:microsoftPhi3mini4kinstruct-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.
The model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) that it can support.

The model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.
When assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.""" .

mcro:microsoftPhi3mini4kinstruct-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """Our training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of 
1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 
2) Newly created synthetic, textbook-like data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 
3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.

We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).""" .

mcro:microsoftPhi3mini4kinstruct-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """**Primary use cases**

The model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require 
1) memory/compute constrained environments; 
2) latency bound scenarios; 
3) strong reasoning (especially math and logic). 

Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.

**Out-of-scope use cases**

Our models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  

Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  

**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**""" .

mcro:microsoftPhi4multimodalinstruct-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model is intended for broad multilingual and multimodal commercial and research use." .

mcro:microsoftPhi4multimodalinstruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The model is licensed under the MIT license." .

mcro:microsoftPhi4multimodalinstruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Phi-4-multimodal-instruct has 5.6B parameters and is a multimodal transformer model." .

mcro:microsoftPhi4multimodalinstruct-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:microsoftPhi4multimodalinstruct-License ;
    mcro:hasModelArchitecture mcro:microsoftPhi4multimodalinstruct-ModelArchitecture ;
    mcro:hasTrainingData mcro:microsoftPhi4multimodalinstruct-TrainingData .

mcro:microsoftPhi4multimodalinstruct-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "5T tokens, 2.3M speech hours, and 1.1T image-text tokens" .

mcro:microsoftbeitlargepatch16224-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "BEIT: BERT Pre-Training of Image Transformers" .

mcro:microsoftbeitlargepatch16224-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "image classification" .

mcro:microsoftbeitlargepatch16224-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Vision Transformer (ViT)" .

mcro:microsoftbeitlargepatch16224-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:microsoftbeitlargepatch16224-Citation ;
    mcro:hasModelArchitecture mcro:microsoftbeitlargepatch16224-ModelArchitecture .

mcro:microsoftbeitlargepatch16224-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "ImageNet",
        "ImageNet-21k" .

mcro:microsoftcodebertbasemlm-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "masked-language-modeling" .

mcro:microsoftcodebertbasemlm-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{zhou2023codebertscore,
  url = {https://arxiv.org/abs/2302.05527},
  author = {Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham},
  title = {CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code},
  publisher = {arXiv},
  year = {2023},
}""" .

mcro:microsoftcodebertbasemlm-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "codeparrot/github-code-clean" .

mcro:microsoftcodebertbasemlm-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "CodeBERTScore" .

mcro:microsoftdebertalargemnli-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}""" .

mcro:microsoftdebertalargemnli-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "DeBERTa: Decoding-enhanced BERT with Disentangled Attention" .

mcro:microsoftdebertav3base-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{he2021debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:microsoftdebertav3base-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}""" .

mcro:microsoftdebertav3base-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2." .

mcro:microsoftdebertav3large-Citation a mcro:CitationInformationSection .

mcro:microsoftdebertav3large-Dataset a mcro:DatasetInformationSection .

mcro:microsoftdebertav3large-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:microsoftdebertav3large-UseCase a mcro:UseCaseInformationSection .

mcro:microsoftdebertaxlargemnli-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}""" .

mcro:microsoftdebertaxlargemnli-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "DeBERTa: Decoding-enhanced BERT with Disentangled Attention" .

mcro:microsoftdebertaxlargemnli-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "fine-tuned with mnli task" .

mcro:microsoftmdebertav3base-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{he2021debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}""" .

mcro:microsoftmdebertav3base-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "This model was trained using the 2.5T CC100 data as XLM-R." .

mcro:microsoftmdebertav3base-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """mDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.
The mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R.""" .

mcro:microsoftmdebertav3base-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Fine-tuning on NLU tasks

We present the dev results on XNLI with zero-shot cross-lingual transfer setting, i.e. training with English data only, test on other languages.

| Model        |avg | en |  fr| es  | de  | el  | bg  | ru  |tr   |ar   |vi   | th  | zh | hi  | sw  | ur  |
|--------------| ----|----|----|---- |--   |--   |--   | --  |--   |--   |--   | --  | -- | --  | --  | --  |
| XLM-R-base   |76.2 |85.8|79.7|80.7 |78.7 |77.5 |79.6 |78.1 |74.2 |73.8 |76.5 |74.6 |76.7| 72.4| 66.5| 68.3|
| mDeBERTa-base|**79.8**+/-0.2|**88.2**|**82.6**|**84.4** |**82.7** |**82.3** |**82.4** |**80.8** |**79.5** |**78.5** |**78.1** |**76.4** |**79.5**| **75.9**| **73.9**| **72.4**|""" .

mcro:microsoftphi2-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The model is licensed under the MIT license" .

mcro:microsoftphi2-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.

* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.

* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.

* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.

* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring training data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.

* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.

* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.""" .

mcro:microsoftphi2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-based model with next-word prediction objective" .

mcro:microsoftphi2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:microsoftphi2-License ;
    mcro:hasModelArchitectureInformation mcro:microsoftphi2-ModelArchitecture .

mcro:microsoftphi2-Training a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """* Architecture: a Transformer-based model with next-word prediction objective

* Context length: 2048 tokens

* Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.

* Training tokens: 1.4T tokens

* GPUs: 96xA100-80G

* Training time: 14 days""" .

mcro:microsoftphi2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format." .

mcro:microsoftwavlmbaseplussv-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """[Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900)

Authors: Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei

**Abstract**
*Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.*""" .

mcro:microsoftwavlmbaseplussv-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)
- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)
- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)""" .

mcro:microsoftwavlmbaseplussv-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The official license can be found [here](https://github.com/microsoft/UniSpeech/blob/main/LICENSE)" .

mcro:microsoftwavlmbaseplussv-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The model was pretrained on 16kHz sampled speech audio with utterance and speaker contrastive loss." .

mcro:microsoftwavlmbaseplussv-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Speaker Verification" .

mcro:mimi-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "audio neural codec" .

mcro:mimi-Citation a mcro:CitationInformationSection .

mcro:mimi-Consideration a mcro:ConsiderationInformationSection .

mcro:mimi-IntendedUseCase a mcro:PrimaryIntendedUseCaseInformationSection .

mcro:mimi-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CC-BY" .

mcro:mimi-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:mimi-Architecture ;
    mcro:hasLicense mcro:mimi-License .

mcro:mimi-OutOfScopeUseCase a mcro:OutOfScopeUseCaseSectionInformation .

mcro:mimi-TrainingData a mcro:TrainingDataInformationSection .

mcro:mimi-UseCase a mcro:UseCaseInformationSection .

mcro:mistralaiMistral7BInstructv02-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "For full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/)." .

mcro:mistralaiMistral7BInstructv02-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue """The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.""" .

mcro:mistralaiMistral7BInstructv02-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.""" .

mcro:mistralaiMistral7BInstructv02-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.

Mistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1
- 32k context window (vs 8k context in v0.1)
- Rope-theta = 1e6
- No Sliding-Window Attention

For full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).""" .

mcro:mistralaiMistral7BInstructv02-Owner a mcro:OwnerInformationSection ;
    prov1:hasTextValue """The Mistral AI Team

Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Llio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thophile Gervet, Thibaut Lavril, Thomas Wang, Timothe Lacroix, William El Sayed.""" .

mcro:mistralaiMistral7BInstructv02-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "In order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id." .

mcro:mistralaiMistral7BInstructv03-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.
It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.""" .

mcro:mistralaiMistral7BInstructv03-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Large Language Model (LLM)" .

mcro:mistralaiMistral7BInstructv03-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:mistralaiMistral7BInstructv03-ModelArchitecture ;
    prov1:hasTextValue "The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3." .

mcro:mistralaiMistral7BInstructv03-OwnerInformation a mcro:OwnerInformationSection ;
    prov1:hasTextValue "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Llio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothe Lacroix, Thophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall" .

mcro:mistralaiMistral7Bv01-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "Mistral 7B is a pretrained base model and therefore does not have any moderation mechanisms." .

mcro:mistralaiMistral7Bv01-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Mistral-7B-v0.1 is a transformer model, with the following architecture choices:
- Grouped-Query Attention
- Sliding-Window Attention
- Byte-fallback BPE tokenizer""" .

mcro:mistralaiMixtral8x7BInstructv01-ArchitectureSection a mcro:ModelArchitectureInformationSection .

mcro:mistralaiMixtral8x7BInstructv01-ConsiderationSection a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue """It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.""" .

mcro:mistralaiMixtral8x7BInstructv01-LicenseSection a mcro:LicenseInformationSection .

mcro:mistralaiMixtral8x7BInstructv01-LimitationSection a mcro:LimitationInformationSection .

mcro:mistralaiMixtral8x7BInstructv01-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:mistralaiMixtral8x7BInstructv01-ArchitectureSection ;
    mcro:hasLicense mcro:mistralaiMixtral8x7BInstructv01-LicenseSection .

mcro:mistralaiMixtral8x7BInstructv01-UseCaseSection a mcro:UseCaseInformationSection .

mcro:mixedbreadaimxbaiembedlargev1-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@online{emb2024mxbai,
  title={Open Source Strikes Bread - New Fluffy Embeddings Model},
  author={Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp},
  year={2024},
  url={https://www.mixedbread.ai/blog/mxbai-embed-large-v1},
}

@article{li2023angle,
  title={AnglE-optimized Text Embeddings},
  author={Li, Xianming and Li, Jing},
  journal={arXiv preprint arXiv:2309.12871},
  year={2023}
}""" .

mcro:mixedbreadaimxbaiembedlargev1-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:mixedbreadaimxbaiembedlargev1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Here, we provide several ways to produce sentence embeddings. Please note that you have to provide the prompt `Represent this sentence for searching relevant passages:` for query if you want to use it for retrieval. Besides that you don't need any prompt. Our model also supports [Matryoshka Representation Learning and binary quantization](https://www.mixedbread.ai/blog/binary-mrl)." .

mcro:mixedbreadaimxbaiembedlargev1-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Here, we provide several ways to produce sentence embeddings. Please note that you have to provide the prompt `Represent this sentence for searching relevant passages: ` for query if you want to use it for retrieval. Besides that you don't need any prompt." .

mcro:mlxcommunitygemma312bitqat4bit-Citation a mcro:CitationInformationSection .

mcro:mlxcommunitygemma312bitqat4bit-License a mcro:LicenseInformationSection .

mcro:mlxcommunitygemma312bitqat4bit-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:mlxcommunitygemma312bitqat4bit-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:mlxcommunitygemma312bitqat4bit-Citation ;
    mcro:hasLicense mcro:mlxcommunitygemma312bitqat4bit-License ;
    mcro:hasModelArchitecture mcro:mlxcommunitygemma312bitqat4bit-ModelArchitecture .

mcro:mlxcommunitygemma312bitqat4bit-UseCase a mcro:UseCaseInformationSection .

mcro:mmlwretrievalrobertalarge-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{dadas2024pirb,
  title={{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods}, 
  author={Sawomir Dadas and Micha Perekiewicz and Rafa Powiata},
  year={2024},
  eprint={2402.13350},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}""" .

mcro:mmlwretrievalrobertalarge-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "[Polish MS MARCO](https://huggingface.co/datasets/clarin-knext/msmarco-pl)" .

mcro:mmlwretrievalrobertalarge-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """MMLW (musz mie lepsz wiadomo) are neural text encoders for Polish.
This model is optimized for information retrieval tasks. It can transform queries and passages to 1024 dimensional vectors. 
The model was developed using a two-step procedure: 
- In the first step, it was initialized with Polish RoBERTa checkpoint, and then trained with [multilingual knowledge distillation method](https://aclanthology.org/2020.emnlp-main.365/) on a diverse corpus of 60 million Polish-English text pairs. We utilised [English FlagEmbeddings (BGE)](https://huggingface.co/BAAI/bge-large-en) as teacher models for distillation. 
- The second step involved fine-tuning the obtained models with contrastrive loss on [Polish MS MARCO](https://huggingface.co/datasets/clarin-knext/msmarco-pl) training split. In order to improve the efficiency of contrastive training, we used large batch sizes - 1152 for small, 768 for base, and 288 for large models. Fine-tuning was conducted on a cluster of 12 A100 GPUs.""" .

mcro:mmlwretrievalrobertalarge-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:mobilebertuncased-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks." .

mcro:mobilenetv3small100lambin1k-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:mobilenetv3small100lambin1k-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}
}
""" .

mcro:mobilenetv3small100lambin1k-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1314--1324},
  year={2019}
}
""" .

mcro:mobilenetv3small100lambin1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:mobilenetv3small100lambin1k-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "https://github.com/huggingface/pytorch-image-models" .

mcro:mobilenetv3small100lambin1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:mobilenetv3small100lambin1k-Citation,
        mcro:mobilenetv3small100lambin1k-Citation2 ;
    mcro:hasLicense mcro:mobilenetv3small100lambin1k-License ;
    mcro:hasModelArchitecture mcro:mobilenetv3small100lambin1k-Architecture .

mcro:mobilenetv3small100lambin1k-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Image Classification" .

mcro:mobilenetv3small100lambin1k-UseCase2 a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Feature Map Extraction" .

mcro:mobilenetv3small100lambin1k-UseCase3 a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Image Embeddings" .

mcro:mobilevitsmall-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers." .

mcro:mobilevitsmall-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{vision-transformer,
title = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},
author = {Sachin Mehta and Mohammad Rastegari},
year = {2022},
URL = {https://arxiv.org/abs/2110.02178}
}""" .

mcro:mobilevitsmall-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apple sample code license" .

mcro:mobilevitsmall-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:mobilevitsmall-License ;
    mcro:hasModelArchitecture mcro:mobilevitsmall-Architecture .

mcro:mobilevitsmall-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The MobileViT model was pretrained on ImageNet-1k, a dataset consisting of 1 million images and 1,000 classes." .

mcro:mobilevitsmall-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for image classification." .

mcro:modelcard123-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Citation Text" .

mcro:modelcard123-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet" .

mcro:modelcard123-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "mit" .

mcro:modelcard123-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CNN" .

mcro:modelcard123-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Image classification" .

mcro:modelid-Citation a mcro:CitationInformationSection .

mcro:modelid-Evaluation a mcro:EvaluationDataInformationSection .

mcro:modelid-License a mcro:LicenseInformationSection .

mcro:modelid-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:modelid-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:modelid-Citation ;
    mcro:hasLicense mcro:modelid-License .

mcro:modelid-TrainingData a mcro:TrainingDataInformationSection .

mcro:modelid-UseCase a mcro:UseCaseInformationSection .

mcro:moondream2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Moondream is a small vision language model designed to run efficiently everywhere." .

mcro:moondream2-Version a mcro:VersionInformationSection ;
    prov1:hasTextValue "This repository contains the latest (**2025-04-14**) release of Moondream, as well as [historical releases](https://huggingface.co/vikhyatk/moondream2/blob/main/versions.txt). The model is updated frequently, so we recommend specifying a revision as shown below if you're using it in a production application." .

mcro:mradermacherDeepSeekV2LiteGGUF-License a mcro:LicenseInformationSection .

mcro:mradermacherDeepSeekV2LiteGGUF-QuantativeAnalysis a mcro:QuantativeAnalysisSection .

mcro:multilinguale5large-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{wang2024multilingual,
  title={Multilingual E5 Text Embeddings: A Technical Report},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2402.05672},
  year={2024}
}""" .

mcro:multilinguale5large-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """This model is initialized from [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)
and continually trained on a mixture of multilingual datasets.
It supports 100 languages from xlm-roberta,
but low-resource languages may see performance degradation.""" .

mcro:multilinguale5large-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Long texts will be truncated to at most 512 tokens." .

mcro:multilinguale5large-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This model has 24 layers and the embedding size is 1024." .

mcro:multilinguale5large-TrainingDetails a mcro:DatasetInformationSection ;
    prov1:hasTextValue """**Initialization**: [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)

**First stage**: contrastive pre-training with weak supervision

| Dataset                                                                                                | Weak supervision                      | # of text pairs |
|--------------------------------------------------------------------------------------------------------|---------------------------------------|-----------------|
| Filtered [mC4](https://huggingface.co/datasets/mc4)                                                    | (title, page content)                 | 1B              |
| [CC News](https://huggingface.co/datasets/intfloat/multilingual_cc_news)                               | (title, news content)                 | 400M            |
| [NLLB](https://huggingface.co/datasets/allenai/nllb)                                                   | translation pairs                     | 2.4B            |
| [Wikipedia](https://huggingface.co/datasets/intfloat/wikipedia)                                        | (hierarchical section title, passage) | 150M            |
| Filtered [Reddit](https://www.reddit.com/)                                                             | (comment, response)                   | 800M            |
| [S2ORC](https://github.com/allenai/s2orc)                                                              | (title, abstract) and citation pairs  | 100M            |
| [Stackexchange](https://stackexchange.com/)                                                            | (question, answer)                    | 50M             |
| [xP3](https://huggingface.co/datasets/bigscience/xP3)                                                  | (input prompt, response)              | 80M             |
| [Miscellaneous unsupervised SBERT data](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | -                                     | 10M             |

**Second stage**: supervised fine-tuning

| Dataset                                                                                | Language     | # of text pairs |
|----------------------------------------------------------------------------------------|--------------|-----------------|
| [MS MARCO](https://microsoft.github.io/msmarco/)                                       | English      | 500k            |
| [NQ](https://github.com/facebookresearch/DPR)                                          | English      | 70k             |
| [Trivia QA](https://github.com/facebookresearch/DPR)                                   | English      | 60k             |
| [NLI from SimCSE](https://github.com/princeton-nlp/SimCSE)                             | English      | <300k           |
| [ELI5](https://huggingface.co/datasets/eli5)                                           | English      | 500k            |
| [DuReader Retrieval](https://github.com/baidu/DuReader/tree/master/DuReader-Retrieval) | Chinese      | 86k             |
| [KILT Fever](https://huggingface.co/datasets/kilt_tasks)                               | English      | 70k             |
| [KILT HotpotQA](https://huggingface.co/datasets/kilt_tasks)                            | English      | 70k             |
| [SQuAD](https://huggingface.co/datasets/squad)                                         | English      | 87k             |
| [Quora](https://huggingface.co/datasets/quora)                                         | English      | 150k            |
| [Mr. TyDi](https://huggingface.co/datasets/castorini/mr-tydi)                                                                           | 11 languages | 50k             |
| [MIRACL](https://huggingface.co/datasets/miracl/miracl)                                                                             | 16 languages | 40k             |

For all labeled datasets, we only use its training set for fine-tuning.

For other training details, please refer to our paper at [https://arxiv.org/pdf/2402.05672](https://arxiv.org/pdf/2402.05672).""" .

mcro:multilinguale5large-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.

import torch.nn.functional as F

from torch import Tensor
from transformers import AutoTokenizer, AutoModel


def average_pool(last_hidden_states: Tensor,
                 attention_mask: Tensor) -> Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]


# Each input text should start with "query: " or "passage: ", even for non-English texts.
# For tasks other than retrieval, you can simply use the "query: " prefix.
input_texts = ['query: how much protein should a female eat',
               'query: ',
               "passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.",
               "passage: 1. : : : 1, 2() 3, 4, 2. :1 : : 1, 28, 3,, 4,, 5, 6, 7,"]

tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')
model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')

# Tokenize the input texts
batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')

outputs = model(**batch_dict)
embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])

# normalize embeddings
embeddings = F.normalize(embeddings, p=2, dim=1)
scores = (embeddings[:2] @ embeddings[2:].T) * 100
print(scores.tolist())
""" .

mcro:multiqaminiLML6cosv1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for **semantic search**. It has been trained on 215M (question, answer) pairs from diverse sources." .

mcro:multiqaminiLML6cosv1-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """We use the concatenation from multiple datasets to fine-tune our model. In total we have about 215M (question, answer) pairs.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.""" .

mcro:multiqaminiLML6cosv1-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Our model is intented to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages." .

mcro:mxbai-rerank-xsmall-v1-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@online{rerank2024mxbai,
  title={Boost Your Search With The Crispy Mixedbread Rerank Models},
  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},
  year={2024},
  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},
}""" .

mcro:mxbai-rerank-xsmall-v1-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:mxbai-rerank-xsmall-v1-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:mxbai-rerank-xsmall-v1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:mxbai-rerank-xsmall-v1-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:myshellaiMeloTTS-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@software{zhao2024melo,
  author={Zhao, Wenliang and Yu, Xumin and Qin, Zengyi},
  title = {MeloTTS: High-quality Multi-lingual Multi-accent Text-to-Speech},
  url = {https://github.com/myshell-ai/MeloTTS},
  year = {2023}
}""" .

mcro:myshellaiMeloTTS-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "This library is under MIT License, which means it is free for both commercial and non-commercial use." .

mcro:myshellaiMeloTTS-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MeloTTS is a **high-quality multi-lingual** text-to-speech library by [MIT](https://www.mit.edu/) and [MyShell.ai](https://myshell.ai)." .

mcro:myshellaiMeloTTS-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "MeloTTS is a **high-quality multi-lingual** text-to-speech library" .

mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Consideration a mcro:ConsiderationInformationSection .

mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Dataset a mcro:DatasetInformationSection .

mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-License a mcro:LicenseInformationSection .

mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-License ;
    mcro:hasModelArchitecture mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelArchitecture .

mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-QuantativeAnalysis a mcro:QuantativeAnalysisSection .

mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-UseCase a mcro:UseCaseInformationSection .

mcro:nguyenvulebinhwav2vec2basevi-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """Our self-supervised model is pre-trained on a massive audio set of 13k hours of Vietnamese youtube audio, which includes:
  - Clean audio
  - Noise audio
  - Conversation
  - Multi-gender and dialects""" .

mcro:nguyenvulebinhwav2vec2basevi-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "We use wav2vec2 architecture for doing Self-Supervised learning" .

mcro:nguyenvulebinhwav2vec2basevi-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Since our model has the same architecture as the English wav2vec2 version, you can use [this notebook](https://colab.research.google.com/drive/1FjTsqbYKphl9kL-eILgUc-bl4zVThL8F?usp=sharing) for more information on how to fine-tune the model." .

mcro:nlpconnectvitgpt2imagecaptioning-License a mcro:LicenseInformationSection .

mcro:nlpconnectvitgpt2imagecaptioning-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:nlpconnectvitgpt2imagecaptioning-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:nlpconnectvitgpt2imagecaptioning-License .

mcro:nomicainomicembedtextv1-CitationInformationSection a mcro:CitationInformationSection .

mcro:nomicainomicembedtextv1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:nomicainomicembedtextv1-TrainingDataInformationSection a mcro:TrainingDataInformationSection .

mcro:nomicainomicembedtextv1-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:nomicainomicembedtextv15-Arch a mcro:ModelArchitectureInformationSection .

mcro:nomicainomicembedtextv15-Citation a mcro:CitationInformationSection .

mcro:nomicainomicembedtextv15-UseCase a mcro:UseCaseInformationSection .

mcro:nvidiatitanetlargeenus-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """[1] [TitaNet: Neural Model for Speaker Representation with 1D Depth-wise Separable convolutions and global context](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746806) 
[2] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)""" .

mcro:nvidiatitanetlargeenus-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """All the models in this collection are trained on a composite dataset comprising several thousand hours of English speech:

- Voxceleb-1
- Voxceleb-2
- Fisher
- Switchboard
- Librispeech
- SRE (2004-2010)""" .

mcro:nvidiatitanetlargeenus-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "License to use this model is covered by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). By downloading the public and release version of the model, you accept the terms and conditions of the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license." .

mcro:nvidiatitanetlargeenus-LimitationInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "This model is trained on both telephonic and non-telephonic speech from voxceleb datasets, Fisher and switch board. If your domain of data differs from trained data or doesnot show relatively good performance consider finetuning for that speech domain." .

mcro:nvidiatitanetlargeenus-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "TitaNet model is a depth-wise separable conv1D model [1] for Speaker Verification and diarization tasks." .

mcro:nvidiatitanetlargeenus-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model extracts speaker embeddings from given speech, which is the backbone for speaker verification and diarization tasks." .

mcro:obideidrobertai2b2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "* A RoBERTa" .

mcro:obideidrobertai2b2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "* The I2B2 2014" .

mcro:obideidrobertai2b2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:obideidrobertai2b2-Citation .

mcro:obideidrobertai2b2-ModelParameter a mcro:ModelParameterSection ;
    prov1:hasTextValue "* Training details:" .

mcro:obideidrobertai2b2-Training a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "* Steps on how this model was trained can be found here: [Training](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/train). The \"model_name_or_path\" was set to: \"roberta-large\"." .

mcro:obideidrobertai2b2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "* A demo on how the model works (using model predictions to de-identify a medical note) is on this space:" .

mcro:ocr-equation-images-and-text-to-latex-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "See [texify](https://github.com/VikParuchuri/texify)." .

mcro:oliverguhrfullstoppunctuationmultilanglarge-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "confusion matrix t/p 0 . , ? - : 0 1.0 0.0 0.0 0.0 0.0 0.0 . 0.0 1.0 0.0 0.0 0.0 0.0 , 0.1 0.0 0.9 0.0 0.0 0.0 ? 0.0 0.1 0.0 0.8 0.0 0.0 - 0.1 0.1 0.5 0.0 0.3 0.0 : 0.0 0.3 0.1 0.0 0.0 0.5" .

mcro:oliverguhrfullstoppunctuationmultilanglarge-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian" .

mcro:oliverguhrfullstoppunctuationmultilanglarge-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "This multilanguage model was trained on the [Europarl Dataset](https://huggingface.co/datasets/wmt/europarl) provided by the [SEPP-NLG Shared Task](https://sites.google.com/view/sentence-segmentation). *Please note that this dataset consists of political speeches. Therefore the model might perform differently on texts from other domains.*" .

mcro:oliverguhrfullstoppunctuationmultilanglarge-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "xlm-roberta-base" .

mcro:oliverguhrfullstoppunctuationmultilanglarge-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "precision recall f1-score support 0 0.99 0.99 0.99 73317475 . 0.94 0.95 0.95 4484845 , 0.86 0.86 0.86 6100650 ? 0.88 0.85 0.86 136479 - 0.60 0.29 0.39 233630 : 0.71 0.49 0.58 152424 accuracy 0.98 84425503 macro avg 0.83 0.74 0.77 84425503 weighted avg 0.98 0.98 0.98 84425503" .

mcro:oliverguhrfullstoppunctuationmultilanglarge-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model predicts the punctuation of English, Italian, French and German texts. We developed it to restore the punctuation of transcribed spoken language." .

mcro:openaiclip-Citation a mcro:CitationInformationSection .

mcro:openaiclip-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasEthicalConsideration mcro:openaiclip-EthicalConsideration .

mcro:openaiclip-Dataset a mcro:DatasetInformationSection ;
    mcro:hasTrainingData mcro:openaiclip-TrainingData .

mcro:openaiclip-EthicalConsideration a mcro:EthicalConsiderationSection .

mcro:openaiclip-Limitation a mcro:LimitationInformationSection .

mcro:openaiclip-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:openaiclip-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:openaiclip-Citation ;
    mcro:hasModelArchitecture mcro:openaiclip-ModelArchitecture .

mcro:openaiclip-OutOfScopeUseCase a mcro:OutOfScopeUseCaseSectionInformation .

mcro:openaiclip-PerformanceMetric a mcro:PerformanceMetricInformationSection .

mcro:openaiclip-PrimaryIntendedUseCase a mcro:PrimaryIntendedUseCaseInformationSection .

mcro:openaiclip-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    mcro:hasPerformanceMetric mcro:openaiclip-PerformanceMetric .

mcro:openaiclip-TrainingData a mcro:TrainingDataInformationSection .

mcro:openaiclip-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasOutOfScopeUseCase mcro:openaiclip-OutOfScopeUseCase ;
    mcro:hasPrimaryIntendedUseCase mcro:openaiclip-PrimaryIntendedUseCase .

mcro:openaiwhisperlargev3turbo-Citation a mcro:CitationInformationSection .

mcro:openaiwhisperlargev3turbo-License a mcro:LicenseInformationSection .

mcro:openaiwhisperlargev3turbo-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:openaiwhisperlargev3turbo-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:openaiwhisperlargev3turbo-Citation ;
    mcro:hasLicense mcro:openaiwhisperlargev3turbo-License ;
    mcro:hasModelArchitecture mcro:openaiwhisperlargev3turbo-ModelArchitecture .

mcro:openaiwhisperlargev3turbo-UseCase a mcro:UseCaseInformationSection .

mcro:openbmbMiniCPMo26-CitationInformationSection a mcro:CitationInformationSection .

mcro:openbmbMiniCPMo26-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:openbmbMiniCPMo26-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers" .

mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "MS MARCO",
        "S2ORC_title_abstract",
        "WikiAnswers",
        "eli5_question_answer",
        "gooaq_pairs",
        "searchQA_top5_snippets",
        "squad_pairs",
        "stackexchange_duplicate_questions_body_body",
        "stackexchange_duplicate_questions_title-body_title-body",
        "stackexchange_duplicate_questions_title_title",
        "wikihow",
        "yahoo_answers_question_answer",
        "yahoo_answers_title_answer",
        "yahoo_answers_title_question" .

mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache v2.0 License" .

mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "sparse vectors" .

mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "learned sparse retrieval" .

mcro:openvla7b-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "[OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)" .

mcro:openvla7b-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "[Open X-Embodiment](https://robotics-transformer-x.github.io/) -- specific component datasets can be found [here](https://github.com/openvla/openvla)." .

mcro:openvla7b-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT" .

mcro:openvla7b-Limitation a mcro:LimitationInformationSection .

mcro:openvla7b-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Vision-language-action (language, image => robot actions)" .

mcro:openvla7b-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasDataset mcro:openvla7b-Dataset ;
    mcro:hasLicense mcro:openvla7b-License ;
    mcro:hasModelArchitecture mcro:openvla7b-ModelArchitecture .

mcro:openvla7b-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasLimitation mcro:openvla7b-Limitation ;
    prov1:hasTextValue """**Out-of-Scope:** OpenVLA models do not zero-shot generalize to new (unseen) robot embodiments, or setups that are not represented in the pretraining mix; in these cases,
we suggest collecting a dataset of demonstrations on the desired setup, and fine-tuning OpenVLA models instead.""",
        """OpenVLA models take a language instruction and a camera image of a robot workspace as input, and predict (normalized) robot actions consisting of 7-DoF end-effector deltas
of the form (x, y, z, roll, pitch, yaw, gripper). To execute on an actual robot platform, actions need to be *un-normalized* subject to statistics computed on a per-robot,
per-dataset basis. See [our repository](https://github.com/openvla/openvla) for more information.

OpenVLA models can be used zero-shot to control robots for specific combinations of embodiments and domains seen in the Open-X pretraining mixture (e.g., for 
[BridgeV2 environments with a Widow-X robot](https://rail-berkeley.github.io/bridgedata/)). They can also be efficiently *fine-tuned* for new tasks and robot setups
given minimal demonstration data; [see here](https://github.com/openvla/openvla/blob/main/scripts/finetune.py).""" .

mcro:opusmtenfr-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "opus" .

mcro:opusmtenfr-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformer-align" .

mcro:opusmtnlen-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "opus" .

mcro:opusmtnlen-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformer-align" .

mcro:opusmtruen-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-align" .

mcro:opusmtruen-Citation a mcro:CitationInformationSection .

mcro:opusmtruen-EvaluationData a mcro:DatasetInformationSection .

mcro:opusmtruen-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CC-BY-4.0" .

mcro:opusmtruen-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:opusmtruen-Citation ;
    mcro:hasLicense mcro:opusmtruen-License .

mcro:opusmtruen-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "* Dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)" .

mcro:opusmtruen-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model can be used for translation and text-to-text generation." .

mcro:owlv2-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The model uses a CLIP backbone with a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective." .

mcro:owlv2-Citation a mcro:CitationInformationSection .

mcro:owlv2-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{minderer2023scaling,
      title={Scaling Open-Vocabulary Object Detection}, 
      author={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},
      year={2023},
      eprint={2306.09683},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:owlv2-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{minderer2023scaling,
      title={Scaling Open-Vocabulary Object Detection}, 
      author={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},
      year={2023},
      eprint={2306.09683},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:owlv2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """The CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html).

(to be updated for v2)""" .

mcro:owlv2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The model uses a CLIP backbone with a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective." .

mcro:owlv2-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:owlv2-Citation,
        mcro:owlv2-Citation2 ;
    mcro:hasModelArchitecture mcro:owlv2-Architecture ;
    prov1:hasTextValue """The OWLv2 model (short for Open-World Localization) was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.

The model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.""" .

mcro:owlv2-ModelDetails a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:owlv2-CitationInformationSection ;
    prov1:hasTextValue "June 2023",
        """The OWLv2 model (short for Open-World Localization) was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.

The model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.""" .

mcro:owlv2-PrimaryIntendedUseCase a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue """The primary intended users of these models are AI researchers.

We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.""" .

mcro:owlv2-PrimaryUse a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue """The primary intended users of these models are AI researchers.

We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.""" .

mcro:owlv2-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasPrimaryIntendedUse mcro:owlv2-PrimaryUse ;
    mcro:hasPrimaryIntendedUseCase mcro:owlv2-PrimaryIntendedUseCase ;
    prov1:hasTextValue "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training." .

mcro:owlvitbasepatch32-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{minderer2022simple,
  title={Simple Open-Vocabulary Object Detection with Vision Transformers},
  author={Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby},
  journal={arXiv preprint arXiv:2205.06230},
  year={2022},
}""" .

mcro:owlvitbasepatch32-Dataset a mcro:DatasetInformationSection ;
    mcro:hasTrainingData mcro:owlvitbasepatch32-TrainingData .

mcro:owlvitbasepatch32-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The model uses a CLIP backbone with a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective." .

mcro:owlvitbasepatch32-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:owlvitbasepatch32-Citation ;
    mcro:hasModelArchitecture mcro:owlvitbasepatch32-ModelArchitecture .

mcro:owlvitbasepatch32-PrimaryIntendedUseCase a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training." .

mcro:owlvitbasepatch32-PrimaryIntendedUser a mcro:PrimaryIntendedUserInformationSection ;
    prov1:hasTextValue "The primary intended users of these models are AI researchers." .

mcro:owlvitbasepatch32-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)." .

mcro:owlvitbasepatch32-UseCase a mcro:UseCaseInformationSection ;
    mcro:hasPrimaryIntendedUseCase mcro:owlvitbasepatch32-PrimaryIntendedUseCase ;
    mcro:hasUser mcro:owlvitbasepatch32-User .

mcro:owlvitbasepatch32-User a mcro:UserInformationSection ;
    mcro:hasPrimaryIntendedUser mcro:owlvitbasepatch32-PrimaryIntendedUser .

mcro:parakeet_rnnt_06b-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """The model was trained on 64K hours of English speech collected and prepared by NVIDIA NeMo and Suno teams.

The training dataset consists of private subset with 40K hours of English speech plus 24K hours from the following public datasets:

- Librispeech 960 hours of English speech
- Fisher Corpus
- Switchboard-1 Dataset
- WSJ-0 and WSJ-1
- National Speech Corpus (Part 1, Part 6)
- VCTK
- VoxPopuli (EN)
- Europarl-ASR (EN)
- Multilingual Librispeech (MLS EN) - 2,000 hour subset
- Mozilla Common Voice (v7.0)
- People's Speech  - 12,000 hour subset""" .

mcro:parakeet_rnnt_06b-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "License to use this model is covered by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). By downloading the public and release version of the model, you accept the terms and conditions of the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license." .

mcro:parakeet_rnnt_06b-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "FastConformer [1] is an optimized version of the Conformer model with 8x depthwise-separable convolutional downsampling. The model is trained in a multitask setup with a Transducer decoder (RNNT) loss. You may find more information on the details of FastConformer here: [Fast-Conformer Model](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer)." .

mcro:parakeet_rnnt_06b-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue """[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)

[2] [Google Sentencepiece Tokenizer](https://github.com/google/sentencepiece)

[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)

[4] [Suno.ai](https://suno.ai/)

[5] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)""" .

mcro:parakeet_rnnt_06b-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """`parakeet-rnnt-0.6b` is an ASR model that transcribes speech in lower case English alphabet. This model is jointly developed by [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) and [Suno.ai](https://www.suno.ai/) teams.
It is an XL version of FastConformer Transducer [1] (around 600M parameters) model.
See the [model architecture](#model-architecture) section and [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) for complete architecture details.""" .

mcro:parakeettdt06bv2-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """**Architecture Type**:

FastConformer-TDT

**Network Architecture**:

* This model was developed based on [FastConformer encoder](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) architecture[1] and TDT decoder[2]
* This model has 600 million model parameters.""" .

mcro:parakeettdt06bv2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """The model was trained on the Granary dataset[8], consisting of approximately 120,000 hours of English speech data:

- 10,000 hours from human-transcribed NeMo ASR Set 3.0, including:
  - LibriSpeech (960 hours)
  - Fisher Corpus
  - National Speech Corpus Part 1
  - VCTK
  - VoxPopuli (English)
  - Europarl-ASR (English)
  - Multilingual LibriSpeech (MLS English)  2,000-hour subset
  - Mozilla Common Voice (v7.0)
  - AMI

- 110,000 hours of pseudo-labeled data from:
  - YTC (YouTube-Commons) dataset[4]
  - YODAS dataset [5]
  - Librilight [7]

All transcriptions preserve punctuation and capitalization. The Granary dataset[8] will be made publicly available after presentation at Interspeech 2025.

**Data Collection Method by dataset**

* Hybrid: Automated, Human

**Labeling Method by dataset**

* Hybrid: Synthetic, Human

**Properties:**

* Noise robust data from various sources
* Single channel, 16kHz sampled data""" .

mcro:parakeettdt06bv2-EvaluationDataset a mcro:EvaluationDataInformationSection ;
    prov1:hasTextValue """Huggingface Open ASR Leaderboard datasets are used to evaluate the performance of this model.

**Data Collection Method by dataset**
* Human

**Labeling Method by dataset**
* Human

**Properties:**

* All are commonly used for benchmarking English ASR systems.
* Audio data is typically processed into a 16kHz mono channel format for ASR evaluation, consistent with benchmarks like the [Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).""" .

mcro:parakeettdt06bv2-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "GOVERNING TERMS: Use of this model is governed by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode.en) license." .

mcro:parakeettdt06bv2-Performance a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue """#### Huggingface Open-ASR-Leaderboard Performance
The performance of Automatic Speech Recognition (ASR) models is measured using Word Error Rate (WER). Given that this model is trained on a large and diverse dataset spanning multiple domains, it is generally more robust and accurate across various types of audio.

### Base Performance
The table below summarizes the WER (%) using a Transducer decoder with greedy decoding (without an external language model):

| **Model** | **Avg WER** | **AMI** | **Earnings-22** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI Speech** | **TEDLIUM-v3** | **VoxPopuli** |
|:-------------|:-------------:|:---------:|:------------------:|:----------------:|:-----------------:|:-----------------:|:------------------:|:----------------:|:---------------:|
| parakeet-tdt-0.6b-v2 | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | -

### Noise Robustness
Performance across different Signal-to-Noise Ratios (SNR) using MUSAN music and noise samples:

| **SNR Level** | **Avg WER** | **AMI** | **Earnings** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI** | **Tedlium** | **VoxPopuli** | **Relative Change** |
|:---------------|:-------------:|:----------:|:------------:|:----------------:|:-----------------:|:-----------------:|:-----------:|:-------------:|:---------------:|:-----------------:|
| Clean | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |
| SNR 50 | 6.04 | 11.11 | 11.12 | 9.74 | 1.70 | 3.18 | 2.18 | 3.34 | 5.98 | +0.25% |
| SNR 25 | 6.50 | 12.76 | 11.50 | 9.98 | 1.78 | 3.63 | 2.54 | 3.46 | 6.34 | -7.04% |
| SNR 5 | 8.39 | 19.33 | 13.83 | 11.28 | 2.36 | 5.50 | 3.91 | 3.91 | 6.96 | -38.11% |

### Telephony Audio Performance
Performance comparison between standard 16kHz audio and telephony-style audio (using -law encoding with 16kHz8kHz16kHz conversion):

| **Audio Format** | **Avg WER** | **AMI** | **Earnings** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI** | **Tedlium** | **VoxPopuli** | **Relative Change** |
|:-----------------|:-------------:|:----------:|:------------:|:----------------:|:-----------------:|:-----------------:|:-----------:|:-------------:|:---------------:|:-----------------:|
| Standard 16kHz | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |
| -law 8kHz | 6.32 | 11.98 | 11.16 | 10.02 | 1.78 | 3.52 | 2.20 | 3.38 | 6.52 | -4.10% |

These WER scores were obtained using greedy decoding without an external language model. Additional evaluation details are available on the [Hugging Face ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).[6]""" .

mcro:parakeettdt06bv2-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue """[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)

[2] [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations](https://arxiv.org/abs/2304.06795)

[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)

[4] [Youtube-commons: A massive open corpus for conversational and multimodal data](https://huggingface.co/blog/Pclanglais/youtube-commons)

[5] [Yodas: Youtube-oriented dataset for audio and speech](https://arxiv.org/abs/2406.00899)

[6] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)

[7] [MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages](https://arxiv.org/abs/2410.01036)

[8] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/pdf/2505.13404)""" .

mcro:parakeettdt06bv2-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """This model was trained using the NeMo toolkit [3], following the strategies below:

- Initialized from a FastConformer SSL checkpoint that was pretrained with a wav2vec method on the LibriLight dataset[7].
- Trained for 150,000 steps on 64 A100 GPUs.
- Dataset corpora were balanced using a temperature sampling value of 0.5.
- Stage 2 fine-tuning was performed for 2,500 steps on 4 A100 GPUs using approximately 500 hours of high-quality, human-transcribed data of NeMo ASR Set 3.0.

Training was conducted using this [example script](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py) and [TDT configuration](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/fastconformer/hybrid_transducer_ctc/fastconformer_hybrid_tdt_ctc_bpe.yaml).

The tokenizer was constructed from the training set transcripts using this [script](https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py).""" .

mcro:parakeettdt06bv2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms." .

mcro:patrickjohncyhfashionclip-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder." .

mcro:patrickjohncyhfashionclip-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@Article{Chia2022,
    title="Contrastive language and vision learning of general fashion concepts",
    author="Chia, Patrick John
            and Attanasio, Giuseppe
            and Bianchi, Federico
            and Terragni, Silvia
            and Magalh{\\~a}es, Ana Rita
            and Goncalves, Diogo
            and Greco, Ciro
            and Tagliabue, Jacopo",
    journal="Scientific Reports",
    year="2022",
    month="Nov",
    day="08",
    volume="12",
    number="1",
    abstract="The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from general and transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model adapted for the fashion industry. We demonstrate the effectiveness of the representations learned by FashionCLIP with extensive tests across a variety of tasks, datasets and generalization probes. We argue that adaptations of large pre-trained models such as CLIP offer new perspectives in terms of scalability and sustainability for certain types of players in the industry. Finally, we detail the costs and environmental impact of training, and release the model weights and code as open source contribution to the community.",
    issn="2045-2322",
    doi="10.1038/s41598-022-23052-9",
    url="https://doi.org/10.1038/s41598-022-23052-9"
}""" .

mcro:patrickjohncyhfashionclip-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., stripes, long sleeves, Armani) and _short description_ (80s styled t-shirt)) available in the Farfetch dataset." .

mcro:patrickjohncyhfashionclip-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:patrickjohncyhfashionclip-Architecture ;
    mcro:hasCitation mcro:patrickjohncyhfashionclip-Citation .

mcro:patrickjohncyhfashionclip-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts." .

mcro:petalsteamStableBeluga2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{StableBelugaModels,
      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)},
      title={Stable Beluga models},
      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}
}""" .

mcro:petalsteamStableBeluga2-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:petalsteamStableBeluga2-Citation3 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{mukherjee2023orca,
      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4},
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:petalsteamStableBeluga2-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "Beluga is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Beluga's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model." .

mcro:petalsteamStableBeluga2-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "` Stable Beluga 2` is trained on our internal Orca-style dataset" .

mcro:petalsteamStableBeluga2-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue "`Stable Beluga 2` is a Llama2 70B model finetuned on an Orca style Dataset" .

mcro:petalsteamStableBeluga2-ModelDetail2 a mcro:ModelDetailSection ;
    prov1:hasTextValue """* **Developed by**: [Stability AI](https://stability.ai/)
* **Model type**: Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.
* **Language(s)**: English
* **Library**: [HuggingFace Transformers](https://github.com/huggingface/transformers)
* **License**: Fine-tuned checkpoints (`Stable Beluga 2`) is licensed under the [STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt)
* **Contact**: For questions and comments about the model, please email `lm@stability.ai`""" .

mcro:petalsteamStableBeluga2-ModelParameter a mcro:ModelParameterSection ;
    prov1:hasTextValue """Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:

| Dataset           | Batch Size | Learning Rate |Learning Rate Decay| Warm-up | Weight Decay | Betas       |
|-------------------|------------|---------------|-------------------|---------|--------------|-------------|
| Orca pt1 packed   | 256        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |
| Orca pt2 unpacked | 512        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |""" .

mcro:petalsteamStableBeluga2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Start chatting with `Stable Beluga 2` using the following code snippet:" .

mcro:phi4-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "14B parameters, dense decoder-only Transformer model" .

mcro:phi4-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "[Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)" .

mcro:phi4-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """Our training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:

1. Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code.

2. Newly created synthetic, textbook-like data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).

3. Acquired academic books and Q&A datasets.

4. High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.

Multilingual data constitutes about 8% of our overall data. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge.""" .

mcro:phi4-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT" .

mcro:phi4-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:phi4-License ;
    mcro:hasModelArchitecture mcro:phi4-Architecture .

mcro:phi4-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Our model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:

1. Memory/compute constrained environments.
2. Latency bound scenarios.
3. Reasoning and logic.""" .

mcro:phobertpretrainedlanguagemodels-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{phobert,
    title     = {{PhoBERT: Pre-trained language models for Vietnamese}},
    author    = {Dat Quoc Nguyen and Anh Tuan Nguyen},
    journal   = {Findings of EMNLP},
    year      = {2020}
    }""" .

mcro:phobertpretrainedlanguagemodels-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "PhoBERT pre-training approach is based on [RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)  which optimizes the [BERT](https://github.com/google-research/bert) pre-training procedure for more robust performance." .

mcro:playgroundaiplaygroundv251024pxaesthetic-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{li2024playground,
      title={Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation}, 
      author={Daiqing Li and Aleks Kamko and Ehsan Akhgari and Ali Sabet and Linmiao Xu and Suhail Doshi},
      year={2024},
      eprint={2402.17245},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:playgroundaiplaygroundv251024pxaesthetic-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Playground v2.5 Community License" .

mcro:playgroundaiplaygroundv251024pxaesthetic-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-image generative model" .

mcro:playgroundaiplaygroundv251024pxaesthetic-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:playgroundaiplaygroundv251024pxaesthetic-ModelArchitecture .

mcro:playgroundaiplaygroundv251024pxaesthetic-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "generates highly aesthetic images of resolution 1024x1024, as well as portrait and landscape aspect ratios" .

mcro:potion-base-8M-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@software{minishlab2024model2vec,
  authors = {Stephan Tulkens, Thomas van Dongen},
  title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},
  year = {2024},
  url = {https://github.com/MinishLab/model2vec},
}""" .

mcro:potion-base-8M-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "It is a distilled version of the [baai/bge-base-en-v1.5](https://huggingface.co/baai/bge-base-en-v1.5) Sentence Transformer. It uses static embeddings, allowing text embeddings to be computed orders of magnitude faster on both GPU and CPU." .

mcro:potion-base-8M-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It is designed for applications where computational resources are limited or where real-time performance is critical." .

mcro:prajjwal1bertmedium-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Config of this model:
- `prajjwal1/bert-medium` (L=8, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-medium)""" .

mcro:prajjwal1bertmedium-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bhargava2021generalization,
      title={Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics}, 
      author={Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},
      year={2021},
      eprint={2110.01518},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-1908-08962,
  author    = {Iulia Turc and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {Well-Read Students Learn Better: The Impact of Student Initialization
               on Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/1908.08962},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.08962},
  eprinttype = {arXiv},
  eprint    = {1908.08962},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
""" .

mcro:prajjwal1bertmedium-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "These models are supposed to be trained on a downstream task." .

mcro:prajjwal1berttiny-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{bhargava2021generalization,
      title={Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics}, 
      author={Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},
      year={2021},
      eprint={2110.01518},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-1908-08962,
  author    = {Iulia Turc and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {Well-Read Students Learn Better: The Impact of Student Initialization
               on Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/1908.08962},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.08962},
  eprinttype = {arXiv},
  eprint    = {1908.08962},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
""" .

mcro:prajjwal1berttiny-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """The following model is a Pytorch pre-trained model obtained from converting Tensorflow checkpoint found in the [official Google BERT repository](https://github.com/google-research/bert).

This is one of the smaller pre-trained BERT variants, together with [bert-mini](https://huggingface.co/prajjwal1/bert-mini) [bert-small](https://huggingface.co/prajjwal1/bert-small) and [bert-medium](https://huggingface.co/prajjwal1/bert-medium). They were introduced in the study `Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962)), and ported to HF for the study `Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics` ([arXiv](https://arxiv.org/abs/2110.01518)). These models are supposed to be trained on a downstream task.
""" .

mcro:prithividaparrotparaphraseronT5-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:prithividaparrotparaphraseronT5-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "prithivida/parrot_paraphraser_on_T5" .

mcro:prithividaparrotparaphraseronT5-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models" .

mcro:pyannotesegmentation30-Architecture a mcro:ModelArchitectureInformationSection .

mcro:pyannotesegmentation30-Citation a mcro:CitationInformationSection .

mcro:pyannotesegmentation30-Dataset a mcro:DatasetInformationSection .

mcro:pyannotesegmentation30-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:pyannotesegmentation30-Architecture ;
    mcro:hasCitation mcro:pyannotesegmentation30-Citation ;
    mcro:hasIntendedUseCase mcro:pyannotesegmentation30-UseCase .

mcro:pyannotesegmentation30-UseCase a mcro:UseCaseInformationSection .

mcro:pyannotespeakerdiarization30-Benchmark a mcro:QuantativeAnalysisSection .

mcro:pyannotespeakerdiarization30-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{Bredin23,
  author={Herv Bredin},
  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}""",
        """@inproceedings{Plaquet23,
  author={Alexis Plaquet and Herv Bredin},
  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}""" .

mcro:pyannotespeakerdiarization30-Consideration a mcro:ConsiderationInformationSection .

mcro:pyannotespeakerdiarization30-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "AISHELL",
        "AMI",
        "AVA-AVD",
        "AliMeeting",
        "DIHARD",
        "Ego4D",
        "MSDWild",
        "REPERE",
        "VoxConverse" .

mcro:pyannotespeakerdiarization30-License a mcro:LicenseInformationSection .

mcro:pyannotespeakerdiarization30-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "pipeline",
        "pyannote.audio" .

mcro:pyannotespeakerdiarization30-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:pyannotespeakerdiarization30-License ;
    mcro:hasModelArchitecture mcro:pyannotespeakerdiarization30-ModelArchitecture .

mcro:pyannotespeakerdiarization30-UseCase a mcro:UseCaseInformationSection .

mcro:pyannotespeakerdiarization31-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{Bredin23,
  author={Herv Bredin},
  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
""",
        """@inproceedings{Plaquet23,
  author={Alexis Plaquet and Herv Bredin},
  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
""" .

mcro:pyannotespeakerdiarization31-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:pyannotespeakerdiarization31-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:pyannotespeakerdiarization31-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:pyannotewespeakervoxcelebresnet34LM-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{Bredin23,
  author={Herv Bredin},
  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={1983--1987},
  doi={10.21437/Interspeech.2023-105}
}""",
        """@inproceedings{Wang2023,
  title={Wespeaker: A research and production oriented speaker embedding learning toolkit},
  author={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},
  booktitle={ICASSP 2023, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}""" .

mcro:pyannotewespeakervoxcelebresnet34LM-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The pretrained model in WeNet follows the license of it's corresponding dataset. For example, the pretrained model on VoxCeleb follows Creative Commons Attribution 4.0 International License., since it is used as license of the VoxCeleb dataset, see https://mm.kaist.ac.kr/datasets/voxceleb/." .

mcro:pyannotewespeakervoxcelebresnet34LM-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:pyannotewespeakervoxcelebresnet34LM-UseCase a mcro:UseCaseInformationSection .

mcro:rbhatia46financialragmatryoshka-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Sentence Transformer" .

mcro:rbhatia46financialragmatryoshka-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
""",
        """@misc{henderson2017efficient,
    title={Efficient Natural Language Response Suggestion for Smart Reply}, 
    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
    year={2017},
    eprint={1705.00652},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
""",
        """@misc{kusupati2024matryoshka,
    title={Matryoshka Representation Learning}, 
    author={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},
    year={2024},
    eprint={2205.13147},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
""" .

mcro:rbhatia46financialragmatryoshka-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "apache-2.0" .

mcro:rbhatia46financialragmatryoshka-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:rbhatia46financialragmatryoshka-Architecture ;
    mcro:hasLicense mcro:rbhatia46financialragmatryoshka-License .

mcro:rbhatia46financialragmatryoshka-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "financial use-cases" .

mcro:resnet18a1in1k-Citation1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{wightman2021resnet,
  title={ResNet strikes back: An improved training procedure in timm},
  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},
  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}
}""" .

mcro:resnet18a1in1k-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}
}""" .

mcro:resnet18a1in1k-Citation3 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{He2015,
  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title = {Deep Residual Learning for Image Recognition},
  journal = {arXiv preprint arXiv:1512.03385},
  year = {2015}
}""" .

mcro:resnet18a1in1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:resnet18a1in1k-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "1x1 convolution shortcut downsample",
        "ReLU activations",
        "single layer 7x7 convolution with pooling" .

mcro:resnet18a1in1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:resnet18a1in1k-Citation1,
        mcro:resnet18a1in1k-Citation2,
        mcro:resnet18a1in1k-Citation3 ;
    mcro:hasModelArchitecture mcro:resnet18a1in1k-ModelArchitecture .

mcro:resnet18a1in1k-UseCase a mcro:UseCaseInformationSection .

mcro:resnet50.a1_in1k-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "1x1 convolution shortcut downsample",
        "ReLU activations",
        "single layer 7x7 convolution with pooling" .

mcro:resnet50.a1_in1k-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385",
        "ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476",
        "https://github.com/huggingface/pytorch-image-models" .

mcro:resnet50.a1_in1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:resnet50.a1_in1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:resnet50.a1_in1k-Citation ;
    prov1:hasTextValue "Activations (M): 11.1",
        "GMACs: 4.1",
        "Image classification / feature backbone",
        "Image size: train = 224 x 224, test = 288 x 288",
        "Params (M): 25.6" .

mcro:resnet50.a1_in1k-UseCase a mcro:UseCaseInformationSection .

mcro:resnet50v1.5-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections." .

mcro:resnet50v1.5-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}""" .

mcro:resnet50v1.5-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:resnet50v1.5-Citation ;
    mcro:hasModelArchitecture mcro:resnet50v1.5-Architecture .

mcro:resnet50v1.5-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for image classification" .

mcro:rinnajapanesecloobvitb16-Citation1 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{rinna-japanese-cloob-vit-b-16,
    title = {rinna/japanese-cloob-vit-b-16},
    author = {Shing, Makoto and Zhao, Tianyu and Sawada, Kei},
    url = {https://huggingface.co/rinna/japanese-cloob-vit-b-16}
}""" .

mcro:rinnajapanesecloobvitb16-Citation2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{sawada2024release,
    title = {Release of Pre-Trained Models for the {J}apanese Language},
    author = {Sawada, Kei and Zhao, Tianyu and Shing, Makoto and Mitsui, Kentaro and Kaga, Akio and Hono, Yukiya and Wakatsuki, Toshiaki and Mitsuda, Koh},
    booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
    month = {5},
    year = {2024},
    pages = {13898--13905},
    url = {https://aclanthology.org/2024.lrec-main.1213},
    note = {\\url{https://arxiv.org/abs/2404.01657}}
}""" .

mcro:rinnajapanesecloobvitb16-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The model was trained on [CC12M](https://github.com/google-research-datasets/conceptual-12m) translated the captions to Japanese." .

mcro:rinnajapanesecloobvitb16-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "[The Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)" .

mcro:rinnajapanesecloobvitb16-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The model was trained  a ViT-B/16 Transformer architecture as an image encoder and uses a 12-layer BERT as a text encoder. The image encoder was initialized from the [AugReg `vit-base-patch16-224` model](https://github.com/google-research/vision_transformer)." .

mcro:robertBase-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT" .

mcro:robertBase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{masala2020robert,
  title={RoBERT--A Romanian BERT Model},
  author={Masala, Mihai and Ruseti, Stefan and Dascalu, Mihai},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={6626--6637},
  year={2020}
}""" .

mcro:robertBase-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Oscar",
        "RoTex",
        "RoWiki" .

mcro:robertBase-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:robertBase-Citation ;
    mcro:hasModelArchitecture mcro:robertBase-Architecture .

mcro:robertBase-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "masked language modeling (MLM)",
        "next sentence prediction (NSP)" .

mcro:roberta-baseforextractiveqa-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "SQuAD 2.0" .

mcro:roberta-baseforextractiveqa-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:roberta-baseforextractiveqa-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "roberta-base" .

mcro:roberta-baseforextractiveqa-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:roberta-baseforextractiveqa-LicenseInformationSection .

mcro:roberta-baseforextractiveqa-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Extractive QA" .

mcro:rorshark-vit-base-Dataset a mcro:DatasetInformationSection .

mcro:rorshark-vit-base-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:rorshark-vit-base-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:rorshark-vit-base-ModelArchitecture .

mcro:rorshark-vit-base-UseCase a mcro:UseCaseInformationSection .

mcro:rubertbasecased-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "[1]: Kuratov, Y., Arkhipov, M. (2019). Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language. arXiv preprint [arXiv:1905.07213](https://arxiv.org/abs/1905.07213)." .

mcro:rubertbasecased-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """RuBERT (Russian, cased, 12layer, 768hidden, 12heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERTbase as an initialization for RuBERT[1].

08.11.2021: upload model with MLM and NSP heads""" .

mcro:rubertbasecasedsentimentrusentiment-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "A. Rogers A. Romanov A. Rumshisky S. Volkova M. Gronas A. Gribov RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian. Proceedings of COLING 2018." .

mcro:rubertbasecasedsentimentrusentiment-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "RuSentiment" .

mcro:rubertbasecasedsentimentrusentiment-UseCase a mcro:UseCaseInformationSection .

mcro:ruri-small-v2-CitationInformationSection a mcro:CitationInformationSection .

mcro:ruri-small-v2-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:ruri-small-v2-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:ruri-small-v2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Sentence Transformer" .

mcro:ruri-small-v2-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:ruri-small-v2-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:ruri-small-v2-ModelArchitectureInformationSection .

mcro:ruri-small-v2-ReferenceInformationSection a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "https://arxiv.org/abs/2409.07737" .

mcro:ruri-small-v2-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:sarvamaisarvamm-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "text-only language model built on Mistral-Small" .

mcro:sarvamaisarvamm-ModelDetail a mcro:ModelDetailSection .

mcro:sarvamaisarvamm-UseCase a mcro:UseCaseInformationSection .

mcro:sat-3l-sm-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "[`Segment any Text` paper](arxiv.org/abs/2406.16678)" .

mcro:sat-3l-sm-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "3 Transfomer layers" .

mcro:scb10xtyphoon21gemma312b-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{typhoon2,
      title={Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models}, 
      author={Kunat Pipatanakul and Potsawee Manakul and Natapong Nitarach and Warit Sirichotedumrong and Surapon Nonesung and Teetouch Jaknamon and Parinthapat Pengpun and Pittawat Taveekitworachai and Adisai Na-Thalang and Sittipong Sripaisarnmongkol and Krisanapong Jirayoot and Kasima Tharnpipitchai},
      year={2024},
      eprint={2412.13702},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13702}, 
}""" .

mcro:scb10xtyphoon21gemma312b-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "instructional model" .

mcro:scb10xtyphoon21gemma312b-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Gemma License" .

mcro:scb10xtyphoon21gemma312b-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "A 12B instruct decoder-only model based on Gemma3 architecture" .

mcro:scb10xtyphoon21gemma312b-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:scb10xtyphoon21gemma312b-Citation ;
    mcro:hasLicense mcro:scb10xtyphoon21gemma312b-License ;
    mcro:hasModelArchitecture mcro:scb10xtyphoon21gemma312b-ModelArchitecture .

mcro:scibert-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{beltagy-etal-2019-scibert,
    title = "SciBERT: A Pretrained Language Model for Scientific Text",
    author = "Beltagy, Iz  and Lo, Kyle  and Cohan, Arman",
    booktitle = "EMNLP",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1371"
}""" .

mcro:scibert-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT model trained on scientific text" .

mcro:sdxl10base-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CreativeML Open RAIL++-M License" .

mcro:sdxl10base-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to A red cube on top of a blue sphere
- Faces and people in general may not be generated properly.
- The autoencoding part of the model is lossy.""" .

mcro:sdxl10base-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-image generative model" .

mcro:sdxl10base-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:sdxl10base-License ;
    mcro:hasModelArchitecture mcro:sdxl10base-ModelArchitecture .

mcro:sdxl10base-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "research purposes only" .

mcro:sdxl10refinermodel-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-image generative model" .

mcro:sdxl10refinermodel-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CreativeML Open RAIL++-M License" .

mcro:sdxl10refinermodel-Limitation a mcro:LimitationInformationSection .

mcro:sdxl10refinermodel-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:sdxl10refinermodel-Architecture ;
    mcro:hasLicense mcro:sdxl10refinermodel-License .

mcro:sdxl10refinermodel-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "research purposes only" .

mcro:sdxlinpainting01-Arch a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-image generative model" .

mcro:sdxlinpainting01-Bias a mcro:ConsiderationInformationSection .

mcro:sdxlinpainting01-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CreativeML Open RAIL++-M License" .

mcro:sdxlinpainting01-Limitation a mcro:LimitationInformationSection .

mcro:sdxlinpainting01-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:sdxlinpainting01-License .

mcro:sdxlinpainting01-UseCase a mcro:UseCaseInformationSection .

mcro:sdxlinpainting01-intendeduse a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue "research purposes only" .

mcro:sdxlinpainting01-outofscopeuse a mcro:OutOfScopeUseCaseSectionInformation ;
    prov1:hasTextValue "not trained to be factual or true representations of people or events" .

mcro:sdxlturbo-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """- The generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.
- The model cannot render legible text.
- Faces and people in general may not be generated properly.
- The autoencoding part of the model is lossy.""" .

mcro:sdxlturbo-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SDXL-Turbo is a distilled version of [SDXL 1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), trained for real-time synthesis." .

mcro:sdxlturbo-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:sdxlturbo-ModelArchitecture ;
    prov1:hasTextValue "SDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation." .

mcro:sdxlturbo-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model is intended for both non-commercial and commercial usage." .

mcro:sdxlvae-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "COCO 2017" .

mcro:sdxlvae-EvaluationInformationSection a mcro:QuantativeAnalysisSection .

mcro:sdxlvae-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SDXL is a latent diffusion model, where the diffusion operates in a pretrained, 
learned (and fixed) latent space of an autoencoder. 
While the bulk of the semantic composition is done by the latent diffusion model, 
we can improve _local_, high-frequency details in generated images by improving the quality of the autoencoder. 
To this end, we train the same autoencoder architecture used for the original Stable Diffusion at a larger batch-size (256 vs 9) 
and additionally track the weights with an exponential moving average (EMA). 
The resulting autoencoder outperforms the original model in all evaluated reconstruction metrics, see the table below.""" .

mcro:segformerb0finetunedade20k-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2105-15203,
  author    = {Enze Xie and
               Wenhai Wang and
               Zhiding Yu and
               Anima Anandkumar and
               Jose M. Alvarez and
               Ping Luo},
  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with
               Transformers},
  journal   = {CoRR},
  volume    = {abs/2105.15203},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.15203},
  eprinttype = {arXiv},
  eprint    = {2105.15203},
  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:segformerb0finetunedade20k-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you." .

mcro:segformerb0finetunedade20k-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE)." .

mcro:segformerb0finetunedade20k-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset." .

mcro:segformerb1finetunedade20k-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset." .

mcro:segformerb1finetunedade20k-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2105-15203,
  author    = {Enze Xie and
               Wenhai Wang and
               Zhiding Yu and
               Anima Anandkumar and
               Jose M. Alvarez and
               Ping Luo},
  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with
               Transformers},
  journal   = {CoRR},
  volume    = {abs/2105.15203},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.15203},
  eprinttype = {arXiv},
  eprint    = {2105.15203},
  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:segformerb1finetunedade20k-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you." .

mcro:segformerb1finetunedade20k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:segformerb1finetunedade20k-Architecture .

mcro:segformerb2finetunedforclothessegmentation-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2105-15203,
  author    = {Enze Xie and
               Wenhai Wang and
               Zhiding Yu and
               Anima Anandkumar and
               Jose M. Alvarez and
               Ping Luo},
  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with
               Transformers},
  journal   = {CoRR},
  volume    = {abs/2105.15203},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.15203},
  eprinttype = {arXiv},
  eprint    = {2105.15203},
  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:segformerb2finetunedforclothessegmentation-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ATR dataset",
        "mattmdjaga/human_parsing_dataset" .

mcro:segformerb2finetunedforclothessegmentation-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "https://github.com/NVlabs/SegFormer/blob/master/LICENSE" .

mcro:segformerb2finetunedforclothessegmentation-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SegFormer model" .

mcro:segformerb2finetunedforclothessegmentation-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clothes segmentation",
        "human segmentation" .

mcro:segformerb5finetunedade20k-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2105-15203,
  author    = {Enze Xie and
               Wenhai Wang and
               Zhiding Yu and
               Anima Anandkumar and
               Jose M. Alvarez and
               Ping Luo},
  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with
               Transformers},
  journal   = {CoRR},
  volume    = {abs/2105.15203},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.15203},
  eprinttype = {arXiv},
  eprint    = {2105.15203},
  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:segformerb5finetunedade20k-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE)." .

mcro:segformerb5finetunedade20k-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset." .

mcro:segformerb5finetunedade20k-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you." .

mcro:sentencetransformersLaBSE-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Have a look at [LaBSE](https://tfhub.dev/google/LaBSE/1) for the respective publication that describes LaBSE." .

mcro:sentencetransformersLaBSE-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
  (2): Dense({'in_features': 768, 'out_features': 768, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})
  (3): Normalize()
)""" .

mcro:sentencetransformersLaBSE-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It can be used to map 109 languages to a shared vector space." .

mcro:sentencetransformersbertbasenlimeantokens-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersbertbasenlimeantokens-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersbertbasenlimeantokens-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:sentencetransformersdistilbertbasenlimeantokens-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersdistilbertbasenlimeantokens-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersdistilbertbasenlimeantokens-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:sentencetransformersdistilusebasemultilingualcasedv1-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersdistilusebasemultilingualcasedv1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})""" .

mcro:sentencetransformersdistilusebasemultilingualcasedv1-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:sentencetransformersdistilusebasemultilingualcasedv2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}
""" .

mcro:sentencetransformersdistilusebasemultilingualcasedv2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})
)
""" .

mcro:sentencetransformersdistilusebasemultilingualcasedv2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:sentencetransformersmsmarcodistilbertbasev4-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersmsmarcodistilbertbasev4-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersmsmarcodistilbertbasev4-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:sentencetransformersmultiqampnetbasedotv1-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "It has been trained on 215M (question, answer) pairs from diverse sources." .

mcro:sentencetransformersmultiqampnetbasedotv1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for **semantic search**." .

mcro:sentencetransformersmultiqampnetbasedotv1-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:sentencetransformersmultiqampnetbasedotv1-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:sentencetransformersmultiqampnetbasedotv1-DatasetInformationSection .

mcro:sentencetransformersmultiqampnetbasedotv1-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Our model is intented to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.

Note that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.""" .

mcro:sentencetransformersparaphraseMiniLML3v2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersparaphraseMiniLML3v2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersparaphraseMiniLML3v2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:sentencetransformersparaphraseMiniLML6v2-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersparaphraseMiniLML6v2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersparaphraseMiniLML6v2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:sentencetransformersparaphrasempnetbasev2-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersparaphrasempnetbasev2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersparaphrasempnetbasev2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering",
        "semantic search" .

mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)
""" .

mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:sentencetransformersparaphrasexlmrmultilingualv1-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersparaphrasexlmrmultilingualv1-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersparaphrasexlmrmultilingualv1-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:sentencetransformersrobertabasenlimeantokens-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Reimers, Nils and Gurevych, Iryna",
        "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks" .

mcro:sentencetransformersrobertabasenlimeantokens-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})",
        "Transformer({'max_seq_length': 128, 'do_lower_case': True}) with Transformer model: RobertaModel" .

mcro:sentencetransformersrobertabasenlimeantokens-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:sentencetransformersstsbrobertabase-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersstsbrobertabase-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': True}) with Transformer model: RobertaModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersstsbrobertabase-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:sentencetransformersstsbxlmrmultilingual-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}""" .

mcro:sentencetransformersstsbxlmrmultilingual-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:sentencetransformersstsbxlmrmultilingual-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "clustering or semantic search" .

mcro:siglip2so400mpatch16naflex-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{tschannen2025siglip2multilingualvisionlanguage,
      title={SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features}, 
      author={Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier Hnaff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai},
      year={2025},
      eprint={2502.14786},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.14786}, 
}""" .

mcro:siglip2so400mpatch16naflex-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "SigLIP 2 is pre-trained on the WebLI dataset [(Chen et al., 2023)](https://arxiv.org/abs/2209.06794)." .

mcro:siglip2so400mpatch16naflex-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for tasks like zero-shot image classification and
image-text retrieval, or as a vision encoder for VLMs (and other vision tasks).""" .

mcro:siglipso400mpatch14384-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{zhai2023sigmoid,
      title={Sigmoid Loss for Language Image Pre-Training}, 
      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},
      year={2023},
      eprint={2303.15343},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:siglipso400mpatch14384-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "zero-shot image classification and image-text retrieval" .

mcro:siglipso400mpatch14384-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "SoViT-400m architecture" .

mcro:siglipso400mpatch14384-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:siglipso400mpatch14384-Citation ;
    mcro:hasModelArchitecture mcro:siglipso400mpatch14384-ModelArchitecture .

mcro:siglipso400mpatch14384-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "WebLI dataset" .

mcro:snunlpKRSBERTV40KklueNLIaugSTS-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{kr-sbert,
  author = {Park, Suzi and Hyopil Shin},
  title = {KR-SBERT: A Pre-trained Korean-specific Sentence-BERT model},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\\url{https://github.com/snunlp/KR-SBERT}}
}""" .

mcro:snunlpKRSBERTV40KklueNLIaugSTS-License a mcro:LicenseInformationSection .

mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
)""" .

mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:snunlpKRSBERTV40KklueNLIaugSTS-Citation ;
    mcro:hasLicense mcro:snunlpKRSBERTV40KklueNLIaugSTS-License ;
    mcro:hasModelArchitecture mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelArchitecture ;
    prov1:hasTextValue "Basic information about the model that includes licensing information, owner information, the architecture of the model (algorthim employed), references (cited papers), and versioning information." .

mcro:snunlpKRSBERTV40KklueNLIaugSTS-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:spa-eng-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "English",
        "Spanish" .

mcro:spa-eng-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformer" .

mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{speechbrain,
  title={{SpeechBrain}: A General-Purpose Speech Toolkit},
  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Franois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
  year={2021},
  eprint={2106.04624},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  note={arXiv:2106.04624}
}""" .

mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "This system is composed of an wav2vec2 model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss. Speaker Verification is performed using cosine distance between speaker embeddings." .

mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:speechbrainspkrececapavoxceleb-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Additive Margin Softmax Loss",
        "ECAPA-TDNN",
        "attentive statistical pooling",
        "convolutional and residual blocks" .

mcro:speechbrainspkrececapavoxceleb-Citation-DBLP a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{DBLP:conf/interspeech/DesplanquesTD20,
  author    = {Brecht Desplanques and
               Jenthe Thienpondt and
               Kris Demuynck},
  editor    = {Helen Meng and
               Bo Xu and
               Thomas Fang Zheng},
  title     = {{ECAPA-TDNN:} Emphasized Channel Attention, Propagation and Aggregation
               in {TDNN} Based Speaker Verification},
  booktitle = {Interspeech 2020},
  pages     = {3830--3834},
  publisher = {{ISCA}},
  year      = {2020},
}""" .

mcro:speechbrainspkrececapavoxceleb-Citation-SpeechBrain a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{speechbrain,
  title={{SpeechBrain}: A General-Purpose Speech Toolkit},
  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Franois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
  year={2021},
  eprint={2106.04624},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  note={arXiv:2106.04624}
}""" .

mcro:speechbrainspkrececapavoxceleb-License a mcro:LicenseInformationSection .

mcro:speechbrainspkrececapavoxceleb-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:speechbrainspkrececapavoxceleb-Architecture ;
    mcro:hasCitation mcro:speechbrainspkrececapavoxceleb-Citation-DBLP,
        mcro:speechbrainspkrececapavoxceleb-Citation-SpeechBrain .

mcro:speechbrainspkrececapavoxceleb-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "extract speaker embeddings",
        "speaker verification" .

mcro:spladecocondenserensembledistil-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{https://doi.org/10.48550/arxiv.2205.04733,
  doi = {10.48550/ARXIV.2205.04733},
  url = {https://arxiv.org/abs/2205.04733},
  author = {Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, Stphane},
  keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}""" .

mcro:stabilityaisdturbo-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Faces and people in general may not be generated properly.",
        "The generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.",
        "The model cannot render legible text.",
        "The quality and prompt alignment is lower than that of [SDXL-Turbo](https://huggingface.co/stabilityai/sdxl-turbo)." .

mcro:stabilityaisdturbo-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Generative text-to-image model" .

mcro:stabilityaisdturbo-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasModelArchitecture mcro:stabilityaisdturbo-ModelArchitecture ;
    mcro:hasReference mcro:stabilityaisdturbo-Reference .

mcro:stabilityaisdturbo-OutOfScopeUse a mcro:OutOfScopeUseCaseSectionInformation ;
    prov1:hasTextValue """The model was not trained to be factual or true representations of people or events, 
and therefore using the model to generate such content is out-of-scope for the abilities of this model.""" .

mcro:stabilityaisdturbo-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "https://github.com/Stability-AI/generative-models",
        "https://stability.ai/research/adversarial-diffusion-distillation" .

mcro:stabilityaisdturbo-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Generation of artworks and use in design and other artistic processes.",
        "Research on generative models.",
        "Research on real-time applications of generative models.",
        "Safe deployment of models which have the potential to generate harmful content." .

mcro:stabilityaistablediffusion35medium-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasLimitation mcro:stabilityaistablediffusion35medium-Limitation .

mcro:stabilityaistablediffusion35medium-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue """Free for research, non-commercial, and commercial use for organizations or individuals with less than $1M in total annual revenue. More details can be found in the Community License Agreement. Read more at https://stability.ai/license.
- For individuals and organizations with annual revenue above $1M: please contact us to get an Enterprise License.""" .

mcro:stabilityaistablediffusion35medium-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """While this model can handle long prompts, you may observe artifacts on the edge of generations when T5 tokens go over 256. Pay attention to the token limits when using this model in your workflow, and shortern prompts if artifacts becomes too obvious.
- The medium model has a different training data distribution than the large model, so it may not respond to the same prompt similarly.
- We recommend sampling with **Skip Layer Guidance** for better structure and anatomy coherency.""" .

mcro:stabilityaistablediffusion35medium-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "MMDiT-X text-to-image generative model" .

mcro:stabilityaistablediffusion35medium-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:stabilityaistablediffusion35medium-License ;
    mcro:hasModelArchitecture mcro:stabilityaistablediffusion35medium-ModelArchitecture .

mcro:stabilityaistablediffusion35medium-OutofScopeUseCase a mcro:OutofScopeUseCaseSectionInformation ;
    prov1:hasTextValue "The model was not trained to be factual or true representations of people or events. As such, using the model to generate such content is out-of-scope of the abilities of this model." .

mcro:stabilityaistablediffusion35medium-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Intended uses include the following:
* Generation of artworks and use in design and other artistic processes.
* Applications in educational or creative tools.
* Research on generative models, including understanding the limitations of generative models.

All uses of the model must be in accordance with our Acceptable Use Policy.""" .

mcro:stablediffusioninpaintingmodelcard-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@InProceedings{Rombach_2022_CVPR,
          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj"orn},
          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
          month     = {June},
          year      = {2022},
          pages     = {10684-10695}
      }""" .

mcro:stablediffusioninpaintingmodelcard-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "LAION-2B (en) and subsets thereof" .

mcro:stablediffusioninpaintingmodelcard-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The CreativeML OpenRAIL M license" .

mcro:stablediffusioninpaintingmodelcard-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-image generation model" .

mcro:stablediffusioninpaintingmodelcard-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:stablediffusioninpaintingmodelcard-CitationInformationSection ;
    mcro:hasLicense mcro:stablediffusioninpaintingmodelcard-LicenseInformationSection .

mcro:stablediffusioninpaintingmodelcard-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model is intended for research purposes only." .

mcro:stablediffusionv15modelcard-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-image generation model" .

mcro:stablediffusionv15modelcard-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@InProceedings{Rombach_2022_CVPR,
 author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\"orn},
 title = {High-Resolution Image Synthesis With Latent Diffusion Models},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 month = {June},
 year = {2022},
 pages = {10684-10695}
}""" .

mcro:stablediffusionv15modelcard-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasLimitation mcro:stablediffusionv15modelcard-Limitation ;
    mcro:hasTrainingData mcro:stablediffusionv15modelcard-TrainingData .

mcro:stablediffusionv15modelcard-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The CreativeML OpenRAIL M license" .

mcro:stablediffusionv15modelcard-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue """- The model does not achieve perfect photorealism
- The model cannot render legible text
- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to A red cube on top of a blue sphere
- Faces and people in general may not be generated properly.
- The model was trained mainly with English captions and will not work as well in other languages.
- The autoencoding part of the model is lossy
- The model was trained on a large-scale dataset
 [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material
 and is not fit for product use without additional safety mechanisms and
 considerations.
- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.
 The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.""" .

mcro:stablediffusionv15modelcard-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:stablediffusionv15modelcard-Architecture ;
    mcro:hasCitation mcro:stablediffusionv15modelcard-Citation ;
    mcro:hasLicense mcro:stablediffusionv15modelcard-License .

mcro:stablediffusionv15modelcard-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """The model developers used the following dataset for training the model:

- LAION-2B (en) and subsets thereof (see next section)""" .

mcro:stablediffusionv15modelcard-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """The model is intended for research purposes only. Possible research areas and
tasks include

- Safe deployment of models which have the potential to generate harmful content.
- Probing and understanding the limitations and biases of generative models.
- Generation of artworks and use in design and other artistic processes.
- Applications in educational or creative tools.
- Research on generative models.""" .

mcro:stablediffusionv2-1-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@InProceedings{Rombach_2022_CVPR,
        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\"orn},
        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        month     = {June},
        year      = {2022},
        pages     = {10684-10695}
    }""" .

mcro:stablediffusionv2-1-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "LAION-5B and subsets" .

mcro:stablediffusionv2-1-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "CreativeML Open RAIL++-M License" .

mcro:stablediffusionv2-1-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Diffusion-based text-to-image generation model" .

mcro:stablediffusionv2-1-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:stablediffusionv2-1-Citation ;
    mcro:hasLicense mcro:stablediffusionv2-1-License ;
    mcro:hasModelArchitecture mcro:stablediffusionv2-1-ModelArchitecture .

mcro:stablediffusionv2-1-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The model is intended for research purposes only." .

mcro:surya-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "OCR model for [surya](https://www.github.com/VikParuchuri/surya)" .

mcro:swin_transformer_v2_tiny_sized_model-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Swin Transformer v2" .

mcro:swin_transformer_v2_tiny_sized_model-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{DBLP:journals/corr/abs-2111-09883,
  author    = {Ze Liu and
               Han Hu and
               Yutong Lin and
               Zhuliang Yao and
               Zhenda Xie and
               Yixuan Wei and
               Jia Ning and
               Yue Cao and
               Zheng Zhang and
               Li Dong and
               Furu Wei and
               Baining Guo},
  title     = {Swin Transformer {V2:} Scaling Up Capacity and Resolution},
  journal   = {CoRR},
  volume    = {abs/2111.09883},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09883},
  eprinttype = {arXiv},
  eprint    = {2111.09883},
  timestamp = {Thu, 02 Dec 2021 15:54:22 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09883.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}""" .

mcro:swin_transformer_v2_tiny_sized_model-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:swin_transformer_v2_tiny_sized_model-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:swin_transformer_v2_tiny_sized_model-Citation ;
    mcro:hasModelArchitecture mcro:swin_transformer_v2_tiny_sized_model-Architecture .

mcro:swin_transformer_v2_tiny_sized_model-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "image classification" .

mcro:t53b-Citation a mcro:CitationInformationSection .

mcro:t53b-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Colossal Clean Crawled Corpus (C4)" .

mcro:t53b-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:t53b-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:t53b-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:t53b-Citation ;
    mcro:hasLicense mcro:t53b-License ;
    mcro:hasModelArchitecture mcro:t53b-ModelArchitecture .

mcro:t53b-UseCase a mcro:UseCaseInformationSection .

mcro:t5base-Citation a mcro:CitationInformationSection .

mcro:t5base-Consideration a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "More information needed." .

mcro:t5base-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:t5base-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Language model" .

mcro:t5base-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:t5base-Citation ;
    mcro:hasLicense mcro:t5base-License ;
    mcro:hasModelArchitecture mcro:t5base-ModelArchitecture ;
    prov1:hasTextValue "T5-Base is the checkpoint with 220 million parameters." .

mcro:t5base-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67." .

mcro:t5base-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "The model is pre-trained on the [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), which was developed and released in the context of the same [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) as T5." .

mcro:t5base-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself." .

mcro:t5large-Citation a mcro:CitationInformationSection .

mcro:t5large-Consideration a mcro:ConsiderationInformationSection .

mcro:t5large-Dataset a mcro:DatasetInformationSection .

mcro:t5large-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:t5large-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:t5large-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:t5large-Citation ;
    mcro:hasLicense mcro:t5large-License ;
    mcro:hasModelArchitecture mcro:t5large-ModelArchitecture .

mcro:t5large-ModelParameter a mcro:ModelParameterSection .

mcro:t5large-QuantativeAnalysis a mcro:QuantativeAnalysisSection .

mcro:t5large-UseCase a mcro:UseCaseInformationSection .

mcro:t5small-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Language model" .

mcro:t5small-Citation a mcro:CitationInformationSection .

mcro:t5small-EvaluationData a mcro:DatasetInformationSection .

mcro:t5small-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:t5small-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:t5small-Architecture ;
    mcro:hasCitation mcro:t5small-Citation ;
    mcro:hasLicense mcro:t5small-License .

mcro:t5small-TrainingData a mcro:DatasetInformationSection .

mcro:t5small-UseCase a mcro:UseCaseInformationSection .

mcro:tabletransformerfinetunedfortabledetection-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents" .

mcro:tabletransformerfinetunedfortabledetection-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "PubTables1M" .

mcro:tabletransformerfinetunedfortabledetection-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer-based object detection model" .

mcro:tabletransformerfinetunedfortabledetection-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "detecting tables in documents" .

mcro:tabletransformerfinetunedfortablestructurerecognition-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The Table Transformer is equivalent to [DETR](https://huggingface.co/docs/transformers/model_doc/detr), a Transformer-based object detection model. Note that the authors decided to use the \"normalize before\" setting of DETR, which means that layernorm is applied before self- and cross-attention." .

mcro:tabletransformerfinetunedfortablestructurerecognition-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for detecting the structure (like rows, columns) in tables. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer) for more info." .

mcro:tabletransformerpretrainedfortablestructurerecognition-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "It was introduced in the paper [Aligning benchmark datasets for table structure recognition](https://arxiv.org/abs/2303.00716) by Smock et al. and first released in [this repository](https://github.com/microsoft/table-transformer)." .

mcro:tabletransformerpretrainedfortablestructurerecognition-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "The Table Transformer is equivalent to [DETR](https://huggingface.co/docs/transformers/model_doc/detr), a Transformer-based object detection model. Note that the authors decided to use the \"normalize before\" setting of DETR, which means that layernorm is applied before self- and cross-attention." .

mcro:tabletransformerpretrainedfortablestructurerecognition-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for detecting tables in documents. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer) for more info." .

mcro:testModel-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "citation" .

mcro:testModel-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet" .

mcro:testModel-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "mit" .

mcro:testModel-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CNN" .

mcro:testModel-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "intended use case" .

mcro:testmodel-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "test citation" .

mcro:testmodel-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet" .

mcro:testmodel-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "mit" .

mcro:testmodel-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CNN" .

mcro:testmodel-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "image classification" .

mcro:text-detection-model-for-surya-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Text detection model" .

mcro:tiiuaefalconrw1b-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay" .

mcro:tiiuaefalconrw1b-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "RefinedWeb" .

mcro:tiiuaefalconrw1b-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "Apache 2.0" .

mcro:tiiuaefalconrw1b-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Causal decoder-only" .

mcro:tiiuaefalconrw1b-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Research on large language models, specifically the influence of adequately filtered and deduplicated web data on the properties of large language models (fairness, safety, limitations, capabilities, etc.)." .

mcro:timmViTB16SigLIPi18n256-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  journal={arXiv preprint arXiv:2303.15343},
  year={2023}
}
""",
        """@misc{big_vision,
  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},
  title = {Big Vision},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\\url{https://github.com/google-research/big_vision}}
}
""" .

mcro:timmViTB16SigLIPi18n256-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "WebLI" .

mcro:timmViTB16SigLIPi18n256-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Contrastive Image-Text, Zero-Shot Image Classification." .

mcro:timmViTB16SigLIPi18n256-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:timmViTB16SigLIPi18n256-Citation ;
    mcro:hasDataset mcro:timmViTB16SigLIPi18n256-Dataset ;
    mcro:hasModelArchitecture mcro:timmViTB16SigLIPi18n256-ModelArchitecture ;
    prov1:hasTextValue "A SigLIP (Sigmoid loss for Language-Image Pre-training) model trained on WebLI." .

mcro:timmViTB16SigLIPi18n256-UseCase a mcro:UseCaseInformationSection .

mcro:toxigen-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar." .

mcro:toxigen-Reference a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "Please visit the [Github Repository](https://github.com/microsoft/TOXIGEN) for the training dataset and further details." .

mcro:toxigen-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "This model comes from the paper [ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) and can be used to detect implicit hate speech." .

mcro:trlinternaltestingtinyLlamaForCausalLM32-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue "This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library." .

mcro:trlinternaltestingtinyQwen2ForCausalLM25-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue "This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library." .

mcro:trlinternaltestingtinyT5ForConditionalGeneration-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue "This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library." .

mcro:trocrbasesroie-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{li2021trocr,
      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, 
      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},
      year={2021},
      eprint={2109.10282},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
""" .

mcro:trocrbasesroie-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.

Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.""" .

mcro:trocrbasesroie-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you." .

mcro:trocrsmalliam-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{li2021trocr,
      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, 
      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},
      year={2021},
      eprint={2109.10282},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}""" .

mcro:trocrsmalliam-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.

Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.""" .

mcro:trocrsmalliam-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you." .

mcro:trpakovvitfaceexpression-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "FER2013" .

mcro:trpakovvitfaceexpression-Limitation a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Data Bias" .

mcro:trpakovvitfaceexpression-LimitationGeneralization a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Generalization" .

mcro:trpakovvitfaceexpression-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Vision Transformer (ViT)" .

mcro:trpakovvitfaceexpression-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Facial Expression/Emotion Recognition" .

mcro:tsmatzxlmrobertanerjapanese-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "RobertaModel" .

mcro:tsmatzxlmrobertanerjapanese-ModelParameterSection a mcro:ModelParameterSection .

mcro:tsmatzxlmrobertanerjapanese-TrainingDataInformationSection a mcro:TrainingDataInformationSection .

mcro:tsmatzxlmrobertanerjapanese-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:twitterrobertabasesentiment-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "_TweetEval_ (Findings of EMNLP 2020)" .

mcro:twitterrobertabasesentiment-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "roBERTa-base" .

mcro:twitterrobertabasesentiment-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "sentiment analysis" .

mcro:twitterxlmrobertabasesentiment-CitationInformationSection a mcro:CitationInformationSection .

mcro:twitterxlmrobertabasesentiment-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:twitterxlmrobertabasesentiment-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:twitterxlmrobertabasesentiment-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:twitterxlmrobertabasesentiment-CitationInformationSection ;
    mcro:hasLicense mcro:twitterxlmrobertabasesentiment-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:twitterxlmrobertabasesentiment-ModelArchitectureInformationSection .

mcro:twitterxlmrobertabasesentiment-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "sentiment analysis" .

mcro:unik3d-ReferenceInformationSection a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "https://github.com/lpiccinelli-eth/UniK3D",
        "https://huggingface.co/docs/huggingface_hub/package_reference/mixins#huggingface_hub.PyTorchModelHubMixin" .

mcro:unslothDeepSeekR10528Qwen38BGGUF-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}""" .

mcro:unslothDeepSeekR10528Qwen38BGGUF-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "MIT License" .

mcro:unslothDeepSeekR10528Qwen38BGGUF-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528" .

mcro:unslothDeepSeekR10528Qwen38BGGUF-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B" .

mcro:unslothQwen2505BInstructbnb4bit-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}""" .

mcro:unslothQwen2505BInstructbnb4bit-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "instruction-tuned 0.5B Qwen2.5 model" .

mcro:unslothQwen2505BInstructbnb4bit-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings" .

mcro:upskyybgem3korean-CitationInformationSection a mcro:CitationInformationSection .

mcro:upskyybgem3korean-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:upskyybgem3korean-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Sentence Transformer" .

mcro:upskyybgem3korean-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasDataset mcro:upskyybgem3korean-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:upskyybgem3korean-ModelArchitectureInformationSection .

mcro:upskyybgem3korean-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:variousmodels-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Do not use it in production." .

mcro:vikplayoutsegmenter-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Based on layoutlmv3." .

mcro:vikplayoutsegmenter-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Segments pdf page layout into blocks." .

mcro:visiontransformerbase-sizedmodel-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer)." .

mcro:visiontransformerbase-sizedmodel-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224." .

mcro:visiontransformerbase-sizedmodel-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes." .

mcro:visiontransformerbase-sizedmodel-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for
fine-tuned versions on a task that interests you.""" .

mcro:visiontransformerbase-sizedmodel-hybrid-Citation a mcro:CitationInformationSection .

mcro:visiontransformerbase-sizedmodel-hybrid-EvaluationResults a mcro:QuantativeAnalysisSection .

mcro:visiontransformerbase-sizedmodel-hybrid-IntendedUseCase a mcro:UseCaseInformationSection .

mcro:visiontransformerbase-sizedmodel-hybrid-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:visiontransformerbase-sizedmodel-hybrid-ModelDetail a mcro:ModelDetailSection .

mcro:visiontransformerbase-sizedmodel-hybrid-TrainingData a mcro:DatasetInformationSection .

mcro:visiontransformerbase-sizedmodel-hybrid-TrainingProcedure a mcro:ModelParameterSection .

mcro:visiontransformerbasesizedmodel-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """misc{oquab2023dinov2,
      title={DINOv2: Learning Robust Visual Features without Supervision}, 
      author={Maxime Oquab and Timothe Darcet and Tho Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
      year={2023},
      eprint={2304.07193},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:visiontransformerbasesizedmodel-CitationInformationSection1 a mcro:CitationInformationSection ;
    prov1:hasTextValue "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" .

mcro:visiontransformerbasesizedmodel-CitationInformationSection2 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{wu2020visual,
      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, 
      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
      year={2020},
      eprint={2006.03677},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:visiontransformerbasesizedmodel-CitationInformationSection3 a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}""" .

mcro:visiontransformerbasesizedmodel-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224.",
        """Vision Transformer (ViT) model trained using the DINOv2 method. It was introduced in the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Oquab et al. and first released in [this repository](https://github.com/facebookresearch/dinov2).

Disclaimer: The team releasing DINOv2 did not write a model card for this model so this model card has been written by the Hugging Face team.

## Model description

The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion. 

Images are presented to the model as a sequence of fixed-size patches, which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.

Note that this model does not include any fine-tuned heads. 

By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.""" .

mcro:visiontransformerbasesizedmodel-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes." .

mcro:vit-age-classifier-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "A vision transformer finetuned to classify the age of a given person's face." .

mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}
""",
        """@article{steiner2021augreg,
  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.10270},
  year={2021}
}
""",
        """@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}
}
""" .

mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Citation ;
    mcro:hasDataset mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Dataset,
        mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-PretrainDataset ;
    mcro:hasModelArchitecture mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelArchitecture .

mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-PretrainDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-21k" .

mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Image Classification",
        "Image Embeddings" .

mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{cherti2022reproducible,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  journal={arXiv preprint arXiv:2212.07143},
  year={2022}
}
""",
        """@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}
""",
        """@inproceedings{schuhmann2022laionb,
  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
  author={Christoph Schuhmann and
          Romain Beaumont and
          Richard Vencu and
          Cade W Gordon and
          Ross Wightman and
          Mehdi Cherti and
          Theo Coombes and
          Aarush Katta and
          Clayton Mullis and
          Mitchell Wortsman and
          Patrick Schramowski and
          Srivatsa R Kundurthy and
          Katherine Crowson and
          Ludwig Schmidt and
          Robert Kaczmarczyk and
          Jenia Jitsev},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
  url={https://openreview.net/forum?id=M3Y74vmsMcY}
}
""",
        """@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}
}
""",
        """@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}
""" .

mcro:vitbasepatch32clip448laion2bftin12kin1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation ;
    mcro:hasDataset mcro:vitbasepatch32clip448laion2bftin12kin1k-Dataset ;
    mcro:hasModelArchitecture mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelArchitecture .

mcro:vitbasepatch32clip448laion2bftin12kin1k-UseCase a mcro:UseCaseInformationSection .

mcro:vitbasepatch8224augreg2in21kftin1k-Citation a mcro:CitationInformationSection .

mcro:vitbasepatch8224augreg2in21kftin1k-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "ImageNet-1k" .

mcro:vitbasepatch8224augreg2in21kftin1k-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:vitbasepatch8224augreg2in21kftin1k-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:vitbasepatch8224augreg2in21kftin1k-Citation ;
    mcro:hasDataset mcro:vitbasepatch8224augreg2in21kftin1k-Dataset ;
    mcro:hasModelArchitecture mcro:vitbasepatch8224augreg2in21kftin1k-ModelArchitecture ;
    mcro:hasTrainingData mcro:vitbasepatch8224augreg2in21kftin1k-TrainingData .

mcro:vitbasepatch8224augreg2in21kftin1k-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "ImageNet-21k" .

mcro:vitbasepatch8224augreg2in21kftin1k-UseCase a mcro:UseCaseInformationSection .

mcro:vitlargepatch14reg4dinov2lvd142m-Citation a mcro:CitationInformationSection .

mcro:vitlargepatch14reg4dinov2lvd142m-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "LVD-142M" .

mcro:vitlargepatch14reg4dinov2lvd142m-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Image classification / feature backbone" .

mcro:vitlargepatch14reg4dinov2lvd142m-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:vitlargepatch14reg4dinov2lvd142m-Citation ;
    mcro:hasDataset mcro:vitlargepatch14reg4dinov2lvd142m-Dataset ;
    mcro:hasModelArchitecture mcro:vitlargepatch14reg4dinov2lvd142m-ModelArchitecture .

mcro:vitlargepatch14reg4dinov2lvd142m-UseCase a mcro:UseCaseInformationSection .

mcro:vitmatte-model-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{yao2023vitmatte,
      title={ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers}, 
      author={Jingfeng Yao and Xinggang Wang and Shusheng Yang and Baoyuan Wang},
      year={2023},
      eprint={2305.15272},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}""" .

mcro:vitmatte-model-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "ViTMatte is a simple approach to image matting, the task of accurately estimating the foreground object in an image. The model consists of a Vision Transformer (ViT) with a lightweight head on top." .

mcro:vitmatte-model-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:vitmatte-model-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:vitmatte-model-ModelArchitectureInformationSection .

mcro:vitmatte-model-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for image matting. See the [model hub](https://huggingface.co/models?search=vitmatte) to look for other
fine-tuned versions that may interest you.""" .

mcro:vocos-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{siuzdak2023vocos,
  title={Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis},
  author={Siuzdak, Hubert},
  journal={arXiv preprint arXiv:2306.00814},
  year={2023}
}""" .

mcro:vocos-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The code in this repository is released under the MIT license." .

mcro:vocos-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """Vocos is a fast neural vocoder designed to synthesize audio waveforms from acoustic features. Trained using a Generative
Adversarial Network (GAN) objective, Vocos can generate waveforms in a single forward pass. Unlike other typical
GAN-based vocoders, Vocos does not model audio samples in the time domain. Instead, it generates spectral
coefficients, facilitating rapid audio reconstruction through inverse Fourier transform.""" .

mcro:wav2vec2-baseforemotionrecognition-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{yang2021superb,
  title={SUPERB: Speech processing Universal PERformance Benchmark},
  author={Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y and Liu, Andy T and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and others},
  journal={arXiv preprint arXiv:2105.01051},
  year={2021}
}""" .

mcro:wav2vec2-baseforemotionrecognition-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue """Emotion Recognition (ER) predicts an emotion class for each utterance. The most widely used ER dataset
[IEMOCAP](https://sail.usc.edu/iemocap/) is adopted, and we follow the conventional evaluation protocol: 
we drop the unbalanced emotion classes to leave the final four classes with a similar amount of data points and 
cross-validate on five folds of the standard splits.

For the original model's training and evaluation instructions refer to the 
[S3PRL downstream task README](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#er-emotion-recognition).""" .

mcro:wav2vec2-baseforemotionrecognition-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue """This is a ported version of 
[S3PRL's Wav2Vec2 for the SUPERB Emotion Recognition task](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/emotion).

The base model is [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base), which is pretrained on 16kHz 
sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.

For more information refer to [SUPERB: Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051)""" .

mcro:wav2vec2base-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli" .

mcro:wav2vec2base-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Facebook's Wav2Vec2" .

mcro:wav2vec2base-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "speech recognition" .

mcro:wav2vec2largerobustftlibri960h-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Wav2Vec2" .

mcro:wav2vec2largerobustftlibri960h-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, Michael Auli" .

mcro:wav2vec2largerobustftlibri960h-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Librispeech" .

mcro:wav2vec2largerobustftlibri960h-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "transcribe audio files" .

mcro:wav2vec2largexlsr53-Architecture a mcro:ModelArchitectureInformationSection .

mcro:wav2vec2largexlsr53-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "[Paper](https://arxiv.org/abs/2006.13979)" .

mcro:wav2vec2largexlsr53-License a mcro:LicenseInformationSection .

mcro:wav2vec2largexlsr53-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:wav2vec2largexlsr53-Citation ;
    mcro:hasLicense mcro:wav2vec2largexlsr53-License ;
    mcro:hasModelArchitecture mcro:wav2vec2largexlsr53-Architecture .

mcro:wav2vec2largexlsr53-UseCase a mcro:UseCaseInformationSection .

mcro:wav2vec2largexlsr53espeakcvft-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Paper: Simple and Effective Zero-shot Cross-lingual Phoneme Recognition" .

mcro:wav2vec2largexlsr53espeakcvft-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "CommonVoice" .

mcro:wav2vec2largexlsr53espeakcvft-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Wav2Vec2" .

mcro:wav2vec2largexlsr53espeakcvft-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "recognize phonetic labels in multiple languages" .

mcro:wav2vec2largexlsrhindi-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:wav2vec2largexlsrhindi-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:wav2vec2largexlsrhindi-QuantativeAnalysisSection a mcro:QuantativeAnalysisSection .

mcro:wav2vec2largexlsrhindi-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:wav2vec2xlsr300mcv7turkish-License a mcro:LicenseInformationSection .

mcro:wav2vec2xlsr300mcv7turkish-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:wav2vec2xlsr300mcv7turkish-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasLicense mcro:wav2vec2xlsr300mcv7turkish-License ;
    mcro:hasModelArchitecture mcro:wav2vec2xlsr300mcv7turkish-ModelArchitecture .

mcro:wav2vec2xlsr300mcv7turkish-ModelParameter a mcro:ModelParameterSection ;
    mcro:hasTrainingData mcro:wav2vec2xlsr300mcv7turkish-TrainingData .

mcro:wav2vec2xlsr300mcv7turkish-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """The following datasets were used for finetuning:
 - [Common Voice 7.0 TR](https://huggingface.co/datasets/mozilla-foundation/common_voice_7_0) All `validated` split except `test` split was used for training.
 - [MediaSpeech](https://www.openslr.org/108/)""" .

mcro:wav2vec2xlsr300mhebrew-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection .

mcro:wav2vec2xlsr300mhebrew-TrainingDataInformationSection a mcro:DatasetInformationSection .

mcro:wav2vec2xlsr300mhebrew-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:wavlmbaseplus-Citation a mcro:CitationInformationSection .

mcro:wavlmbaseplus-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)
- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)
- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)""" .

mcro:wavlmbaseplus-License a mcro:LicenseInformationSection .

mcro:wavlmbaseplus-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "[Microsoft's WavLM](https://github.com/microsoft/unilm/tree/master/wavlm)" .

mcro:wavlmbaseplus-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:wavlmbaseplus-Citation ;
    mcro:hasLicense mcro:wavlmbaseplus-License .

mcro:wavlmbaseplus-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """This is an English pre-trained speech model that has to be fine-tuned on a downstream task like speech recognition or audio classification before it can be 
used in inference. The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/).""" .

mcro:wavlmlarge-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing" .

mcro:wavlmlarge-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)
- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)
- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)""" .

mcro:wavlmlarge-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "The official license can be found here" .

mcro:wavlmlarge-ModelArchitecture a mcro:ModelArchitectureInformationSection .

mcro:wavlmlarge-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:wavlmlarge-Citation ;
    mcro:hasLicense mcro:wavlmlarge-License ;
    mcro:hasModelArchitecture mcro:wavlmlarge-ModelArchitecture .

mcro:wavlmlarge-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """This is an English pre-trained speech model that has to be fine-tuned on a downstream task like speech recognition or audio classification before it can be 
used in inference.""" .

mcro:whisper-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{radford2022whisper,
  doi = {10.48550/ARXIV.2212.04356},
  url = {https://arxiv.org/abs/2212.04356},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}""" .

mcro:whisper-Consideration a mcro:ConsiderationInformationSection ;
    mcro:hasEthicalConsideration mcro:whisper-EthicalConsideration ;
    mcro:hasLimitation mcro:whisper-Limitation ;
    prov1:hasTextValue "the models combine trying to predict the next word in audio with trying to transcribe the audio itself." .

mcro:whisper-DatasetInformationSection a mcro:DatasetInformationSection .

mcro:whisper-EthicalConsideration a mcro:EthicalConsiderationSection ;
    prov1:hasTextValue "dual use concerns" .

mcro:whisper-LicenseInformationSection a mcro:LicenseInformationSection .

mcro:whisper-Limitation a mcro:LimitationInformationSection .

mcro:whisper-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer based encoder-decoder model" .

mcro:whisper-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasArchitecture mcro:whisper-ModelArchitectureInformationSection ;
    mcro:hasCitation mcro:whisper-Citation,
        mcro:whisper-CitationInformationSection ;
    mcro:hasLicense mcro:whisper-LicenseInformationSection ;
    mcro:hasModelArchitecture mcro:whisper-ModelArchitecture .

mcro:whisper-ModelParameter a mcro:ModelParameterSection ;
    mcro:hasDataset mcro:whisper-Dataset .

mcro:whisper-Performance a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level." .

mcro:whisper-QuantativeAnalysisSection a mcro:QuantativeAnalysisSection .

mcro:whisper-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "680,000 hours of audio and the corresponding transcripts collected from the internet",
        "The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet." .

mcro:whisper-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "automatic speech recognition (ASR) and speech translation" .

mcro:whisper-UseCaseInformationSection a mcro:UseCaseInformationSection .

mcro:whisper-baseen-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Robust Speech Recognition via Large-Scale Weak Supervision" .

mcro:whisper-baseen-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "English speech recognition" .

mcro:whisper-baseen-License a mcro:LicenseInformationSection .

mcro:whisper-baseen-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer based encoder-decoder model" .

mcro:whisper-baseen-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:whisper-baseen-Citation ;
    mcro:hasLicense mcro:whisper-baseen-License ;
    mcro:hasModelArchitecture mcro:whisper-baseen-ModelArchitecture .

mcro:whisper-baseen-PerformanceLimitations a mcro:LimitationInformationSection ;
    prov1:hasTextValue "hallucination" .

mcro:whisper-baseen-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "680,000 hours of audio" .

mcro:whisperbase-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Robust Speech Recognition via Large-Scale Weak Supervision" .

mcro:whisperbase-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "ASR solution for developers, especially for English speech recognition" .

mcro:whisperbase-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer based encoder-decoder model" .

mcro:whisperbase-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:whisperbase-Citation ;
    mcro:hasModelArchitecture mcro:whisperbase-ModelArchitecture .

mcro:whisperbase-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "680,000 hours of audio and the corresponding transcripts collected from the internet" .

mcro:whisperkit-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "WhisperKit is an on-device speech recognition framework for Apple Silicon: https://github.com/argmaxinc/WhisperKit" .

mcro:whisperlargev3-Architecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Whisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model." .

mcro:whisperlargev3-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{radford2022whisper,
  doi = {10.48550/ARXIV.2212.04356},
  url = {https://arxiv.org/abs/2212.04356},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}""" .

mcro:whisperlargev3-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:whisperlargev3-Citation ;
    mcro:hasModelArchitecture mcro:whisperlargev3-Architecture .

mcro:whisperlargev3-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "The large-v3 checkpoint is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected using Whisper large-v2." .

mcro:whisperlargev3-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "The models are primarily trained and evaluated on ASR and speech translation to English tasks." .

mcro:xclipbasesizedmodel-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "It was introduced in the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Ni et al." .

mcro:xclipbasesizedmodel-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "X-CLIP is a minimal extension of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for general video-language understanding." .

mcro:xclipbasesizedmodel-QuantativeAnalysisSection a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "This model achieves a top-1 accuracy of 80.4% and a top-5 accuracy of 95.0%." .

mcro:xclipbasesizedmodel-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "This model was trained on [Kinetics-400](https://www.deepmind.com/open-source/kinetics)." .

mcro:xclipbasesizedmodel-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "You can use the raw model for determining how well text goes with a given video." .

mcro:xlmrobertabaselanguagedetection-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Unsupervised Cross-lingual Representation Learning at Scale" .

mcro:xlmrobertabaselanguagedetection-IntendedUseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "language detector, i.e. for sequence classification tasks" .

mcro:xlmrobertabaselanguagedetection-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "XLM-RoBERTa transformer model with a classification head" .

mcro:xlmrobertabaselanguagedetection-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:xlmrobertabaselanguagedetection-Citation ;
    mcro:hasModelArchitecture mcro:xlmrobertabaselanguagedetection-ModelArchitecture .

mcro:xlmrobertabaselanguagedetection-PerformanceMetric a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "average accuracy on the test set is **99.6%**" .

mcro:xlmrobertabaselanguagedetection-QuantativeAnalysis a mcro:QuantativeAnalysisSection ;
    mcro:hasPerformanceMetric mcro:xlmrobertabaselanguagedetection-PerformanceMetric .

mcro:xlmrobertabaselanguagedetection-TrainingData a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Language Identification dataset, which consists of text sequences in 20 languages" .

mcro:xtunerllavallama38bv11transformers-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{2023xtuner,
    title={XTuner: A Toolkit for Efficiently Fine-tuning LLM},
    author={XTuner Contributors},
    howpublished = {\\url{https://github.com/InternLM/xtuner}},
    year={2023}
}""" .

mcro:xtunerllavallama38bv11transformers-ModelDetail a mcro:ModelDetailSection ;
    prov1:hasTextValue """llava-llama-3-8b-v1_1-hf is a LLaVA model fine-tuned from [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and [CLIP-ViT-Large-patch14-336](https://huggingface.co/openai/clip-vit-large-patch14-336) with [ShareGPT4V-PT](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V) and [InternVL-SFT](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets) by [XTuner](https://github.com/InternLM/xtuner).

**Note: This model is in HuggingFace LLaVA format.**

Resources:

- GitHub: [xtuner](https://github.com/InternLM/xtuner)
- Official LLaVA format model: [xtuner/llava-llama-3-8b-v1_1-hf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-hf)
- XTuner LLaVA format model: [xtuner/llava-llama-3-8b-v1_1](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1)
- GGUF format model: [xtuner/llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf)""" .

mcro:yiyanghkustfinberttone-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "Huang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022)." .

mcro:yiyanghkustfinberttone-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "BERT model" .

mcro:yiyanghkustfinberttone-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "financial tone analysis task" .

mcro:yuvalkirstainPickScorev1-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@inproceedings{Kirstain2023PickaPicAO,
  title={Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation},
  author={Yuval Kirstain and Adam Polyak and Uriel Singer and Shahbuland Matiana and Joe Penna and Omer Levy},
  year={2023}
}""" .

mcro:yuvalkirstainPickScorev1-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Pick-a-Pic dataset" .

mcro:yuvalkirstainPickScorev1-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "CLIP-H" .

mcro:yuvalkirstainPickScorev1-ModelDetail a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:yuvalkirstainPickScorev1-Citation ;
    mcro:hasDataset mcro:yuvalkirstainPickScorev1-Dataset ;
    mcro:hasModelArchitecture mcro:yuvalkirstainPickScorev1-ModelArchitecture .

mcro:yuvalkirstainPickScorev1-TrainingData a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Pick-a-Pic dataset" .

mcro:yuvalkirstainPickScorev1-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "general scoring function",
        "human preference prediction",
        "image ranking",
        "model evaluation" .

mcro:deepseekaiDeepSeekR1-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "reasoning" .

mcro:gemma3-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@article{gemma_2025,
    title={Gemma 3},
    url={https://goo.gle/Gemma3Report},
    publisher={Kaggle},
    author={Gemma Team},
    year={2025}
}""" .

mcro:gemma3-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue """Here are the key data cleaning and filtering methods applied to the training
data:

-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering
    was applied at multiple stages in the data preparation process to ensure
    the exclusion of harmful and illegal content.
-   Sensitive Data Filtering: As part of making Gemma pre-trained models
    safe and reliable, automated techniques were used to filter out certain
    personal information and other sensitive data from training sets.
-   Additional methods: Filtering based on content quality and safety in
    line with [our policies][safety-policies].""",
        """These models were trained on a dataset of text data that includes a wide variety
of sources. The 27B model was trained with 14 trillion tokens, the 12B model was
trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and
1B with 2 trillion tokens. Here are the key components:

-   Web Documents: A diverse collection of web text ensures the model is
    exposed to a broad range of linguistic styles, topics, and vocabulary. The
    training dataset includes content in over 140 languages.
-   Code: Exposing the model to code helps it to learn the syntax and
    patterns of programming languages, which improves its ability to generate
    code and understand code-related questions.
-   Mathematics: Training on mathematical text helps the model learn logical
    reasoning, symbolic representation, and to address mathematical queries.
-   Images: A wide range of images enables the model to perform image
    analysis and visual data extraction tasks.

The combination of these diverse data sources is crucial for training a powerful
multimodal model that can handle a wide variety of different tasks and data
formats.""" .

mcro:gemma3-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """-   Training Data
    -   The quality and diversity of the training data significantly
        influence the model's capabilities. Biases or gaps in the training data
        can lead to limitations in the model's responses.
    -   The scope of the training dataset determines the subject areas
        the model can handle effectively.
-   Context and Task Complexity
    -   Models are better at tasks that can be framed with clear
        prompts and instructions. Open-ended or highly complex tasks might be
        challenging.
    -   A model's performance can be influenced by the amount of context
        provided (longer context generally leads to better outputs, up to a
        certain point).
-   Language Ambiguity and Nuance
    -   Natural language is inherently complex. Models might struggle
        to grasp subtle nuances, sarcasm, or figurative language.
-   Factual Accuracy
    -   Models generate responses based on information they learned
        from their training datasets, but they are not knowledge bases. They
        may generate incorrect or outdated factual statements.
-   Common Sense
    -   Models rely on statistical patterns in language. They might
        lack the ability to apply common sense reasoning in certain situations.""",
        """Open vision-language models (VLMs) models have a wide range of applications
across various industries and domains. The following list of potential uses is
not comprehensive. The purpose of this list is to provide contextual information
about the possible use-cases that the model creators considered as part of model
training and development.

-   Content Creation and Communication
    -   Text Generation: These models can be used to generate creative text
        formats such as poems, scripts, code, marketing copy, and email drafts.
    -   Chatbots and Conversational AI: Power conversational interfaces
        for customer service, virtual assistants, or interactive applications.
    -   Text Summarization: Generate concise summaries of a text corpus,
        research papers, or reports.
    -   Image Data Extraction: These models can be used to extract,
        interpret, and summarize visual data for text communications.
-   Research and Education
    -   Natural Language Processing (NLP) and VLM Research: These
        models can serve as a foundation for researchers to experiment with VLM
        and NLP techniques, develop algorithms, and contribute to the
        advancement of the field.
    -   Language Learning Tools: Support interactive language learning
        experiences, aiding in grammar correction or providing writing practice.
    -   Knowledge Exploration: Assist researchers in exploring large
        bodies of text by generating summaries or answering questions about
        specific topics.""" .

mcro:llama2-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue "\"Llama-2: Open Foundation and Fine-tuned Chat Models\"(arxiv.org/abs/2307.09288)" .

mcro:llama2-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety." .

mcro:llama2-UseCase a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks." .

mcro:metaLlama31Instruct-License a mcro:LicenseInformationSection ;
    prov1:hasTextValue "A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)" .

mcro:metaLlama31Instruct-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety." .

mcro:visiontransformerbasesizedmodel-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """You can use the raw model for feature extraction. See the [model hub](https://huggingface.co/models?search=facebook/dinov2) to look for
fine-tuned versions on a task that interests you.""",
        "You can use the raw model for image classification." .

mcro:whisper-Citation a mcro:CitationInformationSection ;
    prov1:hasTextValue """@misc{radford2022whisper,
  doi = {10.48550/ARXIV.2212.04356},
  url = {https://arxiv.org/abs/2212.04356},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}""",
        "Alec Radford et al from OpenAI" .

mcro:whisper-Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "680k hours of labelled data" .

mcro:whisper-ModelArchitecture a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Transformer based encoder-decoder model" .

