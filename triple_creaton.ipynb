{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7625c86-7352-439a-a913-53e5c0603410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, ModelCard\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, json, string, time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import traceback\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b18890-39e9-4249-b76a-f004e719c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Authentication and Setup\n",
    "LTOKEN = \"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\"\n",
    "GTOKEN = \"AIzaSyDFZ8kE9GbQDjH30fxJsDnrfuJR-TrQYvg\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d58047f-849d-4536-ac7e-c58e78c66344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Gemini Models:\n",
      "- models/gemini-1.0-pro-vision-latest\n",
      "- models/gemini-pro-vision\n",
      "- models/gemini-1.5-pro-latest\n",
      "- models/gemini-1.5-pro-001\n",
      "- models/gemini-1.5-pro-002\n",
      "- models/gemini-1.5-pro\n",
      "- models/gemini-1.5-flash-latest\n",
      "- models/gemini-1.5-flash-001\n",
      "- models/gemini-1.5-flash-001-tuning\n",
      "- models/gemini-1.5-flash\n",
      "- models/gemini-1.5-flash-002\n",
      "- models/gemini-1.5-flash-8b\n",
      "- models/gemini-1.5-flash-8b-001\n",
      "- models/gemini-1.5-flash-8b-latest\n",
      "- models/gemini-1.5-flash-8b-exp-0827\n",
      "- models/gemini-1.5-flash-8b-exp-0924\n",
      "- models/gemini-2.0-flash-exp\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-lite-preview-02-05\n",
      "- models/gemini-2.0-flash-lite-preview\n",
      "- models/gemini-2.0-pro-exp\n",
      "- models/gemini-2.0-pro-exp-02-05\n",
      "- models/gemini-exp-1206\n",
      "- models/gemini-2.0-flash-thinking-exp-01-21\n",
      "- models/gemini-2.0-flash-thinking-exp\n",
      "- models/gemini-2.0-flash-thinking-exp-1219\n",
      "- models/learnlm-1.5-pro-experimental\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GTOKEN)\n",
    "\n",
    "# Verify API connectivity properly\n",
    "print(\"Working Gemini Models:\")\n",
    "for model in genai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(f\"- {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3fc587-e975-4ea4-bbe8-ba1c40a07d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 23:25:42.493738: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-10 23:25:42.512812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f98543ec44a42978b2917b3558182e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Load LLaMA Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=LTOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=LTOKEN,\n",
    "    torch_dtype=torch.float32,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66e01cc-0a59-4e64-afd8-850a255e978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ontology mappings\n",
    "HF_MAPPING = {\n",
    "    \"model_name\": \"ModelCard:Model\",\n",
    "    \"license\": \"ModelCard:License\",\n",
    "    \"architecture\": \"ModelCard:Architecture\",\n",
    "    \"co2_emitted\": \"ModelCard:EnvironmentalImpact\",\n",
    "    \"training_data\": \"ModelCard:TrainingData\",\n",
    "    \n",
    "    # Predicates for each property\n",
    "    \"license_predicate\": \"modelcard:hasLicense\",\n",
    "    \"architecture_predicate\": \"modelcard:hasArchitecture\",\n",
    "    \"co2_predicate\": \"modelcard:hasEnvironmentalImpact\",\n",
    "    \"training_data_predicate\": \"modelcard:hasTrainingData\",\n",
    "    \n",
    "    # New Mapping\n",
    "    \"datasets\": \"ModelCard:Dataset\",\n",
    "    \"language\": \"ModelCard:Language\",\n",
    "    \"metrics\": \"ModelCard:EvaluationMetric\",\n",
    "    \"base_model\": \"ModelCard:BaseModel\",\n",
    "    \"pipeline_tag\": \"ModelCard:PipelineType\",\n",
    "    \"library_name\": \"ModelCard:Library\",\n",
    "    \n",
    "    # Add new predicates\n",
    "    \"dataset_predicate\": \"modelcard:usesDataset\",\n",
    "    \"language_predicate\": \"modelcard:hasLanguage\",\n",
    "    \"metric_predicate\": \"modelcard:usesMetric\",\n",
    "    \"base_model_predicate\": \"modelcard:hasBaseModel\",\n",
    "    \"pipeline_predicate\": \"modelcard:hasPipelineType\",\n",
    "    \"library_predicate\": \"modelcard:usesLibrary\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7897d490-a52f-469c-8076-3357e0a517cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clean identifiers\n",
    "def clean_identifier(text):\n",
    "    \"\"\"Clean text to be used as an identifier\"\"\"\n",
    "    if not text:\n",
    "        return \"unknown\"\n",
    "    # Remove non-alphanumeric chars, replace spaces with underscores\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9-]', '', str(text).replace(' ', '-'))\n",
    "    # Ensure it's not empty\n",
    "    return cleaned if cleaned else \"unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ea035b-087f-48e9-8662-b11eecfe5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hf_entities(text):\n",
    "    \"\"\"Extract entities using ONLY Gemini/LLaMA (no regex fallback)\"\"\"\n",
    "    try:\n",
    "        unified_prompt = f\"\"\"Extract model metadata as JSON with VALID SPDX LICENSE IDS. Include all available fields:\n",
    "{{\n",
    "  \"Model name\": \"string\",\n",
    "  \"License type\": \"string (SPDX ID)\",\n",
    "  \"Architecture\": \"string\",\n",
    "  \"CO2 emitted\": \"float|null\",\n",
    "  \"Training data sources\": \"string\",\n",
    "  \"Datasets\": [\"string\"],\n",
    "  \"Languages\": [\"string\"],\n",
    "  \"Metrics\": [\"string\"],\n",
    "  \"Base model\": \"string\",\n",
    "  \"Pipeline tag\": \"string\",\n",
    "  \"Library name\": \"string\"\n",
    "}}\n",
    "\n",
    "Model card text:\n",
    "{text[:10000]}\"\"\"\n",
    "\n",
    "        # Try Gemini first\n",
    "        if GTOKEN:\n",
    "            genai.configure(api_key=GTOKEN)\n",
    "            gemini_model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "            \n",
    "            response = gemini_model.generate_content(unified_prompt)\n",
    "            json_str = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "            try:\n",
    "                entities = json.loads(json_str)\n",
    "                if not all(entities.get(k) for k in [\"License type\", \"Architecture\"]):\n",
    "                    raise ValueError(\"Missing required fields\")\n",
    "                return entities\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"Gemini validation failed: {str(e)}\")\n",
    "\n",
    "        # Fallback to LLaMA with same unified prompt\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=LTOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=LTOKEN,\n",
    "            torch_dtype=torch.float32,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        inputs = tokenizer(unified_prompt, return_tensors=\"pt\", max_length=4096, truncation=True).to(DEVICE)\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=600,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        json_str = re.search(r'\\{.*\\}', raw_output, re.DOTALL).group()\n",
    "        \n",
    "        # JSON cleaning\n",
    "        json_str = (json_str.replace(\"'\", '\"')\n",
    "                          .replace(\"None\", \"null\")\n",
    "                          .replace(\"True\", \"true\")\n",
    "                          .replace(\"False\", \"false\"))\n",
    "        json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
    "        \n",
    "        entities = json.loads(json_str)\n",
    "        if not entities.get(\"License type\"):\n",
    "            entities[\"License type\"] = \"unknown-license\"\n",
    "        \n",
    "        return entities\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"LLM extraction failed: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28618659-bc23-4fce-9a31-be32594b6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triple generation function\n",
    "def generate_hf_triples(model_data):\n",
    "    triples = []\n",
    "    prefix = \"hf:\"\n",
    "    \n",
    "    # Base model triple\n",
    "    model_uri = prefix + clean_identifier(model_data['id'])\n",
    "    triples.append({\n",
    "        \"s\": model_uri,\n",
    "        \"p\": \"rdf:type\",\n",
    "        \"o\": HF_MAPPING[\"model_name\"]\n",
    "    })\n",
    "    \n",
    "    # Process extracted entities\n",
    "    if \"entities\" in model_data:\n",
    "        # License\n",
    "        if \"License type\" in model_data[\"entities\"] and model_data[\"entities\"][\"License type\"]:\n",
    "            license_value = model_data[\"entities\"][\"License type\"]\n",
    "            license_uri = f\"{prefix}license-{clean_identifier(license_value)}\"\n",
    "            \n",
    "            triples.extend([\n",
    "                {\n",
    "                    \"s\": model_uri,\n",
    "                    \"p\": HF_MAPPING.get(\"license_predicate\", \"modelcard:hasLicense\"),\n",
    "                    \"o\": license_uri\n",
    "                },\n",
    "                {\n",
    "                    \"s\": license_uri,\n",
    "                    \"p\": \"rdfs:subClassOf\",\n",
    "                    \"o\": HF_MAPPING[\"license\"]\n",
    "                },\n",
    "                {\n",
    "                    \"s\": license_uri,\n",
    "                    \"p\": \"dul:hasParameterDataValue\",\n",
    "                    \"o\": str(license_value)\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "        # Architecture\n",
    "        if \"Architecture\" in model_data[\"entities\"] and model_data[\"entities\"][\"Architecture\"]:\n",
    "            arch_value = model_data[\"entities\"][\"Architecture\"]\n",
    "            arch_uri = f\"{prefix}Architecture-{clean_identifier(arch_value)}\"\n",
    "            \n",
    "            triples.extend([\n",
    "                {\n",
    "                    \"s\": model_uri,\n",
    "                    \"p\": HF_MAPPING.get(\"architecture_predicate\", \"modelcard:hasArchitecture\"),\n",
    "                    \"o\": arch_uri\n",
    "                },\n",
    "                {\n",
    "                    \"s\": arch_uri,\n",
    "                    \"p\": \"rdfs:subClassOf\",\n",
    "                    \"o\": HF_MAPPING[\"architecture\"]\n",
    "                },\n",
    "                {\n",
    "                    \"s\": arch_uri,\n",
    "                    \"p\": \"dul:hasParameterDataValue\",\n",
    "                    \"o\": str(arch_value)\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "        # CO2 emissions\n",
    "        if \"CO2 emitted\" in model_data[\"entities\"] and model_data[\"entities\"][\"CO2 emitted\"]:\n",
    "            try:\n",
    "                co2_value = float(model_data[\"entities\"][\"CO2 emitted\"])\n",
    "                co2_uri = f\"{prefix}CO2-{clean_identifier(str(co2_value))}\"\n",
    "                \n",
    "                triples.extend([\n",
    "                    {\n",
    "                        \"s\": model_uri,\n",
    "                        \"p\": HF_MAPPING.get(\"co2_predicate\", \"modelcard:hasEnvironmentalImpact\"),\n",
    "                        \"o\": co2_uri\n",
    "                    },\n",
    "                    {\n",
    "                        \"s\": co2_uri,\n",
    "                        \"p\": \"rdfs:subClassOf\",\n",
    "                        \"o\": HF_MAPPING[\"co2_emitted\"]\n",
    "                    },\n",
    "                    {\n",
    "                        \"s\": co2_uri,\n",
    "                        \"p\": \"dul:hasParameterDataValue\",\n",
    "                        \"o\": str(co2_value)\n",
    "                    }\n",
    "                ])\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Invalid CO2 value: {model_data['entities']['CO2 emitted']}\")\n",
    "        \n",
    "        # Training data\n",
    "        if \"Training data sources\" in model_data[\"entities\"] and model_data[\"entities\"][\"Training data sources\"]:\n",
    "            data_value = model_data[\"entities\"][\"Training data sources\"]\n",
    "            data_uri = f\"{prefix}TrainingData-{clean_identifier(data_value)[:30]}\"\n",
    "            \n",
    "            triples.extend([\n",
    "                {\n",
    "                    \"s\": model_uri,\n",
    "                    \"p\": HF_MAPPING.get(\"training_data_predicate\", \"modelcard:hasTrainingData\"),\n",
    "                    \"o\": data_uri\n",
    "                },\n",
    "                {\n",
    "                    \"s\": data_uri,\n",
    "                    \"p\": \"rdfs:subClassOf\",\n",
    "                    \"o\": HF_MAPPING[\"training_data\"]\n",
    "                },\n",
    "                {\n",
    "                    \"s\": data_uri,\n",
    "                    \"p\": \"dul:hasParameterDataValue\",\n",
    "                    \"o\": str(data_value)\n",
    "                }\n",
    "            ])\n",
    "    \n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac59436-820c-4e5f-8c86-a8bcc6428c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing pipeline\n",
    "def process_huggingface_models(limit=20):  # Reduced default limit for testing\n",
    "    api = HfApi(token=LTOKEN)\n",
    "    \n",
    "    # Get models sorted by downloads\n",
    "    models = list(api.list_models(\n",
    "        sort=\"downloads\",\n",
    "        direction=-1,\n",
    "        limit=limit,\n",
    "        token=LTOKEN\n",
    "    ))\n",
    "    \n",
    "    all_triples = []\n",
    "    for idx, model in enumerate(models):\n",
    "        try:\n",
    "            # Add slight delay to avoid rate limits\n",
    "            if idx % 10 == 0:\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            # Get model card text\n",
    "            try:\n",
    "                card = ModelCard.load(model.modelId, token=LTOKEN)\n",
    "                card_text = card.text\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading card for {model.modelId}: {str(e)}\")\n",
    "                # Create minimal triples even if we can't load the card\n",
    "                model_uri = f\"hf:{clean_identifier(model.modelId)}\"\n",
    "                all_triples.append({\n",
    "                    \"s\": model_uri,\n",
    "                    \"p\": \"rdf:type\",\n",
    "                    \"o\": HF_MAPPING[\"model_name\"]\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Extract entities using regex instead of LLaMA\n",
    "            extracted_entities = extract_hf_entities(card_text)\n",
    "            if not extracted_entities:\n",
    "                print(f\"Skipping {model.modelId} - no entities extracted\")\n",
    "                continue\n",
    "            \n",
    "            # Create model data structure\n",
    "            model_data = {\n",
    "                \"id\": model.modelId,\n",
    "                \"entities\": extracted_entities\n",
    "            }\n",
    "            \n",
    "            # Generate and collect triples\n",
    "            model_triples = generate_hf_triples(model_data)\n",
    "            all_triples.extend(model_triples)\n",
    "            \n",
    "            # Print progress\n",
    "            if idx % 5 == 0:\n",
    "                print(f\"Processed {idx+1}/{len(models)}: {model.modelId}\")\n",
    "                # Print number of triples for this model\n",
    "                print(f\"Generated {len(model_triples)} triples for this model\")\n",
    "                # Show sample triples with different predicates if available\n",
    "                predicates_shown = set()\n",
    "                for triple in model_triples:\n",
    "                    if triple[\"p\"] not in predicates_shown and len(predicates_shown) < 3:\n",
    "                        print(f\"Sample triple: {triple}\")\n",
    "                        predicates_shown.add(triple[\"p\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model.modelId}: {str(e)}\")\n",
    "            traceback.print_exc()  # Print full stack trace for debugging\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"top_models_triples.json\", \"w\") as f:\n",
    "        json.dump(all_triples, f, indent=2)\n",
    "    \n",
    "    print(\"\\n=== STATISTICS ===\")\n",
    "    predicates = {}\n",
    "    for triple in all_triples:\n",
    "        p = triple[\"p\"]\n",
    "        predicates[p] = predicates.get(p, 0) + 1\n",
    "    \n",
    "    print(\"Predicate counts:\")\n",
    "    for p, count in sorted(predicates.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {p}: {count}\")\n",
    "    \n",
    "    return all_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27f374a4-00de-4eb5-bddd-a4671931bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING PROCESS ===\n",
      "Processed 1/20: FacebookAI/xlm-roberta-large\n",
      "Generated 10 triples for this model\n",
      "Sample triple: {'s': 'hf:FacebookAIxlm-roberta-large', 'p': 'rdf:type', 'o': 'ModelCard:Model'}\n",
      "Sample triple: {'s': 'hf:FacebookAIxlm-roberta-large', 'p': 'modelcard:hasLicense', 'o': 'hf:license-MIT'}\n",
      "Sample triple: {'s': 'hf:license-MIT', 'p': 'rdfs:subClassOf', 'o': 'ModelCard:License'}\n",
      "Processed 6/20: timm/mobilenetv3_small_100.lamb_in1k\n",
      "Generated 10 triples for this model\n",
      "Sample triple: {'s': 'hf:timmmobilenetv3small100lambin1k', 'p': 'rdf:type', 'o': 'ModelCard:Model'}\n",
      "Sample triple: {'s': 'hf:timmmobilenetv3small100lambin1k', 'p': 'modelcard:hasLicense', 'o': 'hf:license-Apache-20'}\n",
      "Sample triple: {'s': 'hf:license-Apache-20', 'p': 'rdfs:subClassOf', 'o': 'ModelCard:License'}\n",
      "Processed 11/20: Bingsu/adetailer\n",
      "Generated 10 triples for this model\n",
      "Sample triple: {'s': 'hf:Bingsuadetailer', 'p': 'rdf:type', 'o': 'ModelCard:Model'}\n",
      "Sample triple: {'s': 'hf:Bingsuadetailer', 'p': 'modelcard:hasLicense', 'o': 'hf:license-MIT'}\n",
      "Sample triple: {'s': 'hf:license-MIT', 'p': 'rdfs:subClassOf', 'o': 'ModelCard:License'}\n",
      "Processed 16/20: openai/clip-vit-base-patch32\n",
      "Generated 10 triples for this model\n",
      "Sample triple: {'s': 'hf:openaiclip-vit-base-patch32', 'p': 'rdf:type', 'o': 'ModelCard:Model'}\n",
      "Sample triple: {'s': 'hf:openaiclip-vit-base-patch32', 'p': 'modelcard:hasLicense', 'o': 'hf:license-MIT'}\n",
      "Sample triple: {'s': 'hf:license-MIT', 'p': 'rdfs:subClassOf', 'o': 'ModelCard:License'}\n",
      "Gemini validation failed: Missing required fields\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be59c918c7bf4174a3841d7b32657c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM extraction failed: Extra data: line 15 column 1 (char 324)\n",
      "Skipping WhereIsAI/UAE-Large-V1 - no entities extracted\n",
      "\n",
      "=== STATISTICS ===\n",
      "Predicate counts:\n",
      "  rdfs:subClassOf: 57\n",
      "  dul:hasParameterDataValue: 57\n",
      "  rdf:type: 19\n",
      "  modelcard:hasLicense: 19\n",
      "  modelcard:hasArchitecture: 19\n",
      "  modelcard:hasTrainingData: 18\n",
      "  modelcard:hasEnvironmentalImpact: 1\n",
      "\n",
      "=== RESULTS SAVED TO FILE ===\n",
      "Total triples generated: 190\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== STARTING PROCESS ===\")\n",
    "    triples = process_huggingface_models(limit=20)  # Use smaller limit for testing\n",
    "    \n",
    "    print(\"\\n=== RESULTS SAVED TO FILE ===\")\n",
    "    print(f\"Total triples generated: {len(triples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be344dbd-c70e-4d46-9b70-43ee5a4b1432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
