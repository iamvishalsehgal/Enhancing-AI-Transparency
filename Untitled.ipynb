{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1d79ec-58b3-46d3-a85a-031fb3d2e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * WARNING: ObjectProperty http://www.w3.org/ns/prov#specializationOf belongs to more than one entity types: [owl.AnnotationProperty, owl.ObjectProperty, prov.alternateOf]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: ObjectProperty http://www.w3.org/ns/prov#wasRevisionOf belongs to more than one entity types: [owl.AnnotationProperty, owl.ObjectProperty, prov.wasDerivedFrom]; I'm trying to fix it...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ONTOLOGY-ALIGNED TRIPLE EXTRACTION STARTED ===\n",
      "Processed 1/50: timm/mobilenetv3_small_100.lamb_in1k\n",
      "Processed 6/50: openai/clip-vit-large-patch14\n",
      "Processed 11/50: facebook/esmfold_v1\n",
      "Processed 16/50: timm/resnet50.a1_in1k\n",
      "Processed 21/50: distilbert/distilbert-base-uncased\n",
      "Processed 26/50: CIDAS/clipseg-rd64-refined\n",
      "Processed 31/50: facebook/opt-125m\n",
      "Processed 36/50: google/siglip-so400m-patch14-384\n",
      "Processed 41/50: jonatasgrosman/wav2vec2-large-xlsr-53-japanese\n",
      "Processed 46/50: google-t5/t5-base\n",
      "\n",
      "=== STATISTICS ===\n",
      "Total models processed: 50\n",
      "Total triples generated: 2614\n",
      "\n",
      "=== COMPLETED ===\n",
      "Final triple count: 2614\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, ModelCard\n",
    "import json, re, time\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import owlready2\n",
    "from rdflib import Graph, URIRef\n",
    "from owlready2 import get_ontology, default_world\n",
    "\n",
    "\n",
    "# Authentication\n",
    "HFTOKEN = \"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\"\n",
    "GTOKEN = \"AIzaSyB8HMqIGvscURWPF75CwnZlXnFFsGh0Vlg\"\n",
    "onto = get_ontology(\"http://purl.obolibrary.org/obo/mcro.owl\").load()\n",
    "ONTOLOGY_BASE = \"http://purl.obolibrary.org/obo/mcro.owl#\"\n",
    "\n",
    "def clean_identifier(text):\n",
    "    \"\"\"Clean text for URI safety\"\"\"\n",
    "    if not text:\n",
    "        return \"unknown\"\n",
    "    return re.sub(r'[^a-zA-Z0-9-]', '', str(text).replace(' ', '-'))[:50]\n",
    "\n",
    "def get_ontology_mapping(entity_key):\n",
    "    # Check if class exists in ontology\n",
    "    clean_key = ''.join(w.capitalize() for w in re.split('[^a-zA-Z0-9]', entity_key))\n",
    "    ontology_class = onto.search_one(iri=f\"*{clean_key}\")\n",
    "    \n",
    "    if ontology_class:\n",
    "        return {\n",
    "            \"class_uri\": ontology_class.iri,\n",
    "            \"predicate_uri\": f\"{ontology_class.namespace.base_iri}has{clean_key}\"\n",
    "        }\n",
    "    else:\n",
    "        # Fallback if class not found\n",
    "        return {\n",
    "            \"class_uri\": f\"mcro:Custom{clean_key}\",\n",
    "            \"predicate_uri\": f\"mcro:has{clean_key}\"\n",
    "        }\n",
    "\n",
    "def extract_hf_entities(text):\n",
    "    \"\"\"Ontology-aware entity extraction with enhanced prompt\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "** STRICT JSON EXTRACTION INSTRUCTIONS **\n",
    "\n",
    "Extract technical metadata from the model card below, following the ontology: \n",
    "{ONTOLOGY_BASE}\n",
    "\n",
    "** REQUIRED FIELDS (use ontology terms when possible): **\n",
    "- License (license)\n",
    "- Architecture (architecture)\n",
    "- Training datasets (trainingData) [array]\n",
    "- Evaluation metrics (metrics) [array]\n",
    "- Hardware used (hardware) [array]\n",
    "- Libraries/frameworks (libraries) [array]\n",
    "- Model type (modelType)\n",
    "- Languages supported (languages) [array]\n",
    "- Pre-trained model (pretrainedModel)\n",
    "- Publication citations (citation) [array]\n",
    "\n",
    "** OUTPUT SPECIFICATIONS: **\n",
    "1. Use STRICT JSON format (NO markdown)\n",
    "2. ALL keys must be in double quotes\n",
    "3. Strings with special characters must use escaped quotes (\\\")\n",
    "4. URLs MUST be complete (http://... not \"http:\")\n",
    "5. Arrays must have proper comma separation\n",
    "6. Use ontology URIs when possible (e.g., \"{ONTOLOGY_BASE}Transformer\")\n",
    "7. Omit fields with no valid data\n",
    "\n",
    "** EXAMPLE FORMAT: **\n",
    "{{\n",
    "  \"license\": \"MIT\",\n",
    "  \"architecture\": \"{ONTOLOGY_BASE}Transformer\",\n",
    "  \"trainingData\": [\n",
    "    \"http://purl.obolibrary.org/obo/mcro.owl#ImageNet-1k\",\n",
    "    \"Wikipedia\"\n",
    "  ],\n",
    "  \"metrics\": {{\n",
    "    \"accuracy\": 0.95,\n",
    "    \"f1\": 0.93\n",
    "  }},\n",
    "  \"libraries\": [\"PyTorch\", \"HuggingFace\"]\n",
    "}}\n",
    "\n",
    "** MODEL CARD TEXT: **\n",
    "{text[:10000]}\n",
    "\"\"\"\n",
    "        \n",
    "        genai.configure(api_key=GTOKEN)\n",
    "        model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "        response = model.generate_content(prompt)\n",
    "        json_str = response.text.strip()\n",
    "        json_str = re.sub(r'^```json\\s*|```.*$', '', json_str, flags=re.IGNORECASE)\n",
    "        return json.loads(json_str)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def generate_hf_triples(model_data):\n",
    "    triples = []\n",
    "    model_uri = f\"{clean_identifier(model_data['id'])}\"\n",
    "    \n",
    "    # Base type assertion using ontology\n",
    "    triples.append({\n",
    "        \"s\": model_uri,\n",
    "        \"p\": \"rdf:type\",\n",
    "        \"o\": f\"{ONTOLOGY_BASE}Model\"\n",
    "    })\n",
    "\n",
    "    if \"entities\" in model_data:\n",
    "        for key, value in model_data[\"entities\"].items():\n",
    "            if not value or key.lower() in ['id', 'model']:\n",
    "                continue\n",
    "\n",
    "            mapping = get_ontology_mapping(key)\n",
    "            values = value if isinstance(value, list) else [value]\n",
    "\n",
    "            for val in values:\n",
    "                if pd.isna(val) or str(val).lower() in ['none', 'null', 'nan']:\n",
    "                    continue\n",
    "\n",
    "                entity_uri = f\"{model_uri}-{clean_identifier(key)}-{clean_identifier(str(val))}\"\n",
    "                \n",
    "                triples.extend([\n",
    "                    {\"s\": model_uri, \"p\": mapping['predicate_uri'], \"o\": entity_uri},\n",
    "                    {\"s\": entity_uri, \"p\": \"rdf:type\", \"o\": mapping['class_uri']},\n",
    "                    {\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": str(val)},\n",
    "                    {\"s\": mapping['class_uri'], \"p\": \"rdfs:subClassOf\", \n",
    "                     \"o\": f\"{ONTOLOGY_BASE}Component\"}\n",
    "                ])\n",
    "    \n",
    "    return triples\n",
    "\n",
    "def process_huggingface_models(limit=20):\n",
    "    api = HfApi(token=HFTOKEN)\n",
    "    models = list(api.list_models(sort=\"downloads\", direction=-1, limit=limit))\n",
    "    all_triples = []\n",
    "\n",
    "    for idx, model in enumerate(models):\n",
    "        try:\n",
    "            card = ModelCard.load(model.modelId, token=HFTOKEN)\n",
    "            entities = extract_hf_entities(card.text)\n",
    "            \n",
    "            if not entities:\n",
    "                print(f\"Skipping {model.modelId} - no entities found\")\n",
    "                continue\n",
    "\n",
    "            model_data = {\"id\": model.modelId, \"entities\": entities}\n",
    "            triples = generate_hf_triples(model_data)\n",
    "            all_triples.extend(triples)\n",
    "\n",
    "            if idx % 5 == 0:\n",
    "                print(f\"Processed {idx+1}/{len(models)}: {model.modelId}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model.modelId}: {str(e)}\")\n",
    "\n",
    "    with open(\"ontology_aligned_triples.json\", \"w\") as f:\n",
    "        json.dump(all_triples, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== STATISTICS ===\")\n",
    "    print(f\"Total models processed: {len(models)}\")\n",
    "    print(f\"Total triples generated: {len(all_triples)}\")\n",
    "    \n",
    "    return all_triples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== ONTOLOGY-ALIGNED TRIPLE EXTRACTION STARTED ===\")\n",
    "    triples = process_huggingface_models(limit=50)\n",
    "    print(\"\\n=== COMPLETED ===\")\n",
    "    print(f\"Final triple count: {len(triples)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
