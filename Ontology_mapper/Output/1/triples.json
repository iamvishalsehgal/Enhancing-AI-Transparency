[
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:mobilenetv3small100lambin1k-ModelDetail"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:mobilenetv3small100lambin1k-Citation"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{howard2019searching,\n  title={Searching for mobilenetv3},\n  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},\n  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},\n  pages={1314--1324},\n  year={2019}\n}"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mobilenetv3small100lambin1k-ModelArchitecture"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:mobilenetv3small100lambin1k-Dataset"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:mobilenetv3small100lambin1k-UseCase"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Classification"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "mcro:hasModelParameter",
    "o": "mcro:mobilenetv3small100lambin1k-ModelParameter"
  },
  {
    "s": "mcro:allMiniLML6v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:allMiniLML6v2",
    "p": "mcro:hasUseCaseInformationSection",
    "o": "mcro:allMiniLML6v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated."
  },
  {
    "s": "mcro:allMiniLML6v2",
    "p": "mcro:hasTrainingDataInformationSection",
    "o": "mcro:allMiniLML6v2-TrainingDataInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file."
  },
  {
    "s": "mcro:allMiniLML6v2",
    "p": "mcro:hasModelArchitectureInformationSection",
    "o": "mcro:allMiniLML6v2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset."
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Falconsainsfwimagedetection-ModelDetail"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "mcro:hasUseCase",
    "o": "mcro:Falconsainsfwimagedetection-UseCase"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "mcro:hasTrainingData",
    "o": "mcro:Falconsainsfwimagedetection-TrainingData"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "mcro:hasReference",
    "o": "mcro:Falconsainsfwimagedetection-Reference"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:Falconsainsfwimagedetection-License"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:Falconsainsfwimagedetection-Architecture"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-UseCase",
    "p": "mcro:hasLimitation",
    "o": "mcro:Falconsainsfwimagedetection-Limitation"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-UseCase",
    "p": "prov:hasTextValue",
    "o": "NSFW Image Classification"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Architecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT)"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Architecture",
    "p": "prov:hasTextValue",
    "o": "transformer encoder"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "google/vit-base-patch16-224-in21k"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-TrainingData",
    "p": "prov:hasTextValue",
    "o": "proprietary dataset"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-TrainingData",
    "p": "prov:hasTextValue",
    "o": "80,000 images"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Reference",
    "p": "prov:hasTextValue",
    "o": "Hugging Face Model Hub"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Reference",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT) Paper"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Reference",
    "p": "prov:hasTextValue",
    "o": "ImageNet-21k Dataset"
  },
  {
    "s": "mcro:dima806fairfaceageimagedetection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:dima806fairfaceageimagedetection",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:dima806fairfaceageimagedetection-Performance"
  },
  {
    "s": "mcro:dima806fairfaceageimagedetection-Performance",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:dima806fairfaceageimagedetection-Performance",
    "p": "prov:hasTextValue",
    "o": "59% accuracy"
  },
  {
    "s": "mcro:bertbasemodeluncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertbasemodeluncased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertbasemodeluncased-ModelDetail"
  },
  {
    "s": "mcro:bertbasemodeluncased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertbasemodeluncased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertbasemodeluncased-Citation"
  },
  {
    "s": "mcro:bertbasemodeluncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertbasemodeluncased-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertbasemodeluncased-Architecture"
  },
  {
    "s": "mcro:bertbasemodeluncased-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertbasemodeluncased",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bertbasemodeluncased-IntendedUseCase"
  },
  {
    "s": "mcro:bertbasemodeluncased-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertbasemodeluncased",
    "p": "mcro:hasConsideration",
    "o": "mcro:bertbasemodeluncased-Consideration"
  },
  {
    "s": "mcro:bertbasemodeluncased-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:bertbasemodeluncased-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:bertbasemodeluncased-Limitation"
  },
  {
    "s": "mcro:bertbasemodeluncased-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:bertbasemodeluncased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertbasemodeluncased-TrainingData"
  },
  {
    "s": "mcro:bertbasemodeluncased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:clip",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasModelDetail",
    "o": "mcro:clip-ModelDetail"
  },
  {
    "s": "mcro:clip-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:clip-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:clip-Citation"
  },
  {
    "s": "mcro:clip-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clip-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:clip-ModelArchitecture"
  },
  {
    "s": "mcro:clip-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasUseCase",
    "o": "mcro:clip-UseCase"
  },
  {
    "s": "mcro:clip-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clip-UseCase",
    "p": "mcro:hasPrimaryIntendedUseCase",
    "o": "mcro:clip-PrimaryIntendedUseCase"
  },
  {
    "s": "mcro:clip-PrimaryIntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:clip-UseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:clip-OutOfScopeUseCase"
  },
  {
    "s": "mcro:clip-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasDataset",
    "o": "mcro:clip-Dataset"
  },
  {
    "s": "mcro:clip-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clip-Dataset",
    "p": "mcro:hasTrainingData",
    "o": "mcro:clip-TrainingData"
  },
  {
    "s": "mcro:clip-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:clip-QuantativeAnalysis"
  },
  {
    "s": "mcro:clip-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:clip-QuantativeAnalysis",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:clip-Performance"
  },
  {
    "s": "mcro:clip-Performance",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasConsideration",
    "o": "mcro:clip-Consideration"
  },
  {
    "s": "mcro:clip-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:clip-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:clip-Limitation"
  },
  {
    "s": "mcro:clip-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:clip-Limitation",
    "p": "prov:hasTextValue",
    "o": "CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance."
  },
  {
    "s": "mcro:clip-Consideration",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:clip-BiasAndFairness"
  },
  {
    "s": "mcro:clip-BiasAndFairness",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:clip-BiasAndFairness",
    "p": "prov:hasTextValue",
    "o": "We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks."
  },
  {
    "s": "mcro:clip-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer."
  },
  {
    "s": "mcro:clip-Citation",
    "p": "prov:hasTextValue",
    "o": "- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)"
  },
  {
    "s": "mcro:clip-PrimaryIntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis."
  },
  {
    "s": "mcro:clip-OutOfScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases."
  },
  {
    "s": "mcro:clip-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users."
  },
  {
    "s": "mcro:clip-Performance",
    "p": "prov:hasTextValue",
    "o": "We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid"
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasDescription",
    "o": "mcro:TheBlokephi2GGUF-Description"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Description",
    "p": "rdf:type",
    "o": "obo:IAO_0000310"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Description",
    "p": "prov:hasTextValue",
    "o": "This repo contains GGUF format model files for [Microsoft's Phi 2](https://huggingface.co/microsoft/phi-2)."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasLicense",
    "o": "mcro:TheBlokephi2GGUF-License"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-License",
    "p": "prov:hasTextValue",
    "o": "The model is licensed under the [microsoft-research-license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE)."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:TheBlokephi2GGUF-ModelArchitecture"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Phi-2 is a Transformer with **2.7 billion** parameters."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:TheBlokephi2GGUF-IntendedUseCase"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "Phi-2 is intended for research purposes only."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasLimitation",
    "o": "mcro:TheBlokephi2GGUF-Limitation"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Limitation",
    "p": "prov:hasTextValue",
    "o": "* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n\n* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n\n* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n\n* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring trainig data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n\n* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model for research purposes only -- We hope to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n\n* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasDataset",
    "o": "mcro:TheBlokephi2GGUF-Dataset"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Dataset",
    "p": "prov:hasTextValue",
    "o": "Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasCitation",
    "o": "mcro:TheBlokephi2GGUF-Citation"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Citation",
    "p": "prov:hasTextValue",
    "o": "Microsoft's Phi 2"
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronos-t5-small-architecture"
  },
  {
    "s": "mcro:chronos-t5-small-architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-architecture",
    "p": "prov:hasTextValue",
    "o": "based on the [T5 architecture](https://arxiv.org/abs/1910.10683)"
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasCitation",
    "o": "mcro:chronos-t5-small-citation"
  },
  {
    "s": "mcro:chronos-t5-small-citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasLicense",
    "o": "mcro:chronos-t5-small-license"
  },
  {
    "s": "mcro:chronos-t5-small-license",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-license",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0 License"
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronos-t5-small-usecase"
  },
  {
    "s": "mcro:chronos-t5-small-usecase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-usecase",
    "p": "prov:hasTextValue",
    "o": "pretrained time series forecasting models"
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasDataset",
    "o": "mcro:chronos-t5-small-dataset"
  },
  {
    "s": "mcro:chronos-t5-small-dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-dataset",
    "p": "prov:hasTextValue",
    "o": "large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes."
  },
  {
    "s": "mcro:robertalargemodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:robertalargemodel",
    "p": "mcro:hasArchitecture",
    "o": "mcro:robertalargemodel-Architecture"
  },
  {
    "s": "mcro:robertalargemodel-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:robertalargemodel-Architecture",
    "p": "prov:hasTextValue",
    "o": "transformers model"
  },
  {
    "s": "mcro:robertalargemodel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:robertalargemodel-UseCase"
  },
  {
    "s": "mcro:robertalargemodel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:robertalargemodel-UseCase",
    "p": "prov:hasTextValue",
    "o": "masked language modeling"
  },
  {
    "s": "mcro:robertalargemodel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:robertalargemodel-TrainingData"
  },
  {
    "s": "mcro:robertalargemodel-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:robertalargemodel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "BookCorpus"
  },
  {
    "s": "mcro:robertalargemodel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "English Wikipedia"
  },
  {
    "s": "mcro:robertalargemodel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "CC-News"
  },
  {
    "s": "mcro:robertalargemodel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "OpenWebText"
  },
  {
    "s": "mcro:robertalargemodel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Stories"
  },
  {
    "s": "mcro:ESMFold",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ESMFold",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ESMFold-ModelArchitecture"
  },
  {
    "s": "mcro:ESMFold-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ESMFold-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "ESMFold is a state-of-the-art end-to-end protein folding model based on an ESM-2 backbone."
  },
  {
    "s": "mcro:ESMFold",
    "p": "mcro:hasCitation",
    "o": "mcro:ESMFold-Citation"
  },
  {
    "s": "mcro:ESMFold-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ESMFold-Citation",
    "p": "prov:hasTextValue",
    "o": "For details on the model architecture and training, please refer to the accompanying paper"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "mcro:hasDataset",
    "o": "mcro:YOLOv8DetectionModel-DatasetInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "mcro:hasUseCase",
    "o": "mcro:YOLOv8DetectionModel-UseCaseInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:YOLOv8DetectionModel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:allmpnetbasev2",
    "p": "mcro:hasUseCase",
    "o": "mcro:allmpnetbasev2-UseCaseInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 384 word pieces is truncated."
  },
  {
    "s": "mcro:allmpnetbasev2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:allmpnetbasev2-TrainingDataInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file."
  },
  {
    "s": "mcro:allmpnetbasev2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:allmpnetbasev2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset."
  },
  {
    "s": "mcro:electramodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:electramodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:electramodel-ModelArchitecture"
  },
  {
    "s": "mcro:electramodel-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:electramodel-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformer networks"
  },
  {
    "s": "mcro:electramodel",
    "p": "mcro:hasUseCase",
    "o": "mcro:electramodel-UseCase"
  },
  {
    "s": "mcro:electramodel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:electramodel-UseCase",
    "p": "prov:hasTextValue",
    "o": "classification tasks"
  },
  {
    "s": "mcro:electramodel-UseCase",
    "p": "prov:hasTextValue",
    "o": "QA tasks"
  },
  {
    "s": "mcro:electramodel-UseCase",
    "p": "prov:hasTextValue",
    "o": "sequence tagging tasks"
  },
  {
    "s": "mcro:electramodel",
    "p": "mcro:hasCitation",
    "o": "mcro:electramodel-Citation"
  },
  {
    "s": "mcro:electramodel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:electramodel-Citation",
    "p": "prov:hasTextValue",
    "o": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "mcro:hasLicense",
    "o": "mcro:pyannotewespeakervoxcelebresnet34LM-License"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-License",
    "p": "prov:hasTextValue",
    "o": "According to [this page](https://github.com/wenet-e2e/wespeaker/blob/master/docs/pretrained.md):\n\n> The pretrained model in WeNet follows the license of it's corresponding dataset. For example, the pretrained model on VoxCeleb follows Creative Commons Attribution 4.0 International License., since it is used as license of the VoxCeleb dataset, see https://mm.kaist.ac.kr/datasets/voxceleb/."
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation1"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation1",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Wang2023,\n  title={Wespeaker: A research and production oriented speaker embedding learning toolkit},\n  author={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},\n  booktitle={ICASSP 2023, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={1--5},\n  year={2023},\n  organization={IEEE}\n}"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation2"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation2",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n  pages={1983--1987},\n  doi={10.21437/Interspeech.2023-105}\n}"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:pyannotewespeakervoxcelebresnet34LM-ModelArchitecture"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "wrapper around [WeSpeaker](https://github.com/wenet-e2e/wespeaker) `wespeaker-voxceleb-resnet34-LM` pretrained speaker embedding model, for use in `pyannote.audio`."
  },
  {
    "s": "mcro:resnet50a1in1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:resnet50a1in1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:resnet50a1in1k-ModelDetail"
  },
  {
    "s": "mcro:resnet50a1in1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:resnet50a1in1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:resnet50a1in1k-ModelArchitecture"
  },
  {
    "s": "mcro:resnet50a1in1k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:resnet50a1in1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "ResNet-B image classification model"
  },
  {
    "s": "mcro:resnet50a1in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:resnet50a1in1k-Citation"
  },
  {
    "s": "mcro:resnet50a1in1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:resnet50a1in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n"
  },
  {
    "s": "mcro:resnet50a1in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n"
  },
  {
    "s": "mcro:resnet50a1in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n"
  },
  {
    "s": "mcro:resnet50a1in1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:resnet50a1in1k-UseCase"
  },
  {
    "s": "mcro:resnet50a1in1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:resnet50a1in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Classification"
  },
  {
    "s": "mcro:resnet50a1in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Feature Map Extraction"
  },
  {
    "s": "mcro:resnet50a1in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Embeddings"
  },
  {
    "s": "mcro:resnet50a1in1k",
    "p": "mcro:hasDataset",
    "o": "mcro:resnet50a1in1k-Dataset"
  },
  {
    "s": "mcro:resnet50a1in1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:resnet50a1in1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:llama318BInstructGGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llama318BInstructGGUF",
    "p": "mcro:hasModelDetail",
    "o": "mcro:llama318BInstructGGUF-ModelDetail"
  },
  {
    "s": "mcro:llama318BInstructGGUF-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llama318BInstructGGUF-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:llama318BInstructGGUF-License"
  },
  {
    "s": "mcro:llama318BInstructGGUF-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llama318BInstructGGUF-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llama318BInstructGGUF-Architecture"
  },
  {
    "s": "mcro:llama318BInstructGGUF-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama318BInstructGGUF",
    "p": "mcro:hasIntendedUse",
    "o": "mcro:llama318BInstructGGUF-IntendedUse"
  },
  {
    "s": "mcro:llama318BInstructGGUF-IntendedUse",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llama318BInstructGGUF",
    "p": "mcro:hasDataset",
    "o": "mcro:llama318BInstructGGUF-Dataset"
  },
  {
    "s": "mcro:llama318BInstructGGUF-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clip",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasCitation",
    "o": "mcro:clip-CitationInformationSection"
  },
  {
    "s": "mcro:clip-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clip-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md)."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasModelDetail",
    "o": "mcro:clip-ModelDetailSection"
  },
  {
    "s": "mcro:clip-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:clip-ModelDetailSection",
    "p": "prov:hasTextValue",
    "o": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within."
  },
  {
    "s": "mcro:clip-ModelDetailSection",
    "p": "mcro:hasVersion",
    "o": "mcro:clip-VersionInformationSection"
  },
  {
    "s": "mcro:clip-VersionInformationSection",
    "p": "rdf:type",
    "o": "mcro:VersionInformationSection"
  },
  {
    "s": "mcro:clip-VersionInformationSection",
    "p": "prov:hasTextValue",
    "o": "January 2021"
  },
  {
    "s": "mcro:clip-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:clip-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clip-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clip-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer."
  },
  {
    "s": "mcro:clip-ModelDetailSection",
    "p": "mcro:hasReference",
    "o": "mcro:clip-ReferenceInformationSection"
  },
  {
    "s": "mcro:clip-ReferenceInformationSection",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:clip-ReferenceInformationSection",
    "p": "prov:hasTextValue",
    "o": "- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasUseCase",
    "o": "mcro:clip-UseCaseInformationSection"
  },
  {
    "s": "mcro:clip-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clip-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis."
  },
  {
    "s": "mcro:clip-UseCaseInformationSection",
    "p": "mcro:hasUser",
    "o": "mcro:clip-UserInformationSection"
  },
  {
    "s": "mcro:clip-UserInformationSection",
    "p": "rdf:type",
    "o": "mcro:UserInformationSection"
  },
  {
    "s": "mcro:clip-UserInformationSection",
    "p": "prov:hasTextValue",
    "o": "The primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
  },
  {
    "s": "mcro:clip-UseCaseInformationSection",
    "p": "mcro:hasConsideration",
    "o": "mcro:clip-ConsiderationInformationSection"
  },
  {
    "s": "mcro:clip-ConsiderationInformationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:clip-ConsiderationInformationSection",
    "p": "prov:hasTextValue",
    "o": "**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasDataset",
    "o": "mcro:clip-DatasetInformationSection"
  },
  {
    "s": "mcro:clip-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clip-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users."
  },
  {
    "s": "mcro:clip-DatasetInformationSection",
    "p": "mcro:hasConsideration",
    "o": "mcro:clip-ConsiderationInformationSection2"
  },
  {
    "s": "mcro:clip-ConsiderationInformationSection2",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:clip-ConsiderationInformationSection2",
    "p": "prov:hasTextValue",
    "o": "Our goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:clip-QuantativeAnalysisSection"
  },
  {
    "s": "mcro:clip-QuantativeAnalysisSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:clip-QuantativeAnalysisSection",
    "p": "prov:hasTextValue",
    "o": "We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid"
  },
  {
    "s": "mcro:clip-QuantativeAnalysisSection",
    "p": "mcro:hasLimitation",
    "o": "mcro:clip-LimitationInformationSection"
  },
  {
    "s": "mcro:clip-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:clip-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance."
  },
  {
    "s": "mcro:clip-QuantativeAnalysisSection",
    "p": "mcro:hasRisk",
    "o": "mcro:clip-RiskInformationSection"
  },
  {
    "s": "mcro:clip-RiskInformationSection",
    "p": "rdf:type",
    "o": "mcro:RiskInformationSection"
  },
  {
    "s": "mcro:clip-RiskInformationSection",
    "p": "prov:hasTextValue",
    "o": "We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks."
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:pyannotesegmentation30-UseCase"
  },
  {
    "s": "mcro:pyannotesegmentation30-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-UseCase",
    "p": "prov:hasTextValue",
    "o": "speaker segmentation"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "mcro:hasInputFormat",
    "o": "mcro:pyannotesegmentation30-InputFormat"
  },
  {
    "s": "mcro:pyannotesegmentation30-InputFormat",
    "p": "rdf:type",
    "o": "mcro:FormatInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-InputFormat",
    "p": "prov:hasTextValue",
    "o": "10 seconds of mono audio sampled at 16kHz"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "mcro:hasOutputFormat",
    "o": "mcro:pyannotesegmentation30-OutputFormat"
  },
  {
    "s": "mcro:pyannotesegmentation30-OutputFormat",
    "p": "rdf:type",
    "o": "mcro:FormatInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-OutputFormat",
    "p": "prov:hasTextValue",
    "o": "(num_frames, num_classes) matrix where the 7 classes are _non-speech_, _speaker #1_, _speaker #2_, _speaker #3_, _speakers #1 and #2_, _speakers #1 and #3_, and _speakers #2 and #3_"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "mcro:hasTrainingDataset",
    "o": "mcro:pyannotesegmentation30-Dataset"
  },
  {
    "s": "mcro:pyannotesegmentation30-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-Dataset",
    "p": "prov:hasTextValue",
    "o": "AISHELL, AliMeeting, AMI, AVA-AVD, DIHARD, Ego4D, MSDWild, REPERE, and VoxConverse"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotesegmentation30-Citation1"
  },
  {
    "s": "mcro:pyannotesegmentation30-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-Citation1",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Plaquet23,\n  author={Alexis Plaquet and Herv\u00e9 Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotesegmentation30-Citation2"
  },
  {
    "s": "mcro:pyannotesegmentation30-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-Citation2",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}"
  },
  {
    "s": "mcro:gpt2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gpt2-ModelDetail"
  },
  {
    "s": "mcro:gpt2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gpt2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:gpt2-Citation"
  },
  {
    "s": "mcro:gpt2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gpt2-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasUseCase",
    "o": "mcro:gpt2-UseCase"
  },
  {
    "s": "mcro:gpt2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasConsideration",
    "o": "mcro:gpt2-Consideration"
  },
  {
    "s": "mcro:gpt2-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:gpt2-TrainingData"
  },
  {
    "s": "mcro:gpt2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasEvaluation",
    "o": "mcro:gpt2-Evaluation"
  },
  {
    "s": "mcro:gpt2-Evaluation",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:distilbertbasemodeluncased-ModelDetailSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:distilbertbasemodeluncased-CitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:distilbertbasemodeluncased-LicenseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:distilbertbasemodeluncased-UseCaseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:distilbertbasemodeluncased-TrainingDataInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers)."
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "mcro:hasLimitation",
    "o": "mcro:distilbertbasemodeluncased-LimitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:clipsegModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clipsegModel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:clipsegModel-UseCase"
  },
  {
    "s": "mcro:clipsegModel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clipsegModel-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model is intended for zero-shot and one-shot image segmentation."
  },
  {
    "s": "mcro:clipsegModel",
    "p": "mcro:hasCitation",
    "o": "mcro:clipsegModel-Citation"
  },
  {
    "s": "mcro:clipsegModel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clipsegModel-Citation",
    "p": "prov:hasTextValue",
    "o": "It was introduced in the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by L\u00fcddecke et al. and first released in [this repository](https://github.com/timojl/clipseg)."
  },
  {
    "s": "mcro:pyannotespeakerdiarization31",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotespeakerdiarization31-Citation"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Plaquet23,\n  author={Alexis Plaquet and Herv\u00e9 Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotespeakerdiarization31-Citation2"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-Citation2",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}"
  },
  {
    "s": "mcro:XLMROBERTaModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:XLMROBERTaModel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:XLMROBERTaModel-ModelArchitecture"
  },
  {
    "s": "mcro:XLMROBERTaModel-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:XLMROBERTaModel-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers"
  },
  {
    "s": "mcro:XLMROBERTaModel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:XLMROBERTaModel-IntendedUseCase"
  },
  {
    "s": "mcro:XLMROBERTaModel-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:XLMROBERTaModel-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "masked language modeling"
  },
  {
    "s": "mcro:XLMROBERTaModel",
    "p": "mcro:hasCitation",
    "o": "mcro:XLMROBERTaModel-Citation"
  },
  {
    "s": "mcro:XLMROBERTaModel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:XLMROBERTaModel-Citation",
    "p": "prov:hasTextValue",
    "o": "Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov"
  },
  {
    "s": "mcro:RoBERTa_base_model",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:RoBERTa_base_model",
    "p": "mcro:hasCitation",
    "o": "mcro:RoBERTa_base_model-CitationInformationSection"
  },
  {
    "s": "mcro:RoBERTa_base_model-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:RoBERTa_base_model-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:RoBERTa_base_model",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:RoBERTa_base_model-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:RoBERTa_base_model-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:RoBERTa_base_model-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion"
  },
  {
    "s": "mcro:RoBERTa_base_model",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:RoBERTa_base_model-UseCaseInformationSection"
  },
  {
    "s": "mcro:RoBERTa_base_model-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:RoBERTa_base_model-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task."
  },
  {
    "s": "mcro:RoBERTa_base_model",
    "p": "mcro:hasTrainingData",
    "o": "mcro:RoBERTa_base_model-DatasetInformationSection"
  },
  {
    "s": "mcro:RoBERTa_base_model-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:RoBERTa_base_model-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "The RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text."
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-UseCase"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-UseCase",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-Architecture"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-Architecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-Citation"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronosboltbase-ModelArchitecture"
  },
  {
    "s": "mcro:chronosboltbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronosboltbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "T5 encoder-decoder architecture"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "mcro:hasCitation",
    "o": "mcro:chronosboltbase-Citation"
  },
  {
    "s": "mcro:chronosboltbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronosboltbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "mcro:hasLicense",
    "o": "mcro:chronosboltbase-License"
  },
  {
    "s": "mcro:chronosboltbase-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronosboltbase-License",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0 License"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronosboltbase-UseCase"
  },
  {
    "s": "mcro:chronosboltbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronosboltbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "zero-shot forecasting"
  },
  {
    "s": "mcro:sentencetransformersusecmlmmultilingual",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersusecmlmmultilingual",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersusecmlmmultilingual-ModelArchitecture"
  },
  {
    "s": "mcro:sentencetransformersusecmlmmultilingual-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersusecmlmmultilingual-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Normalize()\n)"
  },
  {
    "s": "mcro:sentencetransformersusecmlmmultilingual",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersusecmlmmultilingual-Citation"
  },
  {
    "s": "mcro:sentencetransformersusecmlmmultilingual-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersusecmlmmultilingual-Citation",
    "p": "prov:hasTextValue",
    "o": "Have a look at [universal-sentence-encoder-cmlm/multilingual-base-br](https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base-br/1) for the respective publication that describes this model."
  },
  {
    "s": "mcro:sentencetransformersusecmlmmultilingual",
    "p": "prov:hasTextValue",
    "o": "This is a pytorch version of the [universal-sentence-encoder-cmlm/multilingual-base-br](https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base-br/1) model. It can be used to map 109 languages to a shared vector space. As the model is based [LaBSE](https://huggingface.co/sentence-transformers/LaBSE), it perform quite comparable on downstream tasks."
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "mcro:hasCitation",
    "o": "mcro:whisperlargev3-Citation"
  },
  {
    "s": "mcro:whisperlargev3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisperlargev3-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}"
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisperlargev3-ModelArchitecture"
  },
  {
    "s": "mcro:whisperlargev3-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisperlargev3-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Whisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model"
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "mcro:hasTrainingData",
    "o": "mcro:whisperlargev3-TrainingData"
  },
  {
    "s": "mcro:whisperlargev3-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:whisperlargev3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The large-v3 checkpoint is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected using Whisper large-v2."
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:whisperlargev3-IntendedUseCase"
  },
  {
    "s": "mcro:whisperlargev3-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisperlargev3-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The models are primarily trained and evaluated on ASR and speech translation to English tasks."
  },
  {
    "s": "mcro:whisperlargev3turbo",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisperlargev3turbo",
    "p": "mcro:hasCitation",
    "o": "mcro:whisperlargev3turbo-Citation"
  },
  {
    "s": "mcro:whisperlargev3turbo-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisperlargev3turbo-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}"
  },
  {
    "s": "mcro:whisperlargev3turbo",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisperlargev3turbo-Architecture"
  },
  {
    "s": "mcro:whisperlargev3turbo-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisperlargev3turbo-Architecture",
    "p": "prov:hasTextValue",
    "o": "Whisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model."
  },
  {
    "s": "mcro:whisperlargev3turbo",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:whisperlargev3turbo-IntendedUseCase"
  },
  {
    "s": "mcro:whisperlargev3turbo-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisperlargev3turbo-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The models are primarily trained and evaluated on ASR and speech translation to English tasks."
  },
  {
    "s": "mcro:whisperlargev3turbo",
    "p": "mcro:hasDataset",
    "o": "mcro:whisperlargev3turbo-Dataset"
  },
  {
    "s": "mcro:whisperlargev3turbo-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:whisperlargev3turbo-Dataset",
    "p": "prov:hasTextValue",
    "o": "Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many datasets and domains in a zero-shot setting."
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertmultilingualbasemodelcased-ModelDetail"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertmultilingualbasemodelcased-Citation"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bertmultilingualbasemodelcased-UseCase"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertmultilingualbasemodelcased-TrainingData"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasTrainingProcedure",
    "o": "mcro:bertmultilingualbasemodelcased-TrainingProcedure"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-TrainingProcedure",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasCitation",
    "o": "mcro:bertmultilingualbasemodelcased-Citation2"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-Citation2",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:vit-face-expression",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vit-face-expression",
    "p": "mcro:hasDataset",
    "o": "mcro:vit-face-expression-Dataset"
  },
  {
    "s": "mcro:vit-face-expression-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:vit-face-expression-Dataset",
    "p": "prov:hasTextValue",
    "o": "FER2013"
  },
  {
    "s": "mcro:vit-face-expression",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vit-face-expression-ModelArchitecture"
  },
  {
    "s": "mcro:vit-face-expression-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vit-face-expression-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT)"
  },
  {
    "s": "mcro:vit-face-expression",
    "p": "mcro:hasLimitation",
    "o": "mcro:vit-face-expression-Limitation"
  },
  {
    "s": "mcro:vit-face-expression-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:vit-face-expression-Limitation",
    "p": "prov:hasTextValue",
    "o": "Data Bias"
  },
  {
    "s": "mcro:vit-face-expression-Limitation",
    "p": "prov:hasTextValue",
    "o": "Generalization"
  },
  {
    "s": "mcro:opt-125m",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:opt-125m",
    "p": "mcro:hasModelDetail",
    "o": "mcro:opt-125m-ModelDetail"
  },
  {
    "s": "mcro:opt-125m-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:opt-125m-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:opt-125m-ModelArchitecture"
  },
  {
    "s": "mcro:opt-125m-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:opt-125m-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:opt-125m-Citation"
  },
  {
    "s": "mcro:opt-125m-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:opt-125m-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:opt-125m-License"
  },
  {
    "s": "mcro:opt-125m-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:opt-125m",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:opt-125m-IntendedUseCase"
  },
  {
    "s": "mcro:opt-125m-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:opt-125m",
    "p": "mcro:hasDataset",
    "o": "mcro:opt-125m-Dataset"
  },
  {
    "s": "mcro:opt-125m-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:opt-125m-Dataset",
    "p": "prov:hasTextValue",
    "o": "BookCorpus"
  },
  {
    "s": "mcro:opt-125m-Dataset",
    "p": "prov:hasTextValue",
    "o": "CC-Stories"
  },
  {
    "s": "mcro:opt-125m-Dataset",
    "p": "prov:hasTextValue",
    "o": "The Pile"
  },
  {
    "s": "mcro:opt-125m-Dataset",
    "p": "prov:hasTextValue",
    "o": "Pushshift.io Reddit dataset"
  },
  {
    "s": "mcro:opt-125m-Dataset",
    "p": "prov:hasTextValue",
    "o": "CCNewsV2"
  },
  {
    "s": "mcro:opt-125m",
    "p": "mcro:hasConsideration",
    "o": "mcro:opt-125m-Consideration"
  },
  {
    "s": "mcro:opt-125m-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:opt-125m-Consideration",
    "p": "prov:hasTextValue",
    "o": "bias"
  },
  {
    "s": "mcro:opt-125m-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "decoder-only"
  },
  {
    "s": "mcro:opt-125m-Training",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:opt-125m",
    "p": "mcro:hasModelParameter",
    "o": "mcro:opt-125m-ModelParameter"
  },
  {
    "s": "mcro:opt-125m-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:opt-125m-Training",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:opt-125m",
    "p": "mcro:hasTrainingData",
    "o": "mcro:opt-125m-Training"
  },
  {
    "s": "mcro:siglipso400mpatch14384",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:siglipso400mpatch14384",
    "p": "mcro:hasModelDetail",
    "o": "mcro:siglipso400mpatch14384-ModelDetail"
  },
  {
    "s": "mcro:siglipso400mpatch14384-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:siglipso400mpatch14384-Architecture"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Architecture",
    "p": "prov:hasTextValue",
    "o": "SoViT-400m architecture"
  },
  {
    "s": "mcro:siglipso400mpatch14384-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:siglipso400mpatch14384-Citation"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Citation",
    "p": "prov:hasTextValue",
    "o": "Zhai et al."
  },
  {
    "s": "mcro:siglipso400mpatch14384",
    "p": "mcro:hasUseCase",
    "o": "mcro:siglipso400mpatch14384-UseCase"
  },
  {
    "s": "mcro:siglipso400mpatch14384-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-UseCase",
    "p": "prov:hasTextValue",
    "o": "zero-shot image classification and image-text retrieval"
  },
  {
    "s": "mcro:siglipso400mpatch14384",
    "p": "mcro:hasModelParameter",
    "o": "mcro:siglipso400mpatch14384-Parameter"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Parameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Parameter",
    "p": "mcro:hasDataset",
    "o": "mcro:siglipso400mpatch14384-Dataset"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Dataset",
    "p": "prov:hasTextValue",
    "o": "WebLI dataset"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasCitation",
    "o": "mcro:chronosboltsmall-Citation"
  },
  {
    "s": "mcro:chronosboltsmall-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasLicense",
    "o": "mcro:chronosboltsmall-License"
  },
  {
    "s": "mcro:chronosboltsmall-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-License",
    "p": "prov:hasTextValue",
    "o": "This project is licensed under the Apache-2.0 License."
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronosboltsmall-ModelArchitecture"
  },
  {
    "s": "mcro:chronosboltsmall-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations."
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:chronosboltsmall-IntendedUseCase"
  },
  {
    "s": "mcro:chronosboltsmall-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "pretrained time series forecasting models which can be used for zero-shot forecasting"
  },
  {
    "s": "mcro:metaLlama31",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metaLlama31",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metaLlama31-ModelDetail"
  },
  {
    "s": "mcro:metaLlama31-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metaLlama31-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:metaLlama31-License"
  },
  {
    "s": "mcro:metaLlama31-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metaLlama31-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license, the Llama 3.1 Community License"
  },
  {
    "s": "mcro:metaLlama31-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metaLlama31-ModelArchitecture"
  },
  {
    "s": "mcro:metaLlama31-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metaLlama31-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture."
  },
  {
    "s": "mcro:metaLlama31",
    "p": "mcro:hasUseCase",
    "o": "mcro:metaLlama31-UseCase"
  },
  {
    "s": "mcro:metaLlama31-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metaLlama31-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is intended for commercial and research use in multiple languages."
  },
  {
    "s": "mcro:metaLlama31-UseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:metaLlama31-OutOfScopeUseCase"
  },
  {
    "s": "mcro:metaLlama31-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:metaLlama31-OutOfScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "Use in any manner that violates applicable laws or regulations (including trade compliance laws)."
  },
  {
    "s": "mcro:metaLlama31",
    "p": "mcro:hasDataset",
    "o": "mcro:metaLlama31-Dataset"
  },
  {
    "s": "mcro:metaLlama31-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:metaLlama31-Dataset",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources."
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasConsideration",
    "o": "mcro:distilbertbasemultilingualcased-ConsiderationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-ConsiderationInformationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasCitation",
    "o": "mcro:distilbertbasemultilingualcased-CitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasDataset",
    "o": "mcro:distilbertbasemultilingualcased-DatasetInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasLicense",
    "o": "mcro:distilbertbasemultilingualcased-LicenseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasLimitation",
    "o": "mcro:distilbertbasemultilingualcased-LimitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:distilbertbasemultilingualcased-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:distilbertbasemultilingualcased-QuantativeAnalysisSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-QuantativeAnalysisSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasUseCase",
    "o": "mcro:distilbertbasemultilingualcased-UseCaseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Portuguese"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Architecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Architecture",
    "p": "prov:hasTextValue",
    "o": "XLSR-53 large model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Dataset",
    "p": "prov:hasTextValue",
    "o": "Common Voice 6.1"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-portuguese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ortuguese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:XLMrobertalargeModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:XLMrobertalargeModel",
    "p": "mcro:hasCitation",
    "o": "mcro:XLMrobertalargeModel-Citation"
  },
  {
    "s": "mcro:XLMrobertalargeModel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:XLMrobertalargeModel-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:XLMrobertalargeModel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:XLMrobertalargeModel-Arch"
  },
  {
    "s": "mcro:XLMrobertalargeModel-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:XLMrobertalargeModel-Arch",
    "p": "prov:hasTextValue",
    "o": "RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion."
  },
  {
    "s": "mcro:XLMrobertalargeModel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:XLMrobertalargeModel-UseCase"
  },
  {
    "s": "mcro:XLMrobertalargeModel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:XLMrobertalargeModel-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task."
  },
  {
    "s": "mcro:clipvitlargepatch14336",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clipvitlargepatch14336",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:clipvitlargepatch14336-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14336-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14336",
    "p": "mcro:hasDataset",
    "o": "mcro:clipvitlargepatch14336-DatasetInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14336-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14336",
    "p": "mcro:hasUseCase",
    "o": "mcro:clipvitlargepatch14336-UseCaseInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14336-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:esm2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:esm2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:esm2-ModelArchitecture"
  },
  {
    "s": "mcro:esm2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:esm2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:esm2-UseCase"
  },
  {
    "s": "mcro:esm2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:esm2",
    "p": "mcro:hasCitation",
    "o": "mcro:esm2-Citation"
  },
  {
    "s": "mcro:esm2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookesm2t4815BUR50D",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookesm2t363BUR50D",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookesm2t33650MUR50D",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookesm2t30150MUR50D",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookesm2t1235MUR50D",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookesm2t68MUR50D",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clip",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasModelDetail",
    "o": "mcro:clip-ModelDetailSection"
  },
  {
    "s": "mcro:clip-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:clip-ModelDetailSection",
    "p": "mcro:hasModelArchitectureInformation",
    "o": "mcro:clip-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clip-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clip-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The base model uses a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder."
  },
  {
    "s": "mcro:clip-ModelDetailSection",
    "p": "mcro:hasCitationInformation",
    "o": "mcro:clip-CitationInformationSection"
  },
  {
    "s": "mcro:clip-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clip-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Blog Post"
  },
  {
    "s": "mcro:clip-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "CLIP Paper"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasUseCaseInformation",
    "o": "mcro:clip-UseCaseInformationSection"
  },
  {
    "s": "mcro:clip-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clip-UseCaseInformationSection",
    "p": "mcro:hasPrimaryIntendedUseCaseInformation",
    "o": "mcro:clip-PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:clip-PrimaryIntendedUseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:clip-PrimaryIntendedUseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is intended as a research output for research communities."
  },
  {
    "s": "mcro:clip-UseCaseInformationSection",
    "p": "mcro:hasOutofScopeUseCaseInformation",
    "o": "mcro:clip-OutofScopeUseCaseInformationSection"
  },
  {
    "s": "mcro:clip-OutofScopeUseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:OutofScopeUseCaseInformationSection"
  },
  {
    "s": "mcro:clip-OutofScopeUseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "**Any** deployed use case of the model - whether commercial or not - is currently out of scope."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasDatasetInformation",
    "o": "mcro:clip-DatasetInformationSection"
  },
  {
    "s": "mcro:clip-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clip-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model was trained on publicly available image-caption data."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasConsiderationInformation",
    "o": "mcro:clip-ConsiderationInformationSection"
  },
  {
    "s": "mcro:clip-ConsiderationInformationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasLimitationInformation",
    "o": "mcro:clip-LimitationInformationSection"
  },
  {
    "s": "mcro:clip-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:clip-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-chinese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {C}hinese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Arch"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Arch",
    "p": "prov:hasTextValue",
    "o": "wav2vec2-large-xlsr-53"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Dataset",
    "p": "prov:hasTextValue",
    "o": "Common Voice 6.1"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Dataset",
    "p": "prov:hasTextValue",
    "o": "CSS10"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Dataset",
    "p": "prov:hasTextValue",
    "o": "ST-CMDS"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Chinese"
  },
  {
    "s": "mcro:t5base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasLicense",
    "o": "mcro:t5base-License"
  },
  {
    "s": "mcro:t5base-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:t5base-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:t5base-ModelDetails"
  },
  {
    "s": "mcro:t5base-ModelDetails",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasUseCase",
    "o": "mcro:t5base-UseCase"
  },
  {
    "s": "mcro:t5base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasConsideration",
    "o": "mcro:t5base-BiasRisksLimitations"
  },
  {
    "s": "mcro:t5base-BiasRisksLimitations",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasModelParameter",
    "o": "mcro:t5base-TrainingDetails"
  },
  {
    "s": "mcro:t5base-TrainingDetails",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:t5base-Evaluation"
  },
  {
    "s": "mcro:t5base-Evaluation",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasConsideration",
    "o": "mcro:t5base-EnvironmentalImpact"
  },
  {
    "s": "mcro:t5base-EnvironmentalImpact",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasCitation",
    "o": "mcro:t5base-Citation"
  },
  {
    "s": "mcro:t5base-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasOwner",
    "o": "mcro:t5base-ModelCardAuthors"
  },
  {
    "s": "mcro:t5base-ModelCardAuthors",
    "p": "rdf:type",
    "o": "mcro:OwnerInformationSection"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english",
    "p": "mcro:hasModelDetail",
    "o": "mcro:distilbertbaseuncasedfinetunedsst2english-ModelDetail"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english",
    "p": "mcro:hasUseCase",
    "o": "mcro:distilbertbaseuncasedfinetunedsst2english-UseCase"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english",
    "p": "mcro:hasConsideration",
    "o": "mcro:distilbertbaseuncasedfinetunedsst2english-Consideration"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english",
    "p": "mcro:hasModelParameter",
    "o": "mcro:distilbertbaseuncasedfinetunedsst2english-Training"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model."
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-Consideration",
    "p": "prov:hasTextValue",
    "o": "Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aur\u00e9lien G\u00e9ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset)."
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-Training",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-Training",
    "p": "prov:hasTextValue",
    "o": "The authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:distilbertbaseuncasedfinetunedsst2english-License"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilbertbaseuncasedfinetunedsst2english-License",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0"
  },
  {
    "s": "mcro:multilinguale5small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:multilinguale5small",
    "p": "mcro:hasCitation",
    "o": "mcro:multilinguale5small-CitationInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}"
  },
  {
    "s": "mcro:multilinguale5small",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:multilinguale5small-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model has 12 layers and the embedding size is 384."
  },
  {
    "s": "mcro:multilinguale5small",
    "p": "mcro:hasTrainingData",
    "o": "mcro:multilinguale5small-TrainingDataInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:multilinguale5small",
    "p": "mcro:hasBenchmark",
    "o": "mcro:multilinguale5small-BenchmarkResults"
  },
  {
    "s": "mcro:multilinguale5small-BenchmarkResults",
    "p": "rdf:type",
    "o": "obo:IAO_0000310"
  },
  {
    "s": "mcro:multilinguale5small",
    "p": "mcro:hasLimitation",
    "o": "mcro:multilinguale5small-LimitationInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Long texts will be truncated to at most 512 tokens."
  },
  {
    "s": "mcro:multilinguale5small",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:multilinguale5small-IntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-IntendedUseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:multilinguale5small-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model is initialized from [microsoft/Multilingual-MiniLM-L12-H384](https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384)\nand continually trained on a mixture of multilingual datasets."
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:visiontransformerbase-ModelDetail"
  },
  {
    "s": "mcro:visiontransformerbase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:visiontransformerbase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbase-Citation"
  },
  {
    "s": "mcro:visiontransformerbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:visiontransformerbase-License"
  },
  {
    "s": "mcro:visiontransformerbase-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasModelParameter",
    "o": "mcro:visiontransformerbase-ModelParameter"
  },
  {
    "s": "mcro:visiontransformerbase-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:visiontransformerbase-ModelParameter",
    "p": "mcro:hasDataset",
    "o": "mcro:visiontransformerbase-Dataset"
  },
  {
    "s": "mcro:visiontransformerbase-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:visiontransformerbase-IntendedUseCase"
  },
  {
    "s": "mcro:visiontransformerbase-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasTrainingData",
    "o": "mcro:visiontransformerbase-TrainingData"
  },
  {
    "s": "mcro:visiontransformerbase-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-TrainingData",
    "p": "prov:hasTextValue",
    "o": "ImageNet-21k"
  },
  {
    "s": "mcro:visiontransformerbase-TrainingData",
    "p": "prov:hasTextValue",
    "o": "ImageNet"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasTrainingProcedure",
    "o": "mcro:visiontransformerbase-TrainingProcedure"
  },
  {
    "s": "mcro:visiontransformerbase-TrainingProcedure",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:visiontransformerbase-ModelArchitecture"
  },
  {
    "s": "mcro:visiontransformerbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertbasemodelcased-ModelDetail"
  },
  {
    "s": "mcro:bertbasemodelcased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertbasemodelcased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertbasemodelcased-Citation"
  },
  {
    "s": "mcro:bertbasemodelcased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:bertbasemodelcased-Architecture"
  },
  {
    "s": "mcro:bertbasemodelcased-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:bertbasemodelcased-License"
  },
  {
    "s": "mcro:bertbasemodelcased-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertbasemodelcased-UseCase"
  },
  {
    "s": "mcro:bertbasemodelcased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasConsideration",
    "o": "mcro:bertbasemodelcased-Consideration"
  },
  {
    "s": "mcro:bertbasemodelcased-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertbasemodelcased-TrainingData"
  },
  {
    "s": "mcro:bertbasemodelcased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasParameter",
    "o": "mcro:bertbasemodelcased-Parameter"
  },
  {
    "s": "mcro:bertbasemodelcased-Parameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasEvaluation",
    "o": "mcro:bertbasemodelcased-Evaluation"
  },
  {
    "s": "mcro:bertbasemodelcased-Evaluation",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:bertbasemodelcased-Evaluation",
    "p": "mcro:hasDataset",
    "o": "mcro:bertbasemodelcased-Dataset"
  },
  {
    "s": "mcro:bertbasemodelcased-Dataset",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased-Parameter",
    "p": "prov:hasTextValue",
    "o": "WordPiece"
  },
  {
    "s": "mcro:bertbasemodelcased-Parameter",
    "p": "prov:hasTextValue",
    "o": "Adam"
  },
  {
    "s": "mcro:bertbasemodelcased-TrainingData",
    "p": "prov:hasTextValue",
    "o": "BookCorpus"
  },
  {
    "s": "mcro:bertbasemodelcased-TrainingData",
    "p": "prov:hasTextValue",
    "o": "English Wikipedia"
  },
  {
    "s": "mcro:bertbasemodelcased-Architecture",
    "p": "prov:hasTextValue",
    "o": "transformers"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:jinaaijinaembeddingsv3-UseCaseInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "`jina-embeddings-v3` is a **multilingual multi-task text embedding model** designed for a variety of NLP applications."
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jinaaijinaembeddingsv3-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Based on the [Jina-XLM-RoBERTa architecture](https://huggingface.co/jinaai/xlm-roberta-flash-implementation), \nthis model supports Rotary Position Embeddings to handle long input sequences up to **8192 tokens**."
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "mcro:hasLicense",
    "o": "mcro:jinaaijinaembeddingsv3-LicenseInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "CC BY-NC 4.0"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "mcro:hasCitation",
    "o": "mcro:jinaaijinaembeddingsv3-CitationInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,\n      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, \n      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael G\u00fcnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},\n      year={2024},\n      eprint={2409.10173},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2409.10173}, \n}"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphraseMiniLML6v2-ModelArchitecture"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphraseMiniLML6v2-Citation"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:resnet18a1in1k-ModelDetail"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:resnet18a1in1k-Citation"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:resnet18a1in1k-Architecture"
  },
  {
    "s": "mcro:resnet18a1in1k-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-Architecture",
    "p": "prov:hasTextValue",
    "o": "ReLU activations"
  },
  {
    "s": "mcro:resnet18a1in1k-Architecture",
    "p": "prov:hasTextValue",
    "o": "single layer 7x7 convolution with pooling"
  },
  {
    "s": "mcro:resnet18a1in1k-Architecture",
    "p": "prov:hasTextValue",
    "o": "1x1 convolution shortcut downsample"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "mcro:hasDataset",
    "o": "mcro:resnet18a1in1k-Dataset"
  },
  {
    "s": "mcro:resnet18a1in1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:resnet18a1in1k-UseCase"
  },
  {
    "s": "mcro:resnet18a1in1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:flant5base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:flant5base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:flant5base-ModelDetail"
  },
  {
    "s": "mcro:flant5base-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:flant5base-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:flant5base-License"
  },
  {
    "s": "mcro:flant5base-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:flant5base-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:flant5base-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:flant5base-ModelArchitecture"
  },
  {
    "s": "mcro:flant5base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:flant5base-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Language model"
  },
  {
    "s": "mcro:flant5base",
    "p": "mcro:hasCitation",
    "o": "mcro:flant5base-Citation"
  },
  {
    "s": "mcro:flant5base-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:flant5base-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}"
  },
  {
    "s": "mcro:flant5base",
    "p": "mcro:hasUseCase",
    "o": "mcro:flant5base-UseCase"
  },
  {
    "s": "mcro:flant5base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:flant5base-UseCase",
    "p": "prov:hasTextValue",
    "o": "The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models"
  },
  {
    "s": "mcro:fashionclip",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:fashionclip",
    "p": "mcro:hasModelDetail",
    "o": "mcro:fashionclip-ModelDetail"
  },
  {
    "s": "mcro:fashionclip-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:fashionclip-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:fashionclip-Citation"
  },
  {
    "s": "mcro:fashionclip-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:fashionclip-Citation",
    "p": "prov:hasTextValue",
    "o": "@Article{Chia2022,\n    title=\"Contrastive language and vision learning of general fashion concepts\",\n    author=\"Chia, Patrick John\n            and Attanasio, Giuseppe\n            and Bianchi, Federico\n            and Terragni, Silvia\n            and Magalh{\\~a}es, Ana Rita\n            and Goncalves, Diogo\n            and Greco, Ciro\n            and Tagliabue, Jacopo\",\n    journal=\"Scientific Reports\",\n    year=\"2022\",\n    month=\"Nov\",\n    day=\"08\",\n    volume=\"12\",\n    number=\"1\",\n    abstract=\"The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from general and transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model adapted for the fashion industry. We demonstrate the effectiveness of the representations learned by FashionCLIP with extensive tests across a variety of tasks, datasets and generalization probes. We argue that adaptations of large pre-trained models such as CLIP offer new perspectives in terms of scalability and sustainability for certain types of players in the industry. Finally, we detail the costs and environmental impact of training, and release the model weights and code as open source contribution to the community.\",\n    issn=\"2045-2322\",\n    doi=\"10.1038/s41598-022-23052-9\",\n    url=\"https://doi.org/10.1038/s41598-022-23052-9\"\n}\n"
  },
  {
    "s": "mcro:fashionclip-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:fashionclip-Architecture"
  },
  {
    "s": "mcro:fashionclip-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:fashionclip-Architecture",
    "p": "prov:hasTextValue",
    "o": "The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained, starting from a pre-trained checkpoint, to maximize the similarity of (image, text) pairs via a contrastive loss on a fashion dataset containing 800K products."
  },
  {
    "s": "mcro:fashionclip",
    "p": "mcro:hasDataset",
    "o": "mcro:fashionclip-Dataset"
  },
  {
    "s": "mcro:fashionclip-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:fashionclip-Dataset",
    "p": "prov:hasTextValue",
    "o": "The model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., \u201cstripes\u201d, \u201clong sleeves\u201d, \u201cArmani\u201d) and _short description_ (\u201c80s styled t-shirt\u201d)) available in the Farfetch dataset."
  },
  {
    "s": "mcro:fashionclip",
    "p": "mcro:hasConsideration",
    "o": "mcro:fashionclip-Consideration"
  },
  {
    "s": "mcro:fashionclip-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:fashionclip-Consideration",
    "p": "prov:hasTextValue",
    "o": "We acknowledge certain limitations of FashionCLIP and expect that it inherits certain limitations and biases present in the original CLIP model. We do not expect our fine-tuning to significantly augment these limitations: we acknowledge that the fashion data we use makes explicit assumptions about the notion of gender as in \"blue shoes for a woman\" that inevitably associate aspects of clothing with specific people.\n\nOur investigations also suggest that the data used introduces certain limitations in FashionCLIP. From the textual modality, given that most captions derived from the Farfetch dataset are long, we observe that FashionCLIP may be more performant in longer queries than shorter ones. From the image modality, FashionCLIP is also biased towards standard product images (centered, white background).\n\nModel selection, i.e. selecting an appropariate stopping critera during fine-tuning, remains an open challenge. We observed that using loss on an in-domain (i.e. same distribution as test) validation dataset is a poor selection critera when out-of-domain generalization (i.e. across different datasets) is desired, even when the dataset used is relatively diverse and large."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-russian,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {R}ussian},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Architecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Architecture",
    "p": "prov:hasTextValue",
    "o": "Wav2Vec2ForCTC"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Dataset",
    "p": "prov:hasTextValue",
    "o": "Common Voice 6.1"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Dataset",
    "p": "prov:hasTextValue",
    "o": "CSS10"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Russian"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:twitterrobertabasesentiment-ModelArchitecture"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "roBERTa-base"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "mcro:hasDataset",
    "o": "mcro:twitterrobertabasesentiment-Dataset"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-Dataset",
    "p": "prov:hasTextValue",
    "o": "~58M tweets"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "mcro:hasCitation",
    "o": "mcro:twitterrobertabasesentiment-Citation"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-Citation",
    "p": "prov:hasTextValue",
    "o": "_TweetEval_ (Findings of EMNLP 2020)"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:twitterrobertabasesentiment-IntendedUseCase"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "sentiment analysis"
  },
  {
    "s": "mcro:vitmatte",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vitmatte",
    "p": "mcro:hasModelDetail",
    "o": "mcro:vitmatte-ModelDetail"
  },
  {
    "s": "mcro:vitmatte-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:vitmatte-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:vitmatte-Citation"
  },
  {
    "s": "mcro:vitmatte-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:vitmatte-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{yao2023vitmatte,\n      title={ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers}, \n      author={Jingfeng Yao and Xinggang Wang and Shusheng Yang and Baoyuan Wang},\n      year={2023},\n      eprint={2305.15272},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:vitmatte-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vitmatte-ModelArchitecture"
  },
  {
    "s": "mcro:vitmatte-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vitmatte-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT) with a lightweight head on top"
  },
  {
    "s": "mcro:vitmatte",
    "p": "mcro:hasUseCase",
    "o": "mcro:vitmatte-UseCase"
  },
  {
    "s": "mcro:vitmatte-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:vitmatte-UseCase",
    "p": "prov:hasTextValue",
    "o": "image matting"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasCitation",
    "o": "mcro:FlagEmbedding-Citation"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasLicense",
    "o": "mcro:FlagEmbedding-License"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "prov:hasTextValue",
    "o": "FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge."
  },
  {
    "s": "mcro:bartlargecnn",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bartlargecnn",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bartlargecnn-ModelDetail"
  },
  {
    "s": "mcro:bartlargecnn-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bartlargecnn-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bartlargecnn-Citation"
  },
  {
    "s": "mcro:bartlargecnn-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bartlargecnn-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:bartlargecnn-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bartlargecnn-ModelArchitecture"
  },
  {
    "s": "mcro:bartlargecnn-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bartlargecnn-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."
  },
  {
    "s": "mcro:bartlargecnn",
    "p": "mcro:hasUseCase",
    "o": "mcro:bartlargecnn-UseCase"
  },
  {
    "s": "mcro:bartlargecnn-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bartlargecnn-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use this model for text summarization."
  },
  {
    "s": "mcro:bartlargecnn",
    "p": "mcro:hasDataset",
    "o": "mcro:bartlargecnn-Dataset"
  },
  {
    "s": "mcro:bartlargecnn-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bartlargecnn-Dataset",
    "p": "prov:hasTextValue",
    "o": "CNN Daily Mail"
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "mcro:hasLicense",
    "o": "mcro:stablediffusionv15modelcard-License"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-License",
    "p": "prov:hasTextValue",
    "o": "The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing."
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "mcro:hasCitation",
    "o": "mcro:stablediffusionv15modelcard-Citation"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Citation",
    "p": "prov:hasTextValue",
    "o": "@InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n    }"
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:stablediffusionv15modelcard-IntendedUseCase"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models."
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:stablediffusionv15modelcard-Architecture"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Architecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generation model"
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "mcro:hasDataset",
    "o": "mcro:stablediffusionv15modelcard-Dataset"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Dataset",
    "p": "prov:hasTextValue",
    "o": "LAION-2B (en) and subsets thereof"
  },
  {
    "s": "mcro:BGE-M3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BGE-M3",
    "p": "mcro:hasCitation",
    "o": "mcro:BGE-M3-Citation"
  },
  {
    "s": "mcro:BGE-M3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BGE-M3-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:BGE-M3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BGE-M3-Architecture"
  },
  {
    "s": "mcro:BGE-M3-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BGE-M3-Architecture",
    "p": "prov:hasTextValue",
    "o": "BGE-M3"
  },
  {
    "s": "mcro:BGE-M3",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:BGE-M3-UseCase"
  },
  {
    "s": "mcro:BGE-M3-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BGE-M3-UseCase",
    "p": "prov:hasTextValue",
    "o": "dense retrieval, multi-vector retrieval, and sparse retrieval"
  },
  {
    "s": "mcro:BGE-M3",
    "p": "mcro:hasDataset",
    "o": "mcro:BGE-M3-Dataset"
  },
  {
    "s": "mcro:BGE-M3-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:BGE-M3-Dataset",
    "p": "prov:hasTextValue",
    "o": "MLDR"
  },
  {
    "s": "mcro:BGE-M3-Dataset",
    "p": "prov:hasTextValue",
    "o": "bge-m3-data"
  },
  {
    "s": "mcro:YOLOWorldMirror",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:YOLOWorldMirror",
    "p": "mcro:hasDocumentation",
    "o": "mcro:YOLOWorldMirror-Documentation"
  },
  {
    "s": "mcro:YOLOWorldMirror-Documentation",
    "p": "rdf:type",
    "o": "mcro:DocumentationSection"
  },
  {
    "s": "mcro:YOLOWorldMirror-Documentation",
    "p": "prov:hasTextValue",
    "o": "https://docs.ultralytics.com/models/yolo-world/#available-models-supported-tasks-and-operating-modes\n\nmodel weights for ultralytics yolo models"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertbasechinese-ModelDetailSection"
  },
  {
    "s": "mcro:bertbasechinese-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertbasechinese-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:bertbasechinese-License"
  },
  {
    "s": "mcro:bertbasechinese-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertbasechinese-UseCaseSection"
  },
  {
    "s": "mcro:bertbasechinese-UseCaseSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasConsideration",
    "o": "mcro:bertbasechinese-ConsiderationSection"
  },
  {
    "s": "mcro:bertbasechinese-ConsiderationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasModelParameter",
    "o": "mcro:bertbasechinese-ModelParameterSection"
  },
  {
    "s": "mcro:bertbasechinese-ModelParameterSection",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:bertbasechinese-QuantativeAnalysisSection"
  },
  {
    "s": "mcro:bertbasechinese-QuantativeAnalysisSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "mcro:hasCitation",
    "o": "mcro:bartlargemnli-Citation"
  },
  {
    "s": "mcro:bartlargemnli-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bartlargemnli-Citation",
    "p": "prov:hasTextValue",
    "o": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookbartlargemnli-Citation2"
  },
  {
    "s": "mcro:facebookbartlargemnli-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookbartlargemnli-Citation2",
    "p": "prov:hasTextValue",
    "o": "BART fairseq implementation"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "mcro:hasDataset",
    "o": "mcro:MultiNLI"
  },
  {
    "s": "mcro:MultiNLI",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:MultiNLI",
    "p": "prov:hasTextValue",
    "o": "MultiNLI (MNLI)"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BART"
  },
  {
    "s": "mcro:BART",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BART",
    "p": "prov:hasTextValue",
    "o": "bart-large"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:NLIbasedZeroShotTextClassification"
  },
  {
    "s": "mcro:NLIbasedZeroShotTextClassification",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:NLIbasedZeroShotTextClassification",
    "p": "prov:hasTextValue",
    "o": "NLI-based Zero Shot Text Classification"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B",
    "p": "mcro:hasDataset",
    "o": "mcro:CLIPViTB16LAION2B-DatasetInfo"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B-DatasetInfo",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B-DatasetInfo",
    "p": "prov:hasTextValue",
    "o": "This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)."
  },
  {
    "s": "mcro:CLIPViTB16LAION2B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:CLIPViTB16LAION2B-ModelArch"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B-ModelArch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B-ModelArch",
    "p": "prov:hasTextValue",
    "o": "CLIP ViT-B/16"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B",
    "p": "mcro:hasLicense",
    "o": "mcro:CLIPViTB16LAION2B-LicenseInfo"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B-LicenseInfo",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B",
    "p": "mcro:hasCitation",
    "o": "mcro:CLIPViTB16LAION2B-CitationInfo"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B-CitationInfo",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B",
    "p": "mcro:hasUseCase",
    "o": "mcro:CLIPViTB16LAION2B-UseCaseInfo"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B-UseCaseInfo",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:CLIPViTB16LAION2B-UseCaseInfo",
    "p": "prov:hasTextValue",
    "o": "research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model."
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasModelDetail",
    "o": "mcro:stablediffusioninpaintingmodelcard-ModelDetail"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasUseCase",
    "o": "mcro:stablediffusioninpaintingmodelcard-UseCase"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasLimitation",
    "o": "mcro:stablediffusioninpaintingmodelcard-Limitation"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasTrainingData",
    "o": "mcro:stablediffusioninpaintingmodelcard-TrainingData"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasEvaluationResult",
    "o": "mcro:stablediffusioninpaintingmodelcard-EvaluationResult"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-EvaluationResult",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasCitation",
    "o": "mcro:stablediffusioninpaintingmodelcard-Citation"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:stablediffusioninpaintingmodelcard-License"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-License",
    "p": "prov:hasTextValue",
    "o": "The CreativeML OpenRAIL M license"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:stablediffusioninpaintingmodelcard-Architecture"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-Architecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generation model"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-Citation",
    "p": "prov:hasTextValue",
    "o": "@InProceedings{Rombach_2022_CVPR,\n author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n month     = {June},\n year      = {2022},\n pages     = {10684-10695}\n    }"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-TrainingData",
    "p": "mcro:hasDataset",
    "o": "mcro:stablediffusioninpaintingmodelcard-Dataset"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-Dataset",
    "p": "prov:hasTextValue",
    "o": "LAION-2B (en)"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "rdf:type",
    "o": "mcro:ModelCardReport"
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2505BInstruct-UseCase"
  },
  {
    "s": "mcro:Qwen2505BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training.\n\nOnce finetuned, the model can be used as normal in any workflow, for details on how to do this please refer to the [original model documentation](https://qwen.readthedocs.io/en/latest/).\n\nFor more details on the original model, please refer to the original repository [here](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)."
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2505BInstruct-Architecture"
  },
  {
    "s": "mcro:Qwen2505BInstruct-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-Architecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "mcro:hasModelParameter",
    "o": "mcro:Qwen2505BInstruct-Parameter"
  },
  {
    "s": "mcro:Qwen2505BInstruct-Parameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-Parameter",
    "p": "prov:hasTextValue",
    "o": "- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "mcro:hasArchitecture",
    "o": "mcro:Qwen257BInstruct-Architecture"
  },
  {
    "s": "mcro:Qwen257BInstruct-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-Architecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen257BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen257BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:clipViTbigG14laion2B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clipViTbigG14laion2B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:clipViTbigG14laion2B-ModelDetailSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:clipViTbigG14laion2B-License"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-License",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:clipViTbigG14laion2B-CitationLAION"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-CitationLAION",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-CitationLAION",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:clipViTbigG14laion2B-CitationOpenAICLIP"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-CitationOpenAICLIP",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-CitationOpenAICLIP",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Radford2021LearningTV,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n  booktitle={ICML},\n  year={2021}\n}"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:clipViTbigG14laion2B-CitationOpenCLIP"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-CitationOpenCLIP",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-CitationOpenCLIP",
    "p": "prov:hasTextValue",
    "o": "@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:clipViTbigG14laion2B-CitationScalingOpenCLIP"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-CitationScalingOpenCLIP",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-CitationScalingOpenCLIP",
    "p": "prov:hasTextValue",
    "o": "@article{cherti2022reproducible,\n  title={Reproducible scaling laws for contrastive language-image learning},\n  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n  journal={arXiv preprint arXiv:2212.07143},\n  year={2022}\n}"
  },
  {
    "s": "mcro:clipViTbigG14laion2B",
    "p": "mcro:hasUseCase",
    "o": "mcro:clipViTbigG14laion2B-UseCase"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B",
    "p": "mcro:hasModelParameter",
    "o": "mcro:clipViTbigG14laion2B-ModelParameter"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-ModelParameter",
    "p": "mcro:hasDataset",
    "o": "mcro:clipViTbigG14laion2B-Dataset"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clipViTbigG14laion2B",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:clipViTbigG14laion2B-QuantativeAnalysis"
  },
  {
    "s": "mcro:clipViTbigG14laion2B-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:allMiniLML12v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:allMiniLML12v2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:allMiniLML12v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks."
  },
  {
    "s": "mcro:allMiniLML12v2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:allMiniLML12v2-TrainingDataInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file."
  },
  {
    "s": "mcro:allMiniLML12v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:allMiniLML12v2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "We used the pretrained [`microsoft/MiniLM-L12-H384-uncased`](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset."
  },
  {
    "s": "mcro:tabletransformer",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:tabletransformer",
    "p": "mcro:hasModelDetail",
    "o": "mcro:tabletransformer-ModelDetail"
  },
  {
    "s": "mcro:tabletransformer-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:tabletransformer-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:tabletransformer-Citation"
  },
  {
    "s": "mcro:tabletransformer-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:tabletransformer-Citation",
    "p": "prov:hasTextValue",
    "o": "PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents"
  },
  {
    "s": "mcro:tabletransformer-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:tabletransformer-ModelArchitecture"
  },
  {
    "s": "mcro:tabletransformer-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:tabletransformer-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based object detection model"
  },
  {
    "s": "mcro:tabletransformer",
    "p": "mcro:hasUseCase",
    "o": "mcro:tabletransformer-UseCase"
  },
  {
    "s": "mcro:tabletransformer-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:tabletransformer-UseCase",
    "p": "prov:hasTextValue",
    "o": "detecting tables in documents"
  },
  {
    "s": "mcro:t5small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:t5small",
    "p": "mcro:hasModelDetail",
    "o": "mcro:t5small-ModelDetails"
  },
  {
    "s": "mcro:t5small-ModelDetails",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:t5small-ModelDetails",
    "p": "mcro:hasLicense",
    "o": "mcro:t5small-License"
  },
  {
    "s": "mcro:t5small-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:t5small-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:t5small-ModelDetails",
    "p": "mcro:hasCitation",
    "o": "mcro:t5small-Citation"
  },
  {
    "s": "mcro:t5small-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:t5small",
    "p": "mcro:hasUseCase",
    "o": "mcro:t5small-UseCase"
  },
  {
    "s": "mcro:t5small-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:t5small",
    "p": "mcro:hasDataset",
    "o": "mcro:t5small-Dataset"
  },
  {
    "s": "mcro:t5small-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:t5small-Dataset",
    "p": "prov:hasTextValue",
    "o": "Colossal Clean Crawled Corpus (C4)"
  },
  {
    "s": "mcro:t5small-ModelDetails",
    "p": "mcro:hasArchitecture",
    "o": "mcro:t5small-Architecture"
  },
  {
    "s": "mcro:t5small-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:t5small-Architecture",
    "p": "prov:hasTextValue",
    "o": "Language model"
  },
  {
    "s": "mcro:albertbasev2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:albertbasev2-ModelDetail"
  },
  {
    "s": "mcro:albertbasev2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:albertbasev2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:albertbasev2-Citation"
  },
  {
    "s": "mcro:albertbasev2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:albertbasev2-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1909-11942,\n  author    = {Zhenzhong Lan and\n               Mingda Chen and\n               Sebastian Goodman and\n               Kevin Gimpel and\n               Piyush Sharma and\n               Radu Soricut},\n  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language\n               Representations},\n  journal   = {CoRR},\n  volume    = {abs/1909.11942},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.11942},\n  archivePrefix = {arXiv},\n  eprint    = {1909.11942},\n  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:albertbasev2-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:albertbasev2-License"
  },
  {
    "s": "mcro:albertbasev2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:albertbasev2-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:albertbasev2-Architecture"
  },
  {
    "s": "mcro:albertbasev2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:albertbasev2-UseCase"
  },
  {
    "s": "mcro:albertbasev2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:albertbasev2-TrainingData"
  },
  {
    "s": "mcro:albertbasev2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasLimitation",
    "o": "mcro:albertbasev2-Limitation"
  },
  {
    "s": "mcro:albertbasev2-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:distilgpt2-ModelDetail"
  },
  {
    "s": "mcro:distilgpt2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:distilgpt2-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:distilgpt2-License"
  },
  {
    "s": "mcro:distilgpt2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilgpt2-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:distilgpt2-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:distilgpt2-Architecture"
  },
  {
    "s": "mcro:distilgpt2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilgpt2-Architecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based Language Model"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasIntendedUse",
    "o": "mcro:distilgpt2-IntendedUse"
  },
  {
    "s": "mcro:distilgpt2-IntendedUse",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasLimitation",
    "o": "mcro:distilgpt2-Limitation"
  },
  {
    "s": "mcro:distilgpt2-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:distilgpt2-TrainingData"
  },
  {
    "s": "mcro:distilgpt2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasTrainingProcedure",
    "o": "mcro:distilgpt2-TrainingProcedure"
  },
  {
    "s": "mcro:distilgpt2-TrainingProcedure",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasEvaluationResult",
    "o": "mcro:distilgpt2-EvaluationResult"
  },
  {
    "s": "mcro:distilgpt2-EvaluationResult",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasEnvironmentalImpact",
    "o": "mcro:distilgpt2-EnvironmentalImpact"
  },
  {
    "s": "mcro:distilgpt2-EnvironmentalImpact",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasCitation",
    "o": "mcro:distilgpt2-Citation"
  },
  {
    "s": "mcro:distilgpt2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:forcedalignerwithhuggingfacectcmodels",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:forcedalignerwithhuggingfacectcmodels",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:forcedalignerwithhuggingfacectcmodels-intendedusecase"
  },
  {
    "s": "mcro:forcedalignerwithhuggingfacectcmodels-intendedusecase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:forcedalignerwithhuggingfacectcmodels-intendedusecase",
    "p": "prov:hasTextValue",
    "o": "This Python package provides an efficient way to perform forced alignment between text and audio using Hugging Face's pretrained models. it also features an improved implementation to use much less memory than TorchAudio forced alignment API."
  },
  {
    "s": "mcro:forcedalignerwithhuggingfacectcmodels",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:forcedalignerwithhuggingfacectcmodels-modelarchitecture"
  },
  {
    "s": "mcro:forcedalignerwithhuggingfacectcmodels-modelarchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:forcedalignerwithhuggingfacectcmodels-modelarchitecture",
    "p": "prov:hasTextValue",
    "o": "The model checkpoint uploaded here is a conversion from torchaudio to HF Transformers for the MMS-300M checkpoint trained on forced alignment dataset"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasLicense",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-License"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasCitation",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-Citation1"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-Citation1",
    "p": "prov:hasTextValue",
    "o": "@online{emb2024mxbai,\n  title={Open Source Strikes Bread - New Fluffy Embeddings Model},\n  author={Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-embed-large-v1},\n}\n"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasCitation",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-Citation2"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-Citation2",
    "p": "prov:hasTextValue",
    "o": "@article{li2023angle,\n  title={AnglE-optimized Text Embeddings},\n  author={Li, Xianming and Li, Jing},\n  journal={arXiv preprint arXiv:2309.12871},\n  year={2023}\n}\n"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-ModelArchitecture"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasDataset",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-Dataset"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasUseCase",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-UseCase"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookdinov2small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookdinov2small",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookdinov2small-ModelDetail"
  },
  {
    "s": "mcro:facebookdinov2small-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookdinov2small-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookdinov2small-Citation"
  },
  {
    "s": "mcro:facebookdinov2small-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookdinov2small-Citation",
    "p": "prov:hasTextValue",
    "o": "misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision}, \n      author={Maxime Oquab and Timoth\u00e9e Darcet and Th\u00e9o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv\u00e9 Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:facebookdinov2small-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:facebookdinov2small-License"
  },
  {
    "s": "mcro:facebookdinov2small-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:facebookdinov2small-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:facebookdinov2small-Architecture"
  },
  {
    "s": "mcro:facebookdinov2small-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookdinov2small",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookdinov2small-UseCase"
  },
  {
    "s": "mcro:facebookdinov2small-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookdinov2small-Architecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT)"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k",
    "p": "mcro:hasDataset",
    "o": "mcro:efficientnet_b3ra2in1k-Dataset"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:efficientnet_b3ra2in1k-ModelArchitecture"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "EfficientNet"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b3ra2in1k-Citation-1"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-Citation-1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-Citation-1",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{tan2019efficientnet,\n  title={Efficientnet: Rethinking model scaling for convolutional neural networks},\n  author={Tan, Mingxing and Le, Quoc},\n  booktitle={International conference on machine learning},\n  pages={6105--6114},\n  year={2019},\n  organization={PMLR}\n}"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b3ra2in1k-Citation-2"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-Citation-2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-Citation-2",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b3ra2in1k-Citation-3"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-Citation-3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-Citation-3",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:efficientnet_b3ra2in1k-UseCase"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3ra2in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen25VL7BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen25VL7BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:Qwen25VL7BInstruct-IntendedUseCase"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct",
    "p": "mcro:hasDataset",
    "o": "mcro:Qwen25VL7BInstruct-Dataset"
  },
  {
    "s": "mcro:Qwen25VL7BInstruct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:gemma3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gemma3-ModelDetailSection"
  },
  {
    "s": "mcro:gemma3-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gemma3-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:gemma3-CitationSection"
  },
  {
    "s": "mcro:gemma3-CitationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gemma3-CitationSection",
    "p": "prov:hasTextValue",
    "o": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasModelParameter",
    "o": "mcro:gemma3-ModelParameterSection"
  },
  {
    "s": "mcro:gemma3-ModelParameterSection",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:gemma3-ModelParameterSection",
    "p": "mcro:hasDataset",
    "o": "mcro:gemma3-DatasetSection"
  },
  {
    "s": "mcro:gemma3-DatasetSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:gemma3-ModelParameterSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gemma3-ModelArchitectureSection"
  },
  {
    "s": "mcro:gemma3-ModelArchitectureSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:gemma3-QuantativeAnalysisSection"
  },
  {
    "s": "mcro:gemma3-QuantativeAnalysisSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasConsideration",
    "o": "mcro:gemma3-ConsiderationSection"
  },
  {
    "s": "mcro:gemma3-ConsiderationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasUseCase",
    "o": "mcro:gemma3-UseCaseSection"
  },
  {
    "s": "mcro:gemma3-UseCaseSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasModelCardReport",
    "o": "mcro:gemma3-ModelCardReport"
  },
  {
    "s": "mcro:gemma3-ModelCardReport",
    "p": "rdf:type",
    "o": "mcro:ModelCardReport"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-ModelArchitecture"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-Citation"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-UseCase"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasv2-UseCase",
    "p": "prov:hasTextValue",
    "o": "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:Salesforceblipbootstrapping",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping",
    "p": "mcro:hasCitation",
    "o": "mcro:Salesforceblipbootstrapping-Citation"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:Salesforceblipbootstrapping-UseCase"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping",
    "p": "mcro:hasLicense",
    "o": "mcro:Salesforceblipbootstrapping-License"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Salesforceblipbootstrapping-Architecture"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping",
    "p": "mcro:hasConsideration",
    "o": "mcro:Salesforceblipbootstrapping-Consideration"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use this model for conditional and un-conditional image captioning"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-License",
    "p": "prov:hasTextValue",
    "o": "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people\u2019s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP."
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-Architecture",
    "p": "prov:hasTextValue",
    "o": "base architecture (with ViT base backbone)"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipbootstrapping-Consideration",
    "p": "prov:hasTextValue",
    "o": "We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people\u2019s lives, rights, or safety."
  },
  {
    "s": "mcro:Salesforceblipbootstrapping",
    "p": "mcro:hasDataset",
    "o": "mcro:cocoDataset"
  },
  {
    "s": "mcro:cocoDataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:cocoDataset",
    "p": "prov:hasTextValue",
    "o": "COCO dataset"
  },
  {
    "s": "mcro:flux1dev",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:flux1dev",
    "p": "mcro:hasLicense",
    "o": "mcro:flux1dev-License"
  },
  {
    "s": "mcro:flux1dev-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:flux1dev-License",
    "p": "prov:hasTextValue",
    "o": "`FLUX.1 [dev]` Non-Commercial License"
  },
  {
    "s": "mcro:flux1dev",
    "p": "mcro:hasLimitation",
    "o": "mcro:flux1dev-Limitation"
  },
  {
    "s": "mcro:flux1dev-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:flux1dev-Limitation",
    "p": "prov:hasTextValue",
    "o": "This model is not intended or able to provide factual information."
  },
  {
    "s": "mcro:flux1dev-Limitation",
    "p": "prov:hasTextValue",
    "o": "As a statistical model this checkpoint might amplify existing societal biases."
  },
  {
    "s": "mcro:flux1dev-Limitation",
    "p": "prov:hasTextValue",
    "o": "The model may fail to generate output that matches the prompts."
  },
  {
    "s": "mcro:flux1dev-Limitation",
    "p": "prov:hasTextValue",
    "o": "Prompt following is heavily influenced by the prompting-style."
  },
  {
    "s": "mcro:flux1dev",
    "p": "mcro:hasUseCase",
    "o": "mcro:flux1dev-UseCase"
  },
  {
    "s": "mcro:flux1dev-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:flux1dev-UseCase",
    "p": "prov:hasTextValue",
    "o": "personal, scientific, and commercial purposes"
  },
  {
    "s": "mcro:flux1dev",
    "p": "mcro:hasOutOfScopeUse",
    "o": "mcro:flux1dev-OutOfScopeUse"
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "In any way that violates any applicable national, federal, state, local or international law or regulation."
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content."
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "To generate or disseminate verifiably false information and/or content with the purpose of harming others."
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "To generate or disseminate personal identifiable information that can be used to harm an individual."
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "To harass, abuse, threaten, stalk, or bully individuals or groups of individuals."
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "To create non-consensual nudity or illegal pornographic content."
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation."
  },
  {
    "s": "mcro:flux1dev-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "Generating or facilitating large-scale disinformation campaigns."
  },
  {
    "s": "mcro:flux1dev",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:flux1dev-ModelArchitecture"
  },
  {
    "s": "mcro:flux1dev-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:flux1dev-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "12 billion parameter rectified flow transformer"
  },
  {
    "s": "mcro:unik3d",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:unik3d",
    "p": "mcro:hasLibrary",
    "o": "mcro:unik3d-Library"
  },
  {
    "s": "mcro:unik3d-Library",
    "p": "rdf:type",
    "o": "mcro:LibraryInformationSection"
  },
  {
    "s": "mcro:unik3d-Library",
    "p": "prov:hasTextValue",
    "o": "https://github.com/lpiccinelli-eth/UniK3D"
  },
  {
    "s": "mcro:unik3d",
    "p": "mcro:hasDocumentation",
    "o": "mcro:unik3d-Documentation"
  },
  {
    "s": "mcro:unik3d-Documentation",
    "p": "rdf:type",
    "o": "mcro:DocumentationInformationSection"
  },
  {
    "s": "mcro:unik3d-Documentation",
    "p": "prov:hasTextValue",
    "o": "[More Information Needed]"
  },
  {
    "s": "mcro:llama32",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llama32",
    "p": "mcro:hasModelDetail",
    "o": "mcro:llama32-modelDetail"
  },
  {
    "s": "mcro:llama32-modelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llama32-modelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:llama32-license"
  },
  {
    "s": "mcro:llama32-license",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llama32-license",
    "p": "prov:hasTextValue",
    "o": "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)."
  },
  {
    "s": "mcro:llama32-modelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:llama32-citation"
  },
  {
    "s": "mcro:llama32-citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:llama32-citation",
    "p": "prov:hasTextValue",
    "o": "Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes)."
  },
  {
    "s": "mcro:llama32-modelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llama32-architecture"
  },
  {
    "s": "mcro:llama32-architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama32-architecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:llama32",
    "p": "mcro:hasUseCase",
    "o": "mcro:llama32-usecase"
  },
  {
    "s": "mcro:llama32-usecase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llama32-usecase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources."
  },
  {
    "s": "mcro:llama32-usecase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:llama32-outofscope"
  },
  {
    "s": "mcro:llama32-outofscope",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:llama32-outofscope",
    "p": "prov:hasTextValue",
    "o": "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card."
  },
  {
    "s": "mcro:llama32",
    "p": "mcro:hasModelParameter",
    "o": "mcro:llama32-parameter"
  },
  {
    "s": "mcro:llama32-parameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:llama32-parameter",
    "p": "mcro:hasTrainingData",
    "o": "mcro:llama32-trainingdata"
  },
  {
    "s": "mcro:llama32-trainingdata",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:llama32-trainingdata",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)."
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "rdf:type",
    "o": "obo:IAO_0000301"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasCitation",
    "o": "mcro:bertmultilingualbasemodeluncased-Citation"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertmultilingualbasemodeluncased-ModelArchitecture"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the languages in the training set that can then be used to\nextract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a\nstandard classifier using the features produced by the BERT model as inputs."
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bertmultilingualbasemodeluncased-IntendedUseCase"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2."
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasDataset",
    "o": "mcro:bertmultilingualbasemodeluncased-Dataset"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-Dataset",
    "p": "prov:hasTextValue",
    "o": "The BERT model was pretrained on the 102 languages with the largest Wikipedias. You can find the complete list\n[here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages)."
  },
  {
    "s": "mcro:envbreaker",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:envbreaker",
    "p": "prov:hasTextValue",
    "o": "We log statistics to see if any envs are breaking"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasCitation",
    "o": "mcro:Salesforceblipimagecaptioninglarge-Citation"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:Salesforceblipimagecaptioninglarge-IntendedUseCase"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use this model for conditional and un-conditional image captioning"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasConsiderations",
    "o": "mcro:Salesforceblipimagecaptioninglarge-Considerations"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Considerations",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Considerations",
    "p": "prov:hasTextValue",
    "o": "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people\u2019s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP."
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Salesforceblipimagecaptioninglarge-Architecture"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Architecture",
    "p": "prov:hasTextValue",
    "o": "base architecture (with ViT large backbone)"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasDataset",
    "o": "mcro:Salesforceblipimagecaptioninglarge-Dataset"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Dataset",
    "p": "prov:hasTextValue",
    "o": "COCO dataset"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2",
    "p": "mcro:hasUseCaseInformationSection",
    "o": "mcro:crossencodermsmarcoMiniLML6v2-UseCase"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2",
    "p": "mcro:hasPerformanceMetricInformationSection",
    "o": "mcro:crossencodermsmarcoMiniLML6v2-Performance"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-Performance",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-Performance",
    "p": "prov:hasTextValue",
    "o": "In the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset."
  },
  {
    "s": "mcro:sdxl10base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sdxl10base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:sdxl10base-ModelDetail"
  },
  {
    "s": "mcro:sdxl10base-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:sdxl10base-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:sdxl10base-License"
  },
  {
    "s": "mcro:sdxl10base-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:sdxl10base-License",
    "p": "prov:hasTextValue",
    "o": "CreativeML Open RAIL++-M License"
  },
  {
    "s": "mcro:sdxl10base-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sdxl10base-ModelArchitecture"
  },
  {
    "s": "mcro:sdxl10base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sdxl10base-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generative model"
  },
  {
    "s": "mcro:sdxl10base-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:sdxl10base-Citation"
  },
  {
    "s": "mcro:sdxl10base-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sdxl10base-Citation",
    "p": "prov:hasTextValue",
    "o": "SDXL report on arXiv"
  },
  {
    "s": "mcro:sdxl10base",
    "p": "mcro:hasUseCase",
    "o": "mcro:sdxl10base-UseCase"
  },
  {
    "s": "mcro:sdxl10base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sdxl10base-UseCase",
    "p": "prov:hasTextValue",
    "o": "Generation of artworks and use in design and other artistic processes."
  },
  {
    "s": "mcro:sdxl10base-UseCase",
    "p": "prov:hasTextValue",
    "o": "Research on generative models."
  },
  {
    "s": "mcro:sdxl10base-UseCase",
    "p": "prov:hasTextValue",
    "o": "Applications in educational or creative tools."
  },
  {
    "s": "mcro:sdxl10base-UseCase",
    "p": "prov:hasTextValue",
    "o": "Safe deployment of models which have the potential to generate harmful content."
  },
  {
    "s": "mcro:sdxl10base-UseCase",
    "p": "prov:hasTextValue",
    "o": "Probing and understanding the limitations and biases of generative models."
  },
  {
    "s": "mcro:sdxl10base",
    "p": "mcro:hasConsideration",
    "o": "mcro:sdxl10base-Consideration"
  },
  {
    "s": "mcro:sdxl10base-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:sdxl10base-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:sdxl10base-Limitation"
  },
  {
    "s": "mcro:sdxl10base-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:sdxl10base-Consideration",
    "p": "mcro:hasRisk",
    "o": "mcro:sdxl10base-Risk"
  },
  {
    "s": "mcro:sdxl10base-Risk",
    "p": "rdf:type",
    "o": "mcro:RiskInformationSection"
  },
  {
    "s": "mcro:sdxl10base",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:sdxl10base-OutOfScopeUseCase"
  },
  {
    "s": "mcro:sdxl10base-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:sdxl10base",
    "p": "mcro:hasDataset",
    "o": "mcro:sdxl10base-Dataset"
  },
  {
    "s": "mcro:sdxl10base-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:whisper",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasModelDetail",
    "o": "mcro:whisper-ModelDetail"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:whisper-Citation"
  },
  {
    "s": "mcro:whisper-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisper-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisper-ModelArchitecture"
  },
  {
    "s": "mcro:whisper-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisper-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer based encoder-decoder model"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:whisper-IntendedUseCase"
  },
  {
    "s": "mcro:whisper-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisper-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "ASR solution"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasTrainingData",
    "o": "mcro:whisper-TrainingData"
  },
  {
    "s": "mcro:whisper-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:whisper-TrainingData",
    "p": "prov:hasTextValue",
    "o": "680,000 hours of audio and the corresponding transcripts collected from the internet"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-dutch,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {D}utch},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-dutch}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "XLSR-53 large"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset",
    "p": "prov:hasTextValue",
    "o": "Common Voice 6.1"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2",
    "p": "prov:hasTextValue",
    "o": "CSS10"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Dutch"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:visiontransformerbase-ModelArchitecture"
  },
  {
    "s": "mcro:visiontransformerbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbase-Citation1"
  },
  {
    "s": "mcro:visiontransformerbase-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-Citation1",
    "p": "prov:hasTextValue",
    "o": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:visiontransformerbase-IntendedUseCase"
  },
  {
    "s": "mcro:visiontransformerbase-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "image classification"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasTrainingData",
    "o": "mcro:visiontransformerbase-TrainingData"
  },
  {
    "s": "mcro:visiontransformerbase-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-TrainingData",
    "p": "prov:hasTextValue",
    "o": "ImageNet-21k"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbase-Citation2"
  },
  {
    "s": "mcro:visiontransformerbase-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-Citation2",
    "p": "prov:hasTextValue",
    "o": "Visual Transformers: Token-based Image Representation and Processing for Computer Vision"
  },
  {
    "s": "mcro:visiontransformerbase",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbase-Citation3"
  },
  {
    "s": "mcro:visiontransformerbase-Citation3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-Citation3",
    "p": "prov:hasTextValue",
    "o": "Imagenet: A large-scale hierarchical image database"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel",
    "p": "mcro:hasDataset",
    "o": "mcro:clinicalBERT-Bio+ClinicalBERTModel-DatasetInfo"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel-DatasetInfo",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:clinicalBERT-Bio+ClinicalBERTModel-ArchInfo"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel-ArchInfo",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel",
    "p": "mcro:hasCitation",
    "o": "mcro:clinicalBERT-Bio+ClinicalBERTModel-CitationInfo"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel-CitationInfo",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel",
    "p": "mcro:hasUseCase",
    "o": "mcro:clinicalBERT-Bio+ClinicalBERTModel-UseCaseInfo"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel-UseCaseInfo",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:clinicalBERT-Bio+ClinicalBERTModel-TrainingDataInfo"
  },
  {
    "s": "mcro:clinicalBERT-Bio+ClinicalBERTModel-TrainingDataInfo",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasCitation",
    "o": "mcro:FlagEmbedding-CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasLicense",
    "o": "mcro:FlagEmbedding-LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge."
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasCitation",
    "o": "mcro:FlagEmbedding-Citation"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasLicense",
    "o": "mcro:FlagEmbedding-License"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:facebookbartbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookbartbase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookbartbase-ModelDetail"
  },
  {
    "s": "mcro:facebookbartbase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookbartbase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookbartbase-Citation"
  },
  {
    "s": "mcro:facebookbartbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookbartbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n"
  },
  {
    "s": "mcro:facebookbartbase-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookbartbase-ModelArchitecture"
  },
  {
    "s": "mcro:facebookbartbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookbartbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder."
  },
  {
    "s": "mcro:facebookbartbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookbartbase-UseCase"
  },
  {
    "s": "mcro:facebookbartbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookbartbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset."
  }
]