[
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Falconsainsfwimagedetection-ModelDetail"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "mcro:hasUseCase",
    "o": "mcro:Falconsainsfwimagedetection-UseCase"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "mcro:hasTrainingData",
    "o": "mcro:Falconsainsfwimagedetection-TrainingData"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection",
    "p": "mcro:hasReference",
    "o": "mcro:Falconsainsfwimagedetection-Reference"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:Falconsainsfwimagedetection-Citation"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:Falconsainsfwimagedetection-License"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Falconsainsfwimagedetection-ModelArchitecture"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-UseCase",
    "p": "prov:hasTextValue",
    "o": "- **NSFW Image Classification**: The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications.\n- **Specialized Task Fine-Tuning**: While the model is adept at NSFW image classification, its performance may vary when applied to other tasks.\n- Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results."
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The model's training data includes a proprietary dataset comprising approximately 80,000 images. This dataset encompasses a significant amount of variability and consists of two distinct classes: \"normal\" and \"nsfw.\" The training process on this data aimed to equip the model with the ability to distinguish between safe and explicit content effectively."
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Reference",
    "p": "prov:hasTextValue",
    "o": "- [Hugging Face Model Hub](https://huggingface.co/models)\n- [Vision Transformer (ViT) Paper](https://arxiv.org/abs/2010.11929)\n- [ImageNet-21k Dataset](http://www.image-net.org/)"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-Citation",
    "p": "prov:hasTextValue",
    "o": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru. 2019. Model Cards for Model Reporting. In FAT* \u201919: Conference on Fairness, Accountability, and Transparency, January 29\u201331, 2019, Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3287560. 3287596"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-License",
    "p": "prov:hasTextValue",
    "o": "unknown"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Falconsainsfwimagedetection-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Fine-Tuned Vision Transformer (ViT)"
  },
  {
    "s": "mcro:allMiniLML6v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:allMiniLML6v2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:allMiniLML6v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:allMiniLML6v2-TrainingDataInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:allMiniLML6v2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks."
  },
  {
    "s": "mcro:allMiniLML6v2-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file."
  },
  {
    "s": "mcro:allMiniLML6v2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allMiniLML6v2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:dima806fairfaceageimagedetection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:dima806fairfaceageimagedetection",
    "p": "mcro:hasUseCase",
    "o": "mcro:dima806fairfaceageimagedetection-UseCaseInformationSection"
  },
  {
    "s": "mcro:dima806fairfaceageimagedetection-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:dima806fairfaceageimagedetection-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Detects age group with about 59% accuracy based on an image."
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronos-t5-small-ModelArchitecture"
  },
  {
    "s": "mcro:chronos-t5-small-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters."
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasCitation",
    "o": "mcro:chronos-t5-small-Citation"
  },
  {
    "s": "mcro:chronos-t5-small-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasLicense",
    "o": "mcro:chronos-t5-small-License"
  },
  {
    "s": "mcro:chronos-t5-small-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-License",
    "p": "prov:hasTextValue",
    "o": "This project is licensed under the Apache-2.0 License."
  },
  {
    "s": "mcro:chronos-t5-small",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronos-t5-small-UseCase"
  },
  {
    "s": "mcro:chronos-t5-small-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-small-UseCase",
    "p": "prov:hasTextValue",
    "o": "Chronos is a family of **pretrained time series forecasting models** based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes."
  },
  {
    "s": "mcro:bertbaseuncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertbaseuncased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertbaseuncased-ModelDetail"
  },
  {
    "s": "mcro:bertbaseuncased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertbaseuncased-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertbaseuncased-ModelArchitecture"
  },
  {
    "s": "mcro:bertbaseuncased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertbaseuncased-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion."
  },
  {
    "s": "mcro:bertbaseuncased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertbaseuncased-Citation"
  },
  {
    "s": "mcro:bertbaseuncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertbaseuncased-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:bertbaseuncased",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bertbaseuncased-UseCase"
  },
  {
    "s": "mcro:bertbaseuncased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertbaseuncased-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task."
  },
  {
    "s": "mcro:bertbaseuncased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertbaseuncased-TrainingData"
  },
  {
    "s": "mcro:bertbaseuncased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:bertbaseuncased-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders)."
  },
  {
    "s": "mcro:bertbaseuncased",
    "p": "mcro:hasConsideration",
    "o": "mcro:bertbaseuncased-Consideration"
  },
  {
    "s": "mcro:bertbaseuncased-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:bertbaseuncased-Consideration",
    "p": "prov:hasTextValue",
    "o": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:"
  },
  {
    "s": "mcro:clipvitlargepatch14",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clipvitlargepatch14",
    "p": "mcro:hasModelDetail",
    "o": "mcro:clipvitlargepatch14-ModelDetail"
  },
  {
    "s": "mcro:clipvitlargepatch14-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:clipvitlargepatch14-Citation"
  },
  {
    "s": "mcro:clipvitlargepatch14-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:clipvitlargepatch14-ModelArchitecture"
  },
  {
    "s": "mcro:clipvitlargepatch14-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:clipvitlargepatch14-License"
  },
  {
    "s": "mcro:clipvitlargepatch14-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14",
    "p": "mcro:hasUseCase",
    "o": "mcro:clipvitlargepatch14-UseCase"
  },
  {
    "s": "mcro:clipvitlargepatch14-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-UseCase",
    "p": "mcro:hasPrimaryIntendedUseCase",
    "o": "mcro:clipvitlargepatch14-PrimaryIntendedUseCase"
  },
  {
    "s": "mcro:clipvitlargepatch14-PrimaryIntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-UseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:clipvitlargepatch14-OutOfScopeUseCase"
  },
  {
    "s": "mcro:clipvitlargepatch14-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:clipvitlargepatch14",
    "p": "mcro:hasDataset",
    "o": "mcro:clipvitlargepatch14-Dataset"
  },
  {
    "s": "mcro:clipvitlargepatch14-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-Dataset",
    "p": "mcro:hasTrainingDataInformation",
    "o": "mcro:clipvitlargepatch14-TrainingDataInformation"
  },
  {
    "s": "mcro:clipvitlargepatch14-TrainingDataInformation",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-Dataset",
    "p": "mcro:hasEvaluationDataInformation",
    "o": "mcro:clipvitlargepatch14-EvaluationDataInformation"
  },
  {
    "s": "mcro:clipvitlargepatch14-EvaluationDataInformation",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-Dataset",
    "p": "mcro:hasSensitiveDataInformation",
    "o": "mcro:clipvitlargepatch14-SensitiveDataInformation"
  },
  {
    "s": "mcro:clipvitlargepatch14-SensitiveDataInformation",
    "p": "rdf:type",
    "o": "mcro:SensitiveDataInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14",
    "p": "mcro:hasQuantitativeAnalysis",
    "o": "mcro:clipvitlargepatch14-QuantitativeAnalysis"
  },
  {
    "s": "mcro:clipvitlargepatch14-QuantitativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-QuantitativeAnalysis",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:clipvitlargepatch14-PerformanceMetric"
  },
  {
    "s": "mcro:clipvitlargepatch14-PerformanceMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14",
    "p": "mcro:hasConsideration",
    "o": "mcro:clipvitlargepatch14-Consideration"
  },
  {
    "s": "mcro:clipvitlargepatch14-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:clipvitlargepatch14-Limitation"
  },
  {
    "s": "mcro:clipvitlargepatch14-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-Consideration",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:clipvitlargepatch14-EthicalConsideration"
  },
  {
    "s": "mcro:clipvitlargepatch14-EthicalConsideration",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:clipvitlargepatch14-EthicalConsideration",
    "p": "prov:hasTextValue",
    "o": "bias and fairness"
  },
  {
    "s": "mcro:clipvitlargepatch14-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "ViT-L/14 Transformer"
  },
  {
    "s": "mcro:clipvitlargepatch14-TrainingDataInformation",
    "p": "prov:hasTextValue",
    "o": "publicly available image-caption data"
  },
  {
    "s": "mcro:clipvitlargepatch14-PrimaryIntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "research"
  },
  {
    "s": "mcro:clipvitlargepatch14-OutOfScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "Any deployed use case of the model - whether commercial or not - is currently out of scope."
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:mobilenetv3small100lambin1k-ModelDetail"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:mobilenetv3small100lambin1k-License"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-License",
    "p": "prov:hasTextValue",
    "o": "https://github.com/huggingface/pytorch-image-models"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:mobilenetv3small100lambin1k-Citation"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:mobilenetv3small100lambin1k-Citation2"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Citation2",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{howard2019searching,\n  title={Searching for mobilenetv3},\n  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},\n  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},\n  pages={1314--1324},\n  year={2019}\n}\n"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mobilenetv3small100lambin1k-Architecture"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Architecture",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "mcro:hasDataset",
    "o": "mcro:mobilenetv3small100lambin1k-Dataset"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:mobilenetv3small100lambin1k-UseCase"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Classification"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:mobilenetv3small100lambin1k-UseCase2"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-UseCase2",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-UseCase2",
    "p": "prov:hasTextValue",
    "o": "Feature Map Extraction"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:mobilenetv3small100lambin1k-UseCase3"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-UseCase3",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mobilenetv3small100lambin1k-UseCase3",
    "p": "prov:hasTextValue",
    "o": "Image Embeddings"
  },
  {
    "s": "mcro:RoBERTaLargeModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:RoBERTaLargeModel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:RoBERTaLargeModel-ModelArchitecture"
  },
  {
    "s": "mcro:RoBERTaLargeModel-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:RoBERTaLargeModel-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion."
  },
  {
    "s": "mcro:RoBERTaLargeModel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:RoBERTaLargeModel-IntendedUseCase"
  },
  {
    "s": "mcro:RoBERTaLargeModel-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:RoBERTaLargeModel-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task."
  },
  {
    "s": "mcro:RoBERTaLargeModel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:RoBERTaLargeModel-TrainingData"
  },
  {
    "s": "mcro:RoBERTaLargeModel-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:RoBERTaLargeModel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas."
  },
  {
    "s": "mcro:RoBERTaLargeModel",
    "p": "mcro:hasCitation",
    "o": "mcro:RoBERTaLargeModel-Citation"
  },
  {
    "s": "mcro:RoBERTaLargeModel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:RoBERTaLargeModel-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:clip",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasModelDetail",
    "o": "mcro:clip-ModelDetail"
  },
  {
    "s": "mcro:clip-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:clip-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within."
  },
  {
    "s": "mcro:clip-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:clip-Citation"
  },
  {
    "s": "mcro:clip-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:clip-Citation",
    "p": "prov:hasTextValue",
    "o": "- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)"
  },
  {
    "s": "mcro:clip-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:clip-ModelArchitecture"
  },
  {
    "s": "mcro:clip-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clip-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasUseCase",
    "o": "mcro:clip-UseCase"
  },
  {
    "s": "mcro:clip-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clip-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis."
  },
  {
    "s": "mcro:clip-UseCase",
    "p": "mcro:hasPrimaryIntendedUseCase",
    "o": "mcro:clip-PrimaryIntendedUseCase"
  },
  {
    "s": "mcro:clip-PrimaryIntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:clip-PrimaryIntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
  },
  {
    "s": "mcro:clip-UseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:clip-OutOfScopeUseCase"
  },
  {
    "s": "mcro:clip-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:clip-OutOfScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.\n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasDataset",
    "o": "mcro:clip-Dataset"
  },
  {
    "s": "mcro:clip-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clip-Dataset",
    "p": "prov:hasTextValue",
    "o": "The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasConsideration",
    "o": "mcro:clip-Consideration"
  },
  {
    "s": "mcro:clip-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:clip-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:clip-Limitation"
  },
  {
    "s": "mcro:clip-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:clip-Limitation",
    "p": "prov:hasTextValue",
    "o": "CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance."
  },
  {
    "s": "mcro:clip-Consideration",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:clip-EthicalConsideration"
  },
  {
    "s": "mcro:clip-EthicalConsideration",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:clip-EthicalConsideration",
    "p": "prov:hasTextValue",
    "o": "We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks."
  },
  {
    "s": "mcro:clip",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:clip-QuantativeAnalysis"
  },
  {
    "s": "mcro:clip-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:clip-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "mcro:hasDataset",
    "o": "mcro:YOLOv8DetectionModel-DatasetInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:YOLOv8DetectionModel-PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel-PerformanceMetricInformationSection",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "mcro:hasUseCase",
    "o": "mcro:YOLOv8DetectionModel-UseCaseInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel",
    "p": "mcro:hasConsideration",
    "o": "mcro:YOLOv8DetectionModel-ConsiderationInformationSection"
  },
  {
    "s": "mcro:YOLOv8DetectionModel-ConsiderationInformationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:allmpnetbasev2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:allmpnetbasev2-UseCaseInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks."
  },
  {
    "s": "mcro:allmpnetbasev2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:allmpnetbasev2-TrainingDataInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file."
  },
  {
    "s": "mcro:allmpnetbasev2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:allmpnetbasev2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allmpnetbasev2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset."
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "mcro:hasLicense",
    "o": "mcro:pyannotewespeakervoxcelebresnet34LM-License"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:pyannotewespeakervoxcelebresnet34LM-ModelArchitecture"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM",
    "p": "mcro:hasUseCase",
    "o": "mcro:pyannotewespeakervoxcelebresnet34LM-UseCase"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Wang2023,\n  title={Wespeaker: A research and production oriented speaker embedding learning toolkit},\n  author={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},\n  booktitle={ICASSP 2023, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={1--5},\n  year={2023},\n  organization={IEEE}\n}"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n  pages={1983--1987},\n  doi={10.21437/Interspeech.2023-105}\n}"
  },
  {
    "s": "mcro:pyannotewespeakervoxcelebresnet34LM-License",
    "p": "prov:hasTextValue",
    "o": "The pretrained model in WeNet follows the license of it's corresponding dataset. For example, the pretrained model on VoxCeleb follows Creative Commons Attribution 4.0 International License., since it is used as license of the VoxCeleb dataset, see https://mm.kaist.ac.kr/datasets/voxceleb/."
  },
  {
    "s": "mcro:resnet50.a1_in1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:resnet50.a1_in1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:resnet50.a1_in1k-ModelDetail"
  },
  {
    "s": "mcro:resnet50.a1_in1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:resnet50.a1_in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:resnet50.a1_in1k-Citation"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:resnet50.a1_in1k",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:resnet50.a1_in1k-Arch"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Arch",
    "p": "prov:hasTextValue",
    "o": "ReLU activations"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Arch",
    "p": "prov:hasTextValue",
    "o": "single layer 7x7 convolution with pooling"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Arch",
    "p": "prov:hasTextValue",
    "o": "1x1 convolution shortcut downsample"
  },
  {
    "s": "mcro:resnet50.a1_in1k",
    "p": "mcro:hasDataset",
    "o": "mcro:resnet50.a1_in1k-Dataset"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:resnet50.a1_in1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:resnet50.a1_in1k-UseCase"
  },
  {
    "s": "mcro:resnet50.a1_in1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:resnet50.a1_in1k-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:resnet50.a1_in1k-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "Params (M): 25.6"
  },
  {
    "s": "mcro:resnet50.a1_in1k-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "GMACs: 4.1"
  },
  {
    "s": "mcro:resnet50.a1_in1k-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "Activations (M): 11.1"
  },
  {
    "s": "mcro:resnet50.a1_in1k-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "Image size: train = 224 x 224, test = 288 x 288"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385"
  },
  {
    "s": "mcro:resnet50.a1_in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "https://github.com/huggingface/pytorch-image-models"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "mcro:hasModelDetail",
    "o": "mcro:pyannotesegmentation30-ModelDetail"
  },
  {
    "s": "mcro:pyannotesegmentation30-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotesegmentation30-Citation"
  },
  {
    "s": "mcro:pyannotesegmentation30-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:pyannotesegmentation30-Architecture"
  },
  {
    "s": "mcro:pyannotesegmentation30-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30-ModelDetail",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:pyannotesegmentation30-UseCase"
  },
  {
    "s": "mcro:pyannotesegmentation30-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:pyannotesegmentation30",
    "p": "mcro:hasDataset",
    "o": "mcro:pyannotesegmentation30-Dataset"
  },
  {
    "s": "mcro:pyannotesegmentation30-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:googleelectrabasediscriminator",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:googleelectrabasediscriminator",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:googleelectrabasediscriminator-ModelArchitecture"
  },
  {
    "s": "mcro:googleelectrabasediscriminator-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:googleelectrabasediscriminator-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "**ELECTRA** is a new method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish \"real\" input tokens vs \"fake\" input tokens generated by another neural network, similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset."
  },
  {
    "s": "mcro:googleelectrabasediscriminator",
    "p": "mcro:hasCitation",
    "o": "mcro:googleelectrabasediscriminator-Citation"
  },
  {
    "s": "mcro:googleelectrabasediscriminator-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:googleelectrabasediscriminator-Citation",
    "p": "prov:hasTextValue",
    "o": "For a detailed description and experimental results, please refer to our paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB)."
  },
  {
    "s": "mcro:googleelectrabasediscriminator",
    "p": "mcro:hasUseCase",
    "o": "mcro:googleelectrabasediscriminator-UseCase"
  },
  {
    "s": "mcro:googleelectrabasediscriminator-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:googleelectrabasediscriminator-UseCase",
    "p": "prov:hasTextValue",
    "o": "This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/))."
  },
  {
    "s": "mcro:pyannotespeakerdiarization31",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotespeakerdiarization31-CitationInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31",
    "p": "mcro:hasUseCase",
    "o": "mcro:pyannotespeakerdiarization31-UseCaseInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:pyannotespeakerdiarization31-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31",
    "p": "mcro:hasDataset",
    "o": "mcro:pyannotespeakerdiarization31-DatasetInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Plaquet23,\n  author={Alexis Plaquet and Herv\u00e9 Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}\n"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}\n"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization31-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:FacebookAIxlmrobertabase-ModelDetail"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:FacebookAIxlmrobertabase-Citation"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:FacebookAIxlmrobertabase-ModelArchitecture"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase",
    "p": "mcro:hasUseCase",
    "o": "mcro:FacebookAIxlmrobertabase-UseCase"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase",
    "p": "mcro:hasLimitation",
    "o": "mcro:FacebookAIxlmrobertabase-Limitation"
  },
  {
    "s": "mcro:FacebookAIxlmrobertabase-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gpt2-ModelDetail"
  },
  {
    "s": "mcro:gpt2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:gpt2-IntendedUseCase"
  },
  {
    "s": "mcro:gpt2-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:gpt2-TrainingData"
  },
  {
    "s": "mcro:gpt2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasTrainingProcedure",
    "o": "mcro:gpt2-TrainingProcedure"
  },
  {
    "s": "mcro:gpt2-TrainingProcedure",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasEvaluationResults",
    "o": "mcro:gpt2-EvaluationResults"
  },
  {
    "s": "mcro:gpt2-EvaluationResults",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasCitation",
    "o": "mcro:gpt2-Citation"
  },
  {
    "s": "mcro:gpt2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gpt2-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gpt2-ModelArchitecture"
  },
  {
    "s": "mcro:gpt2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gpt2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers model"
  },
  {
    "s": "mcro:gpt2-TrainingData",
    "p": "prov:hasTextValue",
    "o": "all the web pages from outbound links on Reddit which received at least 3 karma"
  },
  {
    "s": "mcro:gpt2",
    "p": "mcro:hasConsideration",
    "o": "mcro:gpt2-Consideration"
  },
  {
    "s": "mcro:gpt2-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:gpt2-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:gpt2-Limitation"
  },
  {
    "s": "mcro:gpt2-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:distilbertbasemodeluncased-ModelDetail"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:distilbertbasemodeluncased-ModelArchitecture"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks."
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:distilbertbasemodeluncased-IntendedUseCase"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2."
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:distilbertbasemodeluncased-TrainingData"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-TrainingData",
    "p": "prov:hasTextValue",
    "o": "DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers)."
  },
  {
    "s": "mcro:distilbertbasemodeluncased",
    "p": "mcro:hasCitation",
    "o": "mcro:distilbertbasemodeluncased-Citation"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemodeluncased-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualMiniLML12v2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:RoBERTaBaseModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:RoBERTaBaseModel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:RoBERTaBaseModel-ModelArchitecture"
  },
  {
    "s": "mcro:RoBERTaBaseModel-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:RoBERTaBaseModel-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers model"
  },
  {
    "s": "mcro:RoBERTaBaseModel",
    "p": "mcro:hasUseCase",
    "o": "mcro:RoBERTaBaseModel-UseCase"
  },
  {
    "s": "mcro:RoBERTaBaseModel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:RoBERTaBaseModel-UseCase",
    "p": "prov:hasTextValue",
    "o": "masked language modeling"
  },
  {
    "s": "mcro:RoBERTaBaseModel-UseCase",
    "p": "prov:hasTextValue",
    "o": "fine-tuned on a downstream task"
  },
  {
    "s": "mcro:RoBERTaBaseModel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:RoBERTaBaseModel-TrainingData"
  },
  {
    "s": "mcro:RoBERTaBaseModel-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:RoBERTaBaseModel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "BookCorpus"
  },
  {
    "s": "mcro:RoBERTaBaseModel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "English Wikipedia"
  },
  {
    "s": "mcro:RoBERTaBaseModel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "CC-News"
  },
  {
    "s": "mcro:RoBERTaBaseModel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "OpenWebText"
  },
  {
    "s": "mcro:RoBERTaBaseModel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Stories"
  },
  {
    "s": "mcro:RoBERTaBaseModel",
    "p": "mcro:hasCitation",
    "o": "mcro:RoBERTaBaseModel-Citation"
  },
  {
    "s": "mcro:RoBERTaBaseModel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:RoBERTaBaseModel-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:CLIPSegModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:CLIPSegModel",
    "p": "mcro:hasCitation",
    "o": "mcro:CLIPSegModel-CitationInformationSection"
  },
  {
    "s": "mcro:CLIPSegModel-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CLIPSegModel-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by L\u00fcddecke et al. and first released in [this repository](https://github.com/timojl/clipseg)."
  },
  {
    "s": "mcro:CLIPSegModel",
    "p": "mcro:hasUseCase",
    "o": "mcro:CLIPSegModel-UseCaseInformationSection"
  },
  {
    "s": "mcro:CLIPSegModel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:CLIPSegModel-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model is intended for zero-shot and one-shot image segmentation."
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertmultilingualbasemodelcased-ModelDetail"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertmultilingualbasemodelcased-Citation"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertmultilingualbasemodelcased-UseCase"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertmultilingualbasemodelcased-TrainingData"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertmultilingualbasemodelcased-ModelArchitecture"
  },
  {
    "s": "mcro:bertmultilingualbasemodelcased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-portuguese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ortuguese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53portuguese-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "[Common Voice 6.1](https://huggingface.co/datasets/common_voice)"
  },
  {
    "s": "mcro:microsoftdebertalargemnli",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftdebertalargemnli",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftdebertalargemnli-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftdebertalargemnli-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftdebertalargemnli-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
  },
  {
    "s": "mcro:microsoftdebertalargemnli",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftdebertalargemnli-Citation"
  },
  {
    "s": "mcro:microsoftdebertalargemnli-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftdebertalargemnli-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}"
  },
  {
    "s": "mcro:colbertv2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:colbertv2",
    "p": "mcro:hasCitation",
    "o": "mcro:colbertv2-CitationInformationSection"
  },
  {
    "s": "mcro:colbertv2-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:colbertv2-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "* [**ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT**](https://arxiv.org/abs/2004.12832) (SIGIR'20).\n* [**Relevance-guided Supervision for OpenQA with ColBERT**](https://arxiv.org/abs/2007.00814) (TACL'21).\n* [**Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval**](https://arxiv.org/abs/2101.00436) (NeurIPS'21).\n* [**ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction**](https://arxiv.org/abs/2112.01488) (NAACL'22).\n* [**PLAID: An Efficient Engine for Late Interaction Retrieval**](https://arxiv.org/abs/2205.09707) (CIKM'22)."
  },
  {
    "s": "mcro:colbertv2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:colbertv2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:colbertv2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:colbertv2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "ColBERT is a _fast_ and _accurate_ retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds."
  },
  {
    "s": "mcro:colbertv2",
    "p": "mcro:hasUseCase",
    "o": "mcro:colbertv2-UseCaseInformationSection"
  },
  {
    "s": "mcro:colbertv2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:colbertv2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Using ColBERT on a dataset typically involves the following steps."
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metaLlama31Instruct-ModelArchitecture"
  },
  {
    "s": "mcro:metaLlama31Instruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "mcro:hasLicense",
    "o": "mcro:metaLlama31Instruct-License"
  },
  {
    "s": "mcro:metaLlama31Instruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)"
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:metaLlama31Instruct-IntendedUseCase"
  },
  {
    "s": "mcro:metaLlama31Instruct-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases."
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "mcro:hasDataset",
    "o": "mcro:metaLlama31Instruct-Dataset"
  },
  {
    "s": "mcro:metaLlama31Instruct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-Dataset",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples."
  },
  {
    "s": "mcro:Qwen2515BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2515BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2515BInstruct-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2515BInstruct-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2515BInstruct-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:Qwen2515BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2515BInstruct-UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2515BInstruct-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2515BInstruct-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training."
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:CLIPViTbigG14LAION2B-ModelDetail"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:CLIPViTbigG14LAION2B-License"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-License",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:CLIPViTbigG14LAION2B-Citation1"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Citation1",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:CLIPViTbigG14LAION2B-Citation2"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Citation2",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Radford2021LearningTV,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n  booktitle={ICML},\n  year={2021}\n}"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:CLIPViTbigG14LAION2B-Citation3"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Citation3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Citation3",
    "p": "prov:hasTextValue",
    "o": "@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:CLIPViTbigG14LAION2B-Citation4"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Citation4",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Citation4",
    "p": "prov:hasTextValue",
    "o": "@article{cherti2022reproducible,\n  title={Reproducible scaling laws for contrastive language-image learning},\n  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n  journal={arXiv preprint arXiv:2212.07143},\n  year={2022}\n}"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B",
    "p": "mcro:hasUseCase",
    "o": "mcro:CLIPViTbigG14LAION2B-UseCase"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-UseCase",
    "p": "prov:hasTextValue",
    "o": "As per the original [OpenAI CLIP model card](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/model-card.md), this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. \n\nThe OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset."
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B",
    "p": "mcro:hasTrainingData",
    "o": "mcro:CLIPViTbigG14LAION2B-TrainingData"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-TrainingData",
    "p": "prov:hasTextValue",
    "o": "This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/). \nFine-tuning was also partially done on LAION-A, a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and phash deduplicated.\n\n**IMPORTANT NOTE:** The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a \u201csafe\u201d subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress."
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B",
    "p": "mcro:hasEvaluation",
    "o": "mcro:CLIPViTbigG14LAION2B-Evaluation"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Evaluation",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:CLIPViTbigG14LAION2B-Architecture"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:CLIPViTbigG14LAION2B-Architecture",
    "p": "prov:hasTextValue",
    "o": "A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip)."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-russian,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {R}ussian},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Russian using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [CSS10](https://github.com/Kyubyong/css10)."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53russian-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Russian"
  },
  {
    "s": "mcro:metallamaMetaLlama38B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metallamaMetaLlama38B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metallamaMetaLlama38B-ModelDetail"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metallamaMetaLlama38B",
    "p": "mcro:hasUseCase",
    "o": "mcro:metallamaMetaLlama38B-UseCase"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metallamaMetaLlama38B",
    "p": "mcro:hasTrainingData",
    "o": "mcro:metallamaMetaLlama38B-TrainingData"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:metallamaMetaLlama38B",
    "p": "mcro:hasBenchmark",
    "o": "mcro:metallamaMetaLlama38B-Benchmark"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-Benchmark",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:metallamaMetaLlama38B",
    "p": "mcro:hasConsideration",
    "o": "mcro:metallamaMetaLlama38B-Consideration"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:metallamaMetaLlama38B-Architecture"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-Architecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metallamaMetaLlama38B-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:metallamaMetaLlama38B-License"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:metallamaMetaLlama38B-Citation"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{llama3modelcard,\n  title={Llama 3 Model Card},\n  author={AI@Meta},\n  year={2024},\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n}"
  },
  {
    "s": "mcro:metallamaMetaLlama38B-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks."
  },
  {
    "s": "mcro:metallamaMetaLlama38B-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data."
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertbasemodelcased-ModelDetail"
  },
  {
    "s": "mcro:bertbasemodelcased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertbasemodelcased-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:bertbasemodelcased-License"
  },
  {
    "s": "mcro:bertbasemodelcased-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertbasemodelcased-Citation"
  },
  {
    "s": "mcro:bertbasemodelcased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:bertbasemodelcased-Architecture"
  },
  {
    "s": "mcro:bertbasemodelcased-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertbasemodelcased-UseCase"
  },
  {
    "s": "mcro:bertbasemodelcased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasConsideration",
    "o": "mcro:bertbasemodelcased-Consideration"
  },
  {
    "s": "mcro:bertbasemodelcased-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertbasemodelcased-TrainingData"
  },
  {
    "s": "mcro:bertbasemodelcased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:bertbasemodelcased",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:bertbasemodelcased-EvaluationData"
  },
  {
    "s": "mcro:bertbasemodelcased-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:ESMFold",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ESMFold",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ESMFold-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ESMFold-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ESMFold-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "ESMFold is a state-of-the-art end-to-end protein folding model based on an ESM-2 backbone. It does not require any lookup or MSA step, and therefore does not require any external databases to be present in order to make predictions. As a result, inference time is very significantly faster than AlphaFold2."
  },
  {
    "s": "mcro:ESMFold",
    "p": "mcro:hasCitation",
    "o": "mcro:ESMFold-CitationInformationSection"
  },
  {
    "s": "mcro:ESMFold-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ESMFold-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "For details on the model architecture and training, please refer to the [accompanying paper](https://www.science.org/doi/10.1126/science.ade2574)."
  },
  {
    "s": "mcro:ESMFold",
    "p": "mcro:hasUseCase",
    "o": "mcro:ESMFold-UseCaseInformationSection"
  },
  {
    "s": "mcro:ESMFold-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:ESMFold-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "If you're interested in using ESMFold in practice, please check out the associated [tutorial notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb)."
  },
  {
    "s": "mcro:allMiniLML12v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:allMiniLML12v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:allMiniLML12v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated."
  },
  {
    "s": "mcro:allMiniLML12v2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:allMiniLML12v2-TrainingDataInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file."
  },
  {
    "s": "mcro:allMiniLML12v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:allMiniLML12v2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allMiniLML12v2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge",
    "p": "mcro:hasModelDetail",
    "o": "mcro:FacebookAIxlmrobertalarge-ModelDetail"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge",
    "p": "mcro:hasUseCase",
    "o": "mcro:FacebookAIxlmrobertalarge-UseCase"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge",
    "p": "mcro:hasCitation",
    "o": "mcro:FacebookAIxlmrobertalarge-Citation"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:FacebookAIxlmrobertalarge-ModelArchitecture"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:FacebookAIxlmrobertalarge-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronosboltbase-ModelArchitecture"
  },
  {
    "s": "mcro:chronosboltbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronosboltbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "T5 encoder-decoder architecture"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "mcro:hasCitation",
    "o": "mcro:chronosboltbase-Citation"
  },
  {
    "s": "mcro:chronosboltbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronosboltbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "mcro:hasLicense",
    "o": "mcro:chronosboltbase-License"
  },
  {
    "s": "mcro:chronosboltbase-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronosboltbase-License",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0 License"
  },
  {
    "s": "mcro:chronosboltbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronosboltbase-UseCase"
  },
  {
    "s": "mcro:chronosboltbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronosboltbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "time series forecasting"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Chinese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [ST-CMDS](http://www.openslr.org/38/).\nWhen using this model, make sure that your speech input is sampled at 16kHz."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Chinese"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-chinese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {C}hinese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:siglipso400mpatch14384",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:siglipso400mpatch14384",
    "p": "mcro:hasModelDetail",
    "o": "mcro:siglipso400mpatch14384-ModelDetail"
  },
  {
    "s": "mcro:siglipso400mpatch14384-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:siglipso400mpatch14384-ModelArchitecture"
  },
  {
    "s": "mcro:siglipso400mpatch14384-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SoViT-400m architecture"
  },
  {
    "s": "mcro:siglipso400mpatch14384-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:siglipso400mpatch14384-Citation"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training}, \n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:siglipso400mpatch14384",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:siglipso400mpatch14384-IntendedUseCase"
  },
  {
    "s": "mcro:siglipso400mpatch14384-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "zero-shot image classification and image-text retrieval"
  },
  {
    "s": "mcro:siglipso400mpatch14384",
    "p": "mcro:hasTrainingData",
    "o": "mcro:siglipso400mpatch14384-TrainingData"
  },
  {
    "s": "mcro:siglipso400mpatch14384-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:siglipso400mpatch14384-TrainingData",
    "p": "prov:hasTextValue",
    "o": "WebLI dataset"
  },
  {
    "s": "mcro:facebookopt125m",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookopt125m",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:facebookopt125m-UseCase"
  },
  {
    "s": "mcro:facebookopt125m-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookopt125m-UseCase",
    "p": "prov:hasTextValue",
    "o": "The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\nIn addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling). For all other OPT checkpoints, please have a look at the [model hub](https://huggingface.co/models?filter=opt)."
  },
  {
    "s": "mcro:facebookopt125m",
    "p": "mcro:hasLimitation",
    "o": "mcro:facebookopt125m-Limitation"
  },
  {
    "s": "mcro:facebookopt125m-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:facebookopt125m-Limitation",
    "p": "prov:hasTextValue",
    "o": "As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of\nunfiltered content from the internet, which is far from neutral the model is strongly biased : \n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models. \n\nThis bias will also affect all fine-tuned versions of this model."
  },
  {
    "s": "mcro:facebookopt125m",
    "p": "mcro:hasTrainingData",
    "o": "mcro:facebookopt125m-TrainingData"
  },
  {
    "s": "mcro:facebookopt125m-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:facebookopt125m-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents: \n\n  - BookCorpus, which consists of more than 10K unpublished books,\n  - CC-Stories, which contains a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas,\n  - The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included. \n  - Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021)\n  - CCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n\nThe final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally\nto each dataset\u2019s size in the pretraining corpus. \n\nThe dataset might contains offensive content as parts of the dataset are a subset of\npublic Common Crawl data, along with a subset of public Reddit data, which could contain sentences\nthat, if viewed directly, can be insulting, threatening, or might otherwise cause anxiety."
  },
  {
    "s": "mcro:facebookopt125m",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookopt125m-Citation"
  },
  {
    "s": "mcro:facebookopt125m-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookopt125m-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{zhang2022opt,\n      title={OPT: Open Pre-trained Transformer Language Models}, \n      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},\n      year={2022},\n      eprint={2205.01068},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:facebookopt125m",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookopt125m-Arch"
  },
  {
    "s": "mcro:facebookopt125m-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookopt125m-Arch",
    "p": "prov:hasTextValue",
    "o": "OPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\nOPT belongs to the same family of decoder-only models like [GPT-3](https://arxiv.org/abs/2005.14165). As such, it was pretrained using the self-supervised causal language modedling objective."
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis",
    "p": "mcro:hasModelDetail",
    "o": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelDetail"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-License"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-License",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelArchitecture"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Text Classification"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-UseCase"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-UseCase",
    "p": "prov:hasTextValue",
    "o": "topic classification"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis",
    "p": "mcro:hasTrainingData",
    "o": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-TrainingData"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:distilbertdistilbertbaseuncasedfinetunedsst2englis-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Stanford Sentiment Treebank"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model",
    "p": "mcro:hasModelDetail",
    "o": "mcro:granitetimeseriesttmr2model-ModelDetail"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model",
    "p": "mcro:hasUseCaseInformation",
    "o": "mcro:granitetimeseriesttmr2model-UseCaseInformation"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-UseCaseInformation",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model",
    "p": "mcro:hasTrainingDataInformation",
    "o": "mcro:granitetimeseriesttmr2model-TrainingDataInformation"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-TrainingDataInformation",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model",
    "p": "mcro:hasCitationInformation",
    "o": "mcro:granitetimeseriesttmr2model-CitationInformation"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-CitationInformation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-ModelDetail",
    "p": "mcro:hasCitationInformation",
    "o": "mcro:granitetimeseriesttmr2model-Citation"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{ekambaram2024tinytimemixersttms,\n      title={Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series},\n      author={Vijay Ekambaram and Arindam Jati and Pankaj Dayama and Sumanta Mukherjee and Nam H. Nguyen and Wesley M. Gifford and Chandra Reddy and Jayant Kalagnanam},\n      booktitle={Advances in Neural Information Processing Systems (NeurIPS 2024)},\n      year={2024},\n}"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-ModelDetail",
    "p": "mcro:hasModelArchitectureInformation",
    "o": "mcro:granitetimeseriesttmr2model-ModelArchitecture"
  },
  {
    "s": "mcro:granitetimeseriesttmr2model-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:trpakovvitfaceexpression",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:trpakovvitfaceexpression",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:trpakovvitfaceexpression-ModelArchitecture"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT)"
  },
  {
    "s": "mcro:trpakovvitfaceexpression",
    "p": "mcro:hasDataset",
    "o": "mcro:trpakovvitfaceexpression-Dataset"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-Dataset",
    "p": "prov:hasTextValue",
    "o": "FER2013"
  },
  {
    "s": "mcro:trpakovvitfaceexpression",
    "p": "mcro:hasUseCase",
    "o": "mcro:trpakovvitfaceexpression-UseCase"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-UseCase",
    "p": "prov:hasTextValue",
    "o": "Facial Expression/Emotion Recognition"
  },
  {
    "s": "mcro:trpakovvitfaceexpression",
    "p": "mcro:hasLimitation",
    "o": "mcro:trpakovvitfaceexpression-Limitation"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-Limitation",
    "p": "prov:hasTextValue",
    "o": "Data Bias"
  },
  {
    "s": "mcro:trpakovvitfaceexpression",
    "p": "mcro:hasLimitation",
    "o": "mcro:trpakovvitfaceexpression-LimitationGeneralization"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-LimitationGeneralization",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:trpakovvitfaceexpression-LimitationGeneralization",
    "p": "prov:hasTextValue",
    "o": "Generalization"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "mcro:hasUseCase",
    "o": "mcro:jinaaijinaembeddingsv3-UseCaseInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "`jina-embeddings-v3` is a **multilingual multi-task text embedding model** designed for a variety of NLP applications.\nBased on the [Jina-XLM-RoBERTa architecture](https://huggingface.co/jinaai/xlm-roberta-flash-implementation), \nthis model supports Rotary Position Embeddings to handle long input sequences up to **8192 tokens**.\nAdditionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently."
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jinaaijinaembeddingsv3-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Jina-XLM-RoBERTa architecture"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "mcro:hasLicense",
    "o": "mcro:jinaaijinaembeddingsv3-LicenseInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "`jina-embeddings-v3` is listed on AWS & Azure. If you need to use it beyond those platforms or on-premises within your company, note that the models is licensed under CC BY-NC 4.0. For commercial usage inquiries, feel free to [contact us](https://jina.ai/contact-sales/)."
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3",
    "p": "mcro:hasCitation",
    "o": "mcro:jinaaijinaembeddingsv3-CitationInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jinaaijinaembeddingsv3-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,\n      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, \n      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael G\u00fcnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},\n      year={2024},\n      eprint={2409.10173},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2409.10173}, \n}"
  },
  {
    "s": "mcro:autogluonchronosboltbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:autogluonchronosboltbase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:autogluonchronosboltbase-ModelDetail"
  },
  {
    "s": "mcro:autogluonchronosboltbase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:autogluonchronosboltbase-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:autogluonchronosboltbase-License"
  },
  {
    "s": "mcro:autogluonchronosboltbase-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:autogluonchronosboltbase-License",
    "p": "prov:hasTextValue",
    "o": "This project is licensed under the Apache-2.0 License."
  },
  {
    "s": "mcro:autogluonchronosboltbase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:autogluonchronosboltbase-Citation"
  },
  {
    "s": "mcro:autogluonchronosboltbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:autogluonchronosboltbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:autogluonchronosboltbase",
    "p": "mcro:hasModelParameter",
    "o": "mcro:autogluonchronosboltbase-ModelParameter"
  },
  {
    "s": "mcro:autogluonchronosboltbase-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:autogluonchronosboltbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:autogluonchronosboltbase-ModelArchitecture"
  },
  {
    "s": "mcro:autogluonchronosboltbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:autogluonchronosboltbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683)"
  },
  {
    "s": "mcro:autogluonchronosboltbase",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:autogluonchronosboltbase-QuantativeAnalysis"
  },
  {
    "s": "mcro:autogluonchronosboltbase-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:autogluonchronosboltbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:autogluonchronosboltbase-UseCase"
  },
  {
    "s": "mcro:autogluonchronosboltbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:autogluonchronosboltbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting."
  },
  {
    "s": "mcro:vitmatte-model",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vitmatte-model",
    "p": "mcro:hasModelDetail",
    "o": "mcro:vitmatte-model-ModelDetail"
  },
  {
    "s": "mcro:vitmatte-model-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:vitmatte-model-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:vitmatte-model-CitationInformationSection"
  },
  {
    "s": "mcro:vitmatte-model-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:vitmatte-model-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{yao2023vitmatte,\n      title={ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers}, \n      author={Jingfeng Yao and Xinggang Wang and Shusheng Yang and Baoyuan Wang},\n      year={2023},\n      eprint={2305.15272},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:vitmatte-model-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vitmatte-model-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vitmatte-model-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vitmatte-model-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "ViTMatte is a simple approach to image matting, the task of accurately estimating the foreground object in an image. The model consists of a Vision Transformer (ViT) with a lightweight head on top."
  },
  {
    "s": "mcro:vitmatte-model",
    "p": "mcro:hasUseCase",
    "o": "mcro:vitmatte-model-UseCaseInformationSection"
  },
  {
    "s": "mcro:vitmatte-model-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:vitmatte-model-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for image matting. See the [model hub](https://huggingface.co/models?search=vitmatte) to look for other\nfine-tuned versions that may interest you."
  },
  {
    "s": "mcro:clip-vit-large-patch14-336",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336",
    "p": "mcro:hasDataset",
    "o": "mcro:clip-vit-large-patch14-336-DatasetInformationSection"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "More information needed"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:clip-vit-large-patch14-336-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "More information needed"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336",
    "p": "mcro:hasUseCase",
    "o": "mcro:clip-vit-large-patch14-336-UseCaseInformationSection"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:clip-vit-large-patch14-336-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "More information needed"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b",
    "p": "mcro:hasLicense",
    "o": "mcro:cognitivecomputationsdolphin291yi1534b-License"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b-License",
    "p": "prov:hasTextValue",
    "o": "apache 2.0 license"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:cognitivecomputationsdolphin291yi1534b-UseCase"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b-UseCase",
    "p": "prov:hasTextValue",
    "o": "instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b",
    "p": "mcro:hasTrainingData",
    "o": "mcro:cognitivecomputationsdolphin291yi1534b-TrainingData"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b-TrainingData",
    "p": "prov:hasTextValue",
    "o": "/workspace/datasets/dolphin-2.9/dolphin201-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/dolphin-coder-translate-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/dolphin-coder-codegen-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/m-a-p_Code-Feedback-sharegpt-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/m-a-p_CodeFeedback-Filtered-Instruction-sharegpt-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/not_samantha_norefusals.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/Orca-Math-resort-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/agent_instruct_react_unfiltered.jsonl\n    type: sharegpt  \n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_instruct_j1s1_3k_unfiltered.jsonl\n    type: sharegpt  \n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_negative_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_react_10p_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_tflan_cot_30p_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/openhermes200k_unfiltered.jsonl\n    type: sharegpt \n    conversation: chatml"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:cognitivecomputationsdolphin291yi1534b-Arch"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:cognitivecomputationsdolphin291yi1534b-Arch",
    "p": "prov:hasTextValue",
    "o": "Yi-1.5-34B"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasLicense",
    "o": "mcro:FlagEmbedding-License"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasCitation",
    "o": "mcro:FlagEmbedding-Citation"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:FlagEmbedding-ModelArchitecture"
  },
  {
    "s": "mcro:FlagEmbedding-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasDataset",
    "o": "mcro:FlagEmbedding-Dataset"
  },
  {
    "s": "mcro:FlagEmbedding-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasUseCase",
    "o": "mcro:FlagEmbedding-UseCase"
  },
  {
    "s": "mcro:FlagEmbedding-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:visiontransformerbase-sizedmodel-ModelArchitecture"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224."
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbase-sizedmodel-Citation"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-Citation",
    "p": "prov:hasTextValue",
    "o": "It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer)."
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:visiontransformerbase-sizedmodel-UseCase"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:visiontransformerbase-sizedmodel-TrainingData"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes."
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:tabletransformerfinetunedfortabledetection-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Transformer-based object detection model"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection",
    "p": "mcro:hasDataset",
    "o": "mcro:tabletransformerfinetunedfortabledetection-DatasetInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "PubTables1M"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection",
    "p": "mcro:hasCitation",
    "o": "mcro:tabletransformerfinetunedfortabledetection-CitationInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection",
    "p": "mcro:hasUseCase",
    "o": "mcro:tabletransformerfinetunedfortabledetection-UseCaseInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortabledetection-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "detecting tables in documents"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:distilbertbasemultilingualcased-ModelDetail"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:distilbertbasemultilingualcased-License"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:distilbertbasemultilingualcased-Citation"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:distilbertbasemultilingualcased-ModelArchitecture"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasUseCase",
    "o": "mcro:distilbertbasemultilingualcased-UseCase"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasConsideration",
    "o": "mcro:distilbertbasemultilingualcased-Consideration"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasTrainingParameter",
    "o": "mcro:distilbertbasemultilingualcased-TrainingParameter"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-TrainingParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:distilbertbasemultilingualcased-QuantativeAnalysis"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased",
    "p": "mcro:hasEnvironmentalImpact",
    "o": "mcro:distilbertbasemultilingualcased-EnvironmentalImpact"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-EnvironmentalImpact",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based language model"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2."
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-Consideration",
    "p": "prov:hasTextValue",
    "o": "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n## Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model."
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-TrainingParameter",
    "p": "prov:hasTextValue",
    "o": "- The model was pretrained with the supervision of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the concatenation of Wikipedia in 104 different languages\n- The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters.\n- Further information about the training procedure and data is included in the [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) model card."
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "The model developers report the following accuracy results for DistilmBERT (see [GitHub Repo](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)): \n\n> Here are the results on the test sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):\n\n| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |\n| :---:                        | :---:   | :---:   | :---:   | :---:  | :---:   | :---:|\n| mBERT base cased (computed)  | 82.1    | 74.6    | 69.1    | 72.3   | 66.4    | 58.5 |\n| mBERT base uncased (reported)| 81.4    | 74.3    | 63.8    | 70.5   | 62.1    | 58.3 |\n| DistilmBERT                  | 78.2    | 69.1    | 64.0    | 66.3   | 59.1    | 54.7 |"
  },
  {
    "s": "mcro:distilbertbasemultilingualcased-EnvironmentalImpact",
    "p": "prov:hasTextValue",
    "o": "Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphraseMiniLML6v2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphraseMiniLML6v2-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersparaphraseMiniLML6v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML6v2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "mcro:hasModelDetail",
    "o": "mcro:whisperlargev3-ModelDetail"
  },
  {
    "s": "mcro:whisperlargev3-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:whisperlargev3-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:whisperlargev3-Citation"
  },
  {
    "s": "mcro:whisperlargev3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisperlargev3-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}"
  },
  {
    "s": "mcro:whisperlargev3-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisperlargev3-Architecture"
  },
  {
    "s": "mcro:whisperlargev3-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisperlargev3-Architecture",
    "p": "prov:hasTextValue",
    "o": "Whisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model."
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "mcro:hasTrainingData",
    "o": "mcro:whisperlargev3-TrainingData"
  },
  {
    "s": "mcro:whisperlargev3-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:whisperlargev3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The large-v3 checkpoint is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected using Whisper large-v2."
  },
  {
    "s": "mcro:whisperlargev3",
    "p": "mcro:hasUseCase",
    "o": "mcro:whisperlargev3-UseCase"
  },
  {
    "s": "mcro:whisperlargev3-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisperlargev3-UseCase",
    "p": "prov:hasTextValue",
    "o": "The models are primarily trained and evaluated on ASR and speech translation to English tasks."
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2505BInstruct-UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training."
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Qwen2505BInstruct-ModelDetailSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ModelDetailSection",
    "p": "prov:hasTextValue",
    "o": "This repo contains an **unmodified version** of the instruction-tuned 0.5B Qwen2.5 model, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ModelDetailSection",
    "p": "mcro:hasArchitecture",
    "o": "mcro:Qwen2505BInstruct-ArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:MahmoudAshrafmms300m1130forcedaligner",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:MahmoudAshrafmms300m1130forcedaligner",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:MahmoudAshrafmms300m1130forcedaligner-ModelArchitecture"
  },
  {
    "s": "mcro:MahmoudAshrafmms300m1130forcedaligner-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:MahmoudAshrafmms300m1130forcedaligner-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "conversion from torchaudio to HF Transformers for the MMS-300M checkpoint trained on forced alignment dataset"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2",
    "p": "mcro:hasDataset",
    "o": "mcro:crossencodermsmarcoMiniLML6v2-Dataset"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-Dataset",
    "p": "prov:hasTextValue",
    "o": "This model was trained on the MS Marco Passage Ranking task."
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:crossencodermsmarcoMiniLML6v2-UseCase"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order."
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:crossencodermsmarcoMiniLML6v2-Architecture"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-Architecture",
    "p": "prov:hasTextValue",
    "o": "Cross-Encoder for MS Marco"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:crossencodermsmarcoMiniLML6v2-QuantativeAnalysis"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML6v2-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset."
  },
  {
    "s": "mcro:facebookesm2t363BUR50D",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookesm2t363BUR50D",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookesm2t363BUR50D-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookesm2t363BUR50D-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookesm2t363BUR50D-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "ESM-2 is a state-of-the-art protein model trained on a masked language modelling objective."
  },
  {
    "s": "mcro:facebookesm2t363BUR50D",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookesm2t363BUR50D-UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookesm2t363BUR50D-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookesm2t363BUR50D-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "It is suitable for fine-tuning on a wide range of tasks that take protein sequences as input."
  },
  {
    "s": "mcro:facebookesm2t363BUR50D",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookesm2t363BUR50D-CitationInformationSection"
  },
  {
    "s": "mcro:facebookesm2t363BUR50D-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookesm2t363BUR50D-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "For detailed information on the model architecture and training data, please refer to the [accompanying paper](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2)."
  },
  {
    "s": "mcro:YOLOWorldMirror",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:YOLOWorldMirror",
    "p": "mcro:hasDocumentation",
    "o": "mcro:YOLOWorldMirror-Documentation"
  },
  {
    "s": "mcro:YOLOWorldMirror-Documentation",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:YOLOWorldMirror-Documentation",
    "p": "prov:hasTextValue",
    "o": "https://docs.ultralytics.com/models/yolo-world/#available-models-supported-tasks-and-operating-modes"
  },
  {
    "s": "mcro:YOLOWorldMirror",
    "p": "mcro:hasModelDetail",
    "o": "mcro:YOLOWorldMirror-ModelDetail"
  },
  {
    "s": "mcro:YOLOWorldMirror-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:YOLOWorldMirror-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "model weights for ultralytics yolo models"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip",
    "p": "mcro:hasModelDetail",
    "o": "mcro:patrickjohncyhfashionclip-ModelDetail"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:patrickjohncyhfashionclip-Architecture"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-Architecture",
    "p": "prov:hasTextValue",
    "o": "The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder."
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:patrickjohncyhfashionclip-Citation"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-Citation",
    "p": "prov:hasTextValue",
    "o": "@Article{Chia2022,\n    title=\"Contrastive language and vision learning of general fashion concepts\",\n    author=\"Chia, Patrick John\n            and Attanasio, Giuseppe\n            and Bianchi, Federico\n            and Terragni, Silvia\n            and Magalh{\\~a}es, Ana Rita\n            and Goncalves, Diogo\n            and Greco, Ciro\n            and Tagliabue, Jacopo\",\n    journal=\"Scientific Reports\",\n    year=\"2022\",\n    month=\"Nov\",\n    day=\"08\",\n    volume=\"12\",\n    number=\"1\",\n    abstract=\"The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from general and transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model adapted for the fashion industry. We demonstrate the effectiveness of the representations learned by FashionCLIP with extensive tests across a variety of tasks, datasets and generalization probes. We argue that adaptations of large pre-trained models such as CLIP offer new perspectives in terms of scalability and sustainability for certain types of players in the industry. Finally, we detail the costs and environmental impact of training, and release the model weights and code as open source contribution to the community.\",\n    issn=\"2045-2322\",\n    doi=\"10.1038/s41598-022-23052-9\",\n    url=\"https://doi.org/10.1038/s41598-022-23052-9\"\n}"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip",
    "p": "mcro:hasDataset",
    "o": "mcro:patrickjohncyhfashionclip-Dataset"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-Dataset",
    "p": "prov:hasTextValue",
    "o": "The model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., \u201cstripes\u201d, \u201clong sleeves\u201d, \u201cArmani\u201d) and _short description_ (\u201c80s styled t-shirt\u201d)) available in the Farfetch dataset."
  },
  {
    "s": "mcro:patrickjohncyhfashionclip",
    "p": "mcro:hasUseCase",
    "o": "mcro:patrickjohncyhfashionclip-UseCase"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:patrickjohncyhfashionclip-UseCase",
    "p": "prov:hasTextValue",
    "o": "FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts."
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasCitation",
    "o": "mcro:FlagEmbedding-Citation"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasLicense",
    "o": "mcro:FlagEmbedding-License"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:FlagEmbedding-ModelArchitecture"
  },
  {
    "s": "mcro:FlagEmbedding-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "retromae and contrastive learning"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasUseCase",
    "o": "mcro:FlagEmbedding-UseCase"
  },
  {
    "s": "mcro:FlagEmbedding-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-UseCase",
    "p": "prov:hasTextValue",
    "o": "Retrieval-augmented LLMs"
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "mcro:hasModelDetail",
    "o": "mcro:stablediffusionv15modelcard-ModelDetail"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:stablediffusionv15modelcard-License"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-License",
    "p": "prov:hasTextValue",
    "o": "The CreativeML OpenRAIL M license"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:stablediffusionv15modelcard-Citation"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Citation",
    "p": "prov:hasTextValue",
    "o": "@InProceedings{Rombach_2022_CVPR,\n author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n title = {High-Resolution Image Synthesis With Latent Diffusion Models},\n booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n month = {June},\n year = {2022},\n pages = {10684-10695}\n}"
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "mcro:hasUseCase",
    "o": "mcro:stablediffusionv15modelcard-UseCase"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models."
  },
  {
    "s": "mcro:stablediffusionv15modelcard-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:stablediffusionv15modelcard-Architecture"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Architecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generation model"
  },
  {
    "s": "mcro:stablediffusionv15modelcard",
    "p": "mcro:hasConsideration",
    "o": "mcro:stablediffusionv15modelcard-Consideration"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:stablediffusionv15modelcard-Limitation"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Limitation",
    "p": "prov:hasTextValue",
    "o": "- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to \u201cA red cube on top of a blue sphere\u201d\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n and is not fit for product use without additional safety mechanisms and\n considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images."
  },
  {
    "s": "mcro:stablediffusionv15modelcard-Consideration",
    "p": "mcro:hasTrainingData",
    "o": "mcro:stablediffusionv15modelcard-TrainingData"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:stablediffusionv15modelcard-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo",
    "p": "mcro:hasModelDetail",
    "o": "mcro:openaiwhisperlargev3turbo-ModelDetail"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:openaiwhisperlargev3turbo-Citation"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:openaiwhisperlargev3turbo-License"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:openaiwhisperlargev3turbo-ModelArchitecture"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:openaiwhisperlargev3turbo-UseCase"
  },
  {
    "s": "mcro:openaiwhisperlargev3turbo-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgem3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BAAIbgem3",
    "p": "mcro:hasModelDetail",
    "o": "mcro:BAAIbgem3-ModelDetail"
  },
  {
    "s": "mcro:BAAIbgem3-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:BAAIbgem3-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:BAAIbgem3-Citation"
  },
  {
    "s": "mcro:BAAIbgem3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgem3-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:BAAIbgem3",
    "p": "mcro:hasUseCase",
    "o": "mcro:BAAIbgem3-UseCase"
  },
  {
    "s": "mcro:BAAIbgem3-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgem3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BAAIbgem3-ModelArchitecture"
  },
  {
    "s": "mcro:BAAIbgem3-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgem3",
    "p": "mcro:hasLicense",
    "o": "mcro:BAAIbgem3-License"
  },
  {
    "s": "mcro:BAAIbgem3-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:BAAIbgem3",
    "p": "mcro:hasDataset",
    "o": "mcro:BAAIbgem3-Dataset"
  },
  {
    "s": "mcro:BAAIbgem3-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bartlargesizedmodelfinetunedoncnndailymail-ModelArchitecture"
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text."
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bartlargesizedmodelfinetunedoncnndailymail-IntendedUseCase"
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use this model for text summarization."
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail",
    "p": "mcro:hasCitation",
    "o": "mcro:bartlargesizedmodelfinetunedoncnndailymail-Citation"
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bartlargesizedmodelfinetunedoncnndailymail-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:resnet18a1in1k-ModelDetail"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:resnet18a1in1k-ModelArchitecture"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "ReLU activations"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "single layer 7x7 convolution with pooling"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "1x1 convolution shortcut downsample"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:resnet18a1in1k-Citation1"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation1",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:resnet18a1in1k-Citation2"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}"
  },
  {
    "s": "mcro:resnet18a1in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:resnet18a1in1k-Citation3"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-Citation3",
    "p": "prov:hasTextValue",
    "o": "@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:resnet18a1in1k-UseCase"
  },
  {
    "s": "mcro:resnet18a1in1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k",
    "p": "mcro:hasDataset",
    "o": "mcro:resnet18a1in1k-Dataset"
  },
  {
    "s": "mcro:resnet18a1in1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:resnet18a1in1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:distilgpt2-ModelDetail"
  },
  {
    "s": "mcro:distilgpt2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:distilgpt2-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:distilgpt2-License"
  },
  {
    "s": "mcro:distilgpt2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilgpt2-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:distilgpt2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:distilgpt2-Citation"
  },
  {
    "s": "mcro:distilgpt2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:distilgpt2-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:distilgpt2-Architecture"
  },
  {
    "s": "mcro:distilgpt2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasUseCase",
    "o": "mcro:distilgpt2-UseCase"
  },
  {
    "s": "mcro:distilgpt2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilgpt2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:distilgpt2-TrainingData"
  },
  {
    "s": "mcro:distilgpt2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:openaiclip",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:openaiclip",
    "p": "mcro:hasModelDetail",
    "o": "mcro:openaiclip-ModelDetail"
  },
  {
    "s": "mcro:openaiclip-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:openaiclip-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:openaiclip-Citation"
  },
  {
    "s": "mcro:openaiclip-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:openaiclip-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:openaiclip-ModelArchitecture"
  },
  {
    "s": "mcro:openaiclip-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:openaiclip",
    "p": "mcro:hasUseCase",
    "o": "mcro:openaiclip-UseCase"
  },
  {
    "s": "mcro:openaiclip-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:openaiclip-UseCase",
    "p": "mcro:hasPrimaryIntendedUseCase",
    "o": "mcro:openaiclip-PrimaryIntendedUseCase"
  },
  {
    "s": "mcro:openaiclip-PrimaryIntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:openaiclip-UseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:openaiclip-OutOfScopeUseCase"
  },
  {
    "s": "mcro:openaiclip-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:openaiclip",
    "p": "mcro:hasDataset",
    "o": "mcro:openaiclip-Dataset"
  },
  {
    "s": "mcro:openaiclip-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:openaiclip-Dataset",
    "p": "mcro:hasTrainingData",
    "o": "mcro:openaiclip-TrainingData"
  },
  {
    "s": "mcro:openaiclip-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:openaiclip",
    "p": "mcro:hasConsideration",
    "o": "mcro:openaiclip-Consideration"
  },
  {
    "s": "mcro:openaiclip-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:openaiclip",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:openaiclip-QuantativeAnalysis"
  },
  {
    "s": "mcro:openaiclip-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:openaiclip-QuantativeAnalysis",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:openaiclip-PerformanceMetric"
  },
  {
    "s": "mcro:openaiclip-PerformanceMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:openaiclip",
    "p": "mcro:hasLimitation",
    "o": "mcro:openaiclip-Limitation"
  },
  {
    "s": "mcro:openaiclip-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:openaiclip-Consideration",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:openaiclip-EthicalConsideration"
  },
  {
    "s": "mcro:openaiclip-EthicalConsideration",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronosboltsmall-ModelArchitecture"
  },
  {
    "s": "mcro:chronosboltsmall-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "T5 encoder-decoder architecture"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasCitation",
    "o": "mcro:chronosboltsmall-Citation"
  },
  {
    "s": "mcro:chronosboltsmall-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasLicense",
    "o": "mcro:chronosboltsmall-License"
  },
  {
    "s": "mcro:chronosboltsmall-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-License",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0 License"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronosboltsmall-UseCase"
  },
  {
    "s": "mcro:chronosboltsmall-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-UseCase",
    "p": "prov:hasTextValue",
    "o": "zero-shot forecasting"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:QwenQwen25VL3BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "* Dynamic Resolution and Frame Rate Training for Video Understanding"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "* Streamlined and Efficient Vision Encoder"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:QwenQwen25VL3BInstruct-UseCase"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "visual agent"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "reasoning"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "tool use"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "computer use"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "phone use"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:QwenQwen25VL3BInstruct-Citation"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5-VL,"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:QwenQwen25VL3BInstruct-Citation2"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-Citation2",
    "p": "prov:hasTextValue",
    "o": "@article{Qwen2VL,"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:QwenQwen25VL3BInstruct-Citation3"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-Citation3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen25VL3BInstruct-Citation3",
    "p": "prov:hasTextValue",
    "o": "@article{Qwen-VL,"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bartlargemnli-ModelDetail"
  },
  {
    "s": "mcro:bartlargemnli-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bartlargemnli-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bartlargemnli-Citation"
  },
  {
    "s": "mcro:bartlargemnli-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bartlargemnli-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bartlargemnli-ModelArchitecture"
  },
  {
    "s": "mcro:bartlargemnli-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bartlargemnli",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bartlargemnli-UseCase"
  },
  {
    "s": "mcro:bartlargemnli-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase",
    "p": "mcro:hasDataset",
    "o": "mcro:Salesforceblipimagecaptioningbase-Dataset"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-Dataset",
    "p": "prov:hasTextValue",
    "o": "image captioning pretrained on COCO dataset"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Salesforceblipimagecaptioningbase-ModelArchitecture"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "base architecture (with ViT base backbone)"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:Salesforceblipimagecaptioningbase-UseCase"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use this model for conditional and un-conditional image captioning"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase",
    "p": "mcro:hasConsideration",
    "o": "mcro:Salesforceblipimagecaptioningbase-Consideration"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-Consideration",
    "p": "prov:hasTextValue",
    "o": "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people\u2019s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP."
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase",
    "p": "mcro:hasCitation",
    "o": "mcro:Salesforceblipimagecaptioningbase-Citation"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioningbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasemultilingualmpnetbasev2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:multilinguale5large",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:multilinguale5large",
    "p": "mcro:hasCitation",
    "o": "mcro:multilinguale5large-Citation"
  },
  {
    "s": "mcro:multilinguale5large-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:multilinguale5large-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}"
  },
  {
    "s": "mcro:multilinguale5large",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:multilinguale5large-ModelArchitecture"
  },
  {
    "s": "mcro:multilinguale5large-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:multilinguale5large-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "This model has 24 layers and the embedding size is 1024."
  },
  {
    "s": "mcro:multilinguale5large",
    "p": "mcro:hasUseCase",
    "o": "mcro:multilinguale5large-UseCase"
  },
  {
    "s": "mcro:multilinguale5large-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:multilinguale5large-UseCase",
    "p": "prov:hasTextValue",
    "o": "Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: \u5357\u74dc\u7684\u5bb6\u5e38\u505a\u6cd5',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: 1.\u6e05\u7092\u5357\u74dc\u4e1d \u539f\u6599:\u5ae9\u5357\u74dc\u534a\u4e2a \u8c03\u6599:\u8471\u3001\u76d0\u3001\u767d\u7cd6\u3001\u9e21\u7cbe \u505a\u6cd5: 1\u3001\u5357\u74dc\u7528\u5200\u8584\u8584\u7684\u524a\u53bb\u8868\u9762\u4e00\u5c42\u76ae,\u7528\u52fa\u5b50\u522e\u53bb\u74e4 2\u3001\u64e6\u6210\u7ec6\u4e1d(\u6ca1\u6709\u64e6\u83dc\u677f\u5c31\u7528\u5200\u6162\u6162\u5207\u6210\u7ec6\u4e1d) 3\u3001\u9505\u70e7\u70ed\u653e\u6cb9,\u5165\u8471\u82b1\u7178\u51fa\u9999\u5473 4\u3001\u5165\u5357\u74dc\u4e1d\u5feb\u901f\u7ffb\u7092\u4e00\u5206\u949f\u5de6\u53f3,\u653e\u76d0\u3001\u4e00\u70b9\u767d\u7cd6\u548c\u9e21\u7cbe\u8c03\u5473\u51fa\u9505 2.\u9999\u8471\u7092\u5357\u74dc \u539f\u6599:\u5357\u74dc1\u53ea \u8c03\u6599:\u9999\u8471\u3001\u849c\u672b\u3001\u6a44\u6984\u6cb9\u3001\u76d0 \u505a\u6cd5: 1\u3001\u5c06\u5357\u74dc\u53bb\u76ae,\u5207\u6210\u7247 2\u3001\u6cb9\u95058\u6210\u70ed\u540e,\u5c06\u849c\u672b\u653e\u5165\u7206\u9999 3\u3001\u7206\u9999\u540e,\u5c06\u5357\u74dc\u7247\u653e\u5165,\u7ffb\u7092 4\u3001\u5728\u7ffb\u7092\u7684\u540c\u65f6,\u53ef\u4ee5\u4e0d\u65f6\u5730\u5f80\u9505\u91cc\u52a0\u6c34,\u4f46\u4e0d\u8981\u592a\u591a 5\u3001\u653e\u5165\u76d0,\u7092\u5300 6\u3001\u5357\u74dc\u5dee\u4e0d\u591a\u8f6f\u548c\u7ef5\u4e86\u4e4b\u540e,\u5c31\u53ef\u4ee5\u5173\u706b 7\u3001\u6492\u5165\u9999\u8471,\u5373\u53ef\u51fa\u9505\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\nmodel = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n"
  },
  {
    "s": "mcro:multilinguale5large",
    "p": "mcro:hasDataset",
    "o": "mcro:multilinguale5large-Dataset"
  },
  {
    "s": "mcro:multilinguale5large-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:multilinguale5large-Dataset",
    "p": "prov:hasTextValue",
    "o": "This model is initialized from [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)\nand continually trained on a mixture of multilingual datasets.\nIt supports 100 languages from xlm-roberta,\nbut low-resource languages may see performance degradation."
  },
  {
    "s": "mcro:multilinguale5large",
    "p": "mcro:hasDataset",
    "o": "mcro:multilinguale5large-TrainingDetails"
  },
  {
    "s": "mcro:multilinguale5large-TrainingDetails",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:multilinguale5large-TrainingDetails",
    "p": "prov:hasTextValue",
    "o": "**Initialization**: [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)\n\n**First stage**: contrastive pre-training with weak supervision\n\n| Dataset                                                                                                | Weak supervision                      | # of text pairs |\n|--------------------------------------------------------------------------------------------------------|---------------------------------------|-----------------|\n| Filtered [mC4](https://huggingface.co/datasets/mc4)                                                    | (title, page content)                 | 1B              |\n| [CC News](https://huggingface.co/datasets/intfloat/multilingual_cc_news)                               | (title, news content)                 | 400M            |\n| [NLLB](https://huggingface.co/datasets/allenai/nllb)                                                   | translation pairs                     | 2.4B            |\n| [Wikipedia](https://huggingface.co/datasets/intfloat/wikipedia)                                        | (hierarchical section title, passage) | 150M            |\n| Filtered [Reddit](https://www.reddit.com/)                                                             | (comment, response)                   | 800M            |\n| [S2ORC](https://github.com/allenai/s2orc)                                                              | (title, abstract) and citation pairs  | 100M            |\n| [Stackexchange](https://stackexchange.com/)                                                            | (question, answer)                    | 50M             |\n| [xP3](https://huggingface.co/datasets/bigscience/xP3)                                                  | (input prompt, response)              | 80M             |\n| [Miscellaneous unsupervised SBERT data](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | -                                     | 10M             |\n\n**Second stage**: supervised fine-tuning\n\n| Dataset                                                                                | Language     | # of text pairs |\n|----------------------------------------------------------------------------------------|--------------|-----------------|\n| [MS MARCO](https://microsoft.github.io/msmarco/)                                       | English      | 500k            |\n| [NQ](https://github.com/facebookresearch/DPR)                                          | English      | 70k             |\n| [Trivia QA](https://github.com/facebookresearch/DPR)                                   | English      | 60k             |\n| [NLI from SimCSE](https://github.com/princeton-nlp/SimCSE)                             | English      | <300k           |\n| [ELI5](https://huggingface.co/datasets/eli5)                                           | English      | 500k            |\n| [DuReader Retrieval](https://github.com/baidu/DuReader/tree/master/DuReader-Retrieval) | Chinese      | 86k             |\n| [KILT Fever](https://huggingface.co/datasets/kilt_tasks)                               | English      | 70k             |\n| [KILT HotpotQA](https://huggingface.co/datasets/kilt_tasks)                            | English      | 70k             |\n| [SQuAD](https://huggingface.co/datasets/squad)                                         | English      | 87k             |\n| [Quora](https://huggingface.co/datasets/quora)                                         | English      | 150k            |\n| [Mr. TyDi](https://huggingface.co/datasets/castorini/mr-tydi)                                                                           | 11 languages | 50k             |\n| [MIRACL](https://huggingface.co/datasets/miracl/miracl)                                                                             | 16 languages | 40k             |\n\nFor all labeled datasets, we only use its training set for fine-tuning.\n\nFor other training details, please refer to our paper at [https://arxiv.org/pdf/2402.05672](https://arxiv.org/pdf/2402.05672)."
  },
  {
    "s": "mcro:multilinguale5large",
    "p": "mcro:hasLimitation",
    "o": "mcro:multilinguale5large-Limitation"
  },
  {
    "s": "mcro:multilinguale5large-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:multilinguale5large-Limitation",
    "p": "prov:hasTextValue",
    "o": "Long texts will be truncated to at most 512 tokens."
  },
  {
    "s": "mcro:layoutlmbaseuncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:layoutlmbaseuncased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:layoutlmbaseuncased-ModelDetail"
  },
  {
    "s": "mcro:layoutlmbaseuncased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:layoutlmbaseuncased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:layoutlmbaseuncased-Citation"
  },
  {
    "s": "mcro:layoutlmbaseuncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:layoutlmbaseuncased-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{xu2019layoutlm,\n    title={LayoutLM: Pre-training of Text and Layout for Document Image Understanding},\n    author={Yiheng Xu and Minghao Li and Lei Cui and Shaohan Huang and Furu Wei and Ming Zhou},\n    year={2019},\n    eprint={1912.13318},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:layoutlmbaseuncased-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:layoutlmbaseuncased-ModelArchitecture"
  },
  {
    "s": "mcro:layoutlmbaseuncased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:layoutlmbaseuncased-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "12-layer, 768-hidden, 12-heads, 113M parameters"
  },
  {
    "s": "mcro:layoutlmbaseuncased",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:layoutlmbaseuncased-UseCase"
  },
  {
    "s": "mcro:layoutlmbaseuncased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:layoutlmbaseuncased-UseCase",
    "p": "prov:hasTextValue",
    "o": "document image understanding and information extraction tasks, such as form understanding and receipt understanding"
  },
  {
    "s": "mcro:layoutlmbaseuncased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:layoutlmbaseuncased-TrainingData"
  },
  {
    "s": "mcro:layoutlmbaseuncased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:layoutlmbaseuncased-TrainingData",
    "p": "prov:hasTextValue",
    "o": "IIT-CDIP Test Collection 1.0"
  },
  {
    "s": "mcro:intfloatmultilinguale5small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:intfloatmultilinguale5small",
    "p": "mcro:hasCitation",
    "o": "mcro:intfloatmultilinguale5small-Citation"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-Citation",
    "p": "prov:hasTextValue",
    "o": "Multilingual E5 Text Embeddings: A Technical Report"
  },
  {
    "s": "mcro:intfloatmultilinguale5small",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:intfloatmultilinguale5small-Architecture"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-Architecture",
    "p": "prov:hasTextValue",
    "o": "This model has 12 layers and the embedding size is 384."
  },
  {
    "s": "mcro:intfloatmultilinguale5small",
    "p": "mcro:hasUseCase",
    "o": "mcro:intfloatmultilinguale5small-UseCase"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-UseCase",
    "p": "prov:hasTextValue",
    "o": "Use \"query: \" prefix for symmetric tasks such as semantic similarity, bitext mining, paraphrase retrieval."
  },
  {
    "s": "mcro:intfloatmultilinguale5small",
    "p": "mcro:hasTrainingData",
    "o": "mcro:intfloatmultilinguale5small-TrainingData"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-TrainingData",
    "p": "prov:hasTextValue",
    "o": "xP3"
  },
  {
    "s": "mcro:intfloatmultilinguale5small",
    "p": "mcro:hasLimitation",
    "o": "mcro:intfloatmultilinguale5small-Limitation"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5small-Limitation",
    "p": "prov:hasTextValue",
    "o": "Long texts will be truncated to at most 512 tokens."
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasModelDetail",
    "o": "mcro:stablediffusioninpaintingmodelcard-ModelDetailSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:stablediffusioninpaintingmodelcard-LicenseInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The CreativeML OpenRAIL M license"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:stablediffusioninpaintingmodelcard-CitationInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasUseCase",
    "o": "mcro:stablediffusioninpaintingmodelcard-UseCaseInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is intended for research purposes only."
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:stablediffusioninpaintingmodelcard-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generation model"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard",
    "p": "mcro:hasDataset",
    "o": "mcro:stablediffusioninpaintingmodelcard-DatasetInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:stablediffusioninpaintingmodelcard-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "LAION-2B (en) and subsets thereof"
  },
  {
    "s": "mcro:distilbartcnn126",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilbartcnn126",
    "p": "mcro:hasModelParameter",
    "o": "mcro:distilbartcnn126-ModelParameterSection"
  },
  {
    "s": "mcro:distilbartcnn126-ModelParameterSection",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:distilbartcnn126-ModelParameterSection",
    "p": "prov:hasTextValue",
    "o": "This checkpoint should be loaded into `BartForConditionalGeneration.from_pretrained`. See the [BART docs](https://huggingface.co/transformers/model_doc/bart.html?#transformers.BartForConditionalGeneration) for more information."
  },
  {
    "s": "mcro:distilbartcnn126",
    "p": "mcro:hasQuantitativeAnalysis",
    "o": "mcro:distilbartcnn126-QuantitativeAnalysisSection"
  },
  {
    "s": "mcro:distilbartcnn126-QuantitativeAnalysisSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:distilbartcnn126-QuantitativeAnalysisSection",
    "p": "prov:hasTextValue",
    "o": "| Model Name                 |   MM Params |   Inference Time (MS) |   Speedup |   Rouge 2 |   Rouge-L |\n|:---------------------------|------------:|----------------------:|----------:|----------:|----------:|\n| distilbart-xsum-12-1       |         222 |                    90 |      2.54 |     18.31 |     33.37 |\n| distilbart-xsum-6-6        |         230 |                   132 |      1.73 |     20.92 |     35.73 |\n| distilbart-xsum-12-3       |         255 |                   106 |      2.16 |     21.37 |     36.39 |\n| distilbart-xsum-9-6        |         268 |                   136 |      1.68 |     21.72 |     36.61 |\n| bart-large-xsum (baseline) |         406 |                   229 |      1    |     21.85 |     36.50 |\n| distilbart-xsum-12-6       |         306 |                   137 |      1.68 |     22.12 |     36.99 |\n| bart-large-cnn (baseline)  |         406 |                   381 |      1    |     21.06 |     30.63 |\n| distilbart-12-3-cnn        |         255 |                   214 |      1.78 |     20.57 |     30.00 |\n| distilbart-12-6-cnn        |         306 |                   307 |      1.24 |     21.26 |     30.59 |\n| distilbart-6-6-cnn         |         230 |                   182 |      2.09 |     20.17 |     29.70 |"
  },
  {
    "s": "mcro:t5base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:t5base-ModelDetail"
  },
  {
    "s": "mcro:t5base-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:t5base-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:t5base-License"
  },
  {
    "s": "mcro:t5base-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:t5base-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:t5base-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:t5base-ModelArchitecture"
  },
  {
    "s": "mcro:t5base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:t5base-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Language model"
  },
  {
    "s": "mcro:t5base-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:t5base-Citation"
  },
  {
    "s": "mcro:t5base-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:t5base-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "T5-Base is the checkpoint with 220 million parameters."
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasUseCase",
    "o": "mcro:t5base-UseCase"
  },
  {
    "s": "mcro:t5base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:t5base-UseCase",
    "p": "prov:hasTextValue",
    "o": "Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself."
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasTrainingData",
    "o": "mcro:t5base-TrainingData"
  },
  {
    "s": "mcro:t5base-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:t5base-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The model is pre-trained on the [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), which was developed and released in the context of the same [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) as T5."
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasConsideration",
    "o": "mcro:t5base-Consideration"
  },
  {
    "s": "mcro:t5base-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:t5base-Consideration",
    "p": "prov:hasTextValue",
    "o": "More information needed."
  },
  {
    "s": "mcro:t5base",
    "p": "mcro:hasReference",
    "o": "mcro:t5base-Reference"
  },
  {
    "s": "mcro:t5base-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:t5base-Reference",
    "p": "prov:hasTextValue",
    "o": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-japanese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {J}apanese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Japanese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [JSUT](https://sites.google.com/site/shinnosuketakamichi/publication/jsut)."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53japanese-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Japanese"
  },
  {
    "s": "mcro:vit-age-classifier",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vit-age-classifier",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vit-age-classifier-ModelArchitecture"
  },
  {
    "s": "mcro:vit-age-classifier-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vit-age-classifier-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "A vision transformer finetuned to classify the age of a given person's face."
  },
  {
    "s": "mcro:t5small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:t5small",
    "p": "mcro:hasModelDetail",
    "o": "mcro:t5small-ModelDetail"
  },
  {
    "s": "mcro:t5small-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:t5small-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:t5small-License"
  },
  {
    "s": "mcro:t5small-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:t5small-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:t5small-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:t5small-Citation"
  },
  {
    "s": "mcro:t5small-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:t5small-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:t5small-Architecture"
  },
  {
    "s": "mcro:t5small-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:t5small-Architecture",
    "p": "prov:hasTextValue",
    "o": "Language model"
  },
  {
    "s": "mcro:t5small",
    "p": "mcro:hasUseCase",
    "o": "mcro:t5small-UseCase"
  },
  {
    "s": "mcro:t5small-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:t5small",
    "p": "mcro:hasTrainingData",
    "o": "mcro:t5small-TrainingData"
  },
  {
    "s": "mcro:t5small-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:t5small",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:t5small-EvaluationData"
  },
  {
    "s": "mcro:t5small-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:albertbasev2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasArchitecture",
    "o": "mcro:albertbasev2-Architecture"
  },
  {
    "s": "mcro:albertbasev2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:albertbasev2-Architecture",
    "p": "prov:hasTextValue",
    "o": "ALBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion."
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:albertbasev2-IntendedUseCase"
  },
  {
    "s": "mcro:albertbasev2-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:albertbasev2-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task."
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasLimitation",
    "o": "mcro:albertbasev2-Limitation"
  },
  {
    "s": "mcro:albertbasev2-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:albertbasev2-Limitation",
    "p": "prov:hasTextValue",
    "o": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions"
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:albertbasev2-TrainingData"
  },
  {
    "s": "mcro:albertbasev2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:albertbasev2-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The ALBERT model was pretrained on BookCorpus, a dataset consisting of 11,038\nunpublished books and English Wikipedia (excluding lists, tables and\nheaders)."
  },
  {
    "s": "mcro:albertbasev2",
    "p": "mcro:hasCitation",
    "o": "mcro:albertbasev2-Citation"
  },
  {
    "s": "mcro:albertbasev2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:albertbasev2-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-1909-11942,\n  author    = {Zhenzhong Lan and\n               Mingda Chen and\n               Sebastian Goodman and\n               Kevin Gimpel and\n               Piyush Sharma and\n               Radu Soricut},\n  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language\n               Representations},\n  journal   = {CoRR},\n  volume    = {abs/1909.11942},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.11942},\n  archivePrefix = {arXiv},\n  eprint    = {1909.11942},\n  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:sdxl10base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sdxl10base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:sdxl10base-ModelDetail"
  },
  {
    "s": "mcro:sdxl10base-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:sdxl10base",
    "p": "mcro:hasUseCase",
    "o": "mcro:sdxl10base-UseCase"
  },
  {
    "s": "mcro:sdxl10base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sdxl10base",
    "p": "mcro:hasLimitation",
    "o": "mcro:sdxl10base-Limitation"
  },
  {
    "s": "mcro:sdxl10base-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:sdxl10base-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:sdxl10base-License"
  },
  {
    "s": "mcro:sdxl10base-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:sdxl10base-License",
    "p": "prov:hasTextValue",
    "o": "CreativeML Open RAIL++-M License"
  },
  {
    "s": "mcro:sdxl10base-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sdxl10base-ModelArchitecture"
  },
  {
    "s": "mcro:sdxl10base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sdxl10base-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generative model"
  },
  {
    "s": "mcro:sdxl10base-UseCase",
    "p": "prov:hasTextValue",
    "o": "research purposes only"
  },
  {
    "s": "mcro:sdxl10base-Limitation",
    "p": "prov:hasTextValue",
    "o": "- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to \u201cA red cube on top of a blue sphere\u201d\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy."
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:QwenQwen25VL7BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "* Dynamic Resolution and Frame Rate Training for Video Understanding"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:QwenQwen25VL7BInstruct-Citation"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5-VL,"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:QwenQwen25VL7BInstruct-UseCase"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:QwenQwen25VL7BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images."
  },
  {
    "s": "mcro:owlv2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:owlv2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:owlv2-ModelDetail"
  },
  {
    "s": "mcro:owlv2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:owlv2-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "The OWLv2 model (short for Open-World Localization) was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.\n\nThe model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."
  },
  {
    "s": "mcro:owlv2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:owlv2-Citation"
  },
  {
    "s": "mcro:owlv2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:owlv2-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:owlv2-Architecture"
  },
  {
    "s": "mcro:owlv2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:owlv2-Architecture",
    "p": "prov:hasTextValue",
    "o": "The model uses a CLIP backbone with a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective."
  },
  {
    "s": "mcro:owlv2",
    "p": "mcro:hasUseCase",
    "o": "mcro:owlv2-UseCase"
  },
  {
    "s": "mcro:owlv2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:owlv2-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training."
  },
  {
    "s": "mcro:owlv2-UseCase",
    "p": "mcro:hasPrimaryIntendedUse",
    "o": "mcro:owlv2-PrimaryUse"
  },
  {
    "s": "mcro:owlv2-PrimaryUse",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:owlv2-PrimaryUse",
    "p": "prov:hasTextValue",
    "o": "The primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
  },
  {
    "s": "mcro:owlv2",
    "p": "mcro:hasDataset",
    "o": "mcro:owlv2-Dataset"
  },
  {
    "s": "mcro:owlv2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:owlv2-Dataset",
    "p": "prov:hasTextValue",
    "o": "The CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html).\n\n(to be updated for v2)"
  },
  {
    "s": "mcro:owlv2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:owlv2-Citation2"
  },
  {
    "s": "mcro:owlv2-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:owlv2-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{minderer2023scaling,\n      title={Scaling Open-Vocabulary Object Detection}, \n      author={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},\n      year={2023},\n      eprint={2306.09683},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:petalsteamStableBeluga2-ModelDetail"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "`Stable Beluga 2` is a Llama2 70B model finetuned on an Orca style Dataset"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasUseCase",
    "o": "mcro:petalsteamStableBeluga2-UseCase"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Start chatting with `Stable Beluga 2` using the following code snippet:"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasCitation",
    "o": "mcro:petalsteamStableBeluga2-Citation"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{StableBelugaModels,\n      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)},\n      title={Stable Beluga models},\n      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}\n}"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasCitation",
    "o": "mcro:petalsteamStableBeluga2-Citation2"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{touvron2023llama,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},\n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasCitation",
    "o": "mcro:petalsteamStableBeluga2-Citation3"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Citation3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Citation3",
    "p": "prov:hasTextValue",
    "o": "@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4},\n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:petalsteamStableBeluga2-ModelDetail2"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-ModelDetail2",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-ModelDetail2",
    "p": "prov:hasTextValue",
    "o": "* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.\n* **Language(s)**: English\n* **Library**: [HuggingFace Transformers](https://github.com/huggingface/transformers)\n* **License**: Fine-tuned checkpoints (`Stable Beluga 2`) is licensed under the [STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt)\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasDataset",
    "o": "mcro:petalsteamStableBeluga2-Dataset"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Dataset",
    "p": "prov:hasTextValue",
    "o": "` Stable Beluga 2` is trained on our internal Orca-style dataset"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasModelParameter",
    "o": "mcro:petalsteamStableBeluga2-ModelParameter"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-ModelParameter",
    "p": "prov:hasTextValue",
    "o": "Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:\n\n| Dataset           | Batch Size | Learning Rate |Learning Rate Decay| Warm-up | Weight Decay | Betas       |\n|-------------------|------------|---------------|-------------------|---------|--------------|-------------|\n| Orca pt1 packed   | 256        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n| Orca pt2 unpacked | 512        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |"
  },
  {
    "s": "mcro:petalsteamStableBeluga2",
    "p": "mcro:hasConsideration",
    "o": "mcro:petalsteamStableBeluga2-Consideration"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:petalsteamStableBeluga2-Consideration",
    "p": "prov:hasTextValue",
    "o": "Beluga is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Beluga's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model."
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasLicense",
    "o": "mcro:FlagEmbedding-License"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-License",
    "p": "prov:hasTextValue",
    "o": "FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge."
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasCitation",
    "o": "mcro:FlagEmbedding-Citation"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:FlagEmbedding-ModelArchitecture"
  },
  {
    "s": "mcro:FlagEmbedding-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning."
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasUseCase",
    "o": "mcro:FlagEmbedding-UseCase"
  },
  {
    "s": "mcro:FlagEmbedding-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-UseCase",
    "p": "prov:hasTextValue",
    "o": "FlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Dense Retrieval**: [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasDataset",
    "o": "mcro:FlagEmbedding-Dataset"
  },
  {
    "s": "mcro:FlagEmbedding-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-Dataset",
    "p": "prov:hasTextValue",
    "o": "We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning."
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertmultilingualbasemodeluncased-ModelDetail"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertmultilingualbasemodeluncased-Citation"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:bertmultilingualbasemodeluncased-License"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertmultilingualbasemodeluncased-ModelArchitecture"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertmultilingualbasemodeluncased-UseCase"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasDataset",
    "o": "mcro:bertmultilingualbasemodeluncased-Dataset"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertmultilingualbasemodeluncased-TrainingData"
  },
  {
    "s": "mcro:bertmultilingualbasemodeluncased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3",
    "p": "mcro:hasModelDetail",
    "o": "mcro:deepseekaiDeepSeekV3-ModelDetail"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekV3-License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-License",
    "p": "prov:hasTextValue",
    "o": "This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use."
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekV3-Citation"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:deepseekaiDeepSeekV3-Architecture"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-Architecture",
    "p": "prov:hasTextValue",
    "o": "Innovative Load Balancing Strategy and Training Objective\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration."
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3",
    "p": "mcro:hasDataset",
    "o": "mcro:deepseekaiDeepSeekV3-Dataset"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-Dataset",
    "p": "prov:hasTextValue",
    "o": "Pre-Training: Towards Ultimate Training Efficiency\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours."
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3",
    "p": "mcro:hasDataset",
    "o": "mcro:deepseekaiDeepSeekV3-Dataset2"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-Dataset2",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV3-Dataset2",
    "p": "prov:hasTextValue",
    "o": "Post-Training: Knowledge Distillation from DeepSeek-R1\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3."
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:visiontransformerbasesizedmodel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224."
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbasesizedmodel-CitationInformationSection1"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-CitationInformationSection1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-CitationInformationSection1",
    "p": "prov:hasTextValue",
    "o": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:visiontransformerbasesizedmodel-UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for image classification."
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:visiontransformerbasesizedmodel-TrainingDataInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "The ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes."
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbasesizedmodel-CitationInformationSection2"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-CitationInformationSection2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-CitationInformationSection2",
    "p": "prov:hasTextValue",
    "o": "@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbasesizedmodel-CitationInformationSection3"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-CitationInformationSection3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-CitationInformationSection3",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:intfloatmultilinguale5largeinstruct-Citation"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "[Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/pdf/2402.05672).\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv 2024"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:intfloatmultilinguale5largeinstruct-Architecture"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-Architecture",
    "p": "prov:hasTextValue",
    "o": "This model has 24 layers and the embedding size is 1024."
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:intfloatmultilinguale5largeinstruct-UseCase"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Below are examples to encode queries and passages from the MS-MARCO passage ranking dataset."
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct",
    "p": "mcro:hasTrainingData",
    "o": "mcro:intfloatmultilinguale5largeinstruct-TrainingData"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-TrainingData",
    "p": "prov:hasTextValue",
    "o": "**Initialization**: [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)\n\n**First stage**: contrastive pre-training with 1 billion weakly supervised text pairs.\n\n**Second stage**: fine-tuning on datasets from the [E5-mistral](https://arxiv.org/abs/2401.00368) paper."
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct",
    "p": "mcro:hasLimitation",
    "o": "mcro:intfloatmultilinguale5largeinstruct-Limitation"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5largeinstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Long texts will be truncated to at most 512 tokens."
  },
  {
    "s": "mcro:whisper-baseen",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisper-baseen",
    "p": "mcro:hasModelDetail",
    "o": "mcro:whisper-baseen-ModelDetail"
  },
  {
    "s": "mcro:whisper-baseen-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:whisper-baseen-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisper-baseen-ModelArchitecture"
  },
  {
    "s": "mcro:whisper-baseen-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisper-baseen-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer based encoder-decoder model"
  },
  {
    "s": "mcro:whisper-baseen-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:whisper-baseen-Citation"
  },
  {
    "s": "mcro:whisper-baseen-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisper-baseen-Citation",
    "p": "prov:hasTextValue",
    "o": "Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "s": "mcro:whisper-baseen-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:whisper-baseen-License"
  },
  {
    "s": "mcro:whisper-baseen-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:whisper-baseen",
    "p": "mcro:hasTrainingData",
    "o": "mcro:whisper-baseen-TrainingData"
  },
  {
    "s": "mcro:whisper-baseen-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:whisper-baseen-TrainingData",
    "p": "prov:hasTextValue",
    "o": "680,000 hours of audio"
  },
  {
    "s": "mcro:whisper-baseen",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:whisper-baseen-IntendedUseCase"
  },
  {
    "s": "mcro:whisper-baseen-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisper-baseen-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "English speech recognition"
  },
  {
    "s": "mcro:whisper-baseen",
    "p": "mcro:hasPerformanceLimitations",
    "o": "mcro:whisper-baseen-PerformanceLimitations"
  },
  {
    "s": "mcro:whisper-baseen-PerformanceLimitations",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:whisper-baseen-PerformanceLimitations",
    "p": "prov:hasTextValue",
    "o": "hallucination"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-Architecture"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-Architecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms",
    "p": "mcro:hasLicense",
    "o": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-License"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-License",
    "p": "prov:hasTextValue",
    "o": "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)."
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms",
    "p": "mcro:hasUseCase",
    "o": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-UseCase"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources."
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms",
    "p": "mcro:hasTrainingData",
    "o": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-TrainingData"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLargeLanguageModelsLlms-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)."
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasCitation",
    "o": "mcro:Salesforceblipimagecaptioninglarge-Citation"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasDataset",
    "o": "mcro:Salesforceblipimagecaptioninglarge-Dataset"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Dataset",
    "p": "prov:hasTextValue",
    "o": "COCO dataset"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Salesforceblipimagecaptioninglarge-ModelArchitecture"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "ViT large backbone"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasUseCase",
    "o": "mcro:Salesforceblipimagecaptioninglarge-UseCase"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-UseCase",
    "p": "prov:hasTextValue",
    "o": "image captioning"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge",
    "p": "mcro:hasConsideration",
    "o": "mcro:Salesforceblipimagecaptioninglarge-Consideration"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipimagecaptioninglarge-Consideration",
    "p": "prov:hasTextValue",
    "o": "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people\u2019s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP."
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasLicense",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-LicenseInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasCitation",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-CitationInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1",
    "p": "mcro:hasUseCase",
    "o": "mcro:mixedbreadaimxbaiembedlargev1-UseCaseInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@online{emb2024mxbai,\n  title={Open Source Strikes Bread - New Fluffy Embeddings Model},\n  author={Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-embed-large-v1},\n}\n\n@article{li2023angle,\n  title={AnglE-optimized Text Embeddings},\n  author={Li, Xianming and Li, Jing},\n  journal={arXiv preprint arXiv:2309.12871},\n  year={2023}\n}"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Here, we provide several ways to produce sentence embeddings. Please note that you have to provide the prompt `Represent this sentence for searching relevant passages:` for query if you want to use it for retrieval. Besides that you don't need any prompt. Our model also supports [Matryoshka Representation Learning and binary quantization](https://www.mixedbread.ai/blog/binary-mrl)."
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mixedbreadaimxbaiembedlargev1-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Here, we provide several ways to produce sentence embeddings. Please note that you have to provide the prompt `Represent this sentence for searching relevant passages: ` for query if you want to use it for retrieval. Besides that you don't need any prompt."
  },
  {
    "s": "mcro:Qwen253BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen253BInstruct",
    "p": "mcro:hasArchitecture",
    "o": "mcro:Qwen253BInstruct-Architecture"
  },
  {
    "s": "mcro:Qwen253BInstruct-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen253BInstruct-Architecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:Qwen253BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen253BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen253BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen253BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:Qwen253BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen253BInstruct-UseCase"
  },
  {
    "s": "mcro:Qwen253BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen253BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:facebookhubertlargels960ft",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookhubertlargels960ft",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookhubertlargels960ft-ModelDetail"
  },
  {
    "s": "mcro:facebookhubertlargels960ft-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookhubertlargels960ft-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookhubertlargels960ft-Citation"
  },
  {
    "s": "mcro:facebookhubertlargels960ft-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookhubertlargels960ft-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookhubertlargels960ft-ModelArchitecture"
  },
  {
    "s": "mcro:facebookhubertlargels960ft-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookhubertlargels960ft-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookhubertlargels960ft-UseCase"
  },
  {
    "s": "mcro:facebookhubertlargels960ft-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftdebertav3base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftdebertav3base",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftdebertav3base-Citation"
  },
  {
    "s": "mcro:microsoftdebertav3base-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftdebertav3base-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:microsoftdebertav3base",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftdebertav3base-Citation2"
  },
  {
    "s": "mcro:microsoftdebertav3base-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftdebertav3base-Citation2",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}"
  },
  {
    "s": "mcro:microsoftdebertav3base",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftdebertav3base-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftdebertav3base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftdebertav3base-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2."
  },
  {
    "s": "mcro:googlegemma31bit",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:googlegemma31bit",
    "p": "mcro:hasCitation",
    "o": "mcro:googlegemma31bit-Citation"
  },
  {
    "s": "mcro:googlegemma31bit-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:googlegemma31bit-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
  },
  {
    "s": "mcro:googlegemma31bit",
    "p": "mcro:hasDataset",
    "o": "mcro:googlegemma31bit-Dataset"
  },
  {
    "s": "mcro:googlegemma31bit-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:googlegemma31bit-Dataset",
    "p": "prov:hasTextValue",
    "o": "These models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats."
  },
  {
    "s": "mcro:googlegemma31bit",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:googlegemma31bit-Arch"
  },
  {
    "s": "mcro:googlegemma31bit-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:googlegemma31bit-Arch",
    "p": "prov:hasTextValue",
    "o": "Gemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone."
  },
  {
    "s": "mcro:googlegemma31bit",
    "p": "mcro:hasUseCase",
    "o": "mcro:googlegemma31bit-UseCase"
  },
  {
    "s": "mcro:googlegemma31bit-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:googlegemma31bit-UseCase",
    "p": "prov:hasTextValue",
    "o": "Open vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics."
  },
  {
    "s": "mcro:bartbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bartbase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bartbase-ModelDetail"
  },
  {
    "s": "mcro:bartbase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bartbase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bartbase-CitationInformationSection"
  },
  {
    "s": "mcro:bartbase-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bartbase-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:bartbase-LicenseInformationSection"
  },
  {
    "s": "mcro:bartbase-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:bartbase-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bartbase-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bartbase-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bartbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:bartbase-UseCaseInformationSection"
  },
  {
    "s": "mcro:bartbase-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:flan-t5-base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:flan-t5-base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:flan-t5-base-ModelDetail"
  },
  {
    "s": "mcro:flan-t5-base-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:flan-t5-base-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:flan-t5-base-License"
  },
  {
    "s": "mcro:flan-t5-base-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:flan-t5-base-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:flan-t5-base",
    "p": "mcro:hasCitation",
    "o": "mcro:flan-t5-base-Citation"
  },
  {
    "s": "mcro:flan-t5-base-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:flan-t5-base",
    "p": "mcro:hasUseCase",
    "o": "mcro:flan-t5-base-UseCase"
  },
  {
    "s": "mcro:flan-t5-base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:flan-t5-base",
    "p": "mcro:hasTrainingData",
    "o": "mcro:flan-t5-base-TrainingData"
  },
  {
    "s": "mcro:flan-t5-base-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:flan-t5-base",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:flan-t5-base-EvaluationData"
  },
  {
    "s": "mcro:flan-t5-base-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:flan-t5-base",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:flan-t5-base-ModelArchitecture"
  },
  {
    "s": "mcro:flan-t5-base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:flan-t5-base",
    "p": "mcro:hasConsideration",
    "o": "mcro:flan-t5-base-Consideration"
  },
  {
    "s": "mcro:flan-t5-base-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen257BInstruct-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen257BInstruct-CitationInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen257BInstruct-UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:vocos",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vocos",
    "p": "mcro:hasCitation",
    "o": "mcro:vocos-CitationInformationSection"
  },
  {
    "s": "mcro:vocos-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:vocos-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{siuzdak2023vocos,\n  title={Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis},\n  author={Siuzdak, Hubert},\n  journal={arXiv preprint arXiv:2306.00814},\n  year={2023}\n}"
  },
  {
    "s": "mcro:vocos",
    "p": "mcro:hasLicense",
    "o": "mcro:vocos-LicenseInformationSection"
  },
  {
    "s": "mcro:vocos-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:vocos-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The code in this repository is released under the MIT license."
  },
  {
    "s": "mcro:vocos",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vocos-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vocos-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vocos-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Vocos is a fast neural vocoder designed to synthesize audio waveforms from acoustic features. Trained using a Generative\nAdversarial Network (GAN) objective, Vocos can generate waveforms in a single forward pass. Unlike other typical\nGAN-based vocoders, Vocos does not model audio samples in the time domain. Instead, it generates spectral\ncoefficients, facilitating rapid audio reconstruction through inverse Fourier transform."
  },
  {
    "s": "mcro:resnet50v1.5",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:resnet50v1.5",
    "p": "mcro:hasModelDetail",
    "o": "mcro:resnet50v1.5-ModelDetail"
  },
  {
    "s": "mcro:resnet50v1.5-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:resnet50v1.5-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:resnet50v1.5-Citation"
  },
  {
    "s": "mcro:resnet50v1.5-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:resnet50v1.5-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{he2016deep,\n  title={Deep residual learning for image recognition},\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages={770--778},\n  year={2016}\n}"
  },
  {
    "s": "mcro:resnet50v1.5-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:resnet50v1.5-Architecture"
  },
  {
    "s": "mcro:resnet50v1.5-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:resnet50v1.5-Architecture",
    "p": "prov:hasTextValue",
    "o": "ResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections."
  },
  {
    "s": "mcro:resnet50v1.5",
    "p": "mcro:hasUseCase",
    "o": "mcro:resnet50v1.5-UseCase"
  },
  {
    "s": "mcro:resnet50v1.5-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:resnet50v1.5-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for image classification"
  },
  {
    "s": "mcro:variousmodels",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:variousmodels",
    "p": "mcro:hasUseCase",
    "o": "mcro:variousmodels-UseCaseInformationSection"
  },
  {
    "s": "mcro:variousmodels-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:variousmodels-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Do not use it in production."
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT",
    "p": "mcro:hasDataset",
    "o": "mcro:emilyalsentzerBioClinicalBERT-DatasetInfo"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:emilyalsentzerBioClinicalBERT-ModelArch"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT",
    "p": "mcro:hasCitation",
    "o": "mcro:emilyalsentzerBioClinicalBERT-Citation"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT",
    "p": "mcro:hasUseCase",
    "o": "mcro:emilyalsentzerBioClinicalBERT-UseCase"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT-DatasetInfo",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT-DatasetInfo",
    "p": "prov:hasTextValue",
    "o": "The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. For more details on MIMIC, see [here](https://mimic.physionet.org/). All notes from the `NOTEEVENTS` table were included (~880M words)."
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT-ModelArch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT-ModelArch",
    "p": "prov:hasTextValue",
    "o": "initialized from [BioBERT](https://arxiv.org/abs/1901.08746) & trained on all MIMIC notes"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT-Citation",
    "p": "prov:hasTextValue",
    "o": "[Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) (NAACL Clinical NLP Workshop 2019)"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:emilyalsentzerBioClinicalBERT-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model card describes the Bio+Clinical BERT model, which was initialized from [BioBERT](https://arxiv.org/abs/1901.08746) & trained on all MIMIC notes."
  },
  {
    "s": "mcro:we_log_statistics",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:we_log_statistics",
    "p": "prov:hasTextValue",
    "o": "We log statistics to see if any envs are breaking"
  },
  {
    "s": "mcro:upskyybgem3korean",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:upskyybgem3korean",
    "p": "mcro:hasModelDetails",
    "o": "mcro:upskyybgem3korean-ModelDetailSection"
  },
  {
    "s": "mcro:upskyybgem3korean-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:upskyybgem3korean-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:upskyybgem3korean-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:upskyybgem3korean-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:upskyybgem3korean-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Sentence Transformer"
  },
  {
    "s": "mcro:upskyybgem3korean-ModelDetailSection",
    "p": "mcro:hasDataset",
    "o": "mcro:upskyybgem3korean-DatasetInformationSection"
  },
  {
    "s": "mcro:upskyybgem3korean-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:upskyybgem3korean",
    "p": "mcro:hasUseCase",
    "o": "mcro:upskyybgem3korean-UseCaseInformationSection"
  },
  {
    "s": "mcro:upskyybgem3korean-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:upskyybgem3korean",
    "p": "mcro:hasCitation",
    "o": "mcro:upskyybgem3korean-CitationInformationSection"
  },
  {
    "s": "mcro:upskyybgem3korean-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment",
    "p": "mcro:hasModelDetail",
    "o": "mcro:twitterxlmrobertabasesentiment-ModelDetailSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:twitterxlmrobertabasesentiment-CitationInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:twitterxlmrobertabasesentiment-LicenseInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:twitterxlmrobertabasesentiment-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment",
    "p": "mcro:hasUseCase",
    "o": "mcro:twitterxlmrobertabasesentiment-UseCaseInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:twitterxlmrobertabasesentiment-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "sentiment analysis"
  },
  {
    "s": "mcro:Supabasegtesmall",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Supabasegtesmall",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Supabasegtesmall-ModelDetail"
  },
  {
    "s": "mcro:Supabasegtesmall-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Supabasegtesmall-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "General Text Embeddings (GTE) model.\n\nThe GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including [GTE-large](https://huggingface.co/thenlper/gte-large), [GTE-base](https://huggingface.co/thenlper/gte-base), and [GTE-small](https://huggingface.co/thenlper/gte-small). The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including **information retrieval**, **semantic textual similarity**, **text reranking**, etc."
  },
  {
    "s": "mcro:Supabasegtesmall",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Supabasegtesmall-ModelArchitecture"
  },
  {
    "s": "mcro:Supabasegtesmall-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Supabasegtesmall-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT framework"
  },
  {
    "s": "mcro:Supabasegtesmall",
    "p": "mcro:hasUseCase",
    "o": "mcro:Supabasegtesmall-UseCase"
  },
  {
    "s": "mcro:Supabasegtesmall-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Supabasegtesmall-UseCase",
    "p": "prov:hasTextValue",
    "o": "information retrieval"
  },
  {
    "s": "mcro:Supabasegtesmall-UseCase",
    "p": "prov:hasTextValue",
    "o": "semantic textual similarity"
  },
  {
    "s": "mcro:Supabasegtesmall-UseCase",
    "p": "prov:hasTextValue",
    "o": "text reranking"
  },
  {
    "s": "mcro:Supabasegtesmall",
    "p": "mcro:hasLimitation",
    "o": "mcro:Supabasegtesmall-Limitation"
  },
  {
    "s": "mcro:Supabasegtesmall-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:Supabasegtesmall-Limitation",
    "p": "prov:hasTextValue",
    "o": "This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens."
  },
  {
    "s": "mcro:Qwen2505B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2505B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2505B-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen2505B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2505B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:Qwen2505B",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen2505B-Citation"
  },
  {
    "s": "mcro:Qwen2505B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen2505B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:Qwen2505B",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2505B-UseCase"
  },
  {
    "s": "mcro:Qwen2505B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2505B-UseCase",
    "p": "prov:hasTextValue",
    "o": "We do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model."
  },
  {
    "s": "mcro:whisper",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasModelDetail",
    "o": "mcro:whisper-ModelDetail"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasUseCase",
    "o": "mcro:whisper-UseCaseInformationSection"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasDataset",
    "o": "mcro:whisper-DatasetInformationSection"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:whisper-QuantativeAnalysisSection"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:whisper-LicenseInformationSection"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:whisper-CitationInformationSection"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:whisper-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisper-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisper-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:whisper-QuantativeAnalysisSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:whisper-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:whisper-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisper-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}"
  },
  {
    "s": "mcro:whisper-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisper-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Transformer based encoder-decoder model"
  },
  {
    "s": "mcro:XTTS",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:XTTS",
    "p": "mcro:hasUseCase",
    "o": "mcro:XTTS-UseCaseInformationSection"
  },
  {
    "s": "mcro:XTTS-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:XTTS-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip."
  },
  {
    "s": "mcro:XTTS",
    "p": "mcro:hasLicense",
    "o": "mcro:XTTS-LicenseInformationSection"
  },
  {
    "s": "mcro:XTTS-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:XTTS-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "[Coqui Public Model License](https://coqui.ai/cpml)"
  },
  {
    "s": "mcro:lftwr4target",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:lftwr4target",
    "p": "mcro:hasCitation",
    "o": "mcro:lftwr4target-CitationInformationSection"
  },
  {
    "s": "mcro:lftwr4target-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:lftwr4target-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{vidgen2021lftw,\n  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},\n  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},\n  booktitle={ACL},\n  year={2021}\n}"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev",
    "p": "mcro:hasLicense",
    "o": "mcro:blackforestlabsFLUX1dev-License"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev-License",
    "p": "prov:hasTextValue",
    "o": "This model falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)."
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev",
    "p": "mcro:hasLimitation",
    "o": "mcro:blackforestlabsFLUX1dev-Limitation"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev-Limitation",
    "p": "prov:hasTextValue",
    "o": "- This model is not intended or able to provide factual information.\n- As a statistical model this checkpoint might amplify existing societal biases.\n- The model may fail to generate output that matches the prompts.\n- Prompt following is heavily influenced by the prompting-style."
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev",
    "p": "mcro:hasUseCase",
    "o": "mcro:blackforestlabsFLUX1dev-UseCase"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model and its derivatives may not be used\n\n- In any way that violates any applicable national, federal, state, local or international law or regulation.\n- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\n- To generate or disseminate verifiably false information and/or content with the purpose of harming others.\n- To generate or disseminate personal identifiable information that can be used to harm an individual.\n- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\n- To create non-consensual nudity or illegal pornographic content.\n- For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\n- Generating or facilitating large-scale disinformation campaigns."
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev",
    "p": "mcro:hasArchitecture",
    "o": "mcro:blackforestlabsFLUX1dev-Architecture"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1dev-Architecture",
    "p": "prov:hasTextValue",
    "o": "`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions."
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill",
    "p": "mcro:hasDataset",
    "o": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "MS MARCO"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "eli5_question_answer"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "squad_pairs"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "WikiAnswers"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "yahoo_answers_title_question"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "gooaq_pairs"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "stackexchange_duplicate_questions_body_body"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "wikihow"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "S2ORC_title_abstract"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "stackexchange_duplicate_questions_title-body_title-body"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "yahoo_answers_question_answer"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "searchQA_top5_snippets"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "stackexchange_duplicate_questions_title_title"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "yahoo_answers_title_answer"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill",
    "p": "mcro:hasLicense",
    "o": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-License"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-License",
    "p": "prov:hasTextValue",
    "o": "Apache v2.0 License"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill",
    "p": "mcro:hasCitation",
    "o": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-Citation"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-Citation",
    "p": "prov:hasTextValue",
    "o": "Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill",
    "p": "mcro:hasUseCase",
    "o": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-UseCaseInformationSection"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "learned sparse retrieval"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:opensearchprojectopensearchneuralsparseencodingdocv2distill-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "sparse vectors"
  },
  {
    "s": "mcro:longformerbase4096",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:longformerbase4096",
    "p": "mcro:hasCitation",
    "o": "mcro:longformerbase4096-CitationInformationSection"
  },
  {
    "s": "mcro:longformerbase4096-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:longformerbase4096-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{Beltagy2020Longformer,\n  title={Longformer: The Long-Document Transformer},\n  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},\n  journal={arXiv:2004.05150},\n  year={2020},\n}"
  },
  {
    "s": "mcro:longformerbase4096",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:longformerbase4096-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:longformerbase4096-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:longformerbase4096-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Longformer is a transformer model for long documents. `longformer-base-4096` is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. \n \nLongformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations."
  },
  {
    "s": "mcro:bertbaseNER",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertbaseNER",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertbaseNER-ModelDetail"
  },
  {
    "s": "mcro:bertbaseNER-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertbaseNER-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertbaseNER-Citation"
  },
  {
    "s": "mcro:bertbaseNER-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertbaseNER-ModelDetail",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bertbaseNER-IntendedUseCase"
  },
  {
    "s": "mcro:bertbaseNER-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertbaseNER-ModelDetail",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertbaseNER-TrainingData"
  },
  {
    "s": "mcro:bertbaseNER-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bertbaseNER-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertbaseNER-ModelArchitecture"
  },
  {
    "s": "mcro:bertbaseNER-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertbaseNER-ModelDetail",
    "p": "mcro:hasLimitation",
    "o": "mcro:bertbaseNER-Limitation"
  },
  {
    "s": "mcro:bertbaseNER-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:bertbaseNER",
    "p": "mcro:hasEvalResults",
    "o": "mcro:bertbaseNER-EvalResults"
  },
  {
    "s": "mcro:bertbaseNER-EvalResults",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:bertbaseNER-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT model"
  },
  {
    "s": "mcro:bertbaseNER-TrainingData",
    "p": "prov:hasTextValue",
    "o": "CoNLL-2003 Named Entity Recognition"
  },
  {
    "s": "mcro:bertbaseNER-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "Named Entity Recognition"
  },
  {
    "s": "mcro:kokoro82M",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:kokoro82M",
    "p": "mcro:hasLicense",
    "o": "mcro:kokoro82M-License"
  },
  {
    "s": "mcro:kokoro82M-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:kokoro82M-License",
    "p": "prov:hasTextValue",
    "o": "Apache-licensed weights"
  },
  {
    "s": "mcro:kokoro82M",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:kokoro82M-Architecture"
  },
  {
    "s": "mcro:kokoro82M-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:kokoro82M-Architecture",
    "p": "prov:hasTextValue",
    "o": "open-weight TTS model with 82 million parameters"
  },
  {
    "s": "mcro:kokoro82M",
    "p": "mcro:hasUseCase",
    "o": "mcro:kokoro82M-UseCase"
  },
  {
    "s": "mcro:kokoro82M-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:kokoro82M-UseCase",
    "p": "prov:hasTextValue",
    "o": "can be deployed anywhere from production environments to personal projects"
  },
  {
    "s": "mcro:kokoro82M",
    "p": "mcro:hasTrainingData",
    "o": "mcro:kokoro82M-TrainingData"
  },
  {
    "s": "mcro:kokoro82M-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:kokoro82M-TrainingData",
    "p": "prov:hasTextValue",
    "o": "permissive/non-copyrighted audio data and IPA phoneme labels"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:QwenQwen2515BInstruct-ModelDetail"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:QwenQwen2515BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:QwenQwen2515BInstruct-Citation"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:QwenQwen2515BInstruct-UseCase"
  },
  {
    "s": "mcro:QwenQwen2515BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:example_model",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:example_model",
    "p": "mcro:hasLicense",
    "o": "mcro:example_model-License"
  },
  {
    "s": "mcro:example_model-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:example_model-License",
    "p": "prov:hasTextValue",
    "o": "mit"
  },
  {
    "s": "mcro:example_model",
    "p": "mcro:hasDataset",
    "o": "mcro:example_model-Dataset"
  },
  {
    "s": "mcro:example_model-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:example_model-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet"
  },
  {
    "s": "mcro:example_model",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:example_model-ModelArchitecture"
  },
  {
    "s": "mcro:example_model-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:example_model-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "CNN"
  },
  {
    "s": "mcro:example_model",
    "p": "mcro:hasCitation",
    "o": "mcro:example_model-Citation"
  },
  {
    "s": "mcro:example_model-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:example_model-Citation",
    "p": "prov:hasTextValue",
    "o": "example citation"
  },
  {
    "s": "mcro:example_model",
    "p": "mcro:hasUseCase",
    "o": "mcro:example_model-UseCase"
  },
  {
    "s": "mcro:example_model-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:example_model-UseCase",
    "p": "prov:hasTextValue",
    "o": "example use case"
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronosbolttiny-ModelArchitecture"
  },
  {
    "s": "mcro:chronosbolttiny-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations."
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasCitation",
    "o": "mcro:chronosbolttiny-Citation"
  },
  {
    "s": "mcro:chronosbolttiny-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasLicense",
    "o": "mcro:chronosbolttiny-License"
  },
  {
    "s": "mcro:chronosbolttiny-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-License",
    "p": "prov:hasTextValue",
    "o": "This project is licensed under the Apache-2.0 License."
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronosbolttiny-UseCase"
  },
  {
    "s": "mcro:chronosbolttiny-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-UseCase",
    "p": "prov:hasTextValue",
    "o": "Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting."
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasDataset",
    "o": "mcro:chronosbolttiny-Dataset"
  },
  {
    "s": "mcro:chronosbolttiny-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-Dataset",
    "p": "prov:hasTextValue",
    "o": "It chunks the historical time series context into patches of multiple observations"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02",
    "p": "mcro:hasModelDetail",
    "o": "mcro:mistralaiMistral7BInstructv02-ModelDetail"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.\n\nMistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/)."
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02",
    "p": "mcro:hasLicense",
    "o": "mcro:mistralaiMistral7BInstructv02-License"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-License",
    "p": "prov:hasTextValue",
    "o": "The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs."
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02",
    "p": "mcro:hasLimitation",
    "o": "mcro:mistralaiMistral7BInstructv02-Limitation"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-Limitation",
    "p": "prov:hasTextValue",
    "o": "The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs."
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02",
    "p": "mcro:hasCitation",
    "o": "mcro:mistralaiMistral7BInstructv02-Citation"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-Citation",
    "p": "prov:hasTextValue",
    "o": "For full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/)."
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02",
    "p": "mcro:hasUseCase",
    "o": "mcro:mistralaiMistral7BInstructv02-UseCase"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-UseCase",
    "p": "prov:hasTextValue",
    "o": "In order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id."
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02",
    "p": "mcro:hasOwner",
    "o": "mcro:mistralaiMistral7BInstructv02-Owner"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-Owner",
    "p": "rdf:type",
    "o": "mcro:OwnerInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv02-Owner",
    "p": "prov:hasTextValue",
    "o": "The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed."
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf",
    "p": "mcro:hasModelDetail",
    "o": "mcro:llavahfllavaonevisionqwen205bovhf-ModelDetail"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:llavahfllavaonevisionqwen205bovhf-Architecture"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf-Architecture",
    "p": "prov:hasTextValue",
    "o": "SO400M + Qwen2"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:llavahfllavaonevisionqwen205bovhf-Citation"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{li2024llavaonevisioneasyvisualtask,\n      title={LLaVA-OneVision: Easy Visual Task Transfer}, \n      author={Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},\n      year={2024},\n      eprint={2408.03326},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2408.03326}, \n}"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf",
    "p": "mcro:hasUseCase",
    "o": "mcro:llavahfllavaonevisionqwen205bovhf-UseCase"
  },
  {
    "s": "mcro:llavahfllavaonevisionqwen205bovhf-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-ModelArchitecture"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n)\n"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-Citation"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-UseCase"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv2-UseCase",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:hubertsiuzdaksnac24khz",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:hubertsiuzdaksnac24khz",
    "p": "mcro:hasUseCase",
    "o": "mcro:hubertsiuzdaksnac24khz-UseCaseInformationSection"
  },
  {
    "s": "mcro:hubertsiuzdaksnac24khz-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:hubertsiuzdaksnac24khz-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "speech synthesis"
  },
  {
    "s": "mcro:hubertsiuzdaksnac24khz",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:hubertsiuzdaksnac24khz-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:hubertsiuzdaksnac24khz-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:hubertsiuzdaksnac24khz-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SNAC encodes audio into hierarchical tokens similarly to SoundStream, EnCodec, and DAC"
  },
  {
    "s": "mcro:facebookwav2vec2base960h",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookwav2vec2base960h",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookwav2vec2base960h-ModelDetail"
  },
  {
    "s": "mcro:facebookwav2vec2base960h-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookwav2vec2base960h-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookwav2vec2base960h-Citation"
  },
  {
    "s": "mcro:facebookwav2vec2base960h-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookwav2vec2base960h-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookwav2vec2base960h-ModelArchitecture"
  },
  {
    "s": "mcro:facebookwav2vec2base960h-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookwav2vec2base960h",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookwav2vec2base960h-UseCase"
  },
  {
    "s": "mcro:facebookwav2vec2base960h-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookwav2vec2base960h",
    "p": "mcro:hasDataset",
    "o": "mcro:facebookwav2vec2base960h-Dataset"
  },
  {
    "s": "mcro:facebookwav2vec2base960h-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:WhereIsAIUAELargeV1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:WhereIsAIUAELargeV1",
    "p": "mcro:hasLicense",
    "o": "mcro:WhereIsAIUAELargeV1-License"
  },
  {
    "s": "mcro:WhereIsAIUAELargeV1-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:WhereIsAIUAELargeV1-License",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:WhereIsAIUAELargeV1",
    "p": "mcro:hasCitation",
    "o": "mcro:WhereIsAIUAELargeV1-Citation"
  },
  {
    "s": "mcro:WhereIsAIUAELargeV1-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:WhereIsAIUAELargeV1",
    "p": "mcro:hasUseCase",
    "o": "mcro:WhereIsAIUAELargeV1-UseCase"
  },
  {
    "s": "mcro:WhereIsAIUAELargeV1-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BAAIbgererankerv2m3-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "reranker"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3",
    "p": "mcro:hasCitation",
    "o": "mcro:BAAIbgererankerv2m3-CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3",
    "p": "mcro:hasUseCase",
    "o": "mcro:BAAIbgererankerv2m3-UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerv2m3-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "reranker uses question and document as input and directly output similarity instead of embedding.\nYou can get a relevance score by inputting query and passage to the reranker.\nAnd the score can be mapped to a float value in [0,1] by sigmoid function."
  },
  {
    "s": "mcro:yiyanghkustfinberttone",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:yiyanghkustfinberttone",
    "p": "mcro:hasCitation",
    "o": "mcro:yiyanghkustfinberttone-Citation"
  },
  {
    "s": "mcro:yiyanghkustfinberttone-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:yiyanghkustfinberttone-Citation",
    "p": "prov:hasTextValue",
    "o": "Huang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022)."
  },
  {
    "s": "mcro:yiyanghkustfinberttone",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:yiyanghkustfinberttone-ModelArchitecture"
  },
  {
    "s": "mcro:yiyanghkustfinberttone-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:yiyanghkustfinberttone-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT model"
  },
  {
    "s": "mcro:yiyanghkustfinberttone",
    "p": "mcro:hasUseCase",
    "o": "mcro:yiyanghkustfinberttone-UseCase"
  },
  {
    "s": "mcro:yiyanghkustfinberttone-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:yiyanghkustfinberttone-UseCase",
    "p": "prov:hasTextValue",
    "o": "financial tone analysis task"
  },
  {
    "s": "mcro:multiqaminiLML6cosv1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:multiqaminiLML6cosv1",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:multiqaminiLML6cosv1-UseCaseInformationSection"
  },
  {
    "s": "mcro:multiqaminiLML6cosv1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:multiqaminiLML6cosv1-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intented to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages."
  },
  {
    "s": "mcro:multiqaminiLML6cosv1",
    "p": "mcro:hasTrainingData",
    "o": "mcro:multiqaminiLML6cosv1-TrainingDataInformationSection"
  },
  {
    "s": "mcro:multiqaminiLML6cosv1-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:multiqaminiLML6cosv1-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. In total we have about 215M (question, answer) pairs.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file."
  },
  {
    "s": "mcro:multiqaminiLML6cosv1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:multiqaminiLML6cosv1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:multiqaminiLML6cosv1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:multiqaminiLML6cosv1-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for **semantic search**. It has been trained on 215M (question, answer) pairs from diverse sources."
  },
  {
    "s": "mcro:appleOpenELM11BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct",
    "p": "mcro:hasLicense",
    "o": "mcro:appleOpenELM11BInstruct-License"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:appleOpenELM11BInstruct-Citation"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{mehtaOpenELMEfficientLanguage2024,\n\ttitle = {{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}},\n\tshorttitle = {{OpenELM}},\n\turl = {https://arxiv.org/abs/2404.14619v1},\n\tlanguage = {en},\n\turldate = {2024-04-24},\n\tjournal = {arXiv.org},\n\tauthor = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},\n\tmonth = apr,\n\tyear = {2024},\n}\n\n@inproceedings{mehta2022cvnets, \n     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, \n     title = {CVNets: High Performance Library for Computer Vision}, \n     year = {2022}, \n     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, \n     series = {MM '22} \n}"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:appleOpenELM11BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct",
    "p": "mcro:hasDataset",
    "o": "mcro:appleOpenELM11BInstruct-Dataset"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:appleOpenELM11BInstruct-UseCase"
  },
  {
    "s": "mcro:appleOpenELM11BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb",
    "p": "mcro:hasModelDetail",
    "o": "mcro:speechbrainspkrececapavoxceleb-ModelDetail"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:speechbrainspkrececapavoxceleb-Architecture"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Architecture",
    "p": "prov:hasTextValue",
    "o": "ECAPA-TDNN"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:speechbrainspkrececapavoxceleb-Citation-DBLP"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Citation-DBLP",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Citation-DBLP",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{DBLP:conf/interspeech/DesplanquesTD20,\n  author    = {Brecht Desplanques and\n               Jenthe Thienpondt and\n               Kris Demuynck},\n  editor    = {Helen Meng and\n               Bo Xu and\n               Thomas Fang Zheng},\n  title     = {{ECAPA-TDNN:} Emphasized Channel Attention, Propagation and Aggregation\n               in {TDNN} Based Speaker Verification},\n  booktitle = {Interspeech 2020},\n  pages     = {3830--3834},\n  publisher = {{ISCA}},\n  year      = {2020},\n}"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Citation-SpeechBrain",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Citation-SpeechBrain",
    "p": "prov:hasTextValue",
    "o": "@misc{speechbrain,\n  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran\u00e7ois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n  year={2021},\n  eprint={2106.04624},\n  archivePrefix={arXiv},\n  primaryClass={eess.AS},\n  note={arXiv:2106.04624}\n}"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:speechbrainspkrececapavoxceleb-Citation-SpeechBrain"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb",
    "p": "mcro:hasLicense",
    "o": "mcro:speechbrainspkrececapavoxceleb-License"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb",
    "p": "mcro:hasUseCase",
    "o": "mcro:speechbrainspkrececapavoxceleb-UseCase"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Architecture",
    "p": "prov:hasTextValue",
    "o": "convolutional and residual blocks"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Architecture",
    "p": "prov:hasTextValue",
    "o": "attentive statistical pooling"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-Architecture",
    "p": "prov:hasTextValue",
    "o": "Additive Margin Softmax Loss"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-UseCase",
    "p": "prov:hasTextValue",
    "o": "speaker verification"
  },
  {
    "s": "mcro:speechbrainspkrececapavoxceleb-UseCase",
    "p": "prov:hasTextValue",
    "o": "extract speaker embeddings"
  },
  {
    "s": "mcro:llama32Collection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llama32Collection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llama32Collection-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama32Collection-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama32Collection-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:llama32Collection",
    "p": "mcro:hasLicense",
    "o": "mcro:llama32Collection-LicenseInformationSection"
  },
  {
    "s": "mcro:llama32Collection-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llama32Collection-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)."
  },
  {
    "s": "mcro:llama32Collection",
    "p": "mcro:hasUseCase",
    "o": "mcro:llama32Collection-UseCaseInformationSection"
  },
  {
    "s": "mcro:llama32Collection-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llama32Collection-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources."
  },
  {
    "s": "mcro:llama32Collection",
    "p": "mcro:hasTrainingData",
    "o": "mcro:llama32Collection-TrainingDataInformationSection"
  },
  {
    "s": "mcro:llama32Collection-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:llama32Collection-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)."
  },
  {
    "s": "mcro:edgenextsmallusiin1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:edgenextsmallusiin1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:edgenextsmallusiin1k-ModelDetail"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:edgenextsmallusiin1k-Citation"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Maaz2022EdgeNeXt,\n  title={EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications},\n    author={Muhammad Maaz and Abdelrahman Shaker and Hisham Cholakkal and Salman Khan and Syed Waqas Zamir and Rao Muhammad Anwer and Fahad Shahbaz Khan},\n  booktitle={International Workshop on Computational Aspects of Deep Learning at 17th European Conference on Computer Vision (CADL2022)},\n  year={2022},\n  organization={Springer}\n}\n"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:edgenextsmallusiin1k-Citation2"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2204.03475,\n  doi = {10.48550/ARXIV.2204.03475},  \n  url = {https://arxiv.org/abs/2204.03475},  \n  author = {Ridnik, Tal and Lawen, Hussam and Ben-Baruch, Emanuel and Noy, Asaf},  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},  \n  title = {Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results},  \n  publisher = {arXiv},  \n  year = {2022},  \n}\n"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:edgenextsmallusiin1k-Arch"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-Arch",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:edgenextsmallusiin1k-Dataset"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:edgenextsmallusiin1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:edgenextsmallusiin1k-UseCase"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:edgenextsmallusiin1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Classification"
  },
  {
    "s": "mcro:nomicainomicembedtextv1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:nomicainomicembedtextv1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:nomicainomicembedtextv1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv1",
    "p": "mcro:hasUseCase",
    "o": "mcro:nomicainomicembedtextv1-UseCaseInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv1",
    "p": "mcro:hasCitation",
    "o": "mcro:nomicainomicembedtextv1-CitationInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv1-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv1",
    "p": "mcro:hasTrainingData",
    "o": "mcro:nomicainomicembedtextv1-TrainingDataInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv1-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:whisper",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasModelDetail",
    "o": "mcro:whisper-ModelDetail"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasUseCase",
    "o": "mcro:whisper-UseCase"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasDataset",
    "o": "mcro:whisper-Dataset"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasPerformance",
    "o": "mcro:whisper-Performance"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasConsideration",
    "o": "mcro:whisper-Consideration"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasTrainingData",
    "o": "mcro:whisper-TrainingData"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasCitation",
    "o": "mcro:whisper-Citation"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisper-ModelArchitecture"
  },
  {
    "s": "mcro:whisper-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisper-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer based encoder-decoder model"
  },
  {
    "s": "mcro:whisper-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisper-UseCase",
    "p": "prov:hasTextValue",
    "o": "automatic speech recognition (ASR) and speech translation"
  },
  {
    "s": "mcro:whisper-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:whisper-Dataset",
    "p": "prov:hasTextValue",
    "o": "680k hours of labelled data"
  },
  {
    "s": "mcro:whisper-Performance",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:whisper-Performance",
    "p": "prov:hasTextValue",
    "o": "improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level."
  },
  {
    "s": "mcro:whisper-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:whisper-Consideration",
    "p": "prov:hasTextValue",
    "o": "the models combine trying to predict the next word in audio with trying to transcribe the audio itself."
  },
  {
    "s": "mcro:whisper-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:whisper-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet."
  },
  {
    "s": "mcro:whisper-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisper-Citation",
    "p": "prov:hasTextValue",
    "o": "Alec Radford et al from OpenAI"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metallamaLlama323BInstruct-ModelDetail"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:metallamaLlama323BInstruct-License"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-License",
    "p": "prov:hasTextValue",
    "o": "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)."
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metallamaLlama323BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metallamaLlama323BInstruct",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:metallamaLlama323BInstruct-IntendedUseCase"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources."
  },
  {
    "s": "mcro:metallamaLlama323BInstruct",
    "p": "mcro:hasDataset",
    "o": "mcro:metallamaLlama323BInstruct-Dataset"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:metallamaLlama323BInstruct-Dataset",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)."
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertbasemultilingualuncasedsentiment-ModelDetail"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertbasemultilingualuncasedsentiment-UseCase"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above or for further finetuning on related sentiment analysis tasks."
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:bertbasemultilingualuncasedsentiment-Dataset"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-Dataset",
    "p": "prov:hasTextValue",
    "o": "Here is the number of product reviews we used for finetuning the model:"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-ModelDetail",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:bertbasemultilingualuncasedsentiment-QuantativeAnalysis"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:bertbasemultilingualuncasedsentiment-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "The fine-tuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:"
  },
  {
    "s": "mcro:trlinternaltestingtinyQwen2ForCausalLM25",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:trlinternaltestingtinyQwen2ForCausalLM25",
    "p": "mcro:hasModelDetail",
    "o": "mcro:trlinternaltestingtinyQwen2ForCausalLM25-ModelDetail"
  },
  {
    "s": "mcro:trlinternaltestingtinyQwen2ForCausalLM25-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:trlinternaltestingtinyQwen2ForCausalLM25-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library."
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertbasechinese-ModelDetail"
  },
  {
    "s": "mcro:bertbasechinese-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertbasechinese-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper).\n\n- **Developed by:** HuggingFace team\n- **Model Type:** Fill-Mask\n- **Language(s):** Chinese\n- **License:** [More Information needed]\n- **Parent Model:** See the [BERT base uncased model](https://huggingface.co/bert-base-uncased) for more information about the BERT base model."
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertbasechinese-UseCase"
  },
  {
    "s": "mcro:bertbasechinese-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertbasechinese-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model can be used for masked language modeling"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasConsideration",
    "o": "mcro:bertbasechinese-Consideration"
  },
  {
    "s": "mcro:bertbasechinese-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:bertbasechinese-Consideration",
    "p": "prov:hasTextValue",
    "o": "**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922))."
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasModelParameter",
    "o": "mcro:bertbasechinese-ModelParameter"
  },
  {
    "s": "mcro:bertbasechinese-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:bertbasechinese-ModelParameter",
    "p": "prov:hasTextValue",
    "o": "* **type_vocab_size:** 2\n* **vocab_size:** 21128\n* **num_hidden_layers:** 12"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertbasechinese-TrainingData"
  },
  {
    "s": "mcro:bertbasechinese-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:bertbasechinese-TrainingData",
    "p": "prov:hasTextValue",
    "o": "[More Information Needed]"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:bertbasechinese-QuantativeAnalysis"
  },
  {
    "s": "mcro:bertbasechinese-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:bertbasechinese-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "[More Information Needed]"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasCitation",
    "o": "mcro:bertbasechinese-Citation"
  },
  {
    "s": "mcro:bertbasechinese-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertbasechinese-Citation",
    "p": "prov:hasTextValue",
    "o": "[BERT](https://arxiv.org/abs/1810.04805)"
  },
  {
    "s": "mcro:bertbasechinese",
    "p": "mcro:hasLicense",
    "o": "mcro:bertbasechinese-License"
  },
  {
    "s": "mcro:bertbasechinese-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:bertbasechinese-License",
    "p": "prov:hasTextValue",
    "o": "[More Information needed]"
  },
  {
    "s": "mcro:lengyue233contentvecbest",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:lengyue233contentvecbest",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:lengyue233contentvecbest-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:lengyue233contentvecbest-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:lengyue233contentvecbest",
    "p": "mcro:hasUseCase",
    "o": "mcro:lengyue233contentvecbest-UseCaseInformationSection"
  },
  {
    "s": "mcro:lengyue233contentvecbest-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookmusicgenmedium",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookmusicgenmedium",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookmusicgenmedium-ModelDetail"
  },
  {
    "s": "mcro:facebookmusicgenmedium-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookmusicgenmedium-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookmusicgenmedium-Citation"
  },
  {
    "s": "mcro:facebookmusicgenmedium-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookmusicgenmedium-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{copet2023simple,\n      title={Simple and Controllable Music Generation}, \n      author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre D\u00e9fossez},\n      year={2023},\n      eprint={2306.05284},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD}\n}"
  },
  {
    "s": "mcro:facebookmusicgenmedium-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:facebookmusicgenmedium-License"
  },
  {
    "s": "mcro:facebookmusicgenmedium-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:facebookmusicgenmedium-License",
    "p": "prov:hasTextValue",
    "o": "Code is released under MIT, model weights are released under CC-BY-NC 4.0."
  },
  {
    "s": "mcro:facebookmusicgenmedium-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookmusicgenmedium-ModelArchitecture"
  },
  {
    "s": "mcro:facebookmusicgenmedium-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookmusicgenmedium-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "MusicGen consists of an EnCodec model for audio tokenization, an auto-regressive language model based on the transformer architecture for music modeling. The model comes in different sizes: 300M, 1.5B and 3.3B parameters ; and two variants: a model trained for text-to-music generation task and a model trained for melody-guided music generation."
  },
  {
    "s": "mcro:facebookmusicgenmedium",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:facebookmusicgenmedium-IntendedUseCase"
  },
  {
    "s": "mcro:facebookmusicgenmedium-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookmusicgenmedium-IntendedUseCase",
    "p": "mcro:hasPrimaryIntendedUseCase",
    "o": "mcro:facebookmusicgenmedium-PrimaryIntendedUseCase"
  },
  {
    "s": "mcro:facebookmusicgenmedium-PrimaryIntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:facebookmusicgenmedium-PrimaryIntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The primary use of MusicGen is research on AI-based music generation, including:\n\n- Research efforts, such as probing and better understanding the limitations of generative models to further improve the state of science\n- Generation of music guided by text or melody to understand current abilities of generative AI models by machine learning amateurs"
  },
  {
    "s": "mcro:facebookmusicgenmedium-IntendedUseCase",
    "p": "mcro:hasPrimaryIntendedUser",
    "o": "mcro:facebookmusicgenmedium-PrimaryIntendedUser"
  },
  {
    "s": "mcro:facebookmusicgenmedium-PrimaryIntendedUser",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUserInformationSection"
  },
  {
    "s": "mcro:facebookmusicgenmedium-PrimaryIntendedUser",
    "p": "prov:hasTextValue",
    "o": "The primary intended users of the model are researchers in audio, machine learning and artificial intelligence, as well as amateur seeking to better understand those models."
  },
  {
    "s": "mcro:facebookmusicgenmedium-IntendedUseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:facebookmusicgenmedium-OutOfScopeUseCase"
  },
  {
    "s": "mcro:facebookmusicgenmedium-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:facebookmusicgenmedium-OutOfScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "The model should not be used on downstream applications without further risk evaluation and mitigation. The model should not be used to intentionally create or disseminate music pieces that create hostile or alienating environments for people. This includes generating music that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes."
  },
  {
    "s": "mcro:TinyLlama11BChatv10",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:TinyLlama11BChatv10",
    "p": "mcro:hasModelDetail",
    "o": "mcro:TinyLlama11BChatv10-ModelDetail"
  },
  {
    "s": "mcro:TinyLlama11BChatv10-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:TinyLlama11BChatv10",
    "p": "mcro:hasUseCase",
    "o": "mcro:TinyLlama11BChatv10-UseCase"
  },
  {
    "s": "mcro:TinyLlama11BChatv10-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:TinyLlama11BChatv10",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:TinyLlama11BChatv10-ModelArchitecture"
  },
  {
    "s": "mcro:TinyLlama11BChatv10-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:TinyLlama11BChatv10-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 2"
  },
  {
    "s": "mcro:TinyLlama11BChatv10",
    "p": "mcro:hasDataset",
    "o": "mcro:TinyLlama11BChatv10-Dataset"
  },
  {
    "s": "mcro:TinyLlama11BChatv10-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:TinyLlama11BChatv10-Dataset",
    "p": "prov:hasTextValue",
    "o": "UltraChat"
  },
  {
    "s": "mcro:TinyLlama11BChatv10-Dataset",
    "p": "prov:hasTextValue",
    "o": "UltraFeedback"
  },
  {
    "s": "mcro:microsoftmdebertav3base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftmdebertav3base",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftmdebertav3base-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftmdebertav3base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftmdebertav3base-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "mDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.\nThe mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R."
  },
  {
    "s": "mcro:microsoftmdebertav3base",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftmdebertav3base-Citation"
  },
  {
    "s": "mcro:microsoftmdebertav3base-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftmdebertav3base-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}"
  },
  {
    "s": "mcro:microsoftmdebertav3base",
    "p": "mcro:hasDataset",
    "o": "mcro:microsoftmdebertav3base-Dataset"
  },
  {
    "s": "mcro:microsoftmdebertav3base-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftmdebertav3base-Dataset",
    "p": "prov:hasTextValue",
    "o": "This model was trained using the 2.5T CC100 data as XLM-R."
  },
  {
    "s": "mcro:microsoftmdebertav3base",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftmdebertav3base-UseCase"
  },
  {
    "s": "mcro:microsoftmdebertav3base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftmdebertav3base-UseCase",
    "p": "prov:hasTextValue",
    "o": "Fine-tuning on NLU tasks\n\nWe present the dev results on XNLI with zero-shot cross-lingual transfer setting, i.e. training with English data only, test on other languages.\n\n| Model        |avg | en |  fr| es  | de  | el  | bg  | ru  |tr   |ar   |vi   | th  | zh | hi  | sw  | ur  |\n|--------------| ----|----|----|---- |--   |--   |--   | --  |--   |--   |--   | --  | -- | --  | --  | --  |\n| XLM-R-base   |76.2 |85.8|79.7|80.7 |78.7 |77.5 |79.6 |78.1 |74.2 |73.8 |76.5 |74.6 |76.7| 72.4| 66.5| 68.3|\n| mDeBERTa-base|**79.8**+/-0.2|**88.2**|**82.6**|**84.4** |**82.7** |**82.3** |**82.4** |**80.8** |**79.5** |**78.5** |**78.1** |**76.4** |**79.5**| **75.9**| **73.9**| **72.4**|"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384",
    "p": "mcro:hasModelDetail",
    "o": "mcro:ViTSO400M14SigLIP384-ModelDetail"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:ViTSO400M14SigLIP384-Dataset"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-Dataset",
    "p": "prov:hasTextValue",
    "o": "WebLI"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:ViTSO400M14SigLIP384-Citation1"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-Citation1",
    "p": "prov:hasTextValue",
    "o": "@article{zhai2023sigmoid,\n  title={Sigmoid loss for language image pre-training},\n  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2303.15343},\n  year={2023}\n}"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:ViTSO400M14SigLIP384-Citation2"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{big_vision,\n  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},\n  title = {Big Vision},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/google-research/big_vision}}\n}"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ViTSO400M14SigLIP384-Architecture"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-Architecture",
    "p": "prov:hasTextValue",
    "o": "Contrastive Image-Text, Zero-Shot Image Classification"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384",
    "p": "mcro:hasUseCase",
    "o": "mcro:ViTSO400M14SigLIP384-UseCase"
  },
  {
    "s": "mcro:ViTSO400M14SigLIP384-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1",
    "p": "mcro:hasCitation",
    "o": "mcro:mxbai-rerank-xsmall-v1-CitationInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1",
    "p": "mcro:hasLicense",
    "o": "mcro:mxbai-rerank-xsmall-v1-LicenseInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mxbai-rerank-xsmall-v1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1",
    "p": "mcro:hasUseCase",
    "o": "mcro:mxbai-rerank-xsmall-v1-UseCaseInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1",
    "p": "mcro:hasDataset",
    "o": "mcro:mxbai-rerank-xsmall-v1-DatasetInformationSection"
  },
  {
    "s": "mcro:mxbai-rerank-xsmall-v1-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:layoutlmv3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:layoutlmv3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:layoutlmv3-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:layoutlmv3-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:layoutlmv3-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis."
  },
  {
    "s": "mcro:layoutlmv3",
    "p": "mcro:hasCitation",
    "o": "mcro:layoutlmv3-CitationInformationSection"
  },
  {
    "s": "mcro:layoutlmv3-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:layoutlmv3-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{huang2022layoutlmv3,\n  author={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},\n  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},\n  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},\n  year={2022}\n}"
  },
  {
    "s": "mcro:layoutlmv3",
    "p": "mcro:hasLicense",
    "o": "mcro:layoutlmv3-LicenseInformationSection"
  },
  {
    "s": "mcro:layoutlmv3-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:layoutlmv3-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The content of this project itself is licensed under the [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/).\nPortions of the source code are based on the [transformers](https://github.com/huggingface/transformers) project.\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersbertbasenlimeantokens-ModelArchitecture"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersbertbasenlimeantokens-Citation"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersbertbasenlimeantokens-UseCase"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersbertbasenlimeantokens-UseCase",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:visiontransformerbasesizedmodel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT) model trained using the DINOv2 method. It was introduced in the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Oquab et al. and first released in [this repository](https://github.com/facebookresearch/dinov2).\n\nDisclaimer: The team releasing DINOv2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion. \n\nImages are presented to the model as a sequence of fixed-size patches, which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nNote that this model does not include any fine-tuned heads. \n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image."
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasUseCase",
    "o": "mcro:visiontransformerbasesizedmodel-UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for feature extraction. See the [model hub](https://huggingface.co/models?search=facebook/dinov2) to look for\nfine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbasesizedmodel-CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:visiontransformerbasesizedmodel-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision}, \n      author={Maxime Oquab and Timoth\u00e9e Darcet and Th\u00e9o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv\u00e9 Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention",
    "p": "mcro:hasCitation",
    "o": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-Citation"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-ModelArchitecture"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention",
    "p": "mcro:hasUseCase",
    "o": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-UseCase"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:DeBERTaDecodingenhancedBERTwithDisentangledAttention-UseCase",
    "p": "prov:hasTextValue",
    "o": "Fine-tuning on NLU tasks"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa",
    "p": "mcro:hasModelDetail",
    "o": "mcro:roberta-baseforextractiveqa-ModelDetailSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:roberta-baseforextractiveqa-LicenseInformationSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:roberta-baseforextractiveqa-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "roberta-base"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa",
    "p": "mcro:hasDataset",
    "o": "mcro:roberta-baseforextractiveqa-DatasetInformationSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "SQuAD 2.0"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa",
    "p": "mcro:hasUseCase",
    "o": "mcro:roberta-baseforextractiveqa-UseCaseInformationSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:roberta-baseforextractiveqa-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Extractive QA"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit",
    "p": "mcro:hasCitation",
    "o": "mcro:unslothQwen2505BInstructbnb4bit-Citation"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:unslothQwen2505BInstructbnb4bit-ModelArchitecture"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:unslothQwen2505BInstructbnb4bit-IntendedUseCase"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:unslothQwen2505BInstructbnb4bit-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "instruction-tuned 0.5B Qwen2.5 model"
  },
  {
    "s": "mcro:debertaV3Small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:debertaV3Small",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:debertaV3Small-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:debertaV3Small-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:debertaV3Small-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "6 layers"
  },
  {
    "s": "mcro:debertaV3Small-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "hidden size of 768"
  },
  {
    "s": "mcro:debertaV3Small-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "44M backbone parameters"
  },
  {
    "s": "mcro:debertaV3Small-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "vocabulary containing 128K tokens"
  },
  {
    "s": "mcro:debertaV3Small-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "98M parameters in the Embedding layer"
  },
  {
    "s": "mcro:debertaV3Small",
    "p": "mcro:hasDataset",
    "o": "mcro:debertaV3Small-DatasetInformationSection"
  },
  {
    "s": "mcro:debertaV3Small-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:debertaV3Small-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "160GB data"
  },
  {
    "s": "mcro:debertaV3Small",
    "p": "mcro:hasCitation",
    "o": "mcro:debertaV3Small-CitationInformationSection"
  },
  {
    "s": "mcro:debertaV3Small-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:debertaV3Small-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:debertaV3Small-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}"
  },
  {
    "s": "mcro:InternVL378B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:InternVL378B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:InternVL378B-ModelArchitecture"
  },
  {
    "s": "mcro:InternVL378B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:InternVL378B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "InternVL3 retains the same model architecture as InternVL 2.5 and its predecessors, InternVL 1.5 and 2.0, following the \"ViT-MLP-LLM\" paradigm. In this new version, we integrate a newly incrementally pre-trained InternViT with various pre-trained LLMs, including InternLM 3 and Qwen 2.5, using a randomly initialized MLP projector."
  },
  {
    "s": "mcro:InternVL378B",
    "p": "mcro:hasLicense",
    "o": "mcro:InternVL378B-License"
  },
  {
    "s": "mcro:InternVL378B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:InternVL378B-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:InternVL378B",
    "p": "mcro:hasCitation",
    "o": "mcro:InternVL378B-Citation"
  },
  {
    "s": "mcro:InternVL378B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:InternVL378B-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{chen2024expanding,\n  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},\n  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},\n  journal={arXiv preprint arXiv:2412.05271},\n  year={2024}\n}\n@article{wang2024mpo,\n  title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization},\n  author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and Dai, Jifeng},\n  journal={arXiv preprint arXiv:2411.10442},\n  year={2024}\n}\n@article{chen2024far,\n  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},\n  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},\n  journal={arXiv preprint arXiv:2404.16821},\n  year={2024}\n}\n@inproceedings{chen2024internvl,\n  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},\n  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={24185--24198},\n  year={2024}\n}"
  },
  {
    "s": "mcro:InternVL378B",
    "p": "mcro:hasUseCase",
    "o": "mcro:InternVL378B-UseCase"
  },
  {
    "s": "mcro:InternVL378B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:InternVL378B-UseCase",
    "p": "prov:hasTextValue",
    "o": "an advanced multimodal large language model (MLLM) series that demonstrates superior overall performance"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Stablediffusionsafetychecker-ModelDetail"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:Stablediffusionsafetychecker-License"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "mcro:hasUseCase",
    "o": "mcro:Stablediffusionsafetychecker-UseCase"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "mcro:hasBiasRiskLimitationConsideration",
    "o": "mcro:Stablediffusionsafetychecker-BiasRiskLimitationConsideration"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-BiasRiskLimitationConsideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "mcro:hasTrainingDetail",
    "o": "mcro:Stablediffusionsafetychecker-TrainingDetail"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-TrainingDetail",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "mcro:hasEvaluation",
    "o": "mcro:Stablediffusionsafetychecker-Evaluation"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-Evaluation",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "mcro:hasEnvironmentalImpact",
    "o": "mcro:Stablediffusionsafetychecker-EnvironmentalImpact"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-EnvironmentalImpact",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "mcro:hasTechnicalSpecification",
    "o": "mcro:Stablediffusionsafetychecker-TechnicalSpecification"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-TechnicalSpecification",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker",
    "p": "mcro:hasCitation",
    "o": "mcro:Stablediffusionsafetychecker-Citation"
  },
  {
    "s": "mcro:Stablediffusionsafetychecker-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FinBERT",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FinBERT",
    "p": "mcro:hasModelDetail",
    "o": "mcro:FinBERT-ModelDetail"
  },
  {
    "s": "mcro:FinBERT-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:FinBERT-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:FinBERT-Citation"
  },
  {
    "s": "mcro:FinBERT-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FinBERT-Citation",
    "p": "prov:hasTextValue",
    "o": "[Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014)"
  },
  {
    "s": "mcro:FinBERT-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:FinBERT-Citation2"
  },
  {
    "s": "mcro:FinBERT-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FinBERT-Citation2",
    "p": "prov:hasTextValue",
    "o": "[FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063)"
  },
  {
    "s": "mcro:FinBERT-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:FinBERT-Citation3"
  },
  {
    "s": "mcro:FinBERT-Citation3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FinBERT-Citation3",
    "p": "prov:hasTextValue",
    "o": "[blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium."
  },
  {
    "s": "mcro:FinBERT",
    "p": "mcro:hasUseCase",
    "o": "mcro:FinBERT-UseCase"
  },
  {
    "s": "mcro:FinBERT-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:FinBERT-UseCase",
    "p": "prov:hasTextValue",
    "o": "analyze sentiment of financial text"
  },
  {
    "s": "mcro:FinBERT",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:FinBERT-ModelArchitecture"
  },
  {
    "s": "mcro:FinBERT-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:FinBERT-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT language model"
  },
  {
    "s": "mcro:nomicainomicembedtextv15",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:nomicainomicembedtextv15",
    "p": "mcro:hasUseCase",
    "o": "mcro:nomicainomicembedtextv15-UseCase"
  },
  {
    "s": "mcro:nomicainomicembedtextv15-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv15",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:nomicainomicembedtextv15-Arch"
  },
  {
    "s": "mcro:nomicainomicembedtextv15-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:nomicainomicembedtextv15",
    "p": "mcro:hasCitation",
    "o": "mcro:nomicainomicembedtextv15-Citation"
  },
  {
    "s": "mcro:nomicainomicembedtextv15-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:sentencetransformersmultiqampnetbasedotv1-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intented to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.\n\nNote that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text."
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1",
    "p": "mcro:hasModelDetail",
    "o": "mcro:sentencetransformersmultiqampnetbasedotv1-ModelDetailSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersmultiqampnetbasedotv1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for **semantic search**."
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-ModelDetailSection",
    "p": "mcro:hasTrainingData",
    "o": "mcro:sentencetransformersmultiqampnetbasedotv1-DatasetInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmultiqampnetbasedotv1-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "It has been trained on 215M (question, answer) pairs from diverse sources."
  },
  {
    "s": "mcro:QwenQwen2505BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:QwenQwen2505BInstruct-ModelDetail"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:QwenQwen2505BInstruct-LicenseInformationSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:QwenQwen2505BInstruct-CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:QwenQwen2505BInstruct-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:QwenQwen2505BInstruct-UseCaseInformationSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:QwenQwen2505BInstruct-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinze He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersstsbxlmrmultilingual-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersstsbxlmrmultilingual-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersstsbxlmrmultilingual-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbxlmrmultilingual-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekR1DistillLlama8B-License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:deepseekaiDeepSeekR1DistillLlama8B-ModelArchitecture"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "DeepSeek-R1"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:deepseekaiDeepSeekR1DistillLlama8B-UseCase"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-UseCase",
    "p": "prov:hasTextValue",
    "o": "Reasoning"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B",
    "p": "mcro:hasDataset",
    "o": "mcro:deepseekaiDeepSeekR1DistillLlama8B-Dataset"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-Dataset",
    "p": "prov:hasTextValue",
    "o": "Qwen2.5 and Llama3"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekR1DistillLlama8B-Citation"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillLlama8B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext",
    "p": "mcro:hasDataset",
    "o": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Dataset"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Dataset",
    "p": "prov:hasTextValue",
    "o": "UMLS"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext",
    "p": "mcro:hasModelDetail",
    "o": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-ModelDetail"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Citation"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Architecture"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext",
    "p": "mcro:hasUseCase",
    "o": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-UseCase"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltext-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekR1-LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:deepseekaiDeepSeekR1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "MoE"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasUseCase",
    "o": "mcro:deepseekaiDeepSeekR1-UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekR1-CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasDataset",
    "o": "mcro:deepseekaiDeepSeekR1-DatasetInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger",
    "p": "mcro:hasModelDetail",
    "o": "mcro:indonesianrobertabaseposptagger-ModelDetail"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:indonesianrobertabaseposptagger-License"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger",
    "p": "mcro:hasUseCase",
    "o": "mcro:indonesianrobertabaseposptagger-UseCase"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger",
    "p": "mcro:hasDataset",
    "o": "mcro:indonesianrobertabaseposptagger-Dataset"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:indonesianrobertabaseposptagger-ModelArchitecture"
  },
  {
    "s": "mcro:indonesianrobertabaseposptagger-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:doclingmodels",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:doclingmodels",
    "p": "mcro:hasModelDetail",
    "o": "mcro:doclingmodels-ModelDetail"
  },
  {
    "s": "mcro:doclingmodels-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:doclingmodels",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:doclingmodels-ModelArchitecture"
  },
  {
    "s": "mcro:doclingmodels-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:doclingmodels-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "RT-DETR"
  },
  {
    "s": "mcro:doclingmodels",
    "p": "mcro:hasDataset",
    "o": "mcro:doclingmodels-Dataset"
  },
  {
    "s": "mcro:doclingmodels-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:doclingmodels-Dataset",
    "p": "prov:hasTextValue",
    "o": "DocLayNet"
  },
  {
    "s": "mcro:doclingmodels",
    "p": "mcro:hasReference",
    "o": "mcro:doclingmodels-Reference"
  },
  {
    "s": "mcro:doclingmodels-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:doclingmodels-Reference",
    "p": "prov:hasTextValue",
    "o": "@techreport{Docling,\n  author = {Deep Search Team},\n  month = {8},\n  title = {{Docling Technical Report}},\n  url={https://arxiv.org/abs/2408.09869},\n  eprint={2408.09869},\n  doi = \"10.48550/arXiv.2408.09869\",\n  version = {1.0.0},\n  year = {2024}\n}\n\n@article{doclaynet2022,\n  title = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis},  \n  doi = {10.1145/3534678.353904},\n  url = {https://arxiv.org/abs/2206.01062},\n  author = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J},\n  year = {2022}\n}\n\n@InProceedings{TableFormer2022,\n    author    = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},\n    title     = {TableFormer: Table Structure Understanding With Transformers},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2022},\n    pages     = {4614-4623},\n    doi = {https://doi.org/10.1109/CVPR52688.2022.00457}\n}\n"
  },
  {
    "s": "mcro:doclingmodels",
    "p": "mcro:hasUseCase",
    "o": "mcro:doclingmodels-UseCase"
  },
  {
    "s": "mcro:doclingmodels-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:doclingmodels-UseCase",
    "p": "prov:hasTextValue",
    "o": "identify the structure of the table, starting from an image of a table"
  },
  {
    "s": "mcro:e5largev2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:e5largev2",
    "p": "mcro:hasCitation",
    "o": "mcro:e5largev2-CitationInformationSection"
  },
  {
    "s": "mcro:e5largev2-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:e5largev2-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}"
  },
  {
    "s": "mcro:e5largev2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:e5largev2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:e5largev2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:e5largev2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model has 24 layers and the embedding size is 1024."
  },
  {
    "s": "mcro:e5largev2",
    "p": "mcro:hasUseCase",
    "o": "mcro:e5largev2-UseCaseInformationSection"
  },
  {
    "s": "mcro:e5largev2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:e5largev2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n               'query: summit define',\n               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n               \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\n\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\nmodel = AutoModel.from_pretrained('intfloat/e5-large-v2')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:CLIPViTH14LAION2B-ModelDetail"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B",
    "p": "mcro:hasUseCaseInformationSection",
    "o": "mcro:CLIPViTH14LAION2B-UseCase"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B",
    "p": "mcro:hasTrainingDataInformationSection",
    "o": "mcro:CLIPViTH14LAION2B-TrainingDetails"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:CLIPViTH14LAION2B-Evaluation"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B",
    "p": "mcro:hasCitation",
    "o": "mcro:CLIPViTH14LAION2B-Citation"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-ModelDetail",
    "p": "mcro:hasModelArchitectureInformationSection",
    "o": "mcro:CLIPViTH14LAION2B-ModelArchitecture"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip)."
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "CLIP ViT-H/14"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-UseCase",
    "p": "prov:hasTextValue",
    "o": "this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model."
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-TrainingDetails",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-TrainingDetails",
    "p": "prov:hasTextValue",
    "o": "This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/)."
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-Evaluation",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-Evaluation",
    "p": "prov:hasTextValue",
    "o": "The model achieves a 78.0 zero-shot top-1 accuracy on ImageNet-1k."
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CLIPViTH14LAION2B-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}\n\n@inproceedings{Radford2021LearningTV,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n  booktitle={ICML},\n  year={2021}\n}\n\n@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}"
  },
  {
    "s": "mcro:intfloatmultilinguale5base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:intfloatmultilinguale5base",
    "p": "mcro:hasCitation",
    "o": "mcro:intfloatmultilinguale5base-CitationInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}"
  },
  {
    "s": "mcro:intfloatmultilinguale5base",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:intfloatmultilinguale5base-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model has 12 layers and the embedding size is 768."
  },
  {
    "s": "mcro:intfloatmultilinguale5base",
    "p": "mcro:hasTrainingDataInformation",
    "o": "mcro:intfloatmultilinguale5base-TrainingDataInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base",
    "p": "mcro:hasBenchmarkResults",
    "o": "mcro:intfloatmultilinguale5base-BenchmarkResultsSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-BenchmarkResultsSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base",
    "p": "mcro:hasUseCase",
    "o": "mcro:intfloatmultilinguale5base-UseCaseInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base",
    "p": "mcro:hasLimitation",
    "o": "mcro:intfloatmultilinguale5base-LimitationInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:intfloatmultilinguale5base-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Long texts will be truncated to at most 512 tokens."
  },
  {
    "s": "mcro:NusaBertnerv13",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:NusaBertnerv13",
    "p": "mcro:hasModelDetail",
    "o": "mcro:NusaBertnerv13-ModelDetail"
  },
  {
    "s": "mcro:NusaBertnerv13-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:NusaBertnerv13-ModelDetail",
    "p": "mcro:hasModelArchitectureInformation",
    "o": "mcro:NusaBertnerv13-ModelArchitectureInformation"
  },
  {
    "s": "mcro:NusaBertnerv13-ModelArchitectureInformation",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:NusaBertnerv13-ModelArchitectureInformation",
    "p": "prov:hasTextValue",
    "o": "ModernBERT architecture"
  },
  {
    "s": "mcro:NusaBertnerv13-ModelDetail",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:NusaBertnerv13-IntendedUseCase"
  },
  {
    "s": "mcro:NusaBertnerv13-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:NusaBertnerv13-ModelDetail",
    "p": "mcro:hasDatasetInformation",
    "o": "mcro:NusaBertnerv13-DatasetInformation"
  },
  {
    "s": "mcro:NusaBertnerv13-DatasetInformation",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelDetail"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelArchitecture"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-License"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-IntendedUse"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-IntendedUse",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-IntendedUse",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases."
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm",
    "p": "mcro:hasDataset",
    "o": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-Dataset"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-Dataset",
    "p": "prov:hasTextValue",
    "o": "**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023."
  },
  {
    "s": "mcro:bertlargemodeluncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertlargemodeluncased",
    "p": "mcro:hasModelDetail",
    "o": "mcro:bertlargemodeluncased-ModelDetail"
  },
  {
    "s": "mcro:bertlargemodeluncased-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:bertlargemodeluncased-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:bertlargemodeluncased-Citation"
  },
  {
    "s": "mcro:bertlargemodeluncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertlargemodeluncased-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertlargemodeluncased-Arch"
  },
  {
    "s": "mcro:bertlargemodeluncased-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertlargemodeluncased-Arch",
    "p": "prov:hasTextValue",
    "o": "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion."
  },
  {
    "s": "mcro:bertlargemodeluncased",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertlargemodeluncased-UseCase"
  },
  {
    "s": "mcro:bertlargemodeluncased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertlargemodeluncased-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task."
  },
  {
    "s": "mcro:bertlargemodeluncased",
    "p": "mcro:hasTrainingData",
    "o": "mcro:bertlargemodeluncased-TrainingData"
  },
  {
    "s": "mcro:bertlargemodeluncased-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:bertlargemodeluncased-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders)."
  },
  {
    "s": "mcro:Qwen3-8B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen3-8B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Qwen3-8B-ModelDetail"
  },
  {
    "s": "mcro:Qwen3-8B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Qwen3-8B-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen3-8B-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen3-8B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen3-8B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:Qwen3-8B-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen3-8B-UseCase"
  },
  {
    "s": "mcro:Qwen3-8B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen3-8B-UseCase",
    "p": "prov:hasTextValue",
    "o": "general-purpose dialogue"
  },
  {
    "s": "mcro:Qwen3-8B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen3-8B",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen3-8B-Citation"
  },
  {
    "s": "mcro:Qwen3-8B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}"
  },
  {
    "s": "mcro:llama2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llama2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:llama2-ModelDetail"
  },
  {
    "s": "mcro:llama2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llama2-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:llama2-License"
  },
  {
    "s": "mcro:llama2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llama2-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)"
  },
  {
    "s": "mcro:llama2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:llama2-Citation"
  },
  {
    "s": "mcro:llama2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:llama2-Citation",
    "p": "prov:hasTextValue",
    "o": "\"Llama-2: Open Foundation and Fine-tuned Chat Models\"(arxiv.org/abs/2307.09288)"
  },
  {
    "s": "mcro:llama2",
    "p": "mcro:hasUseCaseInformation",
    "o": "mcro:llama2-UseCase"
  },
  {
    "s": "mcro:llama2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llama2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks."
  },
  {
    "s": "mcro:llama2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llama2-ModelArchitecture"
  },
  {
    "s": "mcro:llama2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:allrobertalargev1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:allrobertalargev1",
    "p": "mcro:hasUseCase",
    "o": "mcro:allrobertalargev1-UseCaseInformationSection"
  },
  {
    "s": "mcro:allrobertalargev1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:allrobertalargev1-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks."
  },
  {
    "s": "mcro:allrobertalargev1",
    "p": "mcro:hasTrainingData",
    "o": "mcro:allrobertalargev1-TrainingDataInformationSection"
  },
  {
    "s": "mcro:allrobertalargev1-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:allrobertalargev1-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file."
  },
  {
    "s": "mcro:allrobertalargev1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:allrobertalargev1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allrobertalargev1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:allrobertalargev1-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "We used the pretrained [`roberta-large`](https://huggingface.co/roberta-large) model and fine-tuned in on a \n1B sentence pairs dataset."
  },
  {
    "s": "mcro:allrobertalargev1",
    "p": "mcro:hasBackground",
    "o": "mcro:allrobertalargev1-BackgroundInformationSection"
  },
  {
    "s": "mcro:allrobertalargev1-BackgroundInformationSection",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:allrobertalargev1-BackgroundInformationSection",
    "p": "prov:hasTextValue",
    "o": "The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`roberta-large`](https://huggingface.co/roberta-large) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developped this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developped this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks."
  },
  {
    "s": "mcro:Qwen257BInstruct1M",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen257BInstruct1M",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen257BInstruct1M-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen257BInstruct1M-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct1M-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:Qwen257BInstruct1M",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen257BInstruct1M-Citation"
  },
  {
    "s": "mcro:Qwen257BInstruct1M-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct1M-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5-1m,\n    title = {Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens},\n    url = {https://qwenlm.github.io/blog/qwen2.5-1m/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{qwen2.5,\n      title={Qwen2.5-1M Technical Report},\n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\n      journal={arXiv preprint arXiv:2501.15383},\n      year={2025}\n}"
  },
  {
    "s": "mcro:Qwen257BInstruct1M",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen257BInstruct1M-UseCase"
  },
  {
    "s": "mcro:Qwen257BInstruct1M-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct1M-UseCase",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored",
    "p": "mcro:hasLicense",
    "o": "mcro:OrengutengLlama38BLexiUncensored-License"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored-License",
    "p": "prov:hasTextValue",
    "o": "META LLAMA 3 COMMUNITY LICENSE AGREEMENT"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored",
    "p": "mcro:hasUseCase",
    "o": "mcro:OrengutengLlama38BLexiUncensored-UseCase"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored-UseCase",
    "p": "prov:hasTextValue",
    "o": "Lexi is uncensored, which makes the model compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones.\n\nYou are responsible for any content you create using this model. Please use it responsibly."
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:OrengutengLlama38BLexiUncensored-QuantativeAnalysis"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "|             Metric              |Value|\n|---------------------------------|----:|\n|Avg.                             |66.18|\n|AI2 Reasoning Challenge (25-Shot)|59.56|\n|HellaSwag (10-Shot)              |77.88|\n|MMLU (5-Shot)                    |67.68|\n|TruthfulQA (0-shot)              |47.72|\n|Winogrande (5-shot)              |75.85|\n|GSM8k (5-shot)                   |68.39|"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:OrengutengLlama38BLexiUncensored-ModelArchitecture"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:OrengutengLlama38BLexiUncensored-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama-3-8b-Instruct"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase",
    "p": "mcro:hasCitation",
    "o": "mcro:IDEAResearchgroundingdinobase-CitationInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:IDEAResearchgroundingdinobase-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "extends a closed-set object detection model with a text encoder, enabling open-set object detection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase",
    "p": "mcro:hasUseCase",
    "o": "mcro:IDEAResearchgroundingdinobase-UseCaseInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinobase-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "zero-shot object detection (the task of detecting things in an image out-of-the-box without labeled data)"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase",
    "p": "mcro:hasCitation",
    "o": "mcro:AlibabaNLPgtemultilingualbase-Citation"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\n  pages={1393--1412},\n  year={2024}\n}"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:AlibabaNLPgtemultilingualbase-Architecture"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase-Architecture",
    "p": "prov:hasTextValue",
    "o": "encoder-only transformers architecture"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:AlibabaNLPgtemultilingualbase-UseCase"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtemultilingualbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "multilingual retrieval, cross-lingual retrieval, long text retrieval, and general text representation"
  },
  {
    "s": "mcro:stablediffusionv2-1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:stablediffusionv2-1",
    "p": "mcro:hasModelDetail",
    "o": "mcro:stablediffusionv2-1-ModelDetail"
  },
  {
    "s": "mcro:stablediffusionv2-1-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:stablediffusionv2-1-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:stablediffusionv2-1-License"
  },
  {
    "s": "mcro:stablediffusionv2-1-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:stablediffusionv2-1-License",
    "p": "prov:hasTextValue",
    "o": "CreativeML Open RAIL++-M License"
  },
  {
    "s": "mcro:stablediffusionv2-1-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:stablediffusionv2-1-Citation"
  },
  {
    "s": "mcro:stablediffusionv2-1-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:stablediffusionv2-1-Citation",
    "p": "prov:hasTextValue",
    "o": "@InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }"
  },
  {
    "s": "mcro:stablediffusionv2-1-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:stablediffusionv2-1-ModelArchitecture"
  },
  {
    "s": "mcro:stablediffusionv2-1-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:stablediffusionv2-1-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generation model"
  },
  {
    "s": "mcro:stablediffusionv2-1",
    "p": "mcro:hasUseCase",
    "o": "mcro:stablediffusionv2-1-UseCase"
  },
  {
    "s": "mcro:stablediffusionv2-1-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:stablediffusionv2-1-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended for research purposes only."
  },
  {
    "s": "mcro:stablediffusionv2-1",
    "p": "mcro:hasDataset",
    "o": "mcro:stablediffusionv2-1-Dataset"
  },
  {
    "s": "mcro:stablediffusionv2-1-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:stablediffusionv2-1-Dataset",
    "p": "prov:hasTextValue",
    "o": "LAION-5B and subsets"
  },
  {
    "s": "mcro:distilrobertabase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilrobertabase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:distilrobertabase-ModelDetail"
  },
  {
    "s": "mcro:distilrobertabase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:distilrobertabase-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:distilrobertabase-License"
  },
  {
    "s": "mcro:distilrobertabase-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilrobertabase-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:distilrobertabase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:distilrobertabase-Citation"
  },
  {
    "s": "mcro:distilrobertabase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:distilrobertabase-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}"
  },
  {
    "s": "mcro:distilrobertabase",
    "p": "mcro:hasUseCase",
    "o": "mcro:distilrobertabase-UseCase"
  },
  {
    "s": "mcro:distilrobertabase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilrobertabase",
    "p": "mcro:hasConsideration",
    "o": "mcro:distilrobertabase-Consideration"
  },
  {
    "s": "mcro:distilrobertabase-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:distilrobertabase",
    "p": "mcro:hasModelParameter",
    "o": "mcro:distilrobertabase-ModelParameter"
  },
  {
    "s": "mcro:distilrobertabase-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:distilrobertabase",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:distilrobertabase-QuantativeAnalysis"
  },
  {
    "s": "mcro:distilrobertabase-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:distilrobertabase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:distilrobertabase-ModelArchitecture"
  },
  {
    "s": "mcro:distilrobertabase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilrobertabase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based language model"
  },
  {
    "s": "mcro:distilrobertabase",
    "p": "mcro:hasDataset",
    "o": "mcro:distilrobertabase-Dataset"
  },
  {
    "s": "mcro:distilrobertabase-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:VitTinyVitSmall",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:VitTinyVitSmall",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:VitTinyVitSmall-ModelArchitecture"
  },
  {
    "s": "mcro:VitTinyVitSmall-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:VitTinyVitSmall-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "vit-tiny and vit-small"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15",
    "p": "mcro:hasModelDetail",
    "o": "mcro:AlibabaNLPgtebaseenv15-ModelDetail"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:AlibabaNLPgtebaseenv15-Citation"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:AlibabaNLPgtebaseenv15-License"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15",
    "p": "mcro:hasModelParameter",
    "o": "mcro:AlibabaNLPgtebaseenv15-ModelParameter"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15",
    "p": "mcro:hasUseCase",
    "o": "mcro:AlibabaNLPgtebaseenv15-UseCase"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15",
    "p": "mcro:hasConsideration",
    "o": "mcro:AlibabaNLPgtebaseenv15-Consideration"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:AlibabaNLPgtebaseenv15-QuantativeAnalysis"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15",
    "p": "mcro:hasDataset",
    "o": "mcro:AlibabaNLPgtebaseenv15-Dataset"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-Dataset",
    "p": "mcro:hasTrainingData",
    "o": "mcro:AlibabaNLPgtebaseenv15-TrainingData"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:AlibabaNLPgtebaseenv15-ModelArchitecture"
  },
  {
    "s": "mcro:AlibabaNLPgtebaseenv15-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mobilevitsmall",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mobilevitsmall",
    "p": "mcro:hasModelDetail",
    "o": "mcro:mobilevitsmall-ModelDetail"
  },
  {
    "s": "mcro:mobilevitsmall-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:mobilevitsmall-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:mobilevitsmall-License"
  },
  {
    "s": "mcro:mobilevitsmall-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mobilevitsmall-License",
    "p": "prov:hasTextValue",
    "o": "Apple sample code license"
  },
  {
    "s": "mcro:mobilevitsmall-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mobilevitsmall-Architecture"
  },
  {
    "s": "mcro:mobilevitsmall-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mobilevitsmall-Architecture",
    "p": "prov:hasTextValue",
    "o": "MobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers."
  },
  {
    "s": "mcro:mobilevitsmall",
    "p": "mcro:hasUseCase",
    "o": "mcro:mobilevitsmall-UseCase"
  },
  {
    "s": "mcro:mobilevitsmall-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mobilevitsmall-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for image classification."
  },
  {
    "s": "mcro:mobilevitsmall",
    "p": "mcro:hasTrainingData",
    "o": "mcro:mobilevitsmall-TrainingData"
  },
  {
    "s": "mcro:mobilevitsmall-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:mobilevitsmall-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The MobileViT model was pretrained on ImageNet-1k, a dataset consisting of 1 million images and 1,000 classes."
  },
  {
    "s": "mcro:mobilevitsmall",
    "p": "mcro:hasCitation",
    "o": "mcro:mobilevitsmall-Citation"
  },
  {
    "s": "mcro:mobilevitsmall-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mobilevitsmall-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{vision-transformer,\ntitle = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\nauthor = {Sachin Mehta and Mohammad Rastegari},\nyear = {2022},\nURL = {https://arxiv.org/abs/2110.02178}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish",
    "p": "mcro:hasModelDetail",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelDetail"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-License"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53polish-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gtelargeenv15",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gtelargeenv15",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gtelargeenv15-ModelArchitecture"
  },
  {
    "s": "mcro:gtelargeenv15-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gtelargeenv15-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformer++"
  },
  {
    "s": "mcro:gtelargeenv15",
    "p": "mcro:hasCitation",
    "o": "mcro:gtelargeenv15-Citation"
  },
  {
    "s": "mcro:gtelargeenv15-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gtelargeenv15-Citation",
    "p": "prov:hasTextValue",
    "o": "mGTE: Generalized Long-Context Text Representation and Reranking\nModels for Multilingual Text Retrieval"
  },
  {
    "s": "mcro:gtelargeenv15",
    "p": "mcro:hasUseCase",
    "o": "mcro:gtelargeenv15-UseCase"
  },
  {
    "s": "mcro:gtelargeenv15-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gtelargeenv15-UseCase",
    "p": "prov:hasTextValue",
    "o": "Text Embeddings"
  },
  {
    "s": "mcro:gemma3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gemma3-ModelDetail"
  },
  {
    "s": "mcro:gemma3-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gemma3-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:gemma3-Citation"
  },
  {
    "s": "mcro:gemma3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gemma3-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
  },
  {
    "s": "mcro:gemma3-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:gemma3-Dataset"
  },
  {
    "s": "mcro:gemma3-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:gemma3-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gemma3-ModelArchitecture"
  },
  {
    "s": "mcro:gemma3-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gemma3-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:gemma3-UseCase"
  },
  {
    "s": "mcro:gemma3-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gemma3-Dataset",
    "p": "prov:hasTextValue",
    "o": "These models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats."
  },
  {
    "s": "mcro:gemma3-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*"
  },
  {
    "s": "mcro:gemma3-UseCase",
    "p": "prov:hasTextValue",
    "o": "Open vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics."
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation",
    "p": "mcro:hasLicense",
    "o": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-LicenseInformationSection"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation",
    "p": "mcro:hasCitation",
    "o": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-CitationInformationSection"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{zheng2024birefnet,\n  title={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},\n  author={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},\n  journal={CAAI Artificial Intelligence Research},\n  volume = {3},\n  pages = {9150038},\n  year={2024}\n}"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation",
    "p": "mcro:hasDataset",
    "o": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-DatasetInformationSection"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "DIS-TR and validated on DIS-TEs and DIS-VD"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation",
    "p": "mcro:hasArchitecture",
    "o": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BilateralReferenceForHighResolutionDichotomousImageSegmentation-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex",
    "p": "mcro:hasUseCase",
    "o": "mcro:siglip2so400mpatch16naflex-UseCaseInformationSection"
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for tasks like zero-shot image classification and\nimage-text retrieval, or as a vision encoder for VLMs (and other vision tasks)."
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex",
    "p": "mcro:hasTrainingData",
    "o": "mcro:siglip2so400mpatch16naflex-TrainingDataInformationSection"
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "SigLIP 2 is pre-trained on the WebLI dataset [(Chen et al., 2023)](https://arxiv.org/abs/2209.06794)."
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex",
    "p": "mcro:hasCitation",
    "o": "mcro:siglip2so400mpatch16naflex-CitationInformationSection"
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:siglip2so400mpatch16naflex-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{tschannen2025siglip2multilingualvisionlanguage,\n      title={SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features}, \n      author={Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier H\u00e9naff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai},\n      year={2025},\n      eprint={2502.14786},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2502.14786}, \n}"
  },
  {
    "s": "mcro:bertMiniatures",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertMiniatures",
    "p": "mcro:hasCitation",
    "o": "mcro:bertMiniatures-Citation"
  },
  {
    "s": "mcro:bertMiniatures-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertMiniatures-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{turc2019,\n  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1908.08962v2 },\n  year={2019}\n}"
  },
  {
    "s": "mcro:bertMiniatures",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertMiniatures-Architecture"
  },
  {
    "s": "mcro:bertMiniatures-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertMiniatures-Architecture",
    "p": "prov:hasTextValue",
    "o": "standard BERT recipe (including model architecture and training objective)"
  },
  {
    "s": "mcro:bertMiniatures",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertMiniatures-UseCase"
  },
  {
    "s": "mcro:bertMiniatures-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertMiniatures-UseCase",
    "p": "prov:hasTextValue",
    "o": "smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher."
  },
  {
    "s": "mcro:googlest5v11",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:googlest5v11",
    "p": "mcro:hasDataset",
    "o": "mcro:googlest5v11-Dataset"
  },
  {
    "s": "mcro:googlest5v11-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:googlest5v11-Dataset",
    "p": "prov:hasTextValue",
    "o": "Pre-trained on C4 only without mixing in the downstream tasks."
  },
  {
    "s": "mcro:googlest5v11",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:googlest5v11-Architecture"
  },
  {
    "s": "mcro:googlest5v11-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:googlest5v11-Architecture",
    "p": "prov:hasTextValue",
    "o": "GEGLU activation in feed-forward hidden layer, rather than ReLU"
  },
  {
    "s": "mcro:googlest5v11-Architecture",
    "p": "prov:hasTextValue",
    "o": "no parameter sharing between embedding and classifier layer"
  },
  {
    "s": "mcro:googlest5v11",
    "p": "mcro:hasCitation",
    "o": "mcro:googlest5v11-Citation"
  },
  {
    "s": "mcro:googlest5v11-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:googlest5v11-Citation",
    "p": "prov:hasTextValue",
    "o": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
  },
  {
    "s": "mcro:googlest5v11",
    "p": "mcro:hasUseCase",
    "o": "mcro:googlest5v11-UseCase"
  },
  {
    "s": "mcro:googlest5v11-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:googlest5v11-UseCase",
    "p": "prov:hasTextValue",
    "o": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftPhi35visioninstruct-UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) General image understanding\n4) Optical character recognition\n5) Chart and table understanding\n6) Multiple image comparison\n7) Multi-image or video clip summarization\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n### Use Case Considerations\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.***"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:microsoftPhi35visioninstruct-ModelDetailSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-ModelDetailSection",
    "p": "prov:hasTextValue",
    "o": "Phi-3.5-vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n\ud83c\udfe1 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n\ud83d\udcf0 [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\n\ud83d\udcd6 [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\n\ud83d\udc69\u200d\ud83c\udf73 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n\ud83d\udda5\ufe0f [Try It](https://aka.ms/try-phi3.5vision) <br>\n\n**Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct",
    "p": "mcro:hasLicense",
    "o": "mcro:microsoftPhi35visioninstruct-LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is licensed under the [MIT license](./LICENSE)."
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct",
    "p": "mcro:hasTrainingData",
    "o": "mcro:microsoftPhi35visioninstruct-TrainingDataInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our training data includes a wide variety of sources, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data and code;\n2) selected high-quality image-text interleave data;\n3) newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides, newly created multi-image and video data, e.g., short video clips/pair of two similar images;\n4) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219)."
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftPhi35visioninstruct-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35visioninstruct-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "**Architecture:** Phi-3.5-vision has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.<br>\n**Inputs:** Text and Image. It\u2019s best suited for prompts using the chat format.<br>\n**Context length:** 128K tokens<br>\n**GPUs:** 256 A100-80G<br>\n**Training time:** 6 days<br>\n**Training data:** 500B tokens (vision tokens + text tokens)<br>\n**Outputs:** Generated text in response to the input<br>\n**Dates:** Trained between July and August 2024<br>\n**Status:** This is a static model trained on an offline text dataset with cutoff date March 15, 2024. Future versions of the tuned models may be released as we improve models.<br>\n**Release date:** August 2024<br>"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-dutch,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {D}utch},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-dutch}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset",
    "p": "prov:hasTextValue",
    "o": "Common Voice 6.1"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2",
    "p": "prov:hasTextValue",
    "o": "CSS10"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53dutch-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Dutch"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-arabic,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {A}rabic},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-arabic}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset",
    "p": "prov:hasTextValue",
    "o": "Common Voice 6.1"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset-2"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset-2",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-Dataset-2",
    "p": "prov:hasTextValue",
    "o": "Arabic Speech Corpus"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Fine-tuned XLSR-53 large model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53arabic-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Arabic"
  },
  {
    "s": "mcro:fnetbasemodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:fnetbasemodel",
    "p": "mcro:hasModelDetail",
    "o": "mcro:fnetbasemodel-ModelDetail"
  },
  {
    "s": "mcro:fnetbasemodel-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:fnetbasemodel-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:fnetbasemodel-Citation"
  },
  {
    "s": "mcro:fnetbasemodel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:fnetbasemodel-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2105-03824,\n  author    = {James Lee{-}Thorp and\n               Joshua Ainslie and\n               Ilya Eckstein and\n               Santiago Onta{\\~{n}}{\\'{o}}n},\n  title     = {FNet: Mixing Tokens with Fourier Transforms},\n  journal   = {CoRR},\n  volume    = {abs/2105.03824},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.03824},\n  archivePrefix = {arXiv},\n  eprint    = {2105.03824},\n  timestamp = {Fri, 14 May 2021 12:13:30 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-03824.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:fnetbasemodel",
    "p": "mcro:hasModelParameter",
    "o": "mcro:fnetbasemodel-ModelParameter"
  },
  {
    "s": "mcro:fnetbasemodel-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:fnetbasemodel-ModelParameter",
    "p": "prov:hasTextValue",
    "o": "The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000."
  },
  {
    "s": "mcro:fnetbasemodel",
    "p": "mcro:hasUseCase",
    "o": "mcro:fnetbasemodel-UseCase"
  },
  {
    "s": "mcro:fnetbasemodel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:fnetbasemodel-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task."
  },
  {
    "s": "mcro:fnetbasemodel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:fnetbasemodel-TrainingData"
  },
  {
    "s": "mcro:fnetbasemodel-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:fnetbasemodel-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The FNet model was pretrained on [C4](https://huggingface.co/datasets/c4), a cleaned version of the Common Crawl dataset."
  },
  {
    "s": "mcro:fnetbasemodel",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:fnetbasemodel-QuantativeAnalysis"
  },
  {
    "s": "mcro:fnetbasemodel-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:fnetbasemodel-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "FNet-base was fine-tuned and evaluated on the validation data of the [GLUE benchamrk](https://huggingface.co/datasets/glue)."
  },
  {
    "s": "mcro:fnetbasemodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:fnetbasemodel-ModelArchitecture"
  },
  {
    "s": "mcro:fnetbasemodel-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:fnetbasemodel-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "FNet is a transformers model with attention replaced with fourier transforms."
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Distilbertbasemultilingualcasednerhrl-ModelDetail"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Distilbertbasemultilingualcasednerhrl-ModelArchitecture"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "distilbert-base-multilingual-cased"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl",
    "p": "mcro:hasUseCase",
    "o": "mcro:Distilbertbasemultilingualcasednerhrl-UseCase"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl",
    "p": "mcro:hasDataset",
    "o": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "prov:hasTextValue",
    "o": "ANERcorp"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "prov:hasTextValue",
    "o": "conll 2003"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "prov:hasTextValue",
    "o": "conll 2002"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "prov:hasTextValue",
    "o": "Europeana Newspapers"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "prov:hasTextValue",
    "o": "Italian I-CAB"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "prov:hasTextValue",
    "o": "Latvian NER"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "prov:hasTextValue",
    "o": "Paramopama + Second Harem"
  },
  {
    "s": "mcro:Distilbertbasemultilingualcasednerhrl-Dataset",
    "p": "prov:hasTextValue",
    "o": "MSRA"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:efficientnet_b3.ra2_in1k-ModelDetail"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:efficientnet_b3.ra2_in1k-Dataset"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b3.ra2_in1k-Citation1"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Citation1",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{tan2019efficientnet,\n  title={Efficientnet: Rethinking model scaling for convolutional neural networks},\n  author={Tan, Mingxing and Le, Quoc},\n  booktitle={International conference on machine learning},\n  pages={6105--6114},\n  year={2019},\n  organization={PMLR}\n}"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b3.ra2_in1k-Citation2"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b3.ra2_in1k-Citation3"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Citation3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Citation3",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:efficientnet_b3.ra2_in1k-Arch"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-Arch",
    "p": "prov:hasTextValue",
    "o": "EfficientNet"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:efficientnet_b3.ra2_in1k-UseCase"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:efficientnet_b3.ra2_in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:potion-base-8M",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:potion-base-8M",
    "p": "mcro:hasCitation",
    "o": "mcro:potion-base-8M-Citation"
  },
  {
    "s": "mcro:potion-base-8M-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:potion-base-8M-Citation",
    "p": "prov:hasTextValue",
    "o": "@software{minishlab2024model2vec,\n  authors = {Stephan Tulkens, Thomas van Dongen},\n  title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n  year = {2024},\n  url = {https://github.com/MinishLab/model2vec},\n}"
  },
  {
    "s": "mcro:potion-base-8M",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:potion-base-8M-ModelArchitecture"
  },
  {
    "s": "mcro:potion-base-8M-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:potion-base-8M-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "It is a distilled version of the [baai/bge-base-en-v1.5](https://huggingface.co/baai/bge-base-en-v1.5) Sentence Transformer. It uses static embeddings, allowing text embeddings to be computed orders of magnitude faster on both GPU and CPU."
  },
  {
    "s": "mcro:potion-base-8M",
    "p": "mcro:hasUseCase",
    "o": "mcro:potion-base-8M-UseCase"
  },
  {
    "s": "mcro:potion-base-8M-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:potion-base-8M-UseCase",
    "p": "prov:hasTextValue",
    "o": "It is designed for applications where computational resources are limited or where real-time performance is critical."
  },
  {
    "s": "mcro:metallamaLlama31",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metallamaLlama31",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metallamaLlama31-ModelDetail"
  },
  {
    "s": "mcro:metallamaLlama31-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metallamaLlama31-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:metallamaLlama31-License"
  },
  {
    "s": "mcro:metallamaLlama31-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metallamaLlama31-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)"
  },
  {
    "s": "mcro:metallamaLlama31-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metallamaLlama31-ModelArchitecture"
  },
  {
    "s": "mcro:metallamaLlama31-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metallamaLlama31-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metallamaLlama31",
    "p": "mcro:hasUseCase",
    "o": "mcro:metallamaLlama31-UseCase"
  },
  {
    "s": "mcro:metallamaLlama31-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metallamaLlama31-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases."
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0",
    "p": "mcro:hasCitation",
    "o": "mcro:avsolatorioGISTsmallEmbeddingv0-CitationInformationSection"
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{solatorio2024gistembed,\n    title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},\n    author={Aivin V. Solatorio},\n    journal={arXiv preprint arXiv:2402.16829},\n    year={2024},\n    URL={https://arxiv.org/abs/2402.16829}\n    eprint={2402.16829},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}"
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0",
    "p": "mcro:hasDataset",
    "o": "mcro:avsolatorioGISTsmallEmbeddingv0-DatasetInformationSection"
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "The dataset used is a compilation of the MEDI and MTEB Classification training datasets. Third-party datasets may be subject to additional terms and conditions under their associated licenses. A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:\n\n- Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets)\n- Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb\n\nThe dataset contains a `task_type` key, which can be used to select only the mteb classification tasks (prefixed with `mteb_`).\n\nThe **MEDI Dataset** is published in the following paper: [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741).\n\nThe MTEB Benchmark results of the GIST embedding model, compared with the base model, suggest that the fine-tuning dataset has perturbed the model considerably, which resulted in significant improvements in certain tasks while adversely degrading performance in some.\n\nThe retrieval performance for the TRECCOVID task is of note. The fine-tuning dataset does not contain significant knowledge about COVID-19, which could have caused the observed performance degradation. We found some evidence, detailed in the paper, that thematic coverage of the fine-tuning data can affect downstream performance."
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:avsolatorioGISTsmallEmbeddingv0-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:avsolatorioGISTsmallEmbeddingv0-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is fine-tuned on top of the [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) using the [MEDI dataset](https://github.com/xlang-ai/instructor-embedding.git) augmented with mined triplets from the [MTEB Classification](https://huggingface.co/mteb) training dataset (excluding data from the Amazon Polarity Classification task).\n\nThe model does not require any instruction for generating embeddings. This means that queries for retrieval tasks can be directly encoded without crafting instructions.\n\nTechnical paper: [GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning](https://arxiv.org/abs/2402.16829)"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:jhartmannemotionenglishdistilrobertabase-ModelDetail"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:jhartmannemotionenglishdistilrobertabase-UseCase"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-ModelDetail",
    "p": "mcro:hasReference",
    "o": "mcro:jhartmannemotionenglishdistilrobertabase-Reference"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-Reference",
    "p": "prov:hasTextValue",
    "o": "Jochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022."
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:jhartmannemotionenglishdistilrobertabase-Citation"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{hartmann2022emotionenglish,\n  author={Hartmann, Jochen},\n  title={Emotion English DistilRoBERTa-base},\n  year={2022},\n  howpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},\n}"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jhartmannemotionenglishdistilrobertabase-Architecture"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jhartmannemotionenglishdistilrobertabase-Architecture",
    "p": "prov:hasTextValue",
    "o": "DistilRoBERTa-base"
  },
  {
    "s": "mcro:facebookencodec24khz",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookencodec24khz",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookencodec24khz-ModelDetailSection"
  },
  {
    "s": "mcro:facebookencodec24khz-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookencodec24khz-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookencodec24khz-CitationInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{d\u00e9fossez2022high,\n      title={High Fidelity Neural Audio Compression}, \n      author={Alexandre D\u00e9fossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},\n      year={2022},\n      eprint={2210.13438},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS}\n}"
  },
  {
    "s": "mcro:facebookencodec24khz-ModelDetailSection",
    "p": "mcro:hasArchitecture",
    "o": "mcro:facebookencodec24khz-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Audio Codec"
  },
  {
    "s": "mcro:facebookencodec24khz-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:facebookencodec24khz-LicenseInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookencodec24khz-UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz",
    "p": "mcro:hasDataset",
    "o": "mcro:facebookencodec24khz-DatasetInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-DatasetInformationSection",
    "p": "mcro:hasTrainingData",
    "o": "mcro:facebookencodec24khz-TrainingDataInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:facebookencodec24khz-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "DNS Challenge 4"
  },
  {
    "s": "mcro:facebookencodec24khz-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Common Voice"
  },
  {
    "s": "mcro:facebookencodec24khz-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "AudioSet"
  },
  {
    "s": "mcro:facebookencodec24khz-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "FSD50K"
  },
  {
    "s": "mcro:facebookencodec24khz-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Jamendo dataset"
  },
  {
    "s": "mcro:SWividF5TTS",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:SWividF5TTS",
    "p": "mcro:hasCitation",
    "o": "mcro:SWividF5TTS-Citation"
  },
  {
    "s": "mcro:SWividF5TTS-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:SWividF5TTS-Citation",
    "p": "prov:hasTextValue",
    "o": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching"
  },
  {
    "s": "mcro:englishnerinflairfastmodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:englishnerinflairfastmodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:englishnerinflairfastmodel-ModelArchitecture"
  },
  {
    "s": "mcro:englishnerinflairfastmodel-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:englishnerinflairfastmodel-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Flair embeddings and LSTM-CRF"
  },
  {
    "s": "mcro:englishnerinflairfastmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:englishnerinflairfastmodel-Citation"
  },
  {
    "s": "mcro:englishnerinflairfastmodel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:englishnerinflairfastmodel-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}"
  },
  {
    "s": "mcro:englishnerinflairfastmodel",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:englishnerinflairfastmodel-PerformanceMetric"
  },
  {
    "s": "mcro:englishnerinflairfastmodel-PerformanceMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:englishnerinflairfastmodel-PerformanceMetric",
    "p": "prov:hasTextValue",
    "o": "92,92"
  },
  {
    "s": "mcro:englishnerinflairfastmodel",
    "p": "mcro:hasUseCase",
    "o": "mcro:englishnerinflairfastmodel-UseCase"
  },
  {
    "s": "mcro:englishnerinflairfastmodel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:englishnerinflairfastmodel-UseCase",
    "p": "prov:hasTextValue",
    "o": "English NER"
  },
  {
    "s": "mcro:metaLlama3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metaLlama3",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metaLlama3-ModelDetail"
  },
  {
    "s": "mcro:metaLlama3-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metaLlama3-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metaLlama3-ModelArchitecture"
  },
  {
    "s": "mcro:metaLlama3-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metaLlama3-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metaLlama3-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:metaLlama3-License"
  },
  {
    "s": "mcro:metaLlama3-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metaLlama3-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)"
  },
  {
    "s": "mcro:metaLlama3",
    "p": "mcro:hasUseCase",
    "o": "mcro:metaLlama3-UseCase"
  },
  {
    "s": "mcro:metaLlama3-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metaLlama3-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks."
  },
  {
    "s": "mcro:metaLlama3-UseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:metaLlama3-OutOfScopeUseCase"
  },
  {
    "s": "mcro:metaLlama3-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:metaLlama3-OutOfScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**."
  },
  {
    "s": "mcro:metaLlama3",
    "p": "mcro:hasCitation",
    "o": "mcro:metaLlama3-Citation"
  },
  {
    "s": "mcro:metaLlama3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:metaLlama3-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}"
  },
  {
    "s": "mcro:metaLlama3",
    "p": "mcro:hasDataset",
    "o": "mcro:metaLlama3-Dataset"
  },
  {
    "s": "mcro:metaLlama3-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:metaLlama3-Dataset",
    "p": "prov:hasTextValue",
    "o": "Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data."
  },
  {
    "s": "mcro:kluerobertabase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:kluerobertabase",
    "p": "mcro:hasCitation",
    "o": "mcro:kluerobertabase-CitationInformationSection"
  },
  {
    "s": "mcro:kluerobertabase-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:kluerobertabase-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{park2021klue,\n      title={KLUE: Korean Language Understanding Evaluation},\n      author={Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n      year={2021},\n      eprint={2105.09680},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:kluerobertabase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:kluerobertabase-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:kluerobertabase-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:kluerobertabase-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Pretrained RoBERTa Model on Korean Language"
  },
  {
    "s": "mcro:kluerobertabase",
    "p": "mcro:hasUseCase",
    "o": "mcro:kluerobertabase-UseCaseInformationSection"
  },
  {
    "s": "mcro:kluerobertabase-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:kluerobertabase-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "_NOTE:_ Use `BertTokenizer` instead of RobertaTokenizer. (`AutoTokenizer` will load `BertTokenizer`)"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metallamaLlama3370BInstruct-ModelDetail"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:metallamaLlama3370BInstruct-License"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license, the Llama 3.3 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE)"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metallamaLlama3370BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:metallamaLlama3370BInstruct-UseCase"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases."
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-UseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:metallamaLlama3370BInstruct-OutOfScopeUseCase"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-OutOfScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct",
    "p": "mcro:hasTrainingData",
    "o": "mcro:metallamaLlama3370BInstruct-TrainingData"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:metallamaLlama3370BInstruct-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples."
  },
  {
    "s": "mcro:nvidiatitanetlargeenus",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:nvidiatitanetlargeenus-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "TitaNet model is a depth-wise separable conv1D model [1] for Speaker Verification and diarization tasks."
  },
  {
    "s": "mcro:nvidiatitanetlargeenus",
    "p": "mcro:hasDataset",
    "o": "mcro:nvidiatitanetlargeenus-DatasetInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "All the models in this collection are trained on a composite dataset comprising several thousand hours of English speech:\n\n- Voxceleb-1\n- Voxceleb-2\n- Fisher\n- Switchboard\n- Librispeech\n- SRE (2004-2010)"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus",
    "p": "mcro:hasLicense",
    "o": "mcro:nvidiatitanetlargeenus-LicenseInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "License to use this model is covered by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). By downloading the public and release version of the model, you accept the terms and conditions of the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license."
  },
  {
    "s": "mcro:nvidiatitanetlargeenus",
    "p": "mcro:hasCitation",
    "o": "mcro:nvidiatitanetlargeenus-CitationInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "[1] [TitaNet: Neural Model for Speaker Representation with 1D Depth-wise Separable convolutions and global context](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746806) \n[2] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus",
    "p": "mcro:hasUseCase",
    "o": "mcro:nvidiatitanetlargeenus-UseCaseInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model extracts speaker embeddings from given speech, which is the backbone for speaker verification and diarization tasks."
  },
  {
    "s": "mcro:nvidiatitanetlargeenus",
    "p": "mcro:hasLimitation",
    "o": "mcro:nvidiatitanetlargeenus-LimitationInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:nvidiatitanetlargeenus-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model is trained on both telephonic and non-telephonic speech from voxceleb datasets, Fisher and switch board. If your domain of data differs from trained data or doesnot show relatively good performance consider finetuning for that speech domain."
  },
  {
    "s": "mcro:Salesforceblip2opt27b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Salesforceblip2opt27b",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Salesforceblip2opt27b-ModelDetail"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Salesforceblip2opt27b-ModelArchitecture"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters).\nIt was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository.\n\nDisclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nBLIP-2 consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model.\n\nThe authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen\nwhile training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of \"query tokens\" to query embeddings,\nwhich bridge the gap between the embedding space of the image encoder and the large language model.\n\nThe goal for the model is simply to predict the next text token, giving the query embeddings and the previous text.\n\nThis allows the model to be used for tasks like:\n\n- image captioning\n- visual question answering (VQA)\n- chat-like conversations by feeding the image and the previous conversation as prompt to the model"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:Salesforceblip2opt27b-UseCase"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for conditional text generation given an image and optional text. See the model hub to look for\nfine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:Salesforceblip2opt27b",
    "p": "mcro:hasConsideration",
    "o": "mcro:Salesforceblip2opt27b-Consideration"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-Consideration",
    "p": "prov:hasTextValue",
    "o": "BLIP2-OPT uses off-the-shelf OPT as the language model. It inherits the same risks and limitations as mentioned in Meta's model card.\n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models.\n>\nBLIP2 is fine-tuned on image-text datasets (e.g. [LAION](https://laion.ai/blog/laion-400-open-dataset/) ) collected from the internet.  As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\nBLIP2 has not been tested in real world applications. It should not be directly deployed in any applications. Researchers should first carefully assess the safety and fairness of the model in relation to the specific context they\u2019re being deployed within."
  },
  {
    "s": "mcro:Salesforceblip2opt27b",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:Salesforceblip2opt27b-EthicalConsideration"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-EthicalConsideration",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:Salesforceblip2opt27b-EthicalConsideration",
    "p": "prov:hasTextValue",
    "o": "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people\u2019s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP."
  },
  {
    "s": "mcro:Qwen3-14B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen3-14B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen3-14B-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen3-14B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen3-14B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:Qwen3-14B",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen3-14B-UseCase"
  },
  {
    "s": "mcro:Qwen3-14B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "mcro:hasCitation",
    "o": "mcro:twitterrobertabasesentiment-Citation"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-Citation",
    "p": "prov:hasTextValue",
    "o": "_TweetEval_ (Findings of EMNLP 2020)"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:twitterrobertabasesentiment-ModelArchitecture"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "roBERTa-base"
  },
  {
    "s": "mcro:twitterrobertabasesentiment",
    "p": "mcro:hasUseCase",
    "o": "mcro:twitterrobertabasesentiment-UseCase"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:twitterrobertabasesentiment-UseCase",
    "p": "prov:hasTextValue",
    "o": "sentiment analysis"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03",
    "p": "mcro:hasModelDetail",
    "o": "mcro:mistralaiMistral7BInstructv03-ModelDetail"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3."
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mistralaiMistral7BInstructv03-ModelArchitecture"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Large Language Model (LLM)"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03",
    "p": "mcro:hasLimitation",
    "o": "mcro:mistralaiMistral7BInstructv03-Limitation"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-Limitation",
    "p": "prov:hasTextValue",
    "o": "The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs."
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03",
    "p": "mcro:hasOwnerInformation",
    "o": "mcro:mistralaiMistral7BInstructv03-OwnerInformation"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-OwnerInformation",
    "p": "rdf:type",
    "o": "mcro:OwnerInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7BInstructv03-OwnerInformation",
    "p": "prov:hasTextValue",
    "o": "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, L\u00e9lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timoth\u00e9e Lacroix, Th\u00e9ophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall"
  },
  {
    "s": "mcro:Xenovabgebaseenv15",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Xenovabgebaseenv15",
    "p": "mcro:hasUseCase",
    "o": "mcro:Xenovabgebaseenv15-UseCaseInformationSection"
  },
  {
    "s": "mcro:Xenovabgebaseenv15-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Xenovabgebaseenv15-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Xenovabgebaseenv15",
    "p": "mcro:hasLicense",
    "o": "mcro:Xenovabgebaseenv15-LicenseInformationSection"
  },
  {
    "s": "mcro:testModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:testModel",
    "p": "mcro:hasLicense",
    "o": "mcro:testModel-LicenseInformationSection"
  },
  {
    "s": "mcro:testModel-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:testModel-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "mit"
  },
  {
    "s": "mcro:testModel",
    "p": "mcro:hasDataset",
    "o": "mcro:testModel-DatasetInformationSection"
  },
  {
    "s": "mcro:testModel-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:testModel-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "ImageNet"
  },
  {
    "s": "mcro:testModel",
    "p": "mcro:hasArchitecture",
    "o": "mcro:testModel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:testModel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:testModel-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "CNN"
  },
  {
    "s": "mcro:testModel",
    "p": "mcro:hasCitation",
    "o": "mcro:testModel-CitationInformationSection"
  },
  {
    "s": "mcro:testModel-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:testModel-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "citation"
  },
  {
    "s": "mcro:testModel",
    "p": "mcro:hasUseCase",
    "o": "mcro:testModel-UseCaseInformationSection"
  },
  {
    "s": "mcro:testModel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:testModel-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "intended use case"
  },
  {
    "s": "mcro:Qwen306B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen306B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen306B-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen306B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen306B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:Qwen306B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Pretraining & Post-training"
  },
  {
    "s": "mcro:Qwen306B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "0.6B"
  },
  {
    "s": "mcro:Qwen306B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "0.44B"
  },
  {
    "s": "mcro:Qwen306B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "28"
  },
  {
    "s": "mcro:Qwen306B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "16 for Q and 8 for KV"
  },
  {
    "s": "mcro:Qwen306B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "32,768"
  },
  {
    "s": "mcro:Qwen306B",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen306B-Citation"
  },
  {
    "s": "mcro:Qwen306B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen306B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}"
  },
  {
    "s": "mcro:Qwen306B",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen306B-UseCase"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "complex logical reasoning, math, and coding"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "efficient, general-purpose dialogue"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "creative writing"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "role-playing"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "multi-turn dialogues"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "instruction following"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "precise integration with external tools in both thinking and unthinking modes"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "multilingual instruction following"
  },
  {
    "s": "mcro:Qwen306B-UseCase",
    "p": "prov:hasTextValue",
    "o": "translation"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersparaphrasempnetbasev2-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "clustering"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "semantic search"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphrasempnetbasev2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphrasempnetbasev2-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasempnetbasev2-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:BiomedParse",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BiomedParse",
    "p": "mcro:hasCitation",
    "o": "mcro:BiomedParse-CitationInformationSection"
  },
  {
    "s": "mcro:BiomedParse",
    "p": "mcro:hasModelDetail",
    "o": "mcro:BiomedParse-ModelDetailSection"
  },
  {
    "s": "mcro:BiomedParse",
    "p": "mcro:hasUseCase",
    "o": "mcro:BiomedParse-UseCaseInformationSection"
  },
  {
    "s": "mcro:BiomedParse",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:BiomedParse-EthicalConsiderationSection"
  },
  {
    "s": "mcro:BiomedParse",
    "p": "mcro:hasDataSpecification",
    "o": "mcro:BiomedParse-DataSpecificationSection"
  },
  {
    "s": "mcro:BiomedParse-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BiomedParse-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Zhao, T., Gu, Y., Yang, J. et al. A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nat Methods 22, 166\u2013176 (2025). https://doi.org/10.1038/s41592-024-02499-w\n\n@article{zhao2025foundation,\n  title={A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities},\n  author={Zhao, Theodore and Gu, Yu and Yang, Jianwei and Usuyama, Naoto and Lee, Ho Hin and Kiblawi, Sid and Naumann, Tristan and Gao, Jianfeng and Crabtree, Angela and Abel, Jacob and others},\n  journal={Nature methods},\n  volume={22},\n  number={1},\n  pages={166--176},\n  year={2025},\n  publisher={Nature Publishing Group US New York}\n}"
  },
  {
    "s": "mcro:BiomedParse-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:BiomedParse-ModelDetailSection",
    "p": "prov:hasTextValue",
    "o": "Basic information about the model that includes licensing information, owner information, the architecture of the model (algorthim employed), references (cited papers), and versioning information."
  },
  {
    "s": "mcro:BiomedParse-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BiomedParse-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "\"This section should allow readers to quickly grasp what the model should and should not be used for, and why it was created. It can also help frame the statistical analysis presented in the rest of the card, including a short description of the user(s), use-case(s), and context(s) for which the model was originally developed.\""
  },
  {
    "s": "mcro:BiomedParse-EthicalConsiderationSection",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:BiomedParse-EthicalConsiderationSection",
    "p": "prov:hasTextValue",
    "o": "Microsoft believes Responsible AI is a shared responsibility and we have identified six principles and practices to help organizations address risks, innovate, and create value: fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant use case and addresses unforeseen product misuse.\u202f\n\nWhile testing the model with images and/or text, ensure that the data is PHI free and that there are no patient information or information that can be tracked to a patient identity.\n\nThe model is not designed for the following use cases:\n\n- Use by clinicians to inform clinical decision-making, as a diagnostic tool or as a medical device - Although MedImageParse is highly accurate in parsing biomedical data, it is not desgined or intended to be deployed in clinical settings as-is not is it for use in the diagnosis, cure, mitigation, treatment, or prevention of disease or other conditions (including to support clinical decision-making), or as a substitute of professional medical advice, diagnosis, treatment, or clinical judgment of a healthcare professional.\u202f\n\n- Scenarios without consent for data -\u202fAny scenario that uses health data for a purpose for which consent was not obtained.\u202f\u202f\n\n- Use outside of health scenarios - Any scenario that uses non-medical related image and/or serving purposes outside of the healthcare domain.\u202f\n\nPlease see Microsoft's Responsible AI Principles and approach available at https://www.microsoft.com/en-us/ai/principles-and-approach/"
  },
  {
    "s": "mcro:BiomedParse-DataSpecificationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:BiomedParse-DataSpecificationSection",
    "p": "prov:hasTextValue",
    "o": "- The model expect 2D 8-bit RGB or grayscale images by default, with pixel values ranging from 0 to 255 and resolution 1024*1024.\n- The model outputs pixel probabilities in the same shape as the input image. We convert the floating point probabilities to 8-bit grayscale outputs. The probability threshold for segmentation mask is 0.5, which corresponds to 127.5 in 8-bit grayscale output.\n- The model takes in text prompts for segmentation and doesn't have a fixed number of targets to handle. However, to ensure quality performance, we recommend the following tasks based on evaluation results. However, as we only evaluated the model on the test split of BiomedParseData, there is no guarantee for the same performance on external datasets even for the same task, due to variation in device, preprocessing, resolution and other distribution shifts. For best performance, we recommend finetuning on your specific tasks.\n  - CT:\n    - abdomen: adrenal gland, aorta, bladder, duodenum, esophagus, gallbladder, kidney, kidney cyst, kidney tumor, left adrenal gland, left kidney, liver, pancreas, postcava, right adrenal gland, right kidney, spleen, stomach, tumor\n    - colon: tumor\n    - liver: liver, tumor\n    - lung: COVID-19 infection, nodule\n    - pelvis: uterus\n  - MRI-FLAIR: brain: edema, lower-grade glioma, tumor, tumor core, whole tumor\n  - MRI-T1-Gd: brain: enhancing tumor, tumor core\n  - MRI-T2: prostate: prostate peripheral zone, prostate transitional zone,\n  - MRI:\n    - abdomen: aorta, esophagus, gallbladder, kidney, left kidney, liver, pancreas, postcava, right kidney, spleen, stomach\n    - brain: anterior hippocampus, posterior hippocampus\n    - heart: left heart atrium, left heart ventricle, myocardium, right heart ventricle\n    - prostate: prostate\n  - OCT: retinal: edema\n  - X-Ray: chest: COVID-19 infection, left lung, lung, lung opacity, right lung, viral pneumonia\n  - Dermoscopy: skin: lesion, melanoma\n  - Endoscope: colon: neoplastic polyp, non-neoplastic polyp, polyp\n  - Fundus: retinal: optic cup, optic disc,\n  - Pathology:\n    - bladder: neoplastic cells\n    - breast: epithelial cells, neoplastic cells\n    - cervix: neoplastic cells\n    - colon: glandular structure, neoplastic cells\n    - esophagus: neoplastic cells\n    - kidney: neoplastic cells\n    - liver: epithelial cells, neoplastic cells\n    - ovarian: epithelial cells, neoplastic cells\n    - prostate: neoplastic cells skin: neoplastic cells\n    - stomach: neoplastic cells\n    - testis: epithelial cells\n    - thyroid: epithelial cells, neoplastic cells\n    - uterus: neoplastic cells\n  - Ultrasound:\n    - breast: benign tumor, malignant tumor, tumor\n    - heart: left heart atrium, left heart ventricle\n    - transperineal: fetal head, public symphysis"
  },
  {
    "s": "mcro:BAAIbgereRankerBase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BAAIbgereRankerBase",
    "p": "mcro:hasCitation",
    "o": "mcro:BAAIbgereRankerBase-CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase",
    "p": "mcro:hasLicense",
    "o": "mcro:BAAIbgereRankerBase-LicenseInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase",
    "p": "mcro:hasUseCase",
    "o": "mcro:BAAIbgereRankerBase-UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BAAIbgereRankerBase-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase",
    "p": "mcro:hasDataset",
    "o": "mcro:BAAIbgereRankerBase-DatasetInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:BAAIbgereRankerBase-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:BAAIbgereRankerBase-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgereRankerBase-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell",
    "p": "mcro:hasLicense",
    "o": "mcro:blackforestlabsFLUX1schnell-LicenseInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "apache-2.0"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell",
    "p": "mcro:hasLimitation",
    "o": "mcro:blackforestlabsFLUX1schnell-LimitationInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell",
    "p": "mcro:hasUseCase",
    "o": "mcro:blackforestlabsFLUX1schnell-UseCaseInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell",
    "p": "mcro:hasOutOfScopeUse",
    "o": "mcro:blackforestlabsFLUX1schnell-OutOfScopeUse"
  },
  {
    "s": "mcro:blackforestlabsFLUX1schnell-OutOfScopeUse",
    "p": "rdf:type",
    "o": "mcro:OutofScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:obideidrobertai2b2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:obideidrobertai2b2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:obideidrobertai2b2-ModelDetail"
  },
  {
    "s": "mcro:obideidrobertai2b2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:obideidrobertai2b2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:obideidrobertai2b2-Citation"
  },
  {
    "s": "mcro:obideidrobertai2b2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:obideidrobertai2b2-Citation",
    "p": "prov:hasTextValue",
    "o": "* A RoBERTa"
  },
  {
    "s": "mcro:obideidrobertai2b2",
    "p": "mcro:hasUseCase",
    "o": "mcro:obideidrobertai2b2-UseCase"
  },
  {
    "s": "mcro:obideidrobertai2b2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:obideidrobertai2b2-UseCase",
    "p": "prov:hasTextValue",
    "o": "* A demo on how the model works (using model predictions to de-identify a medical note) is on this space:"
  },
  {
    "s": "mcro:obideidrobertai2b2",
    "p": "mcro:hasDataset",
    "o": "mcro:obideidrobertai2b2-Dataset"
  },
  {
    "s": "mcro:obideidrobertai2b2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:obideidrobertai2b2-Dataset",
    "p": "prov:hasTextValue",
    "o": "* The I2B2 2014"
  },
  {
    "s": "mcro:obideidrobertai2b2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:obideidrobertai2b2-Training"
  },
  {
    "s": "mcro:obideidrobertai2b2-Training",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:obideidrobertai2b2-Training",
    "p": "prov:hasTextValue",
    "o": "* Steps on how this model was trained can be found here: [Training](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/train). The \"model_name_or_path\" was set to: \"roberta-large\"."
  },
  {
    "s": "mcro:obideidrobertai2b2",
    "p": "mcro:hasModelParameter",
    "o": "mcro:obideidrobertai2b2-ModelParameter"
  },
  {
    "s": "mcro:obideidrobertai2b2-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:obideidrobertai2b2-ModelParameter",
    "p": "prov:hasTextValue",
    "o": "* Training details:"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5",
    "p": "mcro:hasLicense",
    "o": "mcro:prithividaparrotparaphraseronT5-LicenseInformationSection"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:prithividaparrotparaphraseronT5-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5",
    "p": "mcro:hasUseCase",
    "o": "mcro:prithividaparrotparaphraseronT5-UseCaseInformationSection"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "prithivida/parrot_paraphraser_on_T5"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:prithividaparrotparaphraseronT5-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:oliverguhrfullstoppunctuationmultilanglarge-ModelArchitecture"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "xlm-roberta-base"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge",
    "p": "mcro:hasDataset",
    "o": "mcro:oliverguhrfullstoppunctuationmultilanglarge-Dataset"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-Dataset",
    "p": "prov:hasTextValue",
    "o": "English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:oliverguhrfullstoppunctuationmultilanglarge-QuantativeAnalysis"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "precision recall f1-score support 0 0.99 0.99 0.99 73317475 . 0.94 0.95 0.95 4484845 , 0.86 0.86 0.86 6100650 ? 0.88 0.85 0.86 136479 - 0.60 0.29 0.39 233630 : 0.71 0.49 0.58 152424 accuracy 0.98 84425503 macro avg 0.83 0.74 0.77 84425503 weighted avg 0.98 0.98 0.98 84425503"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge",
    "p": "mcro:hasConsideration",
    "o": "mcro:oliverguhrfullstoppunctuationmultilanglarge-Consideration"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-Consideration",
    "p": "prov:hasTextValue",
    "o": "confusion matrix t/p 0 . , ? - : 0 1.0 0.0 0.0 0.0 0.0 0.0 . 0.0 1.0 0.0 0.0 0.0 0.0 , 0.1 0.0 0.9 0.0 0.0 0.0 ? 0.0 0.1 0.0 0.8 0.0 0.0 - 0.1 0.1 0.5 0.0 0.3 0.0 : 0.0 0.3 0.1 0.0 0.0 0.5"
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "mcro:hasLicense",
    "o": "mcro:parakeettdt06bv2-License"
  },
  {
    "s": "mcro:parakeettdt06bv2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:parakeettdt06bv2-License",
    "p": "prov:hasTextValue",
    "o": "GOVERNING TERMS: Use of this model is governed by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode.en) license."
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "mcro:hasUseCaseInformation",
    "o": "mcro:parakeettdt06bv2-UseCase"
  },
  {
    "s": "mcro:parakeettdt06bv2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:parakeettdt06bv2-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms."
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "mcro:hasModelArchitectureInformation",
    "o": "mcro:parakeettdt06bv2-Architecture"
  },
  {
    "s": "mcro:parakeettdt06bv2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:parakeettdt06bv2-Architecture",
    "p": "prov:hasTextValue",
    "o": "**Architecture Type**:\n\nFastConformer-TDT\n\n**Network Architecture**:\n\n* This model was developed based on [FastConformer encoder](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) architecture[1] and TDT decoder[2]\n* This model has 600 million model parameters."
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "mcro:hasTrainingDataInformation",
    "o": "mcro:parakeettdt06bv2-TrainingData"
  },
  {
    "s": "mcro:parakeettdt06bv2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:parakeettdt06bv2-TrainingData",
    "p": "prov:hasTextValue",
    "o": "This model was trained using the NeMo toolkit [3], following the strategies below:\n\n- Initialized from a FastConformer SSL checkpoint that was pretrained with a wav2vec method on the LibriLight dataset[7].\n- Trained for 150,000 steps on 64 A100 GPUs.\n- Dataset corpora were balanced using a temperature sampling value of 0.5.\n- Stage 2 fine-tuning was performed for 2,500 steps on 4 A100 GPUs using approximately 500 hours of high-quality, human-transcribed data of NeMo ASR Set 3.0.\n\nTraining was conducted using this [example script](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py) and [TDT configuration](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/fastconformer/hybrid_transducer_ctc/fastconformer_hybrid_tdt_ctc_bpe.yaml).\n\nThe tokenizer was constructed from the training set transcripts using this [script](https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py)."
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "mcro:hasDatasetInformation",
    "o": "mcro:parakeettdt06bv2-Dataset"
  },
  {
    "s": "mcro:parakeettdt06bv2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:parakeettdt06bv2-Dataset",
    "p": "prov:hasTextValue",
    "o": "The model was trained on the Granary dataset[8], consisting of approximately 120,000 hours of English speech data:\n\n- 10,000 hours from human-transcribed NeMo ASR Set 3.0, including:\n  - LibriSpeech (960 hours)\n  - Fisher Corpus\n  - National Speech Corpus Part 1\n  - VCTK\n  - VoxPopuli (English)\n  - Europarl-ASR (English)\n  - Multilingual LibriSpeech (MLS English) \u2013 2,000-hour subset\n  - Mozilla Common Voice (v7.0)\n  - AMI\n\n- 110,000 hours of pseudo-labeled data from:\n  - YTC (YouTube-Commons) dataset[4]\n  - YODAS dataset [5]\n  - Librilight [7]\n\nAll transcriptions preserve punctuation and capitalization. The Granary dataset[8] will be made publicly available after presentation at Interspeech 2025.\n\n**Data Collection Method by dataset**\n\n* Hybrid: Automated, Human\n\n**Labeling Method by dataset**\n\n* Hybrid: Synthetic, Human\n\n**Properties:**\n\n* Noise robust data from various sources\n* Single channel, 16kHz sampled data"
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "mcro:hasEvaluationDataInformation",
    "o": "mcro:parakeettdt06bv2-EvaluationDataset"
  },
  {
    "s": "mcro:parakeettdt06bv2-EvaluationDataset",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:parakeettdt06bv2-EvaluationDataset",
    "p": "prov:hasTextValue",
    "o": "Huggingface Open ASR Leaderboard datasets are used to evaluate the performance of this model.\n\n**Data Collection Method by dataset**\n* Human\n\n**Labeling Method by dataset**\n* Human\n\n**Properties:**\n\n* All are commonly used for benchmarking English ASR systems.\n* Audio data is typically processed into a 16kHz mono channel format for ASR evaluation, consistent with benchmarks like the [Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)."
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "mcro:hasPerformanceMetricInformation",
    "o": "mcro:parakeettdt06bv2-Performance"
  },
  {
    "s": "mcro:parakeettdt06bv2-Performance",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:parakeettdt06bv2-Performance",
    "p": "prov:hasTextValue",
    "o": "#### Huggingface Open-ASR-Leaderboard Performance\nThe performance of Automatic Speech Recognition (ASR) models is measured using Word Error Rate (WER). Given that this model is trained on a large and diverse dataset spanning multiple domains, it is generally more robust and accurate across various types of audio.\n\n### Base Performance\nThe table below summarizes the WER (%) using a Transducer decoder with greedy decoding (without an external language model):\n\n| **Model** | **Avg WER** | **AMI** | **Earnings-22** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI Speech** | **TEDLIUM-v3** | **VoxPopuli** |\n|:-------------|:-------------:|:---------:|:------------------:|:----------------:|:-----------------:|:-----------------:|:------------------:|:----------------:|:---------------:|\n| parakeet-tdt-0.6b-v2 | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | -\n\n### Noise Robustness\nPerformance across different Signal-to-Noise Ratios (SNR) using MUSAN music and noise samples:\n\n| **SNR Level** | **Avg WER** | **AMI** | **Earnings** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI** | **Tedlium** | **VoxPopuli** | **Relative Change** |\n|:---------------|:-------------:|:----------:|:------------:|:----------------:|:-----------------:|:-----------------:|:-----------:|:-------------:|:---------------:|:-----------------:|\n| Clean | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |\n| SNR 50 | 6.04 | 11.11 | 11.12 | 9.74 | 1.70 | 3.18 | 2.18 | 3.34 | 5.98 | +0.25% |\n| SNR 25 | 6.50 | 12.76 | 11.50 | 9.98 | 1.78 | 3.63 | 2.54 | 3.46 | 6.34 | -7.04% |\n| SNR 5 | 8.39 | 19.33 | 13.83 | 11.28 | 2.36 | 5.50 | 3.91 | 3.91 | 6.96 | -38.11% |\n\n### Telephony Audio Performance\nPerformance comparison between standard 16kHz audio and telephony-style audio (using \u03bc-law encoding with 16kHz\u21928kHz\u219216kHz conversion):\n\n| **Audio Format** | **Avg WER** | **AMI** | **Earnings** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI** | **Tedlium** | **VoxPopuli** | **Relative Change** |\n|:-----------------|:-------------:|:----------:|:------------:|:----------------:|:-----------------:|:-----------------:|:-----------:|:-------------:|:---------------:|:-----------------:|\n| Standard 16kHz | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |\n| \u03bc-law 8kHz | 6.32 | 11.98 | 11.16 | 10.02 | 1.78 | 3.52 | 2.20 | 3.38 | 6.52 | -4.10% |\n\nThese WER scores were obtained using greedy decoding without an external language model. Additional evaluation details are available on the [Hugging Face ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).[6]"
  },
  {
    "s": "mcro:parakeettdt06bv2",
    "p": "mcro:hasReferenceInformation",
    "o": "mcro:parakeettdt06bv2-Reference"
  },
  {
    "s": "mcro:parakeettdt06bv2-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:parakeettdt06bv2-Reference",
    "p": "prov:hasTextValue",
    "o": "[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)\n\n[2] [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations](https://arxiv.org/abs/2304.06795)\n\n[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)\n\n[4] [Youtube-commons: A massive open corpus for conversational and multimodal data](https://huggingface.co/blog/Pclanglais/youtube-commons)\n\n[5] [Yodas: Youtube-oriented dataset for audio and speech](https://arxiv.org/abs/2406.00899)\n\n[6] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n\n[7] [MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages](https://arxiv.org/abs/2410.01036)\n\n[8] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/pdf/2505.13404)"
  },
  {
    "s": "mcro:sarvamaisarvamm",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sarvamaisarvamm",
    "p": "mcro:hasModelDetail",
    "o": "mcro:sarvamaisarvamm-ModelDetail"
  },
  {
    "s": "mcro:sarvamaisarvamm-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:sarvamaisarvamm",
    "p": "mcro:hasUseCase",
    "o": "mcro:sarvamaisarvamm-UseCase"
  },
  {
    "s": "mcro:sarvamaisarvamm-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sarvamaisarvamm",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sarvamaisarvamm-ModelArchitecture"
  },
  {
    "s": "mcro:sarvamaisarvamm-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sarvamaisarvamm-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "text-only language model built on Mistral-Small"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:QwenQwen2VL7BInstruct-ModelDetail"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:QwenQwen2VL7BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Naive Dynamic Resolution"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Multimodal Rotary Position Embedding (M-ROPE)"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct",
    "p": "mcro:hasLicense",
    "o": "mcro:QwenQwen2VL7BInstruct-License"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:QwenQwen2VL7BInstruct-Citation"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:QwenQwen2VL7BInstruct-UseCase"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "SoTA understanding of images of various resolution & ratio"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Understanding videos of 20min+"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Agent that can operate your mobiles, robots, etc."
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Multilingual Support"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct",
    "p": "mcro:hasLimitation",
    "o": "mcro:QwenQwen2VL7BInstruct-Limitation"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Lack of Audio Support: The current model does not comprehend audio information within videos."
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Data timeliness: Our image dataset is updated until June 2023, and information subsequent to this date may not be covered."
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Constraints in Individuals and Intellectual Property (IP): The model's capacity to recognize specific individuals or IPs is limited, potentially failing to comprehensively cover all well-known personalities or brands."
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Limited Capacity for Complex Instruction: When faced with intricate multi-step instructions, the model's understanding and execution capabilities require enhancement."
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Insufficient Counting Accuracy: Particularly in complex scenes, the accuracy of object counting is not high, necessitating further improvements."
  },
  {
    "s": "mcro:QwenQwen2VL7BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Weak Spatial Reasoning Skills: Especially in 3D spaces, the model's inference of object positional relationships is inadequate, making it difficult to precisely judge the relative positions of objects."
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:XuhuiToxDectrobertalarge-ModelArchitecture"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Roberta-large"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:XuhuiToxDectrobertalarge-IntendedUseCase"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "toxic language detection"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge",
    "p": "mcro:hasDataset",
    "o": "mcro:XuhuiToxDectrobertalarge-Dataset"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge-Dataset",
    "p": "prov:hasTextValue",
    "o": "tweets"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge",
    "p": "mcro:hasCitation",
    "o": "mcro:XuhuiToxDectrobertalarge-Citation"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:XuhuiToxDectrobertalarge-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{zhou-etal-2020-debiasing,\n  title = {Challenges in Automated Debiasing for Toxic Language Detection},\n  author = {Zhou, Xuhui and Sap, Maarten and Swayamdipta, Swabha and Choi, Yejin and Smith, Noah A.},\n  booktitle = {EACL},\n  abbr = {EACL},\n  html = {https://www.aclweb.org/anthology/2021.eacl-main.274.pdf},\n  code = {https://github.com/XuhuiZhou/Toxic_Debias},\n  year = {2021},\n  bibtex_show = {true},\n  selected = {true}\n}"
  },
  {
    "s": "mcro:toxigen",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:toxigen",
    "p": "mcro:hasCitation",
    "o": "mcro:toxigen-Citation"
  },
  {
    "s": "mcro:toxigen-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:toxigen-Citation",
    "p": "prov:hasTextValue",
    "o": "Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar."
  },
  {
    "s": "mcro:toxigen",
    "p": "mcro:hasUseCase",
    "o": "mcro:toxigen-UseCase"
  },
  {
    "s": "mcro:toxigen-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:toxigen-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model comes from the paper [ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) and can be used to detect implicit hate speech."
  },
  {
    "s": "mcro:toxigen",
    "p": "mcro:hasReference",
    "o": "mcro:toxigen-Reference"
  },
  {
    "s": "mcro:toxigen-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:toxigen-Reference",
    "p": "prov:hasTextValue",
    "o": "Please visit the [Github Repository](https://github.com/microsoft/TOXIGEN) for the training dataset and further details."
  },
  {
    "s": "mcro:chronos-t5-tiny",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronos-t5-tiny",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronos-t5-tiny-ArchitectureSection"
  },
  {
    "s": "mcro:chronos-t5-tiny-ArchitectureSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronos-t5-tiny-ArchitectureSection",
    "p": "prov:hasTextValue",
    "o": "The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters."
  },
  {
    "s": "mcro:chronos-t5-tiny",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronos-t5-tiny-UseCaseSection"
  },
  {
    "s": "mcro:chronos-t5-tiny-UseCaseSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-tiny-UseCaseSection",
    "p": "prov:hasTextValue",
    "o": "To perform inference with Chronos models, install the package in the GitHub [companion repo](https://github.com/amazon-science/chronos-forecasting) by running:\n\npip install git+https://github.com/amazon-science/chronos-forecasting.git\n\nA minimal example showing how to perform inference using Chronos models:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom chronos import ChronosPipeline\n\npipeline = ChronosPipeline.from_pretrained(\n  \"amazon/chronos-t5-tiny\",\n  device_map=\"cuda\",\n  torch_dtype=torch.bfloat16,\n)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\n\n# context must be either a 1D tensor, a list of 1D tensors,\n# or a left-padded 2D tensor with batch as the first dimension\ncontext = torch.tensor(df[\"#Passengers\"])\nprediction_length = 12\nforecast = pipeline.predict(context, prediction_length)  # shape [num_series, num_samples, prediction_length]\n\n# visualize the forecast\nforecast_index = range(len(df), len(df) + prediction_length)\nlow, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n\nplt.figure(figsize=(8, 4))\nplt.plot(df[\"#Passengers\"], color=\"royalblue\", label=\"historical data\")\nplt.plot(forecast_index, median, color=\"tomato\", label=\"median forecast\")\nplt.fill_between(forecast_index, low, high, color=\"tomato\", alpha=0.3, label=\"80% prediction interval\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "s": "mcro:chronos-t5-tiny",
    "p": "mcro:hasCitation",
    "o": "mcro:chronos-t5-tiny-CitationSection"
  },
  {
    "s": "mcro:chronos-t5-tiny-CitationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronos-t5-tiny-CitationSection",
    "p": "prov:hasTextValue",
    "o": "If you find Chronos models useful for your research, please consider citing the associated [paper](https://arxiv.org/abs/2403.07815):\n\n@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronos-t5-tiny",
    "p": "mcro:hasLicense",
    "o": "mcro:chronos-t5-tiny-LicenseSection"
  },
  {
    "s": "mcro:chronos-t5-tiny-LicenseSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-tiny-LicenseSection",
    "p": "prov:hasTextValue",
    "o": "This project is licensed under the Apache-2.0 License."
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean",
    "p": "mcro:hasDataset",
    "o": "mcro:kresnikwav2vec2largexlsrkorean-DatasetInformationSection"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "kresnik/zeroth_korean"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean",
    "p": "mcro:hasUseCase",
    "o": "mcro:kresnikwav2vec2largexlsrkorean-UseCaseInformationSection"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:kresnikwav2vec2largexlsrkorean-PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean-PerformanceMetricInformationSection",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean-PerformanceMetricInformationSection",
    "p": "prov:hasTextValue",
    "o": "WER: 4.74%"
  },
  {
    "s": "mcro:kresnikwav2vec2largexlsrkorean-PerformanceMetricInformationSection",
    "p": "prov:hasTextValue",
    "o": "CER: 1.78%"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:CLIPViTB32LAION2B-ModelDetail"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:CLIPViTB32LAION2B-ModelArchitecture"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "CLIP ViT-B/32"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:CLIPViTB32LAION2B-Dataset"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-Dataset",
    "p": "prov:hasTextValue",
    "o": "LAION-2B"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:CLIPViTB32LAION2B-Citation"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-Citation",
    "p": "prov:hasTextValue",
    "o": "OpenAI CLIP paper"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-Citation",
    "p": "prov:hasTextValue",
    "o": "OpenCLIP software"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B",
    "p": "mcro:hasUseCase",
    "o": "mcro:CLIPViTB32LAION2B-UseCase"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:CLIPViTB32LAION2B-UseCase",
    "p": "prov:hasTextValue",
    "o": "research output for research communities"
  },
  {
    "s": "mcro:RobertaLargeMnli",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:RobertaLargeMnli",
    "p": "mcro:hasModelDetail",
    "o": "mcro:RobertaLargeMnli-ModelDetail"
  },
  {
    "s": "mcro:RobertaLargeMnli-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:RobertaLargeMnli-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:RobertaLargeMnli-License"
  },
  {
    "s": "mcro:RobertaLargeMnli-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:RobertaLargeMnli-License",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:RobertaLargeMnli-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:RobertaLargeMnli-ModelArchitecture"
  },
  {
    "s": "mcro:RobertaLargeMnli-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:RobertaLargeMnli-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based language model"
  },
  {
    "s": "mcro:RobertaLargeMnli",
    "p": "mcro:hasUseCase",
    "o": "mcro:RobertaLargeMnli-UseCase"
  },
  {
    "s": "mcro:RobertaLargeMnli-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:RobertaLargeMnli",
    "p": "mcro:hasTrainingData",
    "o": "mcro:RobertaLargeMnli-TrainingData"
  },
  {
    "s": "mcro:RobertaLargeMnli-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:RobertaLargeMnli",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:RobertaLargeMnli-EvaluationData"
  },
  {
    "s": "mcro:RobertaLargeMnli-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:RobertaLargeMnli",
    "p": "mcro:hasCitation",
    "o": "mcro:RobertaLargeMnli-Citation"
  },
  {
    "s": "mcro:RobertaLargeMnli-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Stablediffusionv14ModelCard-ModelDetailSection"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:Stablediffusionv14ModelCard-License"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-License",
    "p": "prov:hasTextValue",
    "o": "The CreativeML OpenRAIL M license"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Stablediffusionv14ModelCard-ModelArchitecture"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generation model"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:Stablediffusionv14ModelCard-Citation"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-Citation",
    "p": "prov:hasTextValue",
    "o": "@InProceedings{Rombach_2022_CVPR,\n author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n month     = {June},\n year      = {2022},\n pages     = {10684-10695}\n    }\n"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard",
    "p": "mcro:hasDataset",
    "o": "mcro:Stablediffusionv14ModelCard-Dataset"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-Dataset",
    "p": "prov:hasTextValue",
    "o": "LAION-2B (en) and subsets thereof"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard",
    "p": "mcro:hasUseCase",
    "o": "mcro:Stablediffusionv14ModelCard-UseCase"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Stablediffusionv14ModelCard-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended for research purposes only."
  },
  {
    "s": "mcro:microsoftFlorence2large",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftFlorence2large",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftFlorence2large-Citation"
  },
  {
    "s": "mcro:microsoftFlorence2large-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftFlorence2large-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}"
  },
  {
    "s": "mcro:microsoftFlorence2large",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftFlorence2large-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftFlorence2large-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftFlorence2large-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks. Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model."
  },
  {
    "s": "mcro:microsoftFlorence2large",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftFlorence2large-UseCase"
  },
  {
    "s": "mcro:microsoftFlorence2large-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftFlorence2large-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model is capable of performing different tasks through changing the prompts. Tasks include Caption, Detailed Caption, More Detailed Caption, Caption to Phrase Grounding, Object Detection, Dense Region Caption, Region proposal, OCR, OCR with Region, Output confidence score with Object Detection"
  },
  {
    "s": "mcro:microsoftFlorence2large",
    "p": "mcro:hasDataset",
    "o": "mcro:microsoftFlorence2large-Dataset"
  },
  {
    "s": "mcro:microsoftFlorence2large-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftFlorence2large-Dataset",
    "p": "prov:hasTextValue",
    "o": "FLD-5B dataset, containing 5.4 billion annotations across 126 million images"
  },
  {
    "s": "mcro:robertBase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:robertBase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:robertBase-ModelDetail"
  },
  {
    "s": "mcro:robertBase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:robertBase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:robertBase-Citation"
  },
  {
    "s": "mcro:robertBase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:robertBase-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{masala2020robert,\n  title={RoBERT--A Romanian BERT Model},\n  author={Masala, Mihai and Ruseti, Stefan and Dascalu, Mihai},\n  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},\n  pages={6626--6637},\n  year={2020}\n}"
  },
  {
    "s": "mcro:robertBase-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:robertBase-Architecture"
  },
  {
    "s": "mcro:robertBase-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:robertBase-Architecture",
    "p": "prov:hasTextValue",
    "o": "BERT"
  },
  {
    "s": "mcro:robertBase",
    "p": "mcro:hasDataset",
    "o": "mcro:robertBase-Dataset"
  },
  {
    "s": "mcro:robertBase-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:robertBase-Dataset",
    "p": "prov:hasTextValue",
    "o": "Oscar"
  },
  {
    "s": "mcro:robertBase-Dataset",
    "p": "prov:hasTextValue",
    "o": "RoTex"
  },
  {
    "s": "mcro:robertBase-Dataset",
    "p": "prov:hasTextValue",
    "o": "RoWiki"
  },
  {
    "s": "mcro:robertBase",
    "p": "mcro:hasUseCase",
    "o": "mcro:robertBase-UseCase"
  },
  {
    "s": "mcro:robertBase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:robertBase-UseCase",
    "p": "prov:hasTextValue",
    "o": "masked language modeling (MLM)"
  },
  {
    "s": "mcro:robertBase-UseCase",
    "p": "prov:hasTextValue",
    "o": "next sentence prediction (NSP)"
  },
  {
    "s": "mcro:ESM2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ESM2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ESM2-ModelArchitecture"
  },
  {
    "s": "mcro:ESM2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ESM2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "state-of-the-art protein model trained on a masked language modelling objective"
  },
  {
    "s": "mcro:ESM2",
    "p": "mcro:hasUseCase",
    "o": "mcro:ESM2-UseCase"
  },
  {
    "s": "mcro:ESM2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:ESM2-UseCase",
    "p": "prov:hasTextValue",
    "o": "suitable for fine-tuning on a wide range of tasks that take protein sequences as input"
  },
  {
    "s": "mcro:ESM2",
    "p": "mcro:hasCitation",
    "o": "mcro:ESM2-Citation"
  },
  {
    "s": "mcro:ESM2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ESM2-Citation",
    "p": "prov:hasTextValue",
    "o": "accompanying paper"
  },
  {
    "s": "mcro:googleflant5small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:googleflant5small",
    "p": "mcro:hasModelDetail",
    "o": "mcro:googleflant5small-ModelDetail"
  },
  {
    "s": "mcro:googleflant5small-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:googleflant5small-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:googleflant5small-License"
  },
  {
    "s": "mcro:googleflant5small-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:googleflant5small-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:googleflant5small-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:googleflant5small-ModelArchitecture"
  },
  {
    "s": "mcro:googleflant5small-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:googleflant5small-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:googleflant5small-Citation"
  },
  {
    "s": "mcro:googleflant5small-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:googleflant5small",
    "p": "mcro:hasUseCase",
    "o": "mcro:googleflant5small-UseCase"
  },
  {
    "s": "mcro:googleflant5small-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum",
    "p": "mcro:hasDataset",
    "o": "mcro:Salesforcecodet5basemultisum-Dataset"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum-Dataset",
    "p": "prov:hasTextValue",
    "o": "We employ the filtered version of CodeSearchNet data [[Husain et al., 2019](https://arxiv.org/abs/1909.09436)]\nfrom [CodeXGLUE](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text) benchmark for fine-tuning on\ncode summarization. The data is tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer. One can\nprepare text (or code) for the model using RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base)."
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Salesforcecodet5basemultisum-Architecture"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum-Architecture",
    "p": "prov:hasTextValue",
    "o": "[CodeT5-base](https://huggingface.co/Salesforce/codet5-base) model fine-tuned on CodeSearchNet data in a multi-lingual training setting (\nRuby/JavaScript/Go/Python/Java/PHP) for code summarization."
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum",
    "p": "mcro:hasCitation",
    "o": "mcro:Salesforcecodet5basemultisum-Citation"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{\n    wang2021codet5,\n    title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, \n    author={Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi},\n    booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021},\n    year={2021},\n}"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum",
    "p": "mcro:hasUseCase",
    "o": "mcro:Salesforcecodet5basemultisum-UseCase"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforcecodet5basemultisum-UseCase",
    "p": "prov:hasTextValue",
    "o": "for code summarization"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilusebasemultilingualcasedv1-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:Systranfasterwhisperbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Systranfasterwhisperbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Systranfasterwhisperbase-ModelArchitecture"
  },
  {
    "s": "mcro:Systranfasterwhisperbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Systranfasterwhisperbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "CTranslate2"
  },
  {
    "s": "mcro:sdxlinpainting01",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sdxlinpainting01",
    "p": "mcro:hasModelDetail",
    "o": "mcro:sdxlinpainting01-ModelDetail"
  },
  {
    "s": "mcro:sdxlinpainting01-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:sdxlinpainting01-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:sdxlinpainting01-License"
  },
  {
    "s": "mcro:sdxlinpainting01-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:sdxlinpainting01-License",
    "p": "prov:hasTextValue",
    "o": "CreativeML Open RAIL++-M License"
  },
  {
    "s": "mcro:sdxlinpainting01",
    "p": "mcro:hasUseCase",
    "o": "mcro:sdxlinpainting01-UseCase"
  },
  {
    "s": "mcro:sdxlinpainting01-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sdxlinpainting01",
    "p": "mcro:hasLimitation",
    "o": "mcro:sdxlinpainting01-Limitation"
  },
  {
    "s": "mcro:sdxlinpainting01-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:sdxlinpainting01",
    "p": "mcro:hasConsideration",
    "o": "mcro:sdxlinpainting01-Bias"
  },
  {
    "s": "mcro:sdxlinpainting01-Bias",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:sdxlinpainting01",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sdxlinpainting01-Arch"
  },
  {
    "s": "mcro:sdxlinpainting01-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sdxlinpainting01-Arch",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generative model"
  },
  {
    "s": "mcro:sdxlinpainting01",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:sdxlinpainting01-intendeduse"
  },
  {
    "s": "mcro:sdxlinpainting01-intendeduse",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:sdxlinpainting01-intendeduse",
    "p": "prov:hasTextValue",
    "o": "research purposes only"
  },
  {
    "s": "mcro:sdxlinpainting01",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:sdxlinpainting01-outofscopeuse"
  },
  {
    "s": "mcro:sdxlinpainting01-outofscopeuse",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:sdxlinpainting01-outofscopeuse",
    "p": "prov:hasTextValue",
    "o": "not trained to be factual or true representations of people or events"
  },
  {
    "s": "mcro:Qwen3-4B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen3-4B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Qwen3-4B-ModelDetail"
  },
  {
    "s": "mcro:Qwen3-4B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Qwen3-4B-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen3-4B-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen3-4B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen3-4B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:Qwen3-4B-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen3-4B-UseCase"
  },
  {
    "s": "mcro:Qwen3-4B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:StanfordAIMIstanforddeidentifierbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:StanfordAIMIstanforddeidentifierbase",
    "p": "mcro:hasCitation",
    "o": "mcro:StanfordAIMIstanforddeidentifierbase-Citation"
  },
  {
    "s": "mcro:StanfordAIMIstanforddeidentifierbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:StanfordAIMIstanforddeidentifierbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{10.1093/jamia/ocac219,\n    author = {Chambon, Pierre J and Wu, Christopher and Steinkamp, Jackson M and Adleberg, Jason and Cook, Tessa S and Langlotz, Curtis P},\n    title = \"{Automated deidentification of radiology reports combining transformer and \u201chide in plain sight\u201d rule-based methods}\",\n    journal = {Journal of the American Medical Informatics Association},\n    year = {2022},\n    month = {11},\n    abstract = \"{To develop an automated deidentification pipeline for radiology reports that detect protected health information (PHI) entities and replaces them with realistic surrogates \u201chiding in plain sight.\u201dIn this retrospective study, 999 chest X-ray and CT reports collected between November 2019 and November 2020 were annotated for PHI at the token level and combined with 3001 X-rays and 2193 medical notes previously labeled, forming a large multi-institutional and cross-domain dataset of 6193 documents. Two radiology test sets, from a known and a new institution, as well as i2b2 2006 and 2014 test sets, served as an evaluation set to estimate model performance and to compare it with previously released deidentification tools. Several PHI detection models were developed based on different training datasets, fine-tuning approaches and data augmentation techniques, and a synthetic PHI generation algorithm. These models were compared using metrics such as precision, recall and F1 score, as well as paired samples Wilcoxon tests.Our best PHI detection model achieves 97.9 F1 score on radiology reports from a known institution, 99.6 from a new institution, 99.5 on i2b2 2006, and 98.9 on i2b2 2014. On reports from a known institution, it achieves 99.1 recall of detecting the core of each PHI span.Our model outperforms all deidentifiers it was compared to on all test sets as well as human labelers on i2b2 2014 data. It enables accurate and automatic deidentification of radiology reports.A transformer-based deidentification pipeline can achieve state-of-the-art performance for deidentifying radiology reports and other medical documents.}\",\n    issn = {1527-974X},\n    doi = {10.1093/jamia/ocac219},\n    url = {https://doi.org/10.1093/jamia/ocac219},\n    note = {ocac219},\n    eprint = {https://academic.oup.com/jamia/advance-article-pdf/doi/10.1093/jamia/ocac219/47220191/ocac219.pdf},\n}"
  },
  {
    "s": "mcro:StanfordAIMIstanforddeidentifierbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:StanfordAIMIstanforddeidentifierbase-UseCase"
  },
  {
    "s": "mcro:StanfordAIMIstanforddeidentifierbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:StanfordAIMIstanforddeidentifierbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production."
  },
  {
    "s": "mcro:Qwen3-32B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen3-32B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen3-32B-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen3-32B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen3-32B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:Qwen3-32B",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen3-32B-UseCase"
  },
  {
    "s": "mcro:Qwen3-32B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen3-32B-UseCase",
    "p": "prov:hasTextValue",
    "o": "general-purpose dialogue"
  },
  {
    "s": "mcro:Qwen3-32B",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen3-32B-Citation"
  },
  {
    "s": "mcro:Qwen3-32B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen3-32B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m",
    "p": "mcro:hasModelDetail",
    "o": "mcro:vitlargepatch14reg4dinov2lvd142m-ModelDetail"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vitlargepatch14reg4dinov2lvd142m-ModelArchitecture"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:vitlargepatch14reg4dinov2lvd142m-Citation"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:vitlargepatch14reg4dinov2lvd142m-Dataset"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-Dataset",
    "p": "prov:hasTextValue",
    "o": "LVD-142M"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m",
    "p": "mcro:hasUseCase",
    "o": "mcro:vitlargepatch14reg4dinov2lvd142m-UseCase"
  },
  {
    "s": "mcro:vitlargepatch14reg4dinov2lvd142m-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisperbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisperbase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:whisperbase-ModelDetail"
  },
  {
    "s": "mcro:whisperbase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:whisperbase-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisperbase-ModelArchitecture"
  },
  {
    "s": "mcro:whisperbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisperbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer based encoder-decoder model"
  },
  {
    "s": "mcro:whisperbase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:whisperbase-Citation"
  },
  {
    "s": "mcro:whisperbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisperbase-Citation",
    "p": "prov:hasTextValue",
    "o": "Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "s": "mcro:whisperbase",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:whisperbase-IntendedUseCase"
  },
  {
    "s": "mcro:whisperbase-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisperbase-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "ASR solution for developers, especially for English speech recognition"
  },
  {
    "s": "mcro:whisperbase",
    "p": "mcro:hasTrainingData",
    "o": "mcro:whisperbase-TrainingData"
  },
  {
    "s": "mcro:whisperbase-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:whisperbase-TrainingData",
    "p": "prov:hasTextValue",
    "o": "680,000 hours of audio and the corresponding transcripts collected from the internet"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookmask2formerswintinycocoinstance-ModelDetail"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookmask2formerswintinycocoinstance-Citation"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookmask2formerswintinycocoinstance-Architecture"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookmask2formerswintinycocoinstance-UseCase"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance",
    "p": "mcro:hasLicense",
    "o": "mcro:facebookmask2formerswintinycocoinstance-License"
  },
  {
    "s": "mcro:facebookmask2formerswintinycocoinstance-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection",
    "p": "mcro:hasModelDetail",
    "o": "mcro:TahaDouajidetrdoctabledetection-ModelDetail"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:TahaDouajidetrdoctabledetection-License"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-License",
    "p": "prov:hasTextValue",
    "o": "More information needed"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:TahaDouajidetrdoctabledetection-Citation"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:TahaDouajidetrdoctabledetection-ModelArchitecture"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection",
    "p": "mcro:hasUseCase",
    "o": "mcro:TahaDouajidetrdoctabledetection-UseCase"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-UseCase",
    "p": "mcro:hasPrimaryIntendedUseCase",
    "o": "mcro:TahaDouajidetrdoctabledetection-PrimaryIntendedUseCase"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-PrimaryIntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-UseCase",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:TahaDouajidetrdoctabledetection-OutOfScopeUseCase"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection",
    "p": "mcro:hasConsideration",
    "o": "mcro:TahaDouajidetrdoctabledetection-Consideration"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection",
    "p": "mcro:hasTrainingData",
    "o": "mcro:TahaDouajidetrdoctabledetection-TrainingData"
  },
  {
    "s": "mcro:TahaDouajidetrdoctabledetection-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftdebertaxlargemnli-Citation"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftdebertaxlargemnli-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftdebertaxlargemnli-UseCase"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftdebertaxlargemnli-UseCase",
    "p": "prov:hasTextValue",
    "o": "fine-tuned with mnli task"
  },
  {
    "s": "mcro:facebookdinov2large",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookdinov2large",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookdinov2large-ModelDetail"
  },
  {
    "s": "mcro:facebookdinov2large-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookdinov2large-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookdinov2large-ModelArchitecture"
  },
  {
    "s": "mcro:facebookdinov2large-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookdinov2large-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT)"
  },
  {
    "s": "mcro:facebookdinov2large-ModelDetail",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:facebookdinov2large-IntendedUseCase"
  },
  {
    "s": "mcro:facebookdinov2large-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookdinov2large-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "feature extraction"
  },
  {
    "s": "mcro:facebookdinov2large",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookdinov2large-Citation"
  },
  {
    "s": "mcro:facebookdinov2large-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookdinov2large-Citation",
    "p": "prov:hasTextValue",
    "o": "misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision}, \n      author={Maxime Oquab and Timoth\u00e9e Darcet and Th\u00e9o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv\u00e9 Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:parakeet_rnnt_06b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:parakeet_rnnt_06b",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:parakeet_rnnt_06b-ModelArchitecture"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "FastConformer [1] is an optimized version of the Conformer model with 8x depthwise-separable convolutional downsampling. The model is trained in a multitask setup with a Transducer decoder (RNNT) loss. You may find more information on the details of FastConformer here: [Fast-Conformer Model](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer)."
  },
  {
    "s": "mcro:parakeet_rnnt_06b",
    "p": "mcro:hasDataset",
    "o": "mcro:parakeet_rnnt_06b-Dataset"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-Dataset",
    "p": "prov:hasTextValue",
    "o": "The model was trained on 64K hours of English speech collected and prepared by NVIDIA NeMo and Suno teams.\n\nThe training dataset consists of private subset with 40K hours of English speech plus 24K hours from the following public datasets:\n\n- Librispeech 960 hours of English speech\n- Fisher Corpus\n- Switchboard-1 Dataset\n- WSJ-0 and WSJ-1\n- National Speech Corpus (Part 1, Part 6)\n- VCTK\n- VoxPopuli (EN)\n- Europarl-ASR (EN)\n- Multilingual Librispeech (MLS EN) - 2,000 hour subset\n- Mozilla Common Voice (v7.0)\n- People's Speech  - 12,000 hour subset"
  },
  {
    "s": "mcro:parakeet_rnnt_06b",
    "p": "mcro:hasReference",
    "o": "mcro:parakeet_rnnt_06b-Reference"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-Reference",
    "p": "prov:hasTextValue",
    "o": "[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)\n\n[2] [Google Sentencepiece Tokenizer](https://github.com/google/sentencepiece)\n\n[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)\n\n[4] [Suno.ai](https://suno.ai/)\n\n[5] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)"
  },
  {
    "s": "mcro:parakeet_rnnt_06b",
    "p": "mcro:hasLicense",
    "o": "mcro:parakeet_rnnt_06b-License"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-License",
    "p": "prov:hasTextValue",
    "o": "License to use this model is covered by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). By downloading the public and release version of the model, you accept the terms and conditions of the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license."
  },
  {
    "s": "mcro:parakeet_rnnt_06b",
    "p": "mcro:hasUseCase",
    "o": "mcro:parakeet_rnnt_06b-UseCase"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:parakeet_rnnt_06b-UseCase",
    "p": "prov:hasTextValue",
    "o": "`parakeet-rnnt-0.6b` is an ASR model that transcribes speech in lower case English alphabet. This model is jointly developed by [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) and [Suno.ai](https://www.suno.ai/) teams.\nIt is an XL version of FastConformer Transducer [1] (around 600M parameters) model.\nSee the [model architecture](#model-architecture) section and [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) for complete architecture details."
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection",
    "p": "mcro:hasModelDetail",
    "o": "mcro:xlmrobertabaselanguagedetection-ModelDetail"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:xlmrobertabaselanguagedetection-ModelArchitecture"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "XLM-RoBERTa transformer model with a classification head"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:xlmrobertabaselanguagedetection-Citation"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-Citation",
    "p": "prov:hasTextValue",
    "o": "Unsupervised Cross-lingual Representation Learning at Scale"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:xlmrobertabaselanguagedetection-IntendedUseCase"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "language detector, i.e. for sequence classification tasks"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection",
    "p": "mcro:hasTrainingData",
    "o": "mcro:xlmrobertabaselanguagedetection-TrainingData"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Language Identification dataset, which consists of text sequences in 20 languages"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:xlmrobertabaselanguagedetection-QuantativeAnalysis"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-QuantativeAnalysis",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:xlmrobertabaselanguagedetection-PerformanceMetric"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-PerformanceMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:xlmrobertabaselanguagedetection-PerformanceMetric",
    "p": "prov:hasTextValue",
    "o": "average accuracy on the test set is **99.6%**"
  },
  {
    "s": "mcro:stabilityaisdturbo",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:stabilityaisdturbo",
    "p": "mcro:hasModelDetail",
    "o": "mcro:stabilityaisdturbo-ModelDetail"
  },
  {
    "s": "mcro:stabilityaisdturbo-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:stabilityaisdturbo-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:stabilityaisdturbo-ModelArchitecture"
  },
  {
    "s": "mcro:stabilityaisdturbo-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:stabilityaisdturbo-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Generative text-to-image model"
  },
  {
    "s": "mcro:stabilityaisdturbo-ModelDetail",
    "p": "mcro:hasReference",
    "o": "mcro:stabilityaisdturbo-Reference"
  },
  {
    "s": "mcro:stabilityaisdturbo-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:stabilityaisdturbo-Reference",
    "p": "prov:hasTextValue",
    "o": "https://github.com/Stability-AI/generative-models"
  },
  {
    "s": "mcro:stabilityaisdturbo-Reference",
    "p": "prov:hasTextValue",
    "o": "https://stability.ai/research/adversarial-diffusion-distillation"
  },
  {
    "s": "mcro:stabilityaisdturbo",
    "p": "mcro:hasUseCase",
    "o": "mcro:stabilityaisdturbo-UseCase"
  },
  {
    "s": "mcro:stabilityaisdturbo-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:stabilityaisdturbo-UseCase",
    "p": "prov:hasTextValue",
    "o": "Research on generative models."
  },
  {
    "s": "mcro:stabilityaisdturbo-UseCase",
    "p": "prov:hasTextValue",
    "o": "Research on real-time applications of generative models."
  },
  {
    "s": "mcro:stabilityaisdturbo-UseCase",
    "p": "prov:hasTextValue",
    "o": "Safe deployment of models which have the potential to generate harmful content."
  },
  {
    "s": "mcro:stabilityaisdturbo-UseCase",
    "p": "prov:hasTextValue",
    "o": "Generation of artworks and use in design and other artistic processes."
  },
  {
    "s": "mcro:stabilityaisdturbo",
    "p": "mcro:hasLimitation",
    "o": "mcro:stabilityaisdturbo-Limitation"
  },
  {
    "s": "mcro:stabilityaisdturbo-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:stabilityaisdturbo-Limitation",
    "p": "prov:hasTextValue",
    "o": "The quality and prompt alignment is lower than that of [SDXL-Turbo](https://huggingface.co/stabilityai/sdxl-turbo)."
  },
  {
    "s": "mcro:stabilityaisdturbo-Limitation",
    "p": "prov:hasTextValue",
    "o": "Faces and people in general may not be generated properly."
  },
  {
    "s": "mcro:stabilityaisdturbo-Limitation",
    "p": "prov:hasTextValue",
    "o": "The model cannot render legible text."
  },
  {
    "s": "mcro:stabilityaisdturbo-Limitation",
    "p": "prov:hasTextValue",
    "o": "The generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism."
  },
  {
    "s": "mcro:stabilityaisdturbo",
    "p": "mcro:hasOutOfScopeUse",
    "o": "mcro:stabilityaisdturbo-OutOfScopeUse"
  },
  {
    "s": "mcro:stabilityaisdturbo-OutOfScopeUse",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:stabilityaisdturbo-OutOfScopeUse",
    "p": "prov:hasTextValue",
    "o": "The model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model."
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct",
    "p": "mcro:hasModelArchitectureInformation",
    "o": "mcro:metallamaLlama3211BVisionInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM."
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct",
    "p": "mcro:hasLicense",
    "o": "mcro:metallamaLlama3211BVisionInstruct-License"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-License",
    "p": "prov:hasTextValue",
    "o": "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)."
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct",
    "p": "mcro:hasIntendedUseCaseInformation",
    "o": "mcro:metallamaLlama3211BVisionInstruct-IntendedUse"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-IntendedUse",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-IntendedUse",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2-Vision is intended for commercial and research use. Instruction tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pretrained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2-Vision\u2019s ability to take images and text as inputs, additional use cases could include:\n\n1. Visual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it.   \n2. Document Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image.  \n3. Image Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story.  \n4. Image-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words.  \n5. Visual Grounding: Visual grounding is like connecting the dots between what we see and say. It\u2019s about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions."
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct",
    "p": "mcro:hasOutOfScopeUseCaseInformation",
    "o": "mcro:metallamaLlama3211BVisionInstruct-OutOfScope"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-OutOfScope",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-OutOfScope",
    "p": "prov:hasTextValue",
    "o": "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card."
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct",
    "p": "mcro:hasTrainingDataInformation",
    "o": "mcro:metallamaLlama3211BVisionInstruct-TrainingData"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:metallamaLlama3211BVisionInstruct-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023."
  },
  {
    "s": "mcro:prajjwal1berttiny",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:prajjwal1berttiny",
    "p": "mcro:hasCitation",
    "o": "mcro:prajjwal1berttiny-Citation"
  },
  {
    "s": "mcro:prajjwal1berttiny-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:prajjwal1berttiny-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bhargava2021generalization,\n      title={Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics}, \n      author={Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},\n      year={2021},\n      eprint={2110.01518},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{DBLP:journals/corr/abs-1908-08962,\n  author    = {Iulia Turc and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {Well-Read Students Learn Better: The Impact of Student Initialization\n               on Knowledge Distillation},\n  journal   = {CoRR},\n  volume    = {abs/1908.08962},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1908.08962},\n  eprinttype = {arXiv},\n  eprint    = {1908.08962},\n  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n"
  },
  {
    "s": "mcro:prajjwal1berttiny",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:prajjwal1berttiny-ModelArchitecture"
  },
  {
    "s": "mcro:prajjwal1berttiny-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:prajjwal1berttiny-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The following model is a Pytorch pre-trained model obtained from converting Tensorflow checkpoint found in the [official Google BERT repository](https://github.com/google-research/bert).\n\nThis is one of the smaller pre-trained BERT variants, together with [bert-mini](https://huggingface.co/prajjwal1/bert-mini) [bert-small](https://huggingface.co/prajjwal1/bert-small) and [bert-medium](https://huggingface.co/prajjwal1/bert-medium). They were introduced in the study `Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962)), and ported to HF for the study `Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics` ([arXiv](https://arxiv.org/abs/2110.01518)). These models are supposed to be trained on a downstream task.\n"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1",
    "p": "mcro:hasModelDetails",
    "o": "mcro:yuvalkirstainPickScorev1-ModelDetail"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:yuvalkirstainPickScorev1-Citation"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:yuvalkirstainPickScorev1-Dataset"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:yuvalkirstainPickScorev1-ModelArchitecture"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1",
    "p": "mcro:hasUseCase",
    "o": "mcro:yuvalkirstainPickScorev1-UseCase"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-Dataset",
    "p": "prov:hasTextValue",
    "o": "Pick-a-Pic dataset"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "CLIP-H"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Kirstain2023PickaPicAO,\n  title={Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation},\n  author={Yuval Kirstain and Adam Polyak and Uriel Singer and Shahbuland Matiana and Joe Penna and Omer Levy},\n  year={2023}\n}"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-UseCase",
    "p": "prov:hasTextValue",
    "o": "general scoring function"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-UseCase",
    "p": "prov:hasTextValue",
    "o": "human preference prediction"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-UseCase",
    "p": "prov:hasTextValue",
    "o": "model evaluation"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-UseCase",
    "p": "prov:hasTextValue",
    "o": "image ranking"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1",
    "p": "mcro:hasTrainingData",
    "o": "mcro:yuvalkirstainPickScorev1-TrainingData"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:yuvalkirstainPickScorev1-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Pick-a-Pic dataset"
  },
  {
    "s": "mcro:gte-small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gte-small",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gte-small-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gte-small-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gte-small-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "BERT framework"
  },
  {
    "s": "mcro:gte-small",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:gte-small-UseCaseInformationSection"
  },
  {
    "s": "mcro:gte-small-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gte-small-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "information retrieval"
  },
  {
    "s": "mcro:gte-small-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "semantic textual similarity"
  },
  {
    "s": "mcro:gte-small-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "text reranking"
  },
  {
    "s": "mcro:gte-small",
    "p": "mcro:hasLimitation",
    "o": "mcro:gte-small-LimitationInformationSection"
  },
  {
    "s": "mcro:gte-small-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:gte-small-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens."
  },
  {
    "s": "mcro:gte-small",
    "p": "mcro:hasCitation",
    "o": "mcro:gte-small-CitationInformationSection"
  },
  {
    "s": "mcro:gte-small-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gte-small-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}"
  },
  {
    "s": "mcro:sentencetransformersLaBSE",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersLaBSE",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersLaBSE-ModelArchitecture"
  },
  {
    "s": "mcro:sentencetransformersLaBSE-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersLaBSE-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 768, 'out_features': 768, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n  (3): Normalize()\n)"
  },
  {
    "s": "mcro:sentencetransformersLaBSE",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersLaBSE-Citation"
  },
  {
    "s": "mcro:sentencetransformersLaBSE-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersLaBSE-Citation",
    "p": "prov:hasTextValue",
    "o": "Have a look at [LaBSE](https://tfhub.dev/google/LaBSE/1) for the respective publication that describes LaBSE."
  },
  {
    "s": "mcro:sentencetransformersLaBSE",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersLaBSE-UseCase"
  },
  {
    "s": "mcro:sentencetransformersLaBSE-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersLaBSE-UseCase",
    "p": "prov:hasTextValue",
    "o": "It can be used to map 109 languages to a shared vector space."
  },
  {
    "s": "mcro:granitetimeseriesttmr1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:granitetimeseriesttmr1",
    "p": "mcro:hasModelDetail",
    "o": "mcro:granitetimeseriesttmr1-ModelDetail"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:granitetimeseriesttmr1-CitationInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:granitetimeseriesttmr1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:granitetimeseriesttmr1-LicenseInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1",
    "p": "mcro:hasUseCase",
    "o": "mcro:granitetimeseriesttmr1-UseCaseInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1",
    "p": "mcro:hasTrainingData",
    "o": "mcro:granitetimeseriesttmr1-TrainingDataInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1",
    "p": "mcro:hasDataset",
    "o": "mcro:granitetimeseriesttmr1-DatasetInformationSection"
  },
  {
    "s": "mcro:granitetimeseriesttmr1-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortablestructurerecognition",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortablestructurerecognition",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:tabletransformerfinetunedfortablestructurerecognition-ModelArchitecture"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortablestructurerecognition-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortablestructurerecognition-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The Table Transformer is equivalent to [DETR](https://huggingface.co/docs/transformers/model_doc/detr), a Transformer-based object detection model. Note that the authors decided to use the \"normalize before\" setting of DETR, which means that layernorm is applied before self- and cross-attention."
  },
  {
    "s": "mcro:tabletransformerfinetunedfortablestructurerecognition",
    "p": "mcro:hasUseCase",
    "o": "mcro:tabletransformerfinetunedfortablestructurerecognition-UseCase"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortablestructurerecognition-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:tabletransformerfinetunedfortablestructurerecognition-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for detecting the structure (like rows, columns) in tables. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer) for more info."
  },
  {
    "s": "mcro:MERTv195M",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:MERTv195M",
    "p": "mcro:hasCitation",
    "o": "mcro:MERTv195M-Citation"
  },
  {
    "s": "mcro:MERTv195M-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:MERTv195M-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{li2023mert,\n      title={MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}, \n      author={Yizhi Li and Ruibin Yuan and Ge Zhang and Yinghao Ma and Xingran Chen and Hanzhi Yin and Chenghua Lin and Anton Ragni and Emmanouil Benetos and Norbert Gyenge and Roger Dannenberg and Ruibo Liu and Wenhu Chen and Gus Xia and Yemin Shi and Wenhao Huang and Yike Guo and Jie Fu},\n      year={2023},\n      eprint={2306.00107},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD}\n}"
  },
  {
    "s": "mcro:MERTv195M",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:MERTv195M-ModelArchitecture"
  },
  {
    "s": "mcro:MERTv195M-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:MERTv195M-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer Layer-Dimension"
  },
  {
    "s": "mcro:MERTv195M",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:MERTv195M-IntendedUseCase"
  },
  {
    "s": "mcro:MERTv195M-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:MERTv195M-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "music generation"
  },
  {
    "s": "mcro:byt5small",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:byt5small",
    "p": "mcro:hasModelDetail",
    "o": "mcro:byt5small-ModelDetail"
  },
  {
    "s": "mcro:byt5small-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:byt5small-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:byt5small-ModelArchitecture"
  },
  {
    "s": "mcro:byt5small-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:byt5small-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "the architecture of MT5"
  },
  {
    "s": "mcro:byt5small-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:byt5small-Citation"
  },
  {
    "s": "mcro:byt5small-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:byt5small-Citation",
    "p": "prov:hasTextValue",
    "o": "ByT5: Towards a token-free future with pre-trained byte-to-byte models"
  },
  {
    "s": "mcro:byt5small",
    "p": "mcro:hasDataset",
    "o": "mcro:byt5small-Dataset"
  },
  {
    "s": "mcro:byt5small-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:byt5small-Dataset",
    "p": "prov:hasTextValue",
    "o": "mC4"
  },
  {
    "s": "mcro:byt5small",
    "p": "mcro:hasUseCase",
    "o": "mcro:byt5small-UseCase"
  },
  {
    "s": "mcro:byt5small-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:byt5small-UseCase",
    "p": "prov:hasTextValue",
    "o": "ByT5 works especially well on noisy text data"
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:tabletransformerpretrainedfortablestructurerecognition-ModelArchitecture"
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The Table Transformer is equivalent to [DETR](https://huggingface.co/docs/transformers/model_doc/detr), a Transformer-based object detection model. Note that the authors decided to use the \"normalize before\" setting of DETR, which means that layernorm is applied before self- and cross-attention."
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:tabletransformerpretrainedfortablestructurerecognition-UseCase"
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for detecting tables in documents. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer) for more info."
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition",
    "p": "mcro:hasCitation",
    "o": "mcro:tabletransformerpretrainedfortablestructurerecognition-Citation"
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:tabletransformerpretrainedfortablestructurerecognition-Citation",
    "p": "prov:hasTextValue",
    "o": "It was introduced in the paper [Aligning benchmark datasets for table structure recognition](https://arxiv.org/abs/2303.00716) by Smock et al. and first released in [this repository](https://github.com/microsoft/table-transformer)."
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny",
    "p": "mcro:hasCitation",
    "o": "mcro:IDEAResearchgroundingdinotiny-Citation"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{liu2023grounding,\n      title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}, \n      author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},\n      year={2023},\n      eprint={2303.05499},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:IDEAResearchgroundingdinotiny-ModelArchitecture"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection."
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny",
    "p": "mcro:hasUseCase",
    "o": "mcro:IDEAResearchgroundingdinotiny-UseCase"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:IDEAResearchgroundingdinotiny-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for zero-shot object detection (the task of detecting things in an image out-of-the-box without labeled data)."
  },
  {
    "s": "mcro:microsoftdebertav3large",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftdebertav3large",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftdebertav3large-Citation"
  },
  {
    "s": "mcro:microsoftdebertav3large-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftdebertav3large",
    "p": "mcro:hasDataset",
    "o": "mcro:microsoftdebertav3large-Dataset"
  },
  {
    "s": "mcro:microsoftdebertav3large-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftdebertav3large",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftdebertav3large-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftdebertav3large-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftdebertav3large",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftdebertav3large-UseCase"
  },
  {
    "s": "mcro:microsoftdebertav3large-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:germanbert",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:germanbert",
    "p": "mcro:hasModelDetail",
    "o": "mcro:germanbert-ModelDetail"
  },
  {
    "s": "mcro:germanbert-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:germanbert-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:germanbert-License"
  },
  {
    "s": "mcro:germanbert-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:germanbert",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:germanbert-ModelArchitecture"
  },
  {
    "s": "mcro:germanbert-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:germanbert",
    "p": "mcro:hasDataset",
    "o": "mcro:germanbert-Dataset"
  },
  {
    "s": "mcro:germanbert-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:germanbert",
    "p": "mcro:hasOwner",
    "o": "mcro:germanbert-Owner"
  },
  {
    "s": "mcro:germanbert-Owner",
    "p": "rdf:type",
    "o": "mcro:OwnerInformationSection"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase",
    "p": "mcro:hasDataset",
    "o": "mcro:banglat5_banglaparaphrase-DatasetInformationSection"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase",
    "p": "mcro:hasCitation",
    "o": "mcro:banglat5_banglaparaphrase-CitationInformationSection"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:banglat5_banglaparaphrase-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase",
    "p": "mcro:hasUseCase",
    "o": "mcro:banglat5_banglaparaphrase-UseCaseInformationSection"
  },
  {
    "s": "mcro:banglat5_banglaparaphrase-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:oliverguhrfullstoppunctuationmultilanglarge-UseCaseInformationSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model predicts the punctuation of English, Italian, French and German texts. We developed it to restore the punctuation of transcribed spoken language."
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge",
    "p": "mcro:hasDataset",
    "o": "mcro:oliverguhrfullstoppunctuationmultilanglarge-DatasetInformationSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:oliverguhrfullstoppunctuationmultilanglarge-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "This multilanguage model was trained on the [Europarl Dataset](https://huggingface.co/datasets/wmt/europarl) provided by the [SEPP-NLG Shared Task](https://sites.google.com/view/sentence-segmentation). *Please note that this dataset consists of political speeches. Therefore the model might perform differently on texts from other domains.*"
  },
  {
    "s": "mcro:facebookmmslid256",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookmmslid256",
    "p": "mcro:hasModelDetail",
    "o": "mcro:facebookmmslid256-ModelDetail"
  },
  {
    "s": "mcro:facebookmmslid256-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:facebookmmslid256-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookmmslid256-Citation"
  },
  {
    "s": "mcro:facebookmmslid256-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookmmslid256-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:facebookmmslid256-License"
  },
  {
    "s": "mcro:facebookmmslid256-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:facebookmmslid256-License",
    "p": "prov:hasTextValue",
    "o": "CC-BY-NC 4.0 license"
  },
  {
    "s": "mcro:facebookmmslid256-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookmmslid256-ModelArchitecture"
  },
  {
    "s": "mcro:facebookmmslid256-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookmmslid256-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Wav2Vec2 architecture"
  },
  {
    "s": "mcro:facebookmmslid256",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookmmslid256-UseCase"
  },
  {
    "s": "mcro:facebookmmslid256-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookmmslid256-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech language identification (LID)"
  },
  {
    "s": "mcro:facebookmmslid256",
    "p": "mcro:hasDataset",
    "o": "mcro:facebookmmslid256-Dataset"
  },
  {
    "s": "mcro:facebookmmslid256-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersstsbrobertabase-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': True}) with Transformer model: RobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersstsbrobertabase-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersstsbrobertabase-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersstsbrobertabase-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:e5smallv2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:e5smallv2",
    "p": "mcro:hasCitation",
    "o": "mcro:e5smallv2-Citation"
  },
  {
    "s": "mcro:e5smallv2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:e5smallv2-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}"
  },
  {
    "s": "mcro:e5smallv2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:e5smallv2-ModelArchitecture"
  },
  {
    "s": "mcro:e5smallv2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:e5smallv2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "This model has 12 layers and the embedding size is 384."
  },
  {
    "s": "mcro:e5smallv2",
    "p": "mcro:hasUseCase",
    "o": "mcro:e5smallv2-UseCase"
  },
  {
    "s": "mcro:e5smallv2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:e5smallv2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Use \\\"query: \\\" and \\\"passage: \\\" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\\n\\n- Use \\\"query: \\\" prefix for symmetric tasks such as semantic similarity, paraphrase retrieval.\\n\\n- Use \\\"query: \\\" prefix if you want to use embeddings as features, such as linear probing classification, clustering."
  },
  {
    "s": "mcro:e5smallv2",
    "p": "mcro:hasLimitation",
    "o": "mcro:e5smallv2-Limitation"
  },
  {
    "s": "mcro:e5smallv2-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:e5smallv2-Limitation",
    "p": "prov:hasTextValue",
    "o": "This model only works for English texts. Long texts will be truncated to at most 512 tokens."
  },
  {
    "s": "mcro:bertsmalluncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertsmalluncased",
    "p": "mcro:hasCitation",
    "o": "mcro:bertsmalluncased-Citation"
  },
  {
    "s": "mcro:bertsmalluncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bertsmalluncased-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{turc2019,\n  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1908.08962v2 },\n  year={2019}\n}"
  },
  {
    "s": "mcro:bertsmalluncased",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bertsmalluncased-ModelArchitecture"
  },
  {
    "s": "mcro:bertsmalluncased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bertsmalluncased-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Mini BERT models from https://arxiv.org/abs/1908.08962 that the HF team didn't convert. The original [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py) is used.\n\nSee the original Google repo: [google-research/bert](https://github.com/google-research/bert)\n\nNote: it's not clear if these checkpoints have undergone knowledge distillation."
  },
  {
    "s": "mcro:bertsmalluncased",
    "p": "mcro:hasUseCase",
    "o": "mcro:bertsmalluncased-UseCase"
  },
  {
    "s": "mcro:bertsmalluncased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bertsmalluncased-UseCase",
    "p": "prov:hasTextValue",
    "o": "See other BERT model cards e.g. https://huggingface.co/bert-base-uncased"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr-1b-portuguese,\n  title={Fine-tuned {XLS-R} 1{B} model for speech recognition in {P}ortuguese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-xls-r-1b-portuguese}},\n  year={2022}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Fine-tuned [facebook/wav2vec2-xls-r-1b](https://huggingface.co/facebook/wav2vec2-xls-r-1b) on Portuguese"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-Dataset",
    "p": "prov:hasTextValue",
    "o": "train and validation splits of [Common Voice 8.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_8_0), [CORAA](https://github.com/nilc-nlp/CORAA), [Multilingual TEDx](http://www.openslr.org/100), and [Multilingual LibriSpeech](https://www.openslr.org/94/)."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2xlsr1bportuguese-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Portuguese"
  },
  {
    "s": "mcro:conjunctsditre15",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:conjunctsditre15",
    "p": "mcro:hasModelDetail",
    "o": "mcro:conjunctsditre15-ModelDetail"
  },
  {
    "s": "mcro:conjunctsditre15-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:conjunctsditre15",
    "p": "mcro:hasUseCase",
    "o": "mcro:conjunctsditre15-UseCase"
  },
  {
    "s": "mcro:conjunctsditre15-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:conjunctsditre15",
    "p": "mcro:hasConsideration",
    "o": "mcro:conjunctsditre15-Consideration"
  },
  {
    "s": "mcro:conjunctsditre15-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:conjunctsditre15",
    "p": "mcro:hasTrainingData",
    "o": "mcro:conjunctsditre15-TrainingData"
  },
  {
    "s": "mcro:conjunctsditre15-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:conjunctsditre15",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:conjunctsditre15-EvaluationData"
  },
  {
    "s": "mcro:conjunctsditre15-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:conjunctsditre15",
    "p": "mcro:hasCitation",
    "o": "mcro:conjunctsditre15-Citation"
  },
  {
    "s": "mcro:conjunctsditre15-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:conjunctsditre15",
    "p": "mcro:hasTechnicalSpecification",
    "o": "mcro:conjunctsditre15-TechnicalSpecification"
  },
  {
    "s": "mcro:conjunctsditre15-TechnicalSpecification",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:distilbertbasemultilingualcasedsentimentsstudent",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilbertbasemultilingualcasedsentimentsstudent",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:distilbertbasemultilingualcasedsentimentsstudent-ModelArchitecture"
  },
  {
    "s": "mcro:distilbertbasemultilingualcasedsentimentsstudent-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcasedsentimentsstudent-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "distilbert-base-multilingual-cased"
  },
  {
    "s": "mcro:distilbertbasemultilingualcasedsentimentsstudent",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:distilbertbasemultilingualcasedsentimentsstudent-UseCase"
  },
  {
    "s": "mcro:distilbertbasemultilingualcasedsentimentsstudent-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilbertbasemultilingualcasedsentimentsstudent-UseCase",
    "p": "prov:hasTextValue",
    "o": "sentiment analysis"
  },
  {
    "s": "mcro:ChatGLM2-6B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ChatGLM2-6B",
    "p": "mcro:hasLicense",
    "o": "mcro:ChatGLM2-6B-License"
  },
  {
    "s": "mcro:ChatGLM2-6B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:ChatGLM2-6B-License",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0"
  },
  {
    "s": "mcro:ChatGLM2-6B",
    "p": "mcro:hasCitation",
    "o": "mcro:ChatGLM2-6B-Citation"
  },
  {
    "s": "mcro:ChatGLM2-6B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ChatGLM2-6B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}"
  },
  {
    "s": "mcro:ChatGLM2-6B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ChatGLM2-6B-Arch"
  },
  {
    "s": "mcro:ChatGLM2-6B-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ChatGLM2-6B-Arch",
    "p": "prov:hasTextValue",
    "o": "GLM"
  },
  {
    "s": "mcro:ChatGLM2-6B",
    "p": "mcro:hasDataset",
    "o": "mcro:ChatGLM2-6B-Dataset"
  },
  {
    "s": "mcro:ChatGLM2-6B-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:ChatGLM2-6B-Dataset",
    "p": "prov:hasTextValue",
    "o": "1.4T \u4e2d\u82f1\u6807\u8bc6\u7b26"
  },
  {
    "s": "mcro:ChatGLM2-6B",
    "p": "mcro:hasUseCase",
    "o": "mcro:ChatGLM2-6B-UseCase"
  },
  {
    "s": "mcro:ChatGLM2-6B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:ChatGLM2-6B-UseCase",
    "p": "prov:hasTextValue",
    "o": "\u4e2d\u82f1\u53cc\u8bed\u5bf9\u8bdd"
  },
  {
    "s": "mcro:Whisperlargev3model",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Whisperlargev3model",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Whisperlargev3model-ModelDetail"
  },
  {
    "s": "mcro:Whisperlargev3model-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Whisperlargev3model-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Whisperlargev3model-ModelArchitecture"
  },
  {
    "s": "mcro:Whisperlargev3model-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Whisperlargev3model-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Whisper large-v3"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekR1-License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekR1-Citation"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:deepseekaiDeepSeekR1-ModelArchitecture"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "MoE"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasUseCase",
    "o": "mcro:deepseekaiDeepSeekR1-UseCase"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-UseCase",
    "p": "prov:hasTextValue",
    "o": "reasoning"
  },
  {
    "s": "mcro:facebookesm2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookesm2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:facebookesm2-ModelArchitecture"
  },
  {
    "s": "mcro:facebookesm2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookesm2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "state-of-the-art protein model trained on a masked language modelling objective"
  },
  {
    "s": "mcro:facebookesm2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:facebookesm2-UseCase"
  },
  {
    "s": "mcro:facebookesm2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookesm2-UseCase",
    "p": "prov:hasTextValue",
    "o": "suitable for fine-tuning on a wide range of tasks that take protein sequences as input"
  },
  {
    "s": "mcro:facebookesm2",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookesm2-Citation"
  },
  {
    "s": "mcro:facebookesm2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookesm2-Citation",
    "p": "prov:hasTextValue",
    "o": "https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2"
  },
  {
    "s": "mcro:gte-base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gte-base",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gte-base-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gte-base-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gte-base-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "BERT framework"
  },
  {
    "s": "mcro:gte-base",
    "p": "mcro:hasUseCase",
    "o": "mcro:gte-base-UseCaseInformationSection"
  },
  {
    "s": "mcro:gte-base-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gte-base-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "information retrieval"
  },
  {
    "s": "mcro:gte-base-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "semantic textual similarity"
  },
  {
    "s": "mcro:gte-base-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "text reranking"
  },
  {
    "s": "mcro:gte-base",
    "p": "mcro:hasLimitation",
    "o": "mcro:gte-base-LimitationInformationSection"
  },
  {
    "s": "mcro:gte-base-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:gte-base-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens."
  },
  {
    "s": "mcro:gte-base",
    "p": "mcro:hasCitation",
    "o": "mcro:gte-base-CitationInformationSection"
  },
  {
    "s": "mcro:gte-base-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gte-base-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}"
  },
  {
    "s": "mcro:codebertbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:codebertbase",
    "p": "mcro:hasDataset",
    "o": "mcro:codebertbase-Dataset"
  },
  {
    "s": "mcro:codebertbase-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:codebertbase-Dataset",
    "p": "prov:hasTextValue",
    "o": "The model is trained on bi-modal data (documents & code) of [CodeSearchNet](https://github.com/github/CodeSearchNet)"
  },
  {
    "s": "mcro:codebertbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:codebertbase-ModelArchitecture"
  },
  {
    "s": "mcro:codebertbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:codebertbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "This model is initialized with Roberta-base and trained with MLM+RTD objective (cf. the paper)."
  },
  {
    "s": "mcro:codebertbase",
    "p": "mcro:hasReference",
    "o": "mcro:codebertbase-Reference"
  },
  {
    "s": "mcro:codebertbase-Reference",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:codebertbase-Reference",
    "p": "prov:hasTextValue",
    "o": "Please see [the official repository](https://github.com/microsoft/CodeBERT) for scripts that support \"code search\" and \"code-to-document generation\"."
  },
  {
    "s": "mcro:codebertbase",
    "p": "mcro:hasReference",
    "o": "mcro:codebertbase-Reference2"
  },
  {
    "s": "mcro:codebertbase-Reference2",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:codebertbase-Reference2",
    "p": "prov:hasTextValue",
    "o": "1. [CodeBERT trained with Masked LM objective](https://huggingface.co/microsoft/codebert-base-mlm) (suitable for code completion)\n2. \ud83e\udd17 [Hugging Face's CodeBERTa](https://huggingface.co/huggingface/CodeBERTa-small-v1) (small size, 6 layers)"
  },
  {
    "s": "mcro:codebertbase",
    "p": "mcro:hasCitation",
    "o": "mcro:codebertbase-Citation"
  },
  {
    "s": "mcro:codebertbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:codebertbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{feng2020codebert,\n    title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},\n    author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},\n    year={2020},\n    eprint={2002.08155},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378",
    "p": "mcro:hasModelDetail",
    "o": "mcro:DFN5BCLIPViTH14378-ModelDetail"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:DFN5BCLIPViTH14378-ModelArchitecture"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Contrastive Image-Text, Zero-Shot Image Classification."
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:DFN5BCLIPViTH14378-Dataset"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-Dataset",
    "p": "prov:hasTextValue",
    "o": "DFN-5b"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:DFN5BCLIPViTH14378-Citation"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{fang2023data,\n  title={Data Filtering Networks},\n  author={Fang, Alex and Jose, Albin Madappally and Jain, Amit and Schmidt, Ludwig and Toshev, Alexander and Shankar, Vaishaal},\n  journal={arXiv preprint arXiv:2309.17425},\n  year={2023}\n}\n"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378",
    "p": "mcro:hasUseCase",
    "o": "mcro:DFN5BCLIPViTH14378-UseCase"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:DFN5BCLIPViTH14378-UseCase",
    "p": "prov:hasTextValue",
    "o": "This section details whether the model was developed with general or specific tasks in mind (e.g., plant recognition worldwide or in the Pacific Northwest). The use cases may be as broadly or narrowly defined as the developers intend. For example, if the model was built simply to label images, then this task should be indicated as the primary intended use case."
  },
  {
    "s": "mcro:sat-3l-sm",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sat-3l-sm",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sat-3l-sm-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sat-3l-sm-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sat-3l-sm-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "3 Transfomer layers"
  },
  {
    "s": "mcro:sat-3l-sm",
    "p": "mcro:hasCitation",
    "o": "mcro:sat-3l-sm-CitationInformationSection"
  },
  {
    "s": "mcro:sat-3l-sm-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sat-3l-sm-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "[`Segment any Text` paper](arxiv.org/abs/2406.16678)"
  },
  {
    "s": "mcro:flairnerenglishlarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:flairnerenglishlarge",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:flairnerenglishlarge-PerformanceMetric"
  },
  {
    "s": "mcro:flairnerenglishlarge-PerformanceMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:flairnerenglishlarge-PerformanceMetric",
    "p": "prov:hasTextValue",
    "o": "94,36 (corrected CoNLL-03)"
  },
  {
    "s": "mcro:flairnerenglishlarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:flairnerenglishlarge-ModelArchitecture"
  },
  {
    "s": "mcro:flairnerenglishlarge-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:flairnerenglishlarge-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Based on document-level XLM-R embeddings and FLERT"
  },
  {
    "s": "mcro:flairnerenglishlarge",
    "p": "mcro:hasCitation",
    "o": "mcro:flairnerenglishlarge-Citation"
  },
  {
    "s": "mcro:flairnerenglishlarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:flairnerenglishlarge-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen",
    "p": "mcro:hasModelDetail",
    "o": "mcro:HelsinkiNLPopusmtzhen-ModelDetail"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:HelsinkiNLPopusmtzhen-License"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-License",
    "p": "prov:hasTextValue",
    "o": "CC-BY-4.0"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:HelsinkiNLPopusmtzhen-Citation"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen",
    "p": "mcro:hasArchitecture",
    "o": "mcro:HelsinkiNLPopusmtzhen-Architecture"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-Architecture",
    "p": "prov:hasTextValue",
    "o": "Translation"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen",
    "p": "mcro:hasUseCase",
    "o": "mcro:HelsinkiNLPopusmtzhen-UseCase"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-UseCase",
    "p": "prov:hasTextValue",
    "o": "translation and text-to-text generation"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen",
    "p": "mcro:hasTrainingData",
    "o": "mcro:HelsinkiNLPopusmtzhen-TrainingData"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:HelsinkiNLPopusmtzhen-EvaluationData"
  },
  {
    "s": "mcro:HelsinkiNLPopusmtzhen-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese",
    "p": "mcro:hasUseCase",
    "o": "mcro:tsmatzxlmrobertanerjapanese-UseCaseInformationSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese",
    "p": "mcro:hasTrainingData",
    "o": "mcro:tsmatzxlmrobertanerjapanese-TrainingDataInformationSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese",
    "p": "mcro:hasModelParameter",
    "o": "mcro:tsmatzxlmrobertanerjapanese-ModelParameterSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese-ModelParameterSection",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:tsmatzxlmrobertanerjapanese-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:tsmatzxlmrobertanerjapanese-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "RobertaModel"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphrasexlmrmultilingualv1-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:arabertv1andv2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:arabertv1andv2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:arabertv1andv2-ModelArchitecture"
  },
  {
    "s": "mcro:arabertv1andv2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:arabertv1andv2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "based on Google's BERT architechture"
  },
  {
    "s": "mcro:arabertv1andv2",
    "p": "mcro:hasDataset",
    "o": "mcro:arabertv1andv2-Dataset"
  },
  {
    "s": "mcro:arabertv1andv2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:arabertv1andv2-Dataset",
    "p": "prov:hasTextValue",
    "o": "OSCAR unshuffled and filtered.\n- [Arabic Wikipedia dump](https://archive.org/details/arwiki-20190201) from 2020/09/01\n- [The 1.5B words Arabic Corpus](https://www.semanticscholar.org/paper/1.5-billion-words-Arabic-Corpus-El-Khair/f3eeef4afb81223df96575adadf808fe7fe440b4)\n- [The OSIAN Corpus](https://www.aclweb.org/anthology/W19-4619)\n- Assafir news articles."
  },
  {
    "s": "mcro:arabertv1andv2",
    "p": "mcro:hasCitation",
    "o": "mcro:arabertv1andv2-Citation"
  },
  {
    "s": "mcro:arabertv1andv2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:arabertv1andv2-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{antoun2020arabert,\n  title={AraBERT: Transformer-based Model for Arabic Language Understanding},\n  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},\n  booktitle={LREC 2020 Workshop Language Resources and Evaluation Conference 11--16 May 2020},\n  pages={9}\n}"
  },
  {
    "s": "mcro:arabertv1andv2",
    "p": "mcro:hasUseCase",
    "o": "mcro:arabertv1andv2-UseCase"
  },
  {
    "s": "mcro:arabertv1andv2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:arabertv1andv2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Arabic Language Understanding"
  },
  {
    "s": "mcro:M2M100418M",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:M2M100418M",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:M2M100418M-ModelArchitecture"
  },
  {
    "s": "mcro:M2M100418M-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:M2M100418M-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation."
  },
  {
    "s": "mcro:M2M100418M",
    "p": "mcro:hasCitation",
    "o": "mcro:M2M100418M-Citation"
  },
  {
    "s": "mcro:M2M100418M-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:M2M100418M-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{fan2020englishcentric,\n      title={Beyond English-Centric Multilingual Machine Translation}, \n      author={Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Edouard Grave and Michael Auli and Armand Joulin},\n      year={2020},\n      eprint={2010.11125},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n"
  },
  {
    "s": "mcro:M2M100418M",
    "p": "mcro:hasUseCase",
    "o": "mcro:M2M100418M-UseCase"
  },
  {
    "s": "mcro:M2M100418M-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:M2M100418M-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model that can directly translate between the 9,900 directions of 100 languages.\nTo translate into a target language, the target language id is forced as the first generated token.\nTo force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method."
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic",
    "p": "mcro:hasLicense",
    "o": "mcro:playgroundaiplaygroundv251024pxaesthetic-License"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-License",
    "p": "prov:hasTextValue",
    "o": "Playground v2.5 Community License"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic",
    "p": "mcro:hasModelDetail",
    "o": "mcro:playgroundaiplaygroundv251024pxaesthetic-ModelDetail"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:playgroundaiplaygroundv251024pxaesthetic-ModelArchitecture"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generative model"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic",
    "p": "mcro:hasUseCase",
    "o": "mcro:playgroundaiplaygroundv251024pxaesthetic-UseCase"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-UseCase",
    "p": "prov:hasTextValue",
    "o": "generates highly aesthetic images of resolution 1024x1024, as well as portrait and landscape aspect ratios"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic",
    "p": "mcro:hasCitation",
    "o": "mcro:playgroundaiplaygroundv251024pxaesthetic-Citation"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:playgroundaiplaygroundv251024pxaesthetic-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{li2024playground,\n      title={Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation}, \n      author={Daiqing Li and Aleks Kamko and Ehsan Akhgari and Ali Sabet and Linmiao Xu and Suhail Doshi},\n      year={2024},\n      eprint={2402.17245},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:bigvganv222khz80band256x",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bigvganv222khz80band256x",
    "p": "mcro:hasCitation",
    "o": "mcro:bigvganv222khz80band256x-Citation"
  },
  {
    "s": "mcro:bigvganv222khz80band256x-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bigvganv222khz80band256x-Citation",
    "p": "prov:hasTextValue",
    "o": "Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon"
  },
  {
    "s": "mcro:bigvganv222khz80band256x",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bigvganv222khz80band256x-ModelArchitecture"
  },
  {
    "s": "mcro:bigvganv222khz80band256x-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bigvganv222khz80band256x-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BigVGAN"
  },
  {
    "s": "mcro:bigvganv222khz80band256x",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bigvganv222khz80band256x-UseCase"
  },
  {
    "s": "mcro:bigvganv222khz80band256x-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bigvganv222khz80band256x-UseCase",
    "p": "prov:hasTextValue",
    "o": "universal neural vocoder"
  },
  {
    "s": "mcro:bigvganv222khz80band256x",
    "p": "mcro:hasDataset",
    "o": "mcro:bigvganv222khz80band256x-Dataset"
  },
  {
    "s": "mcro:bigvganv222khz80band256x-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bigvganv222khz80band256x-Dataset",
    "p": "prov:hasTextValue",
    "o": "Large-scale Compilation"
  },
  {
    "s": "mcro:wavlmbaseplus",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wavlmbaseplus",
    "p": "mcro:hasModelDetail",
    "o": "mcro:wavlmbaseplus-ModelDetail"
  },
  {
    "s": "mcro:wavlmbaseplus-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:wavlmbaseplus-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:wavlmbaseplus-Citation"
  },
  {
    "s": "mcro:wavlmbaseplus-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:wavlmbaseplus-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:wavlmbaseplus-License"
  },
  {
    "s": "mcro:wavlmbaseplus-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:wavlmbaseplus",
    "p": "mcro:hasUseCase",
    "o": "mcro:wavlmbaseplus-UseCase"
  },
  {
    "s": "mcro:wavlmbaseplus-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wavlmbaseplus-UseCase",
    "p": "prov:hasTextValue",
    "o": "This is an English pre-trained speech model that has to be fine-tuned on a downstream task like speech recognition or audio classification before it can be \nused in inference. The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/)."
  },
  {
    "s": "mcro:wavlmbaseplus",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wavlmbaseplus-ModelArchitecture"
  },
  {
    "s": "mcro:wavlmbaseplus-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wavlmbaseplus-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "[Microsoft's WavLM](https://github.com/microsoft/unilm/tree/master/wavlm)"
  },
  {
    "s": "mcro:wavlmbaseplus",
    "p": "mcro:hasDataset",
    "o": "mcro:wavlmbaseplus-Dataset"
  },
  {
    "s": "mcro:wavlmbaseplus-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:wavlmbaseplus-Dataset",
    "p": "prov:hasTextValue",
    "o": "- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)\n- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)\n- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli",
    "p": "mcro:hasModelDetail",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelDetail"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Citation"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Citation",
    "p": "prov:hasTextValue",
    "o": "Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. \u2018Less Annotating, More Classifying \u2013 Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI\u2019. Preprint, June. Open Science Framework. https://osf.io/74b8k."
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-License"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Architecture"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli",
    "p": "mcro:hasModelParameter",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelParameter"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelParameter",
    "p": "mcro:hasTrainingData",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-TrainingData"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli",
    "p": "mcro:hasConsideration",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Consideration"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Limitation"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli",
    "p": "mcro:hasUseCase",
    "o": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-UseCase"
  },
  {
    "s": "mcro:MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:NepaliBERT",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:NepaliBERT",
    "p": "mcro:hasDataset",
    "o": "mcro:NepaliBERT-DatasetInformationSection"
  },
  {
    "s": "mcro:NepaliBERT-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:NepaliBERT-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model is a fine-tuned version of [Bert Base Uncased](https://huggingface.co/bert-base-uncased) on dataset composed of different news scrapped from nepali news portals comprising of 4.6 GB of textual data.\nTHe training corpus is developed using 85467 news scrapped from different job portals. This is a preliminary dataset \nfor the experimentation. THe corpus size is about 4.3 GB of textual data. Similary evaluation data contains few news articles about 12 mb of textual data.\nTrained on about 4.6 GB of Nepali text corpus collected from various sources\nThese data were collected from nepali news site, OSCAR nepali corpus"
  },
  {
    "s": "mcro:NepaliBERT",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:NepaliBERT-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:NepaliBERT-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:NepaliBERT-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Pretraining done on bert base architecture."
  },
  {
    "s": "mcro:NepaliBERT",
    "p": "mcro:hasUseCase",
    "o": "mcro:NepaliBERT-UseCaseInformationSection"
  },
  {
    "s": "mcro:NepaliBERT-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:NepaliBERT-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This transformer model can be used for any NLP tasks related to Devenagari language. At the time of training, this is the state of the art model developed \nfor Devanagari dataset. Intrinsic evaluation with Perplexity of 8.56 achieves this state of the art whereas extrinsit evaluation done on sentiment analysis of Nepali tweets outperformed other existing \nmasked language models on Nepali dataset."
  },
  {
    "s": "mcro:NepaliBERT",
    "p": "mcro:hasCitation",
    "o": "mcro:NepaliBERT-CitationInformationSection"
  },
  {
    "s": "mcro:NepaliBERT-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:NepaliBERT-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "S. Pudasaini, S. Shakya, A. Tamang, S. Adhikari, S. Thapa and S. Lamichhane, \"NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus,\" 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), Kirtipur, Nepal, 2023, pp. 325-330, doi: 10.1109/I-SMAC58438.2023.10290690.\n\n@INPROCEEDINGS{10290690,\n  author={Pudasaini, Shushanta and Shakya, Subarna and Tamang, Aakash and Adhikari, Sajjan and Thapa, Sunil and Lamichhane, Sagar},\n  booktitle={2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, \n  title={NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus}, \n  year={2023},\n  volume={},\n  number={},\n  pages={325-330},\n  doi={10.1109/I-SMAC58438.2023.10290690}}"
  },
  {
    "s": "mcro:spladecocondenserensembledistil",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:spladecocondenserensembledistil",
    "p": "mcro:hasCitation",
    "o": "mcro:spladecocondenserensembledistil-Citation"
  },
  {
    "s": "mcro:spladecocondenserensembledistil-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:spladecocondenserensembledistil-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2205.04733,\n  doi = {10.48550/ARXIV.2205.04733},\n  url = {https://arxiv.org/abs/2205.04733},\n  author = {Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St\u00e9phane},\n  keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}"
  },
  {
    "s": "mcro:detoxify",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasCitation",
    "o": "mcro:detoxify-CitationInformationSection"
  },
  {
    "s": "mcro:detoxify-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:detoxify-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. https://github.com/unitaryai/detoxify},\n  year={2020}\n}"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasLicense",
    "o": "mcro:detoxify-LicenseInformationSection"
  },
  {
    "s": "mcro:detoxify-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:detoxify-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "mit"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:detoxify-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:detoxify-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:detoxify-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Transformers"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:detoxify-ModelArchitectureInformationSection_Pytorch_Lightning"
  },
  {
    "s": "mcro:detoxify-ModelArchitectureInformationSection_Pytorch_Lightning",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:detoxify-ModelArchitectureInformationSection_Pytorch_Lightning",
    "p": "prov:hasTextValue",
    "o": "Pytorch Lightning"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasDataset",
    "o": "mcro:detoxify-DatasetInformationSection"
  },
  {
    "s": "mcro:detoxify-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:detoxify-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Toxic comment classification"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasDataset",
    "o": "mcro:detoxify-DatasetInformationSection_Unintended_Bias"
  },
  {
    "s": "mcro:detoxify-DatasetInformationSection_Unintended_Bias",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:detoxify-DatasetInformationSection_Unintended_Bias",
    "p": "prov:hasTextValue",
    "o": "Unintended Bias in Toxic comments"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasDataset",
    "o": "mcro:detoxify-DatasetInformationSection_Multilingual"
  },
  {
    "s": "mcro:detoxify-DatasetInformationSection_Multilingual",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:detoxify-DatasetInformationSection_Multilingual",
    "p": "prov:hasTextValue",
    "o": "Multilingual toxic comment classification"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasUseCase",
    "o": "mcro:detoxify-UseCaseInformationSection"
  },
  {
    "s": "mcro:detoxify-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:detoxify-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "research purposes"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasUseCase",
    "o": "mcro:detoxify-UseCaseInformationSection_fine_tuning"
  },
  {
    "s": "mcro:detoxify-UseCaseInformationSection_fine_tuning",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:detoxify-UseCaseInformationSection_fine_tuning",
    "p": "prov:hasTextValue",
    "o": "fine-tuning"
  },
  {
    "s": "mcro:detoxify",
    "p": "mcro:hasUseCase",
    "o": "mcro:detoxify-UseCaseInformationSection_content_moderators"
  },
  {
    "s": "mcro:detoxify-UseCaseInformationSection_content_moderators",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:detoxify-UseCaseInformationSection_content_moderators",
    "p": "prov:hasTextValue",
    "o": "aid content moderators in flagging out harmful content quicker"
  },
  {
    "s": "mcro:nlpconnectvitgpt2imagecaptioning",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:nlpconnectvitgpt2imagecaptioning",
    "p": "mcro:hasModelDetail",
    "o": "mcro:nlpconnectvitgpt2imagecaptioning-ModelDetail"
  },
  {
    "s": "mcro:nlpconnectvitgpt2imagecaptioning-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:nlpconnectvitgpt2imagecaptioning-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:nlpconnectvitgpt2imagecaptioning-License"
  },
  {
    "s": "mcro:nlpconnectvitgpt2imagecaptioning-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:nlpconnectvitgpt2imagecaptioning",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:nlpconnectvitgpt2imagecaptioning-ModelArchitecture"
  },
  {
    "s": "mcro:nlpconnectvitgpt2imagecaptioning-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelDetail"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelArchitecture"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}\n"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{cherti2022reproducible,\n  title={Reproducible scaling laws for contrastive language-image learning},\n  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n  journal={arXiv preprint arXiv:2212.07143},\n  year={2022}\n}\n"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}\n"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}\n"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Dataset"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:vitbasepatch32clip448laion2bftin12kin1k-UseCase"
  },
  {
    "s": "mcro:vitbasepatch32clip448laion2bftin12kin1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wav2vec2-baseforemotionrecognition-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This is a ported version of \n[S3PRL's Wav2Vec2 for the SUPERB Emotion Recognition task](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/emotion).\n\nThe base model is [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base), which is pretrained on 16kHz \nsampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n\nFor more information refer to [SUPERB: Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051)"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition",
    "p": "mcro:hasDataset",
    "o": "mcro:wav2vec2-baseforemotionrecognition-DatasetInformationSection"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Emotion Recognition (ER) predicts an emotion class for each utterance. The most widely used ER dataset\n[IEMOCAP](https://sail.usc.edu/iemocap/) is adopted, and we follow the conventional evaluation protocol: \nwe drop the unbalanced emotion classes to leave the final four classes with a similar amount of data points and \ncross-validate on five folds of the standard splits.\n\nFor the original model's training and evaluation instructions refer to the \n[S3PRL downstream task README](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#er-emotion-recognition)."
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition",
    "p": "mcro:hasCitation",
    "o": "mcro:wav2vec2-baseforemotionrecognition-CitationInformationSection"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:wav2vec2-baseforemotionrecognition-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{yang2021superb,\n  title={SUPERB: Speech processing Universal PERformance Benchmark},\n  author={Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y and Liu, Andy T and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and others},\n  journal={arXiv preprint arXiv:2105.01051},\n  year={2021}\n}"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase",
    "p": "mcro:hasModelDetail",
    "o": "mcro:answerdotaiModernBERTbase-ModelDetail"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:answerdotaiModernBERTbase-License"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-License",
    "p": "prov:hasTextValue",
    "o": "We release the ModernBERT model architectures, model weights, training codebase under the Apache 2.0 license."
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:answerdotaiModernBERTbase-Citation"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{modernbert,\n      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, \n      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavi\u00e9 and Orion Weller and Oskar Hallstr\u00f6m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\n      year={2024},\n      eprint={2412.13663},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.13663}, \n}"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:answerdotaiModernBERTbase-ModelArchitecture"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "- Architecture: Encoder-only, Pre-Norm Transformer with GeGLU activations.\n- Sequence Length: Pre-trained up to 1,024 tokens, then extended to 8,192 tokens.\n- Data: 2 trillion tokens of English text and code.\n- Optimizer: StableAdamW with trapezoidal LR scheduling and 1-sqrt decay.\n- Hardware: Trained on 8x H100 GPUs.\n\nSee the paper for more details."
  },
  {
    "s": "mcro:answerdotaiModernBERTbase",
    "p": "mcro:hasLimitation",
    "o": "mcro:answerdotaiModernBERTbase-Limitation"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-Limitation",
    "p": "prov:hasTextValue",
    "o": "ModernBERT\u2019s training data is primarily English and code, so performance may be lower for other languages. While it can handle long sequences efficiently, using the full 8,192 tokens window may be slower than short-context inference. Like any large language model, ModernBERT may produce representations that reflect biases present in its training data. Verify critical or sensitive outputs before relying on them."
  },
  {
    "s": "mcro:answerdotaiModernBERTbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:answerdotaiModernBERTbase-UseCase"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:answerdotaiModernBERTbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "ModernBERT\u2019s native long context length makes it ideal for tasks that require processing long documents, such as retrieval, classification, and semantic search within large corpora. The model was trained on a large corpus of text and code, making it suitable for a wide range of downstream tasks, including code retrieval and hybrid (text + code) semantic search."
  },
  {
    "s": "mcro:SmolLM2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:SmolLM2",
    "p": "mcro:hasLicense",
    "o": "mcro:SmolLM2-License"
  },
  {
    "s": "mcro:SmolLM2",
    "p": "mcro:hasCitation",
    "o": "mcro:SmolLM2-Citation"
  },
  {
    "s": "mcro:SmolLM2",
    "p": "mcro:hasLimitation",
    "o": "mcro:SmolLM2-Limitations"
  },
  {
    "s": "mcro:SmolLM2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:SmolLM2-Training"
  },
  {
    "s": "mcro:SmolLM2-Training",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:SmolLM2-Limitations",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:SmolLM2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:SmolLM2-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:SmolLM2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:SmolLM2-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{allal2025smollm2smolgoesbig,\n      title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model}, \n      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Mart\u00edn Bl\u00e1zquez and Guilherme Penedo and Lewis Tunstall and Andr\u00e9s Marafioti and Hynek Kydl\u00ed\u010dek and Agust\u00edn Piqueres Lajar\u00edn and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Cl\u00e9mentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},\n      year={2025},\n      eprint={2502.02737},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.02737}, \n}"
  },
  {
    "s": "mcro:SmolLM2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:SmolLM2-ModelArchitecture"
  },
  {
    "s": "mcro:SmolLM2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:SmolLM2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer decoder"
  },
  {
    "s": "mcro:SmolLM2",
    "p": "mcro:hasUseCase",
    "o": "mcro:SmolLM2-UseCase"
  },
  {
    "s": "mcro:SmolLM2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:SmolLM2-UseCase",
    "p": "prov:hasTextValue",
    "o": "text rewriting, summarization and function calling"
  },
  {
    "s": "mcro:SmolLM2",
    "p": "mcro:hasDataset",
    "o": "mcro:SmolLM2-Dataset"
  },
  {
    "s": "mcro:SmolLM2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:SmolLM2-Dataset",
    "p": "prov:hasTextValue",
    "o": "FineWeb-Edu, DCLM, The Stack"
  },
  {
    "s": "mcro:microsoftphi2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftphi2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:microsoftphi2-ModelDetail"
  },
  {
    "s": "mcro:microsoftphi2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:microsoftphi2-ModelDetail",
    "p": "mcro:hasModelArchitectureInformation",
    "o": "mcro:microsoftphi2-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftphi2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftphi2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based model with next-word prediction objective"
  },
  {
    "s": "mcro:microsoftphi2-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:microsoftphi2-License"
  },
  {
    "s": "mcro:microsoftphi2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftphi2-License",
    "p": "prov:hasTextValue",
    "o": "The model is licensed under the MIT license"
  },
  {
    "s": "mcro:microsoftphi2",
    "p": "mcro:hasLimitationInformation",
    "o": "mcro:microsoftphi2-Limitation"
  },
  {
    "s": "mcro:microsoftphi2-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:microsoftphi2-Limitation",
    "p": "prov:hasTextValue",
    "o": "* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n\n* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n\n* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n\n* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring training data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n\n* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n\n* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses."
  },
  {
    "s": "mcro:microsoftphi2",
    "p": "mcro:hasTrainingDataInformation",
    "o": "mcro:microsoftphi2-Training"
  },
  {
    "s": "mcro:microsoftphi2-Training",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:microsoftphi2-Training",
    "p": "prov:hasTextValue",
    "o": "* Architecture: a Transformer-based model with next-word prediction objective\n\n* Context length: 2048 tokens\n\n* Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\n\n* Training tokens: 1.4T tokens\n\n* GPUs: 96xA100-80G\n\n* Training time: 14 days"
  },
  {
    "s": "mcro:microsoftphi2",
    "p": "mcro:hasUseCaseInformation",
    "o": "mcro:microsoftphi2-UseCase"
  },
  {
    "s": "mcro:microsoftphi2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftphi2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format."
  },
  {
    "s": "mcro:ko-sroberta-multitask",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ko-sroberta-multitask",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ko-sroberta-multitask-ModelArchitecture"
  },
  {
    "s": "mcro:ko-sroberta-multitask-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ko-sroberta-multitask-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: RobertaModel   (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False}))"
  },
  {
    "s": "mcro:ko-sroberta-multitask",
    "p": "mcro:hasCitation",
    "o": "mcro:ko-sroberta-multitask-Citation"
  },
  {
    "s": "mcro:ko-sroberta-multitask-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ko-sroberta-multitask-Citation",
    "p": "prov:hasTextValue",
    "o": "- Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). Kornli and korsts: New benchmark datasets for korean natural language understanding. arXiv\npreprint arXiv:2004.03289\n- Reimers, Nils and Iryna Gurevych. \u201cSentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\u201d ArXiv abs/1908.10084 (2019)\n- Reimers, Nils and Iryna Gurevych. \u201cMaking Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation.\u201d EMNLP (2020)."
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment",
    "p": "mcro:hasDataset",
    "o": "mcro:rubertbasecasedsentimentrusentiment-Dataset"
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment-Dataset",
    "p": "prov:hasTextValue",
    "o": "RuSentiment"
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:rubertbasecasedsentimentrusentiment-UseCase"
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment",
    "p": "mcro:hasCitation",
    "o": "mcro:rubertbasecasedsentimentrusentiment-Citation"
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:rubertbasecasedsentimentrusentiment-Citation",
    "p": "prov:hasTextValue",
    "o": "A. Rogers A. Romanov A. Rumshisky S. Volkova M. Gronas A. Gribov RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian. Proceedings of COLING 2018."
  },
  {
    "s": "mcro:UFNLPgatortronbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:UFNLPgatortronbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:UFNLPgatortronbase-ModelArchitecture"
  },
  {
    "s": "mcro:UFNLPgatortronbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:UFNLPgatortronbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT architecure"
  },
  {
    "s": "mcro:UFNLPgatortronbase",
    "p": "mcro:hasDataset",
    "o": "mcro:UFNLPgatortronbase-Dataset"
  },
  {
    "s": "mcro:UFNLPgatortronbase-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:UFNLPgatortronbase-Dataset",
    "p": "prov:hasTextValue",
    "o": "82B words of de-identified clinical notes from the University of Florida Health System"
  },
  {
    "s": "mcro:UFNLPgatortronbase-Dataset",
    "p": "prov:hasTextValue",
    "o": "6.1B words from PubMed CC0"
  },
  {
    "s": "mcro:UFNLPgatortronbase-Dataset",
    "p": "prov:hasTextValue",
    "o": "2.5B words from WikiText"
  },
  {
    "s": "mcro:UFNLPgatortronbase-Dataset",
    "p": "prov:hasTextValue",
    "o": "0.5B words of de-identified clinical notes from MIMIC-III"
  },
  {
    "s": "mcro:UFNLPgatortronbase",
    "p": "mcro:hasCitation",
    "o": "mcro:UFNLPgatortronbase-Citation"
  },
  {
    "s": "mcro:UFNLPgatortronbase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:UFNLPgatortronbase-Citation",
    "p": "prov:hasTextValue",
    "o": "Yang X, Lyu T, Li Q, Lee C-Y, Bian J, Hogan WR, Wu Y\u2020. A study of deep learning methods for de-identification of clinical notes in cross-institute settings. BMC Med Inform Decis Mak. 2020 Dec 5;19(5):232. https://www.ncbi.nlm.nih.gov/pubmed/31801524."
  },
  {
    "s": "mcro:UFNLPgatortronbase-Citation",
    "p": "prov:hasTextValue",
    "o": "Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien C, Compas C, Martin C, Costa AB, Flores MG, Zhang Y, Magoc T, Harle CA, Lipori G, Mitchell DA, Hogan WR, Shenkman EA, Bian J, Wu Y\u2020. A large language model for electronic health records. Npj Digit Med. Nature Publishing Group; . 2022 Dec 26;5(1):1\u20139. https://www.nature.com/articles/s41746-022-00742-2"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:microsoftPhi3mini4kinstruct-ModelDetail"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftPhi3mini4kinstruct-UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct",
    "p": "mcro:hasTrainingData",
    "o": "mcro:microsoftPhi3mini4kinstruct-TrainingDataInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct",
    "p": "mcro:hasBenchmark",
    "o": "mcro:microsoftPhi3mini4kinstruct-Benchmark"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct",
    "p": "mcro:hasLicense",
    "o": "mcro:microsoftPhi3mini4kinstruct-License"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-Benchmark",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) that it can support.\n\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters."
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require \n1) memory/compute constrained environments; \n2) latency bound scenarios; \n3) strong reasoning (especially math and logic). \n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**"
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)."
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-Benchmark",
    "p": "prov:hasTextValue",
    "o": "We report the results under completion format for Phi-3-Mini-4K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT3.5-Turbo-1106.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k\u2013shot examples is listed per-benchmark. \n\n| Category | Benchmark | Phi-3-Mini-4K-Ins | Gemma-7B | Mistral-7b | Mixtral-8x7b | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |\n|:----------|:-----------|:-------------------|:----------|:------------|:--------------|:----------------|:-------------------|\n| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.0 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |\n| | MMLU <br>5-shot | 70.9 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |\n| | BigBench Hard CoT<br>3-shot| 73.5 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |\n| Language Understanding | ANLI <br>7-shot | 53.6 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |\n| | HellaSwag <br>5-shot| 75.3 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |\n| Reasoning | ARC Challenge <br>10-shot | 86.3 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |\n| | BoolQ <br>0-shot | 78.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |\n| | MedQA <br>2-shot| 56.5 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |\n| | OpenBookQA <br>10-shot| 82.2 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |\n| | PIQA <br>5-shot| 83.5 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |\n| | GPQA <br>0-shot| 30.6 | 2.9 | 15 | 6.9 | 32.4 | 30.8 |\n| | Social IQA <br>5-shot| 77.6 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |\n| | TruthfulQA (MC2) <br>10-shot| 64.7 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |\n| | WinoGrande <br>5-shot| 71.6 | 55.6 | 54.2 | 62 | 65 | 68.8 |\n| Factual Knowledge | TriviaQA <br>5-shot| 61.4 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |\n| Math | GSM8K CoT <br>8-shot| 85.7 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |\n| Code Generation | HumanEval <br>0-shot| 57.3 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |\n| | MBPP <br>3-shot| 69.8 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |\n| **Average** | | **67.6** | **56.0** | **56.4** | **64.4** | **65.5** | **70.4** |\n\n\nWe take a closer look at different categories across 100 public benchmark datasets at the table below: \n\n| Category | Phi-3-Mini-4K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |\n|:----------|:------------------------|:----------|:------------|:--------------|:---------------------|:---------------|\n| Popular aggregated benchmark | 61.1 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |\n| Reasoning | 70.8 | 60.3 | 62.8 | 68.1 | 69.6 | 71.8 |\n| Language understanding | 60.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |\n| Code generation | 60.7 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |\n| Math | 50.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |\n| Factual knowledge | 38.4 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |\n| Multilingual | 56.7 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |\n| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |\n\n\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine. "
  },
  {
    "s": "mcro:microsoftPhi3mini4kinstruct-License",
    "p": "prov:hasTextValue",
    "o": "The model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-mini-4k/resolve/main/LICENSE)."
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2",
    "p": "mcro:hasDataset",
    "o": "mcro:crossencodermsmarcoMiniLML12v2-Dataset"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2-Dataset",
    "p": "prov:hasTextValue",
    "o": "This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task."
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:crossencodermsmarcoMiniLML12v2-UseCase"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:crossencodermsmarcoMiniLML12v2-Performance"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML12v2-Performance",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:Qwen317B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen317B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Qwen317B-ModelDetail"
  },
  {
    "s": "mcro:Qwen317B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Qwen317B-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen317B-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen317B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen317B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:Qwen317B",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen317B-Citation"
  },
  {
    "s": "mcro:Qwen317B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen317B",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen317B-UseCase"
  },
  {
    "s": "mcro:Qwen317B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:depthanythingDepthAnythingV2Smallhf-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Depth Anything V2 leverages the [DPT](https://huggingface.co/docs/transformers/model_doc/dpt) architecture with a [DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2) backbone."
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf",
    "p": "mcro:hasUseCase",
    "o": "mcro:depthanythingDepthAnythingV2Smallhf-UseCaseInformationSection"
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for tasks like zero-shot depth estimation. See the [model hub](https://huggingface.co/models?search=depth-anything) to look for\nother versions on a task that interests you."
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf",
    "p": "mcro:hasCitation",
    "o": "mcro:depthanythingDepthAnythingV2Smallhf-CitationInformationSection"
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:depthanythingDepthAnythingV2Smallhf-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{yang2024depth,\n      title={Depth Anything V2}, \n      author={Lihe Yang and Bingyi Kang and Zilong Huang and Zhen Zhao and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},\n      year={2024},\n      eprint={2406.09414},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}\n}"
  },
  {
    "s": "mcro:llama2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llama2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:llama2-ModelDetail"
  },
  {
    "s": "mcro:llama2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llama2-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:llama2-License"
  },
  {
    "s": "mcro:llama2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llama2-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)"
  },
  {
    "s": "mcro:llama2-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llama2-ModelArchitecture"
  },
  {
    "s": "mcro:llama2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:llama2",
    "p": "mcro:hasUseCase",
    "o": "mcro:llama2-UseCase"
  },
  {
    "s": "mcro:llama2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llama2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks."
  },
  {
    "s": "mcro:llama2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:llama2-TrainingData"
  },
  {
    "s": "mcro:llama2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:llama2-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data."
  },
  {
    "s": "mcro:llama2",
    "p": "mcro:hasCitation",
    "o": "mcro:llama2-Citation"
  },
  {
    "s": "mcro:llama2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:llama2-Citation",
    "p": "prov:hasTextValue",
    "o": "\"Llama-2: Open Foundation and Fine-tuned Chat Models\"(arxiv.org/abs/2307.09288)"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid",
    "p": "mcro:hasModelDetail",
    "o": "mcro:visiontransformerbase-sizedmodel-hybrid-ModelDetail"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:visiontransformerbase-sizedmodel-hybrid-ModelArchitecture"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:visiontransformerbase-sizedmodel-hybrid-IntendedUseCase"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid",
    "p": "mcro:hasTrainingData",
    "o": "mcro:visiontransformerbase-sizedmodel-hybrid-TrainingData"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid",
    "p": "mcro:hasTrainingProcedure",
    "o": "mcro:visiontransformerbase-sizedmodel-hybrid-TrainingProcedure"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid-TrainingProcedure",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid",
    "p": "mcro:hasEvaluationResults",
    "o": "mcro:visiontransformerbase-sizedmodel-hybrid-EvaluationResults"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid-EvaluationResults",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid",
    "p": "mcro:hasCitation",
    "o": "mcro:visiontransformerbase-sizedmodel-hybrid-Citation"
  },
  {
    "s": "mcro:visiontransformerbase-sizedmodel-hybrid-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:myshellaiMeloTTS",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:myshellaiMeloTTS",
    "p": "mcro:hasLicense",
    "o": "mcro:myshellaiMeloTTS-License"
  },
  {
    "s": "mcro:myshellaiMeloTTS-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:myshellaiMeloTTS-License",
    "p": "prov:hasTextValue",
    "o": "This library is under MIT License, which means it is free for both commercial and non-commercial use."
  },
  {
    "s": "mcro:myshellaiMeloTTS",
    "p": "mcro:hasCitation",
    "o": "mcro:myshellaiMeloTTS-Citation"
  },
  {
    "s": "mcro:myshellaiMeloTTS-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:myshellaiMeloTTS-Citation",
    "p": "prov:hasTextValue",
    "o": "@software{zhao2024melo,\n  author={Zhao, Wenliang and Yu, Xumin and Qin, Zengyi},\n  title = {MeloTTS: High-quality Multi-lingual Multi-accent Text-to-Speech},\n  url = {https://github.com/myshell-ai/MeloTTS},\n  year = {2023}\n}"
  },
  {
    "s": "mcro:myshellaiMeloTTS",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:myshellaiMeloTTS-ModelArchitecture"
  },
  {
    "s": "mcro:myshellaiMeloTTS-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:myshellaiMeloTTS-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "MeloTTS is a **high-quality multi-lingual** text-to-speech library by [MIT](https://www.mit.edu/) and [MyShell.ai](https://myshell.ai)."
  },
  {
    "s": "mcro:myshellaiMeloTTS",
    "p": "mcro:hasUseCase",
    "o": "mcro:myshellaiMeloTTS-UseCase"
  },
  {
    "s": "mcro:myshellaiMeloTTS-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:myshellaiMeloTTS-UseCase",
    "p": "prov:hasTextValue",
    "o": "MeloTTS is a **high-quality multi-lingual** text-to-speech library"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelArchitecture"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Citation"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{steiner2021augreg,\n  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\n  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2106.10270},\n  year={2021}\n}\n"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}\n"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Dataset"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-PretrainDataset"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-PretrainDataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-PretrainDataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-21k"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-UseCase"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Classification"
  },
  {
    "s": "mcro:vit_base_patch16_224.augreg2_in21k_ft_in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Embeddings"
  },
  {
    "s": "mcro:mobilebertuncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mobilebertuncased",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mobilebertuncased-ModelArchitecture"
  },
  {
    "s": "mcro:mobilebertuncased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mobilebertuncased-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks."
  },
  {
    "s": "mcro:QdrantallminiLML6v2withattentions",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:QdrantallminiLML6v2withattentions",
    "p": "mcro:hasUseCase",
    "o": "mcro:QdrantallminiLML6v2withattentions-UseCaseInformationSection"
  },
  {
    "s": "mcro:QdrantallminiLML6v2withattentions-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:QdrantallminiLML6v2withattentions-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model is intended to be used for [BM42 searches](https://qdrant.tech/articles/bm42/)."
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronosboltsmall-ModelArchitecture"
  },
  {
    "s": "mcro:chronosboltsmall-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "T5 encoder-decoder architecture"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasCitation",
    "o": "mcro:chronosboltsmall-Citation"
  },
  {
    "s": "mcro:chronosboltsmall-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasLicense",
    "o": "mcro:chronosboltsmall-License"
  },
  {
    "s": "mcro:chronosboltsmall-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-License",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0 License"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronosboltsmall-UseCase"
  },
  {
    "s": "mcro:chronosboltsmall-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-UseCase",
    "p": "prov:hasTextValue",
    "o": "zero-shot forecasting"
  },
  {
    "s": "mcro:chronosboltsmall",
    "p": "mcro:hasDataset",
    "o": "mcro:chronosboltsmall-Dataset"
  },
  {
    "s": "mcro:chronosboltsmall-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:chronosboltsmall-Dataset",
    "p": "prov:hasTextValue",
    "o": "nearly 100 billion time series observations"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:efficientnet_b0.ra_in1k-ModelDetail"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:efficientnet_b0.ra_in1k-ModelArchitecture"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:efficientnet_b0.ra_in1k-Dataset"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b0.ra_in1k-Citation1"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-Citation1",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{tan2019efficientnet,\n  title={Efficientnet: Rethinking model scaling for convolutional neural networks},\n  author={Tan, Mingxing and Le, Quoc},\n  booktitle={International conference on machine learning},\n  pages={6105--6114},\n  year={2019},\n  organization={PMLR}\n}"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b0.ra_in1k-Citation2"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:efficientnet_b0.ra_in1k-Citation3"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-Citation3",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-Citation3",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:efficientnet_b0.ra_in1k-UseCase"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Classification"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Feature Map Extraction"
  },
  {
    "s": "mcro:efficientnet_b0.ra_in1k-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Embeddings"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi",
    "p": "mcro:hasUseCase",
    "o": "mcro:wav2vec2largexlsrhindi-UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wav2vec2largexlsrhindi-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi",
    "p": "mcro:hasDataset",
    "o": "mcro:wav2vec2largexlsrhindi-DatasetInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:wav2vec2largexlsrhindi-QuantativeAnalysisSection"
  },
  {
    "s": "mcro:wav2vec2largexlsrhindi-QuantativeAnalysisSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:prajjwal1bertmedium",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:prajjwal1bertmedium",
    "p": "mcro:hasCitation",
    "o": "mcro:prajjwal1bertmedium-Citation"
  },
  {
    "s": "mcro:prajjwal1bertmedium-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:prajjwal1bertmedium-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bhargava2021generalization,\n      title={Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics}, \n      author={Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},\n      year={2021},\n      eprint={2110.01518},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{DBLP:journals/corr/abs-1908-08962,\n  author    = {Iulia Turc and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {Well-Read Students Learn Better: The Impact of Student Initialization\n               on Knowledge Distillation},\n  journal   = {CoRR},\n  volume    = {abs/1908.08962},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1908.08962},\n  eprinttype = {arXiv},\n  eprint    = {1908.08962},\n  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n"
  },
  {
    "s": "mcro:prajjwal1bertmedium",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:prajjwal1bertmedium-Architecture"
  },
  {
    "s": "mcro:prajjwal1bertmedium-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:prajjwal1bertmedium-Architecture",
    "p": "prov:hasTextValue",
    "o": "Config of this model:\n- `prajjwal1/bert-medium` (L=8, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-medium)"
  },
  {
    "s": "mcro:prajjwal1bertmedium",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:prajjwal1bertmedium-UseCase"
  },
  {
    "s": "mcro:prajjwal1bertmedium-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:prajjwal1bertmedium-UseCase",
    "p": "prov:hasTextValue",
    "o": "These models are supposed to be trained on a downstream task."
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped",
    "p": "mcro:hasModelDetail",
    "o": "mcro:EleutherAIpythia70mdeduped-ModelDetail"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:EleutherAIpythia70mdeduped-License"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:EleutherAIpythia70mdeduped-Architecture"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-Architecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based Language Model"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:EleutherAIpythia70mdeduped-UseCase"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-UseCase",
    "p": "prov:hasTextValue",
    "o": "research on the behavior, functionality, and limitations of large language models"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped",
    "p": "mcro:hasTrainingData",
    "o": "mcro:EleutherAIpythia70mdeduped-TrainingData"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-TrainingData",
    "p": "prov:hasTextValue",
    "o": "the Pile after the dataset has been globally deduplicated"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped",
    "p": "mcro:hasLimitation",
    "o": "mcro:EleutherAIpythia70mdeduped-Limitation"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-Limitation",
    "p": "prov:hasTextValue",
    "o": "Never rely on Pythia-70M-deduped to produce factually accurate output."
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped",
    "p": "mcro:hasCitation",
    "o": "mcro:EleutherAIpythia70mdeduped-Citation"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:EleutherAIpythia70mdeduped-Citation",
    "p": "prov:hasTextValue",
    "o": "https://arxiv.org/pdf/2304.01373.pdf"
  },
  {
    "s": "mcro:Whisperlargev2modelforCTranslate2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Whisperlargev2modelforCTranslate2",
    "p": "mcro:hasUseCase",
    "o": "mcro:Whisperlargev2modelforCTranslate2-UseCaseInformationSection"
  },
  {
    "s": "mcro:Whisperlargev2modelforCTranslate2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Whisperlargev2modelforCTranslate2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper."
  },
  {
    "s": "mcro:Whisperlargev2modelforCTranslate2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Whisperlargev2modelforCTranslate2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Whisperlargev2modelforCTranslate2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Whisperlargev2modelforCTranslate2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "openai/whisper-large-v2"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21",
    "p": "mcro:hasModelDetail",
    "o": "mcro:EmergentMethodsglinermediumnewsv21-ModelDetail"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:EmergentMethodsglinermediumnewsv21-License"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:EmergentMethodsglinermediumnewsv21-ModelArchitecture"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "microsoft/deberta"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21",
    "p": "mcro:hasCitation",
    "o": "mcro:EmergentMethodsglinermediumnewsv21-Citation"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21",
    "p": "mcro:hasUseCase",
    "o": "mcro:EmergentMethodsglinermediumnewsv21-UseCase"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21",
    "p": "mcro:hasTrainingData",
    "o": "mcro:EmergentMethodsglinermediumnewsv21-TrainingData"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:EmergentMethodsglinermediumnewsv21-TrainingData",
    "p": "prov:hasTextValue",
    "o": "AskNews-NER-v0"
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Inteldptlarge-ModelDetail"
  },
  {
    "s": "mcro:Inteldptlarge-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Inteldptlarge-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. \nIt was introduced in the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ranftl et al. (2021) and first released in [this repository](https://github.com/isl-org/DPT). \nDPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dpt_architecture.jpg)\n\nThe model card has been written in combination by the Hugging Face team and Intel."
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasLicense",
    "o": "mcro:Inteldptlarge-License"
  },
  {
    "s": "mcro:Inteldptlarge-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Inteldptlarge-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasUseCase",
    "o": "mcro:Inteldptlarge-UseCase"
  },
  {
    "s": "mcro:Inteldptlarge-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Inteldptlarge-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for zero-shot monocular depth estimation. See the [model hub](https://huggingface.co/models?search=dpt) to look for fine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasConsideration",
    "o": "mcro:Inteldptlarge-Consideration"
  },
  {
    "s": "mcro:Inteldptlarge-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Inteldptlarge-Consideration",
    "p": "prov:hasTextValue",
    "o": "Multiple datasets compiled together"
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasTrainingData",
    "o": "mcro:Inteldptlarge-TrainingData"
  },
  {
    "s": "mcro:Inteldptlarge-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:Inteldptlarge-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The dataset is called MIX 6, and contains around 1.4M images. The model was initialized with ImageNet-pretrained weights."
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:Inteldptlarge-QuantativeAnalysis"
  },
  {
    "s": "mcro:Inteldptlarge-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:Inteldptlarge-QuantativeAnalysis",
    "p": "prov:hasTextValue",
    "o": "Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the\nprotocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. ([Ranftl et al., 2021](https://arxiv.org/abs/2103.13413))"
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:Inteldptlarge-EthicalConsideration"
  },
  {
    "s": "mcro:Inteldptlarge-EthicalConsideration",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:Inteldptlarge-EthicalConsideration",
    "p": "prov:hasTextValue",
    "o": "The training data come from multiple image datasets compiled together."
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasTradeoff",
    "o": "mcro:Inteldptlarge-Tradeoff"
  },
  {
    "s": "mcro:Inteldptlarge-Tradeoff",
    "p": "rdf:type",
    "o": "mcro:Trade-offInformationSection"
  },
  {
    "s": "mcro:Inteldptlarge-Tradeoff",
    "p": "prov:hasTextValue",
    "o": "Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. There are no additional caveats or recommendations for this model."
  },
  {
    "s": "mcro:Inteldptlarge",
    "p": "mcro:hasCitation",
    "o": "mcro:Inteldptlarge-Citation"
  },
  {
    "s": "mcro:Inteldptlarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Inteldptlarge-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2103-13413,\n  author    = {Ren{\\'{e}} Ranftl and\n               Alexey Bochkovskiy and\n               Vladlen Koltun},\n  title     = {Vision Transformers for Dense Prediction},\n  journal   = {CoRR},\n  volume    = {abs/2103.13413},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2103.13413},\n  eprinttype = {arXiv},\n  eprint    = {2103.13413},\n  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13413.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2VL2BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "* Naive Dynamic Resolution"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "* Multimodal Rotary Position Embedding (M-ROPE)"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct",
    "p": "mcro:hasLimitation",
    "o": "mcro:Qwen2VL2BInstruct-Limitation"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Lack of Audio Support"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Data timeliness"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Constraints in Individuals and Intellectual Property (IP)"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Limited Capacity for Complex Instruction"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Insufficient Counting Accuracy"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Weak Spatial Reasoning Skills"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen2VL2BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{Qwen2VL"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{Qwen-VL"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2VL2BInstruct-UseCase"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "SoTA understanding of images of various resolution & ratio"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Understanding videos of 20min+"
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Agent that can operate your mobiles, robots, etc."
  },
  {
    "s": "mcro:Qwen2VL2BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Multilingual Support"
  },
  {
    "s": "mcro:cointegratedruberttiny2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:cointegratedruberttiny2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:cointegratedruberttiny2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:cointegratedruberttiny2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:cointegratedruberttiny2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "a small Russian BERT-based encoder with high-quality sentence embeddings"
  },
  {
    "s": "mcro:cointegratedruberttiny2",
    "p": "mcro:hasUseCase",
    "o": "mcro:cointegratedruberttiny2-UseCaseInformationSection"
  },
  {
    "s": "mcro:cointegratedruberttiny2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:cointegratedruberttiny2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task."
  },
  {
    "s": "mcro:Whispertiny",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Whispertiny",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Whispertiny-ModelDetail"
  },
  {
    "s": "mcro:Whispertiny-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Whispertiny-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:Whispertiny-UseCase"
  },
  {
    "s": "mcro:Whispertiny-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb1finetunedade20k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:segformerb1finetunedade20k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:segformerb1finetunedade20k-ModelDetail"
  },
  {
    "s": "mcro:segformerb1finetunedade20k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:segformerb1finetunedade20k-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:segformerb1finetunedade20k-Architecture"
  },
  {
    "s": "mcro:segformerb1finetunedade20k-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:segformerb1finetunedade20k-Architecture",
    "p": "prov:hasTextValue",
    "o": "SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset."
  },
  {
    "s": "mcro:segformerb1finetunedade20k",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:segformerb1finetunedade20k-IntendedUseCase"
  },
  {
    "s": "mcro:segformerb1finetunedade20k-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb1finetunedade20k-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:segformerb1finetunedade20k",
    "p": "mcro:hasCitation",
    "o": "mcro:segformerb1finetunedade20k-Citation"
  },
  {
    "s": "mcro:segformerb1finetunedade20k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:segformerb1finetunedade20k-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:wav2vec2base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wav2vec2base",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wav2vec2base-ModelArchitecture"
  },
  {
    "s": "mcro:wav2vec2base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2base-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Facebook's Wav2Vec2"
  },
  {
    "s": "mcro:wav2vec2base",
    "p": "mcro:hasCitation",
    "o": "mcro:wav2vec2base-Citation"
  },
  {
    "s": "mcro:wav2vec2base-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:wav2vec2base-Citation",
    "p": "prov:hasTextValue",
    "o": "Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli"
  },
  {
    "s": "mcro:wav2vec2base",
    "p": "mcro:hasUseCase",
    "o": "mcro:wav2vec2base-UseCase"
  },
  {
    "s": "mcro:wav2vec2base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2base-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition"
  },
  {
    "s": "mcro:bertwithflashattention",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bertwithflashattention",
    "p": "mcro:hasModelParameter",
    "o": "mcro:bertwithflashattention-ModelParameterSection"
  },
  {
    "s": "mcro:bertwithflashattention-ModelParameterSection",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:bertwithflashattention-ModelParameterSection",
    "p": "prov:hasTextValue",
    "o": "- `use_flash_attn`: If `True`, always use flash attention. If `None`, use flash attention when GPU is available. If `False`, never use flash attention (works on CPU).\n- `window_size`: Size (left and right) of the local attention window. If `(-1, -1)`, use global attention\n- `dense_seq_output`: If true, we only need to pass the hidden states for the masked out token (around 15%) to the classifier heads. I set this to true for pretraining.\n- `fused_mlp`: Whether to use fused-dense. Useful to reduce VRAM in combination with activation checkpointing\n- `mlp_checkpoint_lvl`: One of `{0, 1, 2}`. Increasing this increases the amount of activation checkpointing within the MLP. Keep this at 0 for pretraining and use gradient accumulation instead. For embedding training, increase this as much as needed.\n- `last_layer_subset`: If true, we only need the compute the last layer for a subset of tokens. I left this to false.\n- `use_qk_norm`: Whether or not to use QK-normalization\n- `num_loras`: Number of LoRAs to use when initializing a `BertLoRA` model. Has no effect on other models."
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersdistilbertbasenlimeantokens-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersdistilbertbasenlimeantokens-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:sentencetransformersdistilbertbasenlimeantokens-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersdistilbertbasenlimeantokens-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Xlmrobertalargenerspanish-ModelDetail"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Xlmrobertalargenerspanish-ModelArchitecture"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "XLM-Roberta-large"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:Xlmrobertalargenerspanish-Dataset"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-Dataset",
    "p": "prov:hasTextValue",
    "o": "CoNLL-2002"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:Xlmrobertalargenerspanish-Performance"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-Performance",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:Xlmrobertalargenerspanish-Performance",
    "p": "prov:hasTextValue",
    "o": "F1-score of 89.17"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:llama4Scout17B16EInstruct-ModelDetail"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llama4Scout17B16EInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality."
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:llama4Scout17B16EInstruct-License"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license, the Llama 4 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE)"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:llama4Scout17B16EInstruct-IntendedUseCase"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases."
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct",
    "p": "mcro:hasTrainingData",
    "o": "mcro:llama4Scout17B16EInstruct-TrainingData"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:llama4Scout17B16EInstruct-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta\u2019s products and services. This includes publicly shared posts from Instagram and Facebook and people\u2019s interactions with Meta AI."
  },
  {
    "s": "mcro:Qwen257B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen257B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen257B-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen257B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen257B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:Qwen257B",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen257B-Citation"
  },
  {
    "s": "mcro:Qwen257B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen257B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:Qwen257B",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:Qwen257B-IntendedUseCase"
  },
  {
    "s": "mcro:Qwen257B-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen257B-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "Causal Language Models"
  },
  {
    "s": "mcro:infoxlm",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:infoxlm",
    "p": "mcro:hasCitation",
    "o": "mcro:infoxlm-Citation"
  },
  {
    "s": "mcro:infoxlm-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:infoxlm-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{chi-etal-2021-infoxlm,\n  title = \"{I}nfo{XLM}: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training\",\n  author={Chi, Zewen and Dong, Li and Wei, Furu and Yang, Nan and Singhal, Saksham and Wang, Wenhui and Song, Xia and Mao, Xian-Ling and Huang, Heyan and Zhou, Ming},\n  booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n  month = jun,\n  year = \"2021\",\n  address = \"Online\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://aclanthology.org/2021.naacl-main.280\",\n  doi = \"10.18653/v1/2021.naacl-main.280\",\n  pages = \"3576--3588\",}"
  },
  {
    "s": "mcro:NLLB200",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasLicense",
    "o": "mcro:NLLB200-License"
  },
  {
    "s": "mcro:NLLB200-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:NLLB200-License",
    "p": "prov:hasTextValue",
    "o": "CC-BY-NC"
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasUseCase",
    "o": "mcro:NLLB200-UseCase"
  },
  {
    "s": "mcro:NLLB200-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:NLLB200-UseCase",
    "p": "prov:hasTextValue",
    "o": "Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\n- Primary intended users: Primary users are researchers and machine translation research community.\n- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasConsideration",
    "o": "mcro:NLLB200-Consideration"
  },
  {
    "s": "mcro:NLLB200-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:NLLB200-Consideration",
    "p": "prov:hasTextValue",
    "o": "\u2022 In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety)."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasTradeoff",
    "o": "mcro:NLLB200-Tradeoff"
  },
  {
    "s": "mcro:NLLB200-Tradeoff",
    "p": "rdf:type",
    "o": "mcro:TradeoffInformationSection"
  },
  {
    "s": "mcro:NLLB200-Tradeoff",
    "p": "prov:hasTextValue",
    "o": "\u2022 Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasCitation",
    "o": "mcro:NLLB200-Citation"
  },
  {
    "s": "mcro:NLLB200-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:NLLB200-Citation",
    "p": "prov:hasTextValue",
    "o": "- Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022"
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasDataset",
    "o": "mcro:NLLB200-EvalData"
  },
  {
    "s": "mcro:NLLB200-EvalData",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:NLLB200-EvalData",
    "p": "prov:hasTextValue",
    "o": "- Datasets: Flores-200 dataset is described in Section 4\n- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\n- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasDataset",
    "o": "mcro:NLLB200-TrainData"
  },
  {
    "s": "mcro:NLLB200-TrainData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:NLLB200-TrainData",
    "p": "prov:hasTextValue",
    "o": "\u2022 We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasArchitecture",
    "o": "mcro:NLLB200-Arch"
  },
  {
    "s": "mcro:NLLB200-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:NLLB200-Arch",
    "p": "prov:hasTextValue",
    "o": "- Information about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:NLLB200-PerfMetric"
  },
  {
    "s": "mcro:NLLB200-PerfMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:NLLB200-PerfMetric",
    "p": "prov:hasTextValue",
    "o": "\u2022 Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations."
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This system is composed of an wav2vec2 model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss. Speaker Verification is performed using cosine distance between speaker embeddings."
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP",
    "p": "mcro:hasLicense",
    "o": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-LicenseInformationSection"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP",
    "p": "mcro:hasCitation",
    "o": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-CitationInformationSection"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{speechbrain,\n  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran\u00e7ois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n  year={2021},\n  eprint={2106.04624},\n  archivePrefix={arXiv},\n  primaryClass={eess.AS},\n  note={arXiv:2106.04624}\n}"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-UseCaseInformationSection"
  },
  {
    "s": "mcro:speechbrainemotionrecognitionwav2vec2IEMOCAP-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersparaphraseMiniLML3v2-ModelArchitecture"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersparaphraseMiniLML3v2-Citation"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersparaphraseMiniLML3v2-UseCase"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersparaphraseMiniLML3v2-UseCase",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:BAAIbgererankerlarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BAAIbgererankerlarge",
    "p": "mcro:hasLicense",
    "o": "mcro:BAAIbgererankerlarge-License"
  },
  {
    "s": "mcro:BAAIbgererankerlarge",
    "p": "mcro:hasCitation",
    "o": "mcro:BAAIbgererankerlarge-Citation"
  },
  {
    "s": "mcro:BAAIbgererankerlarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BAAIbgererankerlarge-ModelArchitecture"
  },
  {
    "s": "mcro:BAAIbgererankerlarge",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:BAAIbgererankerlarge-UseCase"
  },
  {
    "s": "mcro:BAAIbgererankerlarge",
    "p": "mcro:hasDataset",
    "o": "mcro:BAAIbgererankerlarge-Dataset"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "cross-encoder"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-UseCase",
    "p": "prov:hasTextValue",
    "o": "re-rank top-k documents"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:BAAIbgererankerlarge-Dataset",
    "p": "prov:hasTextValue",
    "o": "multilingual pair data"
  },
  {
    "s": "mcro:Qwen2514BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2514BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2514BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen2514BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2514BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:Qwen2514BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen2514BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen2514BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen2514BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2514BInstruct-UseCase"
  },
  {
    "s": "mcro:Qwen2514BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:t53b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:t53b",
    "p": "mcro:hasModelDetail",
    "o": "mcro:t53b-ModelDetail"
  },
  {
    "s": "mcro:t53b-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:t53b-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:t53b-Citation"
  },
  {
    "s": "mcro:t53b-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:t53b-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:t53b-License"
  },
  {
    "s": "mcro:t53b-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:t53b-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:t53b-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:t53b-ModelArchitecture"
  },
  {
    "s": "mcro:t53b-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:t53b",
    "p": "mcro:hasUseCase",
    "o": "mcro:t53b-UseCase"
  },
  {
    "s": "mcro:t53b-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:t53b",
    "p": "mcro:hasDataset",
    "o": "mcro:t53b-Dataset"
  },
  {
    "s": "mcro:t53b-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:t53b-Dataset",
    "p": "prov:hasTextValue",
    "o": "Colossal Clean Crawled Corpus (C4)"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:microsoftPhi4multimodalinstruct-ModelDetail"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftPhi4multimodalinstruct-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Phi-4-multimodal-instruct has 5.6B parameters and is a multimodal transformer model."
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-ModelDetail",
    "p": "mcro:hasTrainingData",
    "o": "mcro:microsoftPhi4multimodalinstruct-TrainingData"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-TrainingData",
    "p": "prov:hasTextValue",
    "o": "5T tokens, 2.3M speech hours, and 1.1T image-text tokens"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:microsoftPhi4multimodalinstruct-License"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-License",
    "p": "prov:hasTextValue",
    "o": "The model is licensed under the MIT license."
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:microsoftPhi4multimodalinstruct-IntendedUseCase"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi4multimodalinstruct-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended for broad multilingual and multimodal commercial and research use."
  },
  {
    "s": "mcro:Marqo-FashionSigLIP",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Marqo-FashionSigLIP",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Marqo-FashionSigLIP-ModelDetail"
  },
  {
    "s": "mcro:Marqo-FashionSigLIP-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Marqo-FashionSigLIP-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Marqo-FashionSigLIP-ModelArchitecture"
  },
  {
    "s": "mcro:Marqo-FashionSigLIP-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Marqo-FashionSigLIP-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:Marqo-FashionSigLIP-UseCase"
  },
  {
    "s": "mcro:Marqo-FashionSigLIP-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit",
    "p": "mcro:hasModelDetail",
    "o": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelDetail"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelArchitecture"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "4-bit quantized version of DeepSeek-R1-0528-Qwen3-8B using MLX, optimized for Apple Silicon."
  },
  {
    "s": "mcro:BAAIbgelargeen",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BAAIbgelargeen",
    "p": "mcro:hasCitation",
    "o": "mcro:BAAIbgelargeen-Citation"
  },
  {
    "s": "mcro:BAAIbgelargeen-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgelargeen-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:BAAIbgelargeen",
    "p": "mcro:hasLicense",
    "o": "mcro:BAAIbgelargeen-License"
  },
  {
    "s": "mcro:BAAIbgelargeen-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:BAAIbgelargeen-License",
    "p": "prov:hasTextValue",
    "o": "FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge."
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:nguyenvulebinhwav2vec2basevi-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use wav2vec2 architecture for doing Self-Supervised learning"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi",
    "p": "mcro:hasDataset",
    "o": "mcro:nguyenvulebinhwav2vec2basevi-DatasetInformationSection"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our self-supervised model is pre-trained on a massive audio set of 13k hours of Vietnamese youtube audio, which includes:\n  - Clean audio\n  - Noise audio\n  - Conversation\n  - Multi-gender and dialects"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi",
    "p": "mcro:hasUseCase",
    "o": "mcro:nguyenvulebinhwav2vec2basevi-UseCaseInformationSection"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:nguyenvulebinhwav2vec2basevi-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Since our model has the same architecture as the English wav2vec2 version, you can use [this notebook](https://colab.research.google.com/drive/1FjTsqbYKphl9kL-eILgUc-bl4zVThL8F?usp=sharing) for more information on how to fine-tune the model."
  },
  {
    "s": "mcro:pyannotespeakerdiarization30",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30",
    "p": "mcro:hasModelDetail",
    "o": "mcro:pyannotespeakerdiarization30-ModelDetail"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30",
    "p": "mcro:hasBenchmark",
    "o": "mcro:pyannotespeakerdiarization30-Benchmark"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30",
    "p": "mcro:hasCitation",
    "o": "mcro:pyannotespeakerdiarization30-Citation"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30",
    "p": "mcro:hasConsideration",
    "o": "mcro:pyannotespeakerdiarization30-Consideration"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30",
    "p": "mcro:hasDataset",
    "o": "mcro:pyannotespeakerdiarization30-Dataset"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30",
    "p": "mcro:hasUseCase",
    "o": "mcro:pyannotespeakerdiarization30-UseCase"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Benchmark",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:pyannotespeakerdiarization30-ModelArchitecture"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:pyannotespeakerdiarization30-License"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "pyannote.audio"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "pipeline"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "AISHELL"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "AliMeeting"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "AMI"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "AVA-AVD"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "DIHARD"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "Ego4D"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "MSDWild"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "REPERE"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Dataset",
    "p": "prov:hasTextValue",
    "o": "VoxConverse"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Plaquet23,\n  author={Alexis Plaquet and Herv\u00e9 Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}"
  },
  {
    "s": "mcro:pyannotespeakerdiarization30-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:intfloate5mistral7binstruct-Citation"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{wang2023improving,\n  title={Improving Text Embeddings with Large Language Models},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2401.00368},\n  year={2023}\n}\n\n@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:intfloate5mistral7binstruct-ModelArchitecture"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "This model has 32 layers and the embedding size is 4096."
  },
  {
    "s": "mcro:intfloate5mistral7binstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:intfloate5mistral7binstruct-UseCase"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\n\nThis model is initialized from [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\nand fine-tuned on a mixture of multilingual datasets.\nAs a result, it has some multilingual capability.\nHowever, since Mistral-7B-v0.1 is mainly trained on English data, we recommend using this model for English only.\nFor multilingual use cases, please refer to [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large)."
  },
  {
    "s": "mcro:intfloate5mistral7binstruct",
    "p": "mcro:hasLimitation",
    "o": "mcro:intfloate5mistral7binstruct-Limitation"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:intfloate5mistral7binstruct-Limitation",
    "p": "prov:hasTextValue",
    "o": "Using this model for inputs longer than 4096 tokens is not recommended.\n\nThis model's multilingual capability is still inferior to [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) for some cases."
  },
  {
    "s": "mcro:unik3d",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:unik3d",
    "p": "mcro:hasReference",
    "o": "mcro:unik3d-ReferenceInformationSection"
  },
  {
    "s": "mcro:unik3d-ReferenceInformationSection",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:unik3d-ReferenceInformationSection",
    "p": "prov:hasTextValue",
    "o": "https://github.com/lpiccinelli-eth/UniK3D"
  },
  {
    "s": "mcro:unik3d-ReferenceInformationSection",
    "p": "prov:hasTextValue",
    "o": "https://huggingface.co/docs/huggingface_hub/package_reference/mixins#huggingface_hub.PyTorchModelHubMixin"
  },
  {
    "s": "mcro:rorshark-vit-base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:rorshark-vit-base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:rorshark-vit-base-ModelDetail"
  },
  {
    "s": "mcro:rorshark-vit-base-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:rorshark-vit-base-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:rorshark-vit-base-ModelArchitecture"
  },
  {
    "s": "mcro:rorshark-vit-base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:rorshark-vit-base",
    "p": "mcro:hasUseCase",
    "o": "mcro:rorshark-vit-base-UseCase"
  },
  {
    "s": "mcro:rorshark-vit-base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:rorshark-vit-base",
    "p": "mcro:hasDataset",
    "o": "mcro:rorshark-vit-base-Dataset"
  },
  {
    "s": "mcro:rorshark-vit-base-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Qwen2.5-1.5B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2.5-1.5B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2.5-1.5B-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2.5-1.5B-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2.5-1.5B-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:Qwen2.5-1.5B",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen2.5-1.5B-CitationInformationSection"
  },
  {
    "s": "mcro:Qwen2.5-1.5B-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen2.5-1.5B-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:Qwen2.5-1.5B",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2.5-1.5B-UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2.5-1.5B-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4",
    "p": "mcro:hasModelDetail",
    "o": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelDetail"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelArchitecture"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Meta Llama 3.1"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-UseCase"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructGPTQINT4-UseCase",
    "p": "prov:hasTextValue",
    "o": "multilingual dialogue use cases"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Qwen2.5-VL-32B-Instruct-ModelDetail"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2.5-VL-32B-Instruct-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen2.5-VL-32B-Instruct-Citation"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2.5-VL-32B-Instruct-UseCase"
  },
  {
    "s": "mcro:Qwen2.5-VL-32B-Instruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit",
    "p": "mcro:hasModelDetail",
    "o": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Citation"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelArchitecture"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail",
    "p": "mcro:hasTechnicalDetail",
    "o": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-TechnicalDetail"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-TechnicalDetail",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-TechnicalDetail",
    "p": "prov:hasTextValue",
    "o": "8-bit quantized version of DeepSeek-R1-0528-Qwen3-8B using MLX, optimized for Apple Silicon."
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail",
    "p": "mcro:hasDisclaimer",
    "o": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Disclaimer"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Disclaimer",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Disclaimer",
    "p": "prov:hasTextValue",
    "o": "LM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model.  You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models.  LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio."
  },
  {
    "s": "mcro:gte-large",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gte-large",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gte-large-ModelDetail"
  },
  {
    "s": "mcro:gte-large-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gte-large-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:gte-large-Citation"
  },
  {
    "s": "mcro:gte-large-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gte-large-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gte-large-ModelArchitecture"
  },
  {
    "s": "mcro:gte-large-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gte-large",
    "p": "mcro:hasUseCase",
    "o": "mcro:gte-large-UseCase"
  },
  {
    "s": "mcro:gte-large-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gte-large-UseCase",
    "p": "prov:hasTextValue",
    "o": "information retrieval"
  },
  {
    "s": "mcro:gte-large-UseCase",
    "p": "prov:hasTextValue",
    "o": "semantic textual similarity"
  },
  {
    "s": "mcro:gte-large-UseCase",
    "p": "prov:hasTextValue",
    "o": "text reranking"
  },
  {
    "s": "mcro:gte-large",
    "p": "mcro:hasLimitation",
    "o": "mcro:gte-large-Limitation"
  },
  {
    "s": "mcro:gte-large-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:gte-large-Limitation",
    "p": "prov:hasTextValue",
    "o": "This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens."
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta",
    "p": "mcro:hasLicense",
    "o": "mcro:HuggingFaceH4zephyr7bbeta-License"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-License",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:HuggingFaceH4zephyr7bbeta-ModelArchitecture"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets."
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:HuggingFaceH4zephyr7bbeta-IntendedUseCase"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "the model can be used for chat"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta",
    "p": "mcro:hasDataset",
    "o": "mcro:HuggingFaceH4zephyr7bbeta-Dataset"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-Dataset",
    "p": "prov:hasTextValue",
    "o": "UltraChat"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta",
    "p": "mcro:hasCitation",
    "o": "mcro:HuggingFaceH4zephyr7bbeta-Citation"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:HuggingFaceH4zephyr7bbeta-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Cl\u00e9mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}"
  },
  {
    "s": "mcro:ai4bharatindictrans2indicen1B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ai4bharatindictrans2indicen1B",
    "p": "mcro:hasCitation",
    "o": "mcro:ai4bharatindictrans2indicen1B-Citation"
  },
  {
    "s": "mcro:ai4bharatindictrans2indicen1B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ai4bharatindictrans2indicen1B-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{gala2023indictrans,\ntitle={IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},\nauthor={Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},\njournal={Transactions on Machine Learning Research},\nissn={2835-8856},\nyear={2023},\nurl={https://openreview.net/forum?id=vfT4YuzAYA},\nnote={}\n}"
  },
  {
    "s": "mcro:ai4bharatindictrans2indicen1B",
    "p": "mcro:hasUseCase",
    "o": "mcro:ai4bharatindictrans2indicen1B-UseCase"
  },
  {
    "s": "mcro:ai4bharatindictrans2indicen1B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:LTXVideo",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:LTXVideo",
    "p": "mcro:hasModelDetail",
    "o": "mcro:LTXVideo-ModelDetail"
  },
  {
    "s": "mcro:LTXVideo-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:LTXVideo-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:LTXVideo-Architecture"
  },
  {
    "s": "mcro:LTXVideo-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:LTXVideo-Architecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-video and image-to-video generation model"
  },
  {
    "s": "mcro:LTXVideo-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:LTXVideo-UseCase"
  },
  {
    "s": "mcro:LTXVideo-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:LTXVideo-UseCase",
    "p": "prov:hasTextValue",
    "o": "text-to-video as well as image+text-to-video usecases"
  },
  {
    "s": "mcro:LTXVideo-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:LTXVideo-License"
  },
  {
    "s": "mcro:LTXVideo-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:LTXVideo-License",
    "p": "prov:hasTextValue",
    "o": "license"
  },
  {
    "s": "mcro:LTXVideo",
    "p": "mcro:hasConsideration",
    "o": "mcro:LTXVideo-Consideration"
  },
  {
    "s": "mcro:LTXVideo-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:LTXVideo-Consideration",
    "p": "prov:hasTextValue",
    "o": "As a statistical model this checkpoint might amplify existing societal biases."
  },
  {
    "s": "mcro:LTXVideo-Consideration",
    "p": "prov:hasTextValue",
    "o": "The model may fail to generate videos that matches the prompts perfectly."
  },
  {
    "s": "mcro:LTXVideo-Consideration",
    "p": "prov:hasTextValue",
    "o": "Prompt following is heavily influenced by the prompting-style."
  },
  {
    "s": "mcro:LTXVideo-Consideration",
    "p": "prov:hasTextValue",
    "o": "This model is not intended or able to provide factual information."
  },
  {
    "s": "mcro:LTXVideo",
    "p": "mcro:hasLimitation",
    "o": "mcro:LTXVideo-Limitation"
  },
  {
    "s": "mcro:LTXVideo-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:LTXVideo-Limitation",
    "p": "prov:hasTextValue",
    "o": "The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames."
  },
  {
    "s": "mcro:LTXVideo-Limitation",
    "p": "prov:hasTextValue",
    "o": "The model works best on resolutions under 720 x 1280 and number of frames below 257."
  },
  {
    "s": "mcro:LTXVideo-Limitation",
    "p": "prov:hasTextValue",
    "o": "Prompts should be in English. The more elaborate the better. Good prompt looks like `The turquoise waves crash against the dark, jagged rocks of the shore, sending white foam spraying into the air. The scene is dominated by the stark contrast between the bright blue water and the dark, almost black rocks. The water is a clear, turquoise color, and the waves are capped with white foam. The rocks are dark and jagged, and they are covered in patches of green moss. The shore is lined with lush green vegetation, including trees and bushes. In the background, there are rolling hills covered in dense forest. The sky is cloudy, and the light is dim.`"
  },
  {
    "s": "mcro:phi4",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:phi4",
    "p": "mcro:hasModelDetail",
    "o": "mcro:phi4-ModelDetail"
  },
  {
    "s": "mcro:phi4-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:phi4-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:phi4-License"
  },
  {
    "s": "mcro:phi4-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:phi4-License",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:phi4-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:phi4-Architecture"
  },
  {
    "s": "mcro:phi4-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:phi4-Architecture",
    "p": "prov:hasTextValue",
    "o": "14B parameters, dense decoder-only Transformer model"
  },
  {
    "s": "mcro:phi4",
    "p": "mcro:hasUseCase",
    "o": "mcro:phi4-UseCase"
  },
  {
    "s": "mcro:phi4-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:phi4-UseCase",
    "p": "prov:hasTextValue",
    "o": "Our model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n\n1. Memory/compute constrained environments.\n2. Latency bound scenarios.\n3. Reasoning and logic."
  },
  {
    "s": "mcro:phi4",
    "p": "mcro:hasDataset",
    "o": "mcro:phi4-Dataset"
  },
  {
    "s": "mcro:phi4-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:phi4-Dataset",
    "p": "prov:hasTextValue",
    "o": "Our training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:\n\n1. Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code.\n\n2. Newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).\n\n3. Acquired academic books and Q&A datasets.\n\n4. High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nMultilingual data constitutes about 8% of our overall data. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge."
  },
  {
    "s": "mcro:phi4",
    "p": "mcro:hasCitation",
    "o": "mcro:phi4-Citation"
  },
  {
    "s": "mcro:phi4-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:phi4-Citation",
    "p": "prov:hasTextValue",
    "o": "[Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen257BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen257BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen257BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen257BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:Qwen257BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen257BInstruct-UseCase"
  },
  {
    "s": "mcro:Qwen257BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen257BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "instruction-tuned 7B Qwen2.5 model"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm",
    "p": "mcro:hasDataset",
    "o": "mcro:microsoftcodebertbasemlm-Dataset"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm-Dataset",
    "p": "prov:hasTextValue",
    "o": "codeparrot/github-code-clean"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftcodebertbasemlm-Architecture"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm-Architecture",
    "p": "prov:hasTextValue",
    "o": "masked-language-modeling"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftcodebertbasemlm-UseCase"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm-UseCase",
    "p": "prov:hasTextValue",
    "o": "CodeBERTScore"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftcodebertbasemlm-Citation"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftcodebertbasemlm-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{zhou2023codebertscore,\n  url = {https://arxiv.org/abs/2302.05527},\n  author = {Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham},\n  title = {CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code},\n  publisher = {arXiv},\n  year = {2023},\n}"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:deberta-v3-base-prompt-injection-v2-ModelDetail"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "This model is a fine-tuned version of [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) specifically developed to detect and classify prompt injection attacks which can manipulate language models into producing unintended outputs."
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:deberta-v3-base-prompt-injection-v2-Citation"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:deberta-v3-base-prompt-injection-v2-UseCase"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model classifies inputs into benign (`0`) and injection-detected (`1`)."
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2",
    "p": "mcro:hasLimitation",
    "o": "mcro:deberta-v3-base-prompt-injection-v2-Limitation"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-Limitation",
    "p": "prov:hasTextValue",
    "o": "`deberta-v3-base-prompt-injection-v2` is highly accurate in identifying prompt injections in English.\nIt does not detect jailbreak attacks or handle non-English prompts, which may limit its applicability in diverse linguistic environments or against advanced adversarial techniques.\n\nAdditionally, we do not recommend using this scanner for system prompts, as it produces false-positives."
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:deberta-v3-base-prompt-injection-v2-License"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-License",
    "p": "prov:hasTextValue",
    "o": "Apache License 2.0"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deberta-v3-base-prompt-injection-v2,\n  author = {ProtectAI.com},\n  title = {Fine-Tuned DeBERTa-v3-base for Prompt Injection Detection},\n  year = {2024},\n  publisher = {HuggingFace},\n  url = {https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2},\n}"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:deberta-v3-base-prompt-injection-v2-Architecture"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deberta-v3-base-prompt-injection-v2-Architecture",
    "p": "prov:hasTextValue",
    "o": "deberta-v3-base"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The Audio Spectrogram Transformer is equivalent to [ViT](https://huggingface.co/docs/transformers/model_doc/vit), but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks."
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet",
    "p": "mcro:hasUseCase",
    "o": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-UseCase"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for classifying audio into one of the AudioSet classes. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info."
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet",
    "p": "mcro:hasCitation",
    "o": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-Citation"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-Citation",
    "p": "prov:hasTextValue",
    "o": "It was introduced in the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al. and first released in [this repository](https://github.com/YuanGongND/ast)."
  },
  {
    "s": "mcro:sdxl10refinermodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sdxl10refinermodel",
    "p": "mcro:hasModelDetail",
    "o": "mcro:sdxl10refinermodel-ModelDetail"
  },
  {
    "s": "mcro:sdxl10refinermodel-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:sdxl10refinermodel-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:sdxl10refinermodel-License"
  },
  {
    "s": "mcro:sdxl10refinermodel-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:sdxl10refinermodel-License",
    "p": "prov:hasTextValue",
    "o": "CreativeML Open RAIL++-M License"
  },
  {
    "s": "mcro:sdxl10refinermodel-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:sdxl10refinermodel-Architecture"
  },
  {
    "s": "mcro:sdxl10refinermodel-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sdxl10refinermodel-Architecture",
    "p": "prov:hasTextValue",
    "o": "Diffusion-based text-to-image generative model"
  },
  {
    "s": "mcro:sdxl10refinermodel",
    "p": "mcro:hasLimitation",
    "o": "mcro:sdxl10refinermodel-Limitation"
  },
  {
    "s": "mcro:sdxl10refinermodel-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:sdxl10refinermodel",
    "p": "mcro:hasUseCase",
    "o": "mcro:sdxl10refinermodel-UseCase"
  },
  {
    "s": "mcro:sdxl10refinermodel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sdxl10refinermodel-UseCase",
    "p": "prov:hasTextValue",
    "o": "research purposes only"
  },
  {
    "s": "mcro:e5-base-v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:e5-base-v2",
    "p": "mcro:hasCitation",
    "o": "mcro:e5-base-v2-Citation"
  },
  {
    "s": "mcro:e5-base-v2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:e5-base-v2-Citation",
    "p": "prov:hasTextValue",
    "o": "[Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/pdf/2212.03533.pdf).\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022"
  },
  {
    "s": "mcro:e5-base-v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:e5-base-v2-ModelArchitecture"
  },
  {
    "s": "mcro:e5-base-v2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:e5-base-v2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "This model has 12 layers and the embedding size is 768."
  },
  {
    "s": "mcro:mimi",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mimi",
    "p": "mcro:hasModelDetail",
    "o": "mcro:mimi-ModelDetail"
  },
  {
    "s": "mcro:mimi-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:mimi-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:mimi-License"
  },
  {
    "s": "mcro:mimi-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mimi-License",
    "p": "prov:hasTextValue",
    "o": "CC-BY"
  },
  {
    "s": "mcro:mimi-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:mimi-Architecture"
  },
  {
    "s": "mcro:mimi-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mimi-Architecture",
    "p": "prov:hasTextValue",
    "o": "audio neural codec"
  },
  {
    "s": "mcro:mimi",
    "p": "mcro:hasUseCase",
    "o": "mcro:mimi-UseCase"
  },
  {
    "s": "mcro:mimi-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mimi",
    "p": "mcro:hasCitation",
    "o": "mcro:mimi-Citation"
  },
  {
    "s": "mcro:mimi-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mimi",
    "p": "mcro:hasConsideration",
    "o": "mcro:mimi-Consideration"
  },
  {
    "s": "mcro:mimi-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:mimi",
    "p": "mcro:hasTrainingData",
    "o": "mcro:mimi-TrainingData"
  },
  {
    "s": "mcro:mimi-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:mimi",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:mimi-IntendedUseCase"
  },
  {
    "s": "mcro:mimi-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:mimi",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:mimi-OutOfScopeUseCase"
  },
  {
    "s": "mcro:mimi-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutOfScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:Protbertmodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Protbertmodel",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Protbertmodel-ModelDetail"
  },
  {
    "s": "mcro:Protbertmodel-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Protbertmodel-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:Protbertmodel-CitationInformationSection"
  },
  {
    "s": "mcro:Protbertmodel-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Protbertmodel-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:Protbertmodel-LicenseInformationSection"
  },
  {
    "s": "mcro:Protbertmodel-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Protbertmodel-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Protbertmodel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Protbertmodel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Protbertmodel",
    "p": "mcro:hasUseCase",
    "o": "mcro:Protbertmodel-UseCaseInformationSection"
  },
  {
    "s": "mcro:Protbertmodel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Protbertmodel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:Protbertmodel-TrainingDataInformationSection"
  },
  {
    "s": "mcro:Protbertmodel-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:Protbertmodel",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:Protbertmodel-EvaluationDataInformationSection"
  },
  {
    "s": "mcro:Protbertmodel-EvaluationDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:vikplayoutsegmenter",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vikplayoutsegmenter",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vikplayoutsegmenter-ModelArchitecture"
  },
  {
    "s": "mcro:vikplayoutsegmenter-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vikplayoutsegmenter-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Based on layoutlmv3."
  },
  {
    "s": "mcro:vikplayoutsegmenter",
    "p": "mcro:hasUseCase",
    "o": "mcro:vikplayoutsegmenter-UseCase"
  },
  {
    "s": "mcro:vikplayoutsegmenter-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:vikplayoutsegmenter-UseCase",
    "p": "prov:hasTextValue",
    "o": "Segments pdf page layout into blocks."
  },
  {
    "s": "mcro:microsoftFlorence2base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftFlorence2base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:microsoftFlorence2base-ModelDetail"
  },
  {
    "s": "mcro:microsoftFlorence2base-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:microsoftFlorence2base-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:microsoftFlorence2base-License"
  },
  {
    "s": "mcro:microsoftFlorence2base-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftFlorence2base",
    "p": "mcro:hasCitation",
    "o": "mcro:Florence-2Citation"
  },
  {
    "s": "mcro:Florence-2Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Florence-2Citation",
    "p": "prov:hasTextValue",
    "o": "@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}"
  },
  {
    "s": "mcro:microsoftFlorence2base",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Florence-2Architecture"
  },
  {
    "s": "mcro:Florence-2Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Florence-2Architecture",
    "p": "prov:hasTextValue",
    "o": "sequence-to-sequence architecture"
  },
  {
    "s": "mcro:microsoftFlorence2base",
    "p": "mcro:hasDataset",
    "o": "mcro:FLD-5B"
  },
  {
    "s": "mcro:FLD-5B",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:FLD-5B",
    "p": "prov:hasTextValue",
    "o": "FLD-5B dataset, containing 5.4 billion annotations across 126 million images"
  },
  {
    "s": "mcro:microsoftFlorence2base",
    "p": "mcro:hasUseCase",
    "o": "mcro:VisionTasksUseCase"
  },
  {
    "s": "mcro:VisionTasksUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:VisionTasksUseCase",
    "p": "prov:hasTextValue",
    "o": "captioning, object detection, and segmentation"
  },
  {
    "s": "mcro:gpt2medium",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gpt2medium",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gpt2medium-ModelDetail"
  },
  {
    "s": "mcro:gpt2medium-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gpt2medium-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:gpt2medium-Architecture"
  },
  {
    "s": "mcro:gpt2medium-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gpt2medium-Architecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based language model"
  },
  {
    "s": "mcro:gpt2medium-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:gpt2medium-License"
  },
  {
    "s": "mcro:gpt2medium-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:gpt2medium-License",
    "p": "prov:hasTextValue",
    "o": "Modified MIT License"
  },
  {
    "s": "mcro:gpt2medium-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:gpt2medium-Citation"
  },
  {
    "s": "mcro:gpt2medium-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gpt2medium-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{radford2019language,\n  title={Language models are unsupervised multitask learners},\n  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},\n  journal={OpenAI blog},\n  volume={1},\n  number={8},\n  pages={9},\n  year={2019}\n}"
  },
  {
    "s": "mcro:gpt2medium",
    "p": "mcro:hasUseCase",
    "o": "mcro:gpt2medium-UseCase"
  },
  {
    "s": "mcro:gpt2medium-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gpt2medium",
    "p": "mcro:hasConsideration",
    "o": "mcro:gpt2medium-Consideration"
  },
  {
    "s": "mcro:gpt2medium-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:gpt2medium",
    "p": "mcro:hasTrainingData",
    "o": "mcro:gpt2medium-TrainingData"
  },
  {
    "s": "mcro:gpt2medium-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:gpt2medium",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:gpt2medium-EvaluationData"
  },
  {
    "s": "mcro:gpt2medium-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Qwen2532BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2532BInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Qwen2532BInstruct-ModelDetail"
  },
  {
    "s": "mcro:Qwen2532BInstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Qwen2532BInstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2532BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen2532BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2532BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:Qwen2532BInstruct-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen2532BInstruct-UseCase"
  },
  {
    "s": "mcro:Qwen2532BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen2532BInstruct-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen2532BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen2532BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen2532BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:Qwen2532BInstruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:Qwen2532BInstruct-License"
  },
  {
    "s": "mcro:Qwen2532BInstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelDetail"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-License"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelArchitecture"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B",
    "p": "mcro:hasUseCase",
    "o": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-UseCase"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B",
    "p": "mcro:hasDataset",
    "o": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Dataset"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-QuantativeAnalysis"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B",
    "p": "mcro:hasConsideration",
    "o": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Consideration"
  },
  {
    "s": "mcro:naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metaLlama32Collection",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metaLlama32Collection-ModelDetailSection"
  },
  {
    "s": "mcro:metaLlama32Collection-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metaLlama32Collection-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:metaLlama32Collection-LicenseInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)."
  },
  {
    "s": "mcro:metaLlama32Collection-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metaLlama32Collection-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metaLlama32Collection",
    "p": "mcro:hasUseCase",
    "o": "mcro:metaLlama32Collection-UseCaseInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources."
  },
  {
    "s": "mcro:metaLlama32Collection",
    "p": "mcro:hasDataset",
    "o": "mcro:metaLlama32Collection-DatasetInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:metaLlama32Collection-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)."
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator",
    "p": "mcro:hasModelDetail",
    "o": "mcro:googleelectrasmalldiscriminator-ModelDetail"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator",
    "p": "mcro:hasUseCase",
    "o": "mcro:googleelectrasmalldiscriminator-UseCase"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:googleelectrasmalldiscriminator-Architecture"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator-Architecture",
    "p": "prov:hasTextValue",
    "o": "transformer networks"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator",
    "p": "mcro:hasCitation",
    "o": "mcro:googleelectrasmalldiscriminator-Citation"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:googleelectrasmalldiscriminator-Citation",
    "p": "prov:hasTextValue",
    "o": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation",
    "p": "mcro:hasDataset",
    "o": "mcro:segformerb2finetunedforclothessegmentation-DatasetInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "ATR dataset"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "mattmdjaga/human_parsing_dataset"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:segformerb2finetunedforclothessegmentation-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SegFormer model"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation",
    "p": "mcro:hasUseCase",
    "o": "mcro:segformerb2finetunedforclothessegmentation-UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "clothes segmentation"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "human segmentation"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation",
    "p": "mcro:hasCitation",
    "o": "mcro:segformerb2finetunedforclothessegmentation-CitationInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation",
    "p": "mcro:hasLicense",
    "o": "mcro:segformerb2finetunedforclothessegmentation-LicenseInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:segformerb2finetunedforclothessegmentation-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "https://github.com/NVlabs/SegFormer/blob/master/LICENSE"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen7B-License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen7B-Citation"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B",
    "p": "mcro:hasArchitecture",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen7B-Architecture"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B-Architecture",
    "p": "prov:hasTextValue",
    "o": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1."
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B",
    "p": "mcro:hasUseCase",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen7B-UseCase"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen7B-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\""
  },
  {
    "s": "mcro:flan-t5-large",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:flan-t5-large",
    "p": "mcro:hasModelDetail",
    "o": "mcro:flan-t5-large-ModelDetail"
  },
  {
    "s": "mcro:flan-t5-large-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:flan-t5-large-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:flan-t5-large-License"
  },
  {
    "s": "mcro:flan-t5-large-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:flan-t5-large-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:flan-t5-large",
    "p": "mcro:hasCitation",
    "o": "mcro:flan-t5-large-Citation"
  },
  {
    "s": "mcro:flan-t5-large-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:flan-t5-large",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:flan-t5-large-ModelArchitecture"
  },
  {
    "s": "mcro:flan-t5-large-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:flan-t5-large",
    "p": "mcro:hasTrainingData",
    "o": "mcro:flan-t5-large-TrainingData"
  },
  {
    "s": "mcro:flan-t5-large-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:flan-t5-large",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:flan-t5-large-EvaluationData"
  },
  {
    "s": "mcro:flan-t5-large-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:flan-t5-large",
    "p": "mcro:hasUseCase",
    "o": "mcro:flan-t5-large-UseCase"
  },
  {
    "s": "mcro:flan-t5-large-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:flan-t5-large",
    "p": "mcro:hasConsideration",
    "o": "mcro:flan-t5-large-Consideration"
  },
  {
    "s": "mcro:flan-t5-large-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:phobertpretrainedlanguagemodels",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:phobertpretrainedlanguagemodels",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:phobertpretrainedlanguagemodels-ModelArchitecture"
  },
  {
    "s": "mcro:phobertpretrainedlanguagemodels-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:phobertpretrainedlanguagemodels-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "PhoBERT pre-training approach is based on [RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)  which optimizes the [BERT](https://github.com/google-research/bert) pre-training procedure for more robust performance."
  },
  {
    "s": "mcro:phobertpretrainedlanguagemodels",
    "p": "mcro:hasCitation",
    "o": "mcro:phobertpretrainedlanguagemodels-Citation"
  },
  {
    "s": "mcro:phobertpretrainedlanguagemodels-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:phobertpretrainedlanguagemodels-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{phobert,\n    title     = {{PhoBERT: Pre-trained language models for Vietnamese}},\n    author    = {Dat Quoc Nguyen and Anh Tuan Nguyen},\n    journal   = {Findings of EMNLP},\n    year      = {2020}\n    }"
  },
  {
    "s": "mcro:trocrsmalliam",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:trocrsmalliam",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:trocrsmalliam-ModelArchitecture"
  },
  {
    "s": "mcro:trocrsmalliam-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:trocrsmalliam-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens."
  },
  {
    "s": "mcro:trocrsmalliam",
    "p": "mcro:hasCitation",
    "o": "mcro:trocrsmalliam-Citation"
  },
  {
    "s": "mcro:trocrsmalliam-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:trocrsmalliam-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{li2021trocr,\n      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n      year={2021},\n      eprint={2109.10282},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:trocrsmalliam",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:trocrsmalliam-UseCase"
  },
  {
    "s": "mcro:trocrsmalliam-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:trocrsmalliam-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Bertbasemultilingualcasednerhrl-ModelArchitecture"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "bert-base-multilingual-cased"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:Bertbasemultilingualcasednerhrl-IntendedUseCase"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "NER"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl",
    "p": "mcro:hasTrainingData",
    "o": "mcro:Bertbasemultilingualcasednerhrl-TrainingData"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "prov:hasTextValue",
    "o": "ANERcorp"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "prov:hasTextValue",
    "o": "conll 2003"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "prov:hasTextValue",
    "o": "conll 2002"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Europeana Newspapers"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Italian I-CAB"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Latvian NER"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Paramopama + Second Harem"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-TrainingData",
    "p": "prov:hasTextValue",
    "o": "MSRA"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl",
    "p": "mcro:hasLimitation",
    "o": "mcro:Bertbasemultilingualcasednerhrl-Limitation"
  },
  {
    "s": "mcro:Bertbasemultilingualcasednerhrl-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:llavahfllava157bhf",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llavahfllava157bhf",
    "p": "mcro:hasModelDetail",
    "o": "mcro:llavahfllava157bhf-ModelDetail"
  },
  {
    "s": "mcro:llavahfllava157bhf-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llavahfllava157bhf-ModelDetail",
    "p": "mcro:hasModelArchitectureInformation",
    "o": "mcro:llavahfllava157bhf-ModelArchitectureInformation"
  },
  {
    "s": "mcro:llavahfllava157bhf-ModelArchitectureInformation",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llavahfllava157bhf-ModelArchitectureInformation",
    "p": "prov:hasTextValue",
    "o": "LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.\nIt is an auto-regressive language model, based on the transformer architecture."
  },
  {
    "s": "mcro:llavahfllava157bhf-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:llavahfllava157bhf-License"
  },
  {
    "s": "mcro:llavahfllava157bhf-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llavahfllava157bhf-License",
    "p": "prov:hasTextValue",
    "o": "Llama 2 is licensed under the LLAMA 2 Community License,\nCopyright (c) Meta Platforms, Inc. All Rights Reserved."
  },
  {
    "s": "mcro:xtunerllavallama38bv11transformers",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:xtunerllavallama38bv11transformers",
    "p": "mcro:hasModelDetail",
    "o": "mcro:xtunerllavallama38bv11transformers-ModelDetail"
  },
  {
    "s": "mcro:xtunerllavallama38bv11transformers-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:xtunerllavallama38bv11transformers-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "llava-llama-3-8b-v1_1-hf is a LLaVA model fine-tuned from [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and [CLIP-ViT-Large-patch14-336](https://huggingface.co/openai/clip-vit-large-patch14-336) with [ShareGPT4V-PT](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V) and [InternVL-SFT](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets) by [XTuner](https://github.com/InternLM/xtuner).\n\n**Note: This model is in HuggingFace LLaVA format.**\n\nResources:\n\n- GitHub: [xtuner](https://github.com/InternLM/xtuner)\n- Official LLaVA format model: [xtuner/llava-llama-3-8b-v1_1-hf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-hf)\n- XTuner LLaVA format model: [xtuner/llava-llama-3-8b-v1_1](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1)\n- GGUF format model: [xtuner/llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf)"
  },
  {
    "s": "mcro:xtunerllavallama38bv11transformers",
    "p": "mcro:hasCitation",
    "o": "mcro:xtunerllavallama38bv11transformers-Citation"
  },
  {
    "s": "mcro:xtunerllavallama38bv11transformers-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:xtunerllavallama38bv11transformers-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{2023xtuner,\n    title={XTuner: A Toolkit for Efficiently Fine-tuning LLM},\n    author={XTuner Contributors},\n    howpublished = {\\url{https://github.com/InternLM/xtuner}},\n    year={2023}\n}"
  },
  {
    "s": "mcro:hallucinationevaluationmodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:hallucinationevaluationmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:hallucinationevaluationmodel-CitationInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc {hhem-2.1-open,\n\tauthor       = {Forrest Bao and Miaoran Li and Rogger Luo and Ofer Mendelevitch},\n\ttitle        = {{HHEM-2.1-Open}},\n\tyear         = 2024,\n\turl          = { https://huggingface.co/vectara/hallucination_evaluation_model },\n\tdoi          = { 10.57967/hf/3240 },\n\tpublisher    = { Hugging Face }\n}"
  },
  {
    "s": "mcro:hallucinationevaluationmodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:hallucinationevaluationmodel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "T5-base"
  },
  {
    "s": "mcro:hallucinationevaluationmodel",
    "p": "mcro:hasLicense",
    "o": "mcro:hallucinationevaluationmodel-LicenseInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:hallucinationevaluationmodel-UseCaseInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "detecting hallucinations in LLMs"
  },
  {
    "s": "mcro:hallucinationevaluationmodel",
    "p": "mcro:hasDataset",
    "o": "mcro:hallucinationevaluationmodel-DatasetInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "AggreFact"
  },
  {
    "s": "mcro:hallucinationevaluationmodel-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "RAGTruth"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Snowflakesnowflakearcticembedm-ModelDetail"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance."
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Snowflakesnowflakearcticembedm-ModelArchitecture"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance. First, the models are trained with large batches of query-document pairs where negatives are derived in-batch\u2014pretraining leverages about 400m samples of a mix of public datasets and proprietary web search data. Following pretraining models are further optimized with long training on a smaller dataset (about 1m samples) of triplets of query, positive document, and negative document derived from hard harmful mining. Mining of the negatives and data curation is crucial to retrieval accuracy."
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm",
    "p": "mcro:hasLicense",
    "o": "mcro:Snowflakesnowflakearcticembedm-License"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-License",
    "p": "prov:hasTextValue",
    "o": "Arctic is licensed under the [Apache-2](https://www.apache.org/licenses/LICENSE-2.0). The released models can be used for commercial purposes free of charge."
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm",
    "p": "mcro:hasUseCase",
    "o": "mcro:Snowflakesnowflakearcticembedm-UseCase"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedm-UseCase",
    "p": "prov:hasTextValue",
    "o": "snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance."
  },
  {
    "s": "mcro:wavlmlarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wavlmlarge",
    "p": "mcro:hasModelDetail",
    "o": "mcro:wavlmlarge-ModelDetail"
  },
  {
    "s": "mcro:wavlmlarge-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:wavlmlarge-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:wavlmlarge-Citation"
  },
  {
    "s": "mcro:wavlmlarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:wavlmlarge-Citation",
    "p": "prov:hasTextValue",
    "o": "Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing"
  },
  {
    "s": "mcro:wavlmlarge-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:wavlmlarge-License"
  },
  {
    "s": "mcro:wavlmlarge-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:wavlmlarge-License",
    "p": "prov:hasTextValue",
    "o": "The official license can be found here"
  },
  {
    "s": "mcro:wavlmlarge-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wavlmlarge-ModelArchitecture"
  },
  {
    "s": "mcro:wavlmlarge-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wavlmlarge",
    "p": "mcro:hasUseCase",
    "o": "mcro:wavlmlarge-UseCase"
  },
  {
    "s": "mcro:wavlmlarge-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wavlmlarge-UseCase",
    "p": "prov:hasTextValue",
    "o": "This is an English pre-trained speech model that has to be fine-tuned on a downstream task like speech recognition or audio classification before it can be \nused in inference."
  },
  {
    "s": "mcro:wavlmlarge",
    "p": "mcro:hasDataset",
    "o": "mcro:wavlmlarge-Dataset"
  },
  {
    "s": "mcro:wavlmlarge-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:wavlmlarge-Dataset",
    "p": "prov:hasTextValue",
    "o": "- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)\n- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)\n- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor",
    "p": "mcro:hasModelDetail",
    "o": "mcro:T5basesummarizationclaimextractor-ModelDetail"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:T5basesummarizationclaimextractor-License"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor",
    "p": "mcro:hasCitation",
    "o": "mcro:T5basesummarizationclaimextractor-Citation"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:T5basesummarizationclaimextractor-ModelArchitecture"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor",
    "p": "mcro:hasUseCase",
    "o": "mcro:T5basesummarizationclaimextractor-UseCase"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-License",
    "p": "prov:hasTextValue",
    "o": "N/A"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{scire-etal-2024-fenice,\n    title = \"{FENICE}: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction\",\n    author = \"Scir{\\`e}, Alessandro and Ghonim, Karim and Navigli, Roberto\",\n    editor = \"Ku, Lun-Wei  and Martins, Andre and Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand and virtual meeting\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.841\",\n    pages = \"14148--14161\",\n}"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "T5"
  },
  {
    "s": "mcro:T5basesummarizationclaimextractor-UseCase",
    "p": "prov:hasTextValue",
    "o": "Extraction of atomic claims from summaries.\nServe as a component in pipelines for factuality evaluation of summaries."
  },
  {
    "s": "mcro:MyAwesomeModel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:MyAwesomeModel",
    "p": "mcro:hasLicense",
    "o": "mcro:MyAwesomeModel-License"
  },
  {
    "s": "mcro:MyAwesomeModel-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:MyAwesomeModel-License",
    "p": "prov:hasTextValue",
    "o": "mit"
  },
  {
    "s": "mcro:MyAwesomeModel",
    "p": "mcro:hasDataset",
    "o": "mcro:MyAwesomeModel-Dataset"
  },
  {
    "s": "mcro:MyAwesomeModel-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:MyAwesomeModel-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet"
  },
  {
    "s": "mcro:MyAwesomeModel",
    "p": "mcro:hasArchitecture",
    "o": "mcro:MyAwesomeModel-Architecture"
  },
  {
    "s": "mcro:MyAwesomeModel-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:MyAwesomeModel-Architecture",
    "p": "prov:hasTextValue",
    "o": "CNN"
  },
  {
    "s": "mcro:MyAwesomeModel",
    "p": "mcro:hasCitation",
    "o": "mcro:MyAwesomeModel-Citation"
  },
  {
    "s": "mcro:MyAwesomeModel-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:MyAwesomeModel-Citation",
    "p": "prov:hasTextValue",
    "o": "Reference Paper"
  },
  {
    "s": "mcro:MyAwesomeModel",
    "p": "mcro:hasUseCase",
    "o": "mcro:MyAwesomeModel-UseCase"
  },
  {
    "s": "mcro:MyAwesomeModel-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:MyAwesomeModel-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image Classification"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model",
    "p": "mcro:hasModelDetail",
    "o": "mcro:swin_transformer_v2_tiny_sized_model-ModelDetail"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:swin_transformer_v2_tiny_sized_model-Architecture"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-Architecture",
    "p": "prov:hasTextValue",
    "o": "Swin Transformer v2"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:swin_transformer_v2_tiny_sized_model-Citation"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2111-09883,\n  author    = {Ze Liu and\n               Han Hu and\n               Yutong Lin and\n               Zhuliang Yao and\n               Zhenda Xie and\n               Yixuan Wei and\n               Jia Ning and\n               Yue Cao and\n               Zheng Zhang and\n               Li Dong and\n               Furu Wei and\n               Baining Guo},\n  title     = {Swin Transformer {V2:} Scaling Up Capacity and Resolution},\n  journal   = {CoRR},\n  volume    = {abs/2111.09883},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2111.09883},\n  eprinttype = {arXiv},\n  eprint    = {2111.09883},\n  timestamp = {Thu, 02 Dec 2021 15:54:22 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09883.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:swin_transformer_v2_tiny_sized_model-UseCase"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-UseCase",
    "p": "prov:hasTextValue",
    "o": "image classification"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model",
    "p": "mcro:hasDataset",
    "o": "mcro:swin_transformer_v2_tiny_sized_model-Dataset"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:swin_transformer_v2_tiny_sized_model-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ibmresearchMoLFormerXLboth10pct-ModelArchitecture"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "MoLFormer is a large-scale chemical language model designed with the intention of learning a model trained on small molecules which are represented as SMILES strings. MoLFormer leverges masked language modeling and employs a linear attention Transformer combined with rotary embeddings."
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:ibmresearchMoLFormerXLboth10pct-IntendedUseCase"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the model for masked language modeling, but it is mainly intended to be used as a feature extractor or to be fine-tuned for a prediction task. The \"frozen\" model embeddings may be used for similarity measurements, visualization, or training predictor models. The model may also be fine-tuned for sequence classification tasks (e.g., solubility, toxicity, etc.)."
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail",
    "p": "mcro:hasLimitation",
    "o": "mcro:ibmresearchMoLFormerXLboth10pct-Limitation"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Limitation",
    "p": "prov:hasTextValue",
    "o": "This model is not intended for molecule generation. It is also not tested for molecules larger than ~200 atoms (i.e., macromolecules). Furthermore, using invalid or noncanonical SMILES may result in worse performance."
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:ibmresearchMoLFormerXLboth10pct-Dataset"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Dataset",
    "p": "prov:hasTextValue",
    "o": "We trained MoLFormer-XL on a combination of molecules from the ZINC15 and PubChem datasets. This repository contains the version trained on 10% ZINC + 10% PubChem."
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Dataset",
    "p": "mcro:hasTrainingData",
    "o": "mcro:ibmresearchMoLFormerXLboth10pct-TrainingData"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Molecules were canonicalized with RDKit prior to training and isomeric information was removed. Also, molecules longer than 202 tokens were dropped."
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:ibmresearchMoLFormerXLboth10pct-Citation1"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Citation1",
    "p": "prov:hasTextValue",
    "o": "@article{10.1038/s42256-022-00580-7,\n  year = {2022},\n  title = {{Large-scale chemical language representations capture molecular structure and   properties}},\n  author = {Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and   Mroueh, Youssef and Das, Payel},\n  journal = {Nature Machine Intelligence},\n  doi = {10.1038/s42256-022-00580-7},\n  pages = {1256--1264},\n  number = {12},\n  volume = {4}\n}"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:ibmresearchMoLFormerXLboth10pct-Citation2"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ibmresearchMoLFormerXLboth10pct-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2106.09553,\n  doi = {10.48550/ARXIV.2106.09553},\n  url = {https://arxiv.org/abs/2106.09553},\n  author = {Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and Mroueh, Youssef and Das, Payel},\n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Biomolecules (q-bio.BM), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},\n  title = {Large-Scale Chemical Language Representations Capture Molecular Structure and Properties},\n  publisher = {arXiv},\n  year = {2021},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekV30324-License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324-License",
    "p": "prov:hasTextValue",
    "o": "This repository and the model weights are licensed under the [MIT License](LICENSE)."
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekV30324-Citation"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:deepseekaiDeepSeekV30324-Architecture"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324-Architecture",
    "p": "prov:hasTextValue",
    "o": "The model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3."
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324",
    "p": "mcro:hasUseCase",
    "o": "mcro:deepseekaiDeepSeekV30324-UseCase"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekV30324-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model supports features such as function calling, JSON output, and FIM completion."
  },
  {
    "s": "mcro:chronos-t5-base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronos-t5-base",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronos-t5-base-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronos-t5-base-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronos-t5-base-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters."
  },
  {
    "s": "mcro:chronos-t5-base",
    "p": "mcro:hasCitation",
    "o": "mcro:chronos-t5-base-CitationInformationSection"
  },
  {
    "s": "mcro:chronos-t5-base-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronos-t5-base-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronos-t5-base",
    "p": "mcro:hasLicense",
    "o": "mcro:chronos-t5-base-LicenseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-base-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-base-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "This project is licensed under the Apache-2.0 License."
  },
  {
    "s": "mcro:chronos-t5-base",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronos-t5-base-UseCaseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-base-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronos-t5-base-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Chronos is a family of **pretrained time series forecasting models** based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes."
  },
  {
    "s": "mcro:trocrbasesroie",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:trocrbasesroie",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:trocrbasesroie-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:trocrbasesroie-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:trocrbasesroie-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens."
  },
  {
    "s": "mcro:trocrbasesroie",
    "p": "mcro:hasUseCase",
    "o": "mcro:trocrbasesroie-UseCaseInformationSection"
  },
  {
    "s": "mcro:trocrbasesroie-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:trocrbasesroie-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:trocrbasesroie",
    "p": "mcro:hasCitation",
    "o": "mcro:trocrbasesroie-CitationInformationSection"
  },
  {
    "s": "mcro:trocrbasesroie-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:trocrbasesroie-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{li2021trocr,\n      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n      year={2021},\n      eprint={2109.10282},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{grosman2021xlsr53-large-persian,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ersian},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-persian}},\n  year={2021}\n}"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-Dataset",
    "p": "prov:hasTextValue",
    "o": "Common Voice 6.1"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-UseCase"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53persian-UseCase",
    "p": "prov:hasTextValue",
    "o": "speech recognition in Persian"
  },
  {
    "s": "mcro:rubertbasecased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:rubertbasecased",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:rubertbasecased-ModelArchitecture"
  },
  {
    "s": "mcro:rubertbasecased-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:rubertbasecased-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\n\n08.11.2021: upload model with MLM and NSP heads"
  },
  {
    "s": "mcro:rubertbasecased",
    "p": "mcro:hasCitation",
    "o": "mcro:rubertbasecased-Citation"
  },
  {
    "s": "mcro:rubertbasecased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:rubertbasecased-Citation",
    "p": "prov:hasTextValue",
    "o": "[1]: Kuratov, Y., Arkhipov, M. (2019). Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language. arXiv preprint [arXiv:1905.07213](https://arxiv.org/abs/1905.07213)."
  },
  {
    "s": "mcro:debertav3basetasksourcenli",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:debertav3basetasksourcenli",
    "p": "mcro:hasModelDetail",
    "o": "mcro:debertav3basetasksourcenli-ModelDetail"
  },
  {
    "s": "mcro:debertav3basetasksourcenli-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:debertav3basetasksourcenli-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:debertav3basetasksourcenli-License"
  },
  {
    "s": "mcro:debertav3basetasksourcenli-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:debertav3basetasksourcenli",
    "p": "mcro:hasCitation",
    "o": "mcro:debertav3basetasksourcenli-Citation"
  },
  {
    "s": "mcro:debertav3basetasksourcenli-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:debertav3basetasksourcenli",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:debertav3basetasksourcenli-Arch"
  },
  {
    "s": "mcro:debertav3basetasksourcenli-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:debertav3basetasksourcenli",
    "p": "mcro:hasUseCase",
    "o": "mcro:debertav3basetasksourcenli-UseCase"
  },
  {
    "s": "mcro:debertav3basetasksourcenli-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:debertav3basetasksourcenli",
    "p": "mcro:hasOwner",
    "o": "mcro:debertav3basetasksourcenli-Owner"
  },
  {
    "s": "mcro:debertav3basetasksourcenli-Owner",
    "p": "rdf:type",
    "o": "mcro:OwnerInformationSection"
  },
  {
    "s": "mcro:debertav3basetasksourcenli-Owner",
    "p": "prov:hasTextValue",
    "o": "damien.sileo@inria.fr"
  },
  {
    "s": "mcro:faceparsing",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:faceparsing",
    "p": "mcro:hasModelDetail",
    "o": "mcro:faceparsing-ModelDetail"
  },
  {
    "s": "mcro:faceparsing-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:faceparsing-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:faceparsing-License"
  },
  {
    "s": "mcro:faceparsing-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:faceparsing-License",
    "p": "prov:hasTextValue",
    "o": "non-commercial research and educational purposes"
  },
  {
    "s": "mcro:faceparsing-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:faceparsing-Architecture"
  },
  {
    "s": "mcro:faceparsing-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:faceparsing-Architecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-based semantic segmentation image model"
  },
  {
    "s": "mcro:faceparsing",
    "p": "mcro:hasConsideration",
    "o": "mcro:faceparsing-Consideration"
  },
  {
    "s": "mcro:faceparsing-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:faceparsing-Consideration",
    "p": "prov:hasTextValue",
    "o": "While the capabilities of computer vision models are impressive, they can also reinforce or exacerbate social biases. The [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ) dataset used for fine-tuning is large but not necessarily perfectly diverse or representative. Also, they are images of.... just celebrities."
  },
  {
    "s": "mcro:faceparsing",
    "p": "mcro:hasUseCase",
    "o": "mcro:faceparsing-UseCase"
  },
  {
    "s": "mcro:faceparsing-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:faceparsing-UseCase",
    "p": "prov:hasTextValue",
    "o": "Semantic segmentation model fine-tuned from nvidia/mit-b5 with CelebAMask-HQ for face parsing"
  },
  {
    "s": "mcro:BAAIbgebaseen",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BAAIbgebaseen",
    "p": "mcro:hasLicense",
    "o": "mcro:BAAIbgebaseen-License"
  },
  {
    "s": "mcro:BAAIbgebaseen-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:BAAIbgebaseen-License",
    "p": "prov:hasTextValue",
    "o": "FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge."
  },
  {
    "s": "mcro:BAAIbgebaseen",
    "p": "mcro:hasCitation",
    "o": "mcro:BAAIbgebaseen-Citation"
  },
  {
    "s": "mcro:BAAIbgebaseen-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgebaseen-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:BAAIbgebaseen",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BAAIbgebaseen-ModelArchitecture"
  },
  {
    "s": "mcro:BAAIbgebaseen-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgebaseen-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning."
  },
  {
    "s": "mcro:BAAIbgebaseen",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:BAAIbgebaseen-UseCase"
  },
  {
    "s": "mcro:BAAIbgebaseen-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgebaseen-UseCase",
    "p": "prov:hasTextValue",
    "o": "FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification,  clustering, or semantic search.\nAnd it also can be used in vector databases for LLMs."
  },
  {
    "s": "mcro:BAAIbgebaseen",
    "p": "mcro:hasDataset",
    "o": "mcro:BAAIbgebaseen-Dataset"
  },
  {
    "s": "mcro:BAAIbgebaseen-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:BAAIbgebaseen-Dataset",
    "p": "prov:hasTextValue",
    "o": "The [masive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released"
  },
  {
    "s": "mcro:NLLB200",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:NLLB200-IntendedUse"
  },
  {
    "s": "mcro:NLLB200-IntendedUse",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:NLLB200-IntendedUse",
    "p": "prov:hasTextValue",
    "o": "- Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\n- Primary intended users: Primary users are researchers and machine translation research community.\n- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasLicense",
    "o": "mcro:NLLB200-License"
  },
  {
    "s": "mcro:NLLB200-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:NLLB200-License",
    "p": "prov:hasTextValue",
    "o": "CC-BY-NC"
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasCitation",
    "o": "mcro:NLLB200-Citation"
  },
  {
    "s": "mcro:NLLB200-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:NLLB200-Citation",
    "p": "prov:hasTextValue",
    "o": "Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022"
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasDataset",
    "o": "mcro:NLLB200-Dataset"
  },
  {
    "s": "mcro:NLLB200-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:NLLB200-Dataset",
    "p": "prov:hasTextValue",
    "o": "- Datasets: Flores-200 dataset is described in Section 4\n- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\n- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasConsideration",
    "o": "mcro:NLLB200-Consideration"
  },
  {
    "s": "mcro:NLLB200-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:NLLB200-Consideration",
    "p": "prov:hasTextValue",
    "o": "In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety)."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasTradeoff",
    "o": "mcro:NLLB200-Tradeoff"
  },
  {
    "s": "mcro:NLLB200-Tradeoff",
    "p": "rdf:type",
    "o": "mcro:TradeoffInformationSection"
  },
  {
    "s": "mcro:NLLB200-Tradeoff",
    "p": "prov:hasTextValue",
    "o": "Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasTrainingData",
    "o": "mcro:NLLB200-TrainingData"
  },
  {
    "s": "mcro:NLLB200-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:NLLB200-TrainingData",
    "p": "prov:hasTextValue",
    "o": "We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2."
  },
  {
    "s": "mcro:NLLB200",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:NLLB200-Metrics"
  },
  {
    "s": "mcro:NLLB200-Metrics",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:NLLB200-Metrics",
    "p": "prov:hasTextValue",
    "o": "\u2022 Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations."
  },
  {
    "s": "mcro:sdxlvae",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sdxlvae",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sdxlvae-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sdxlvae-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sdxlvae-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SDXL is a latent diffusion model, where the diffusion operates in a pretrained, \nlearned (and fixed) latent space of an autoencoder. \nWhile the bulk of the semantic composition is done by the latent diffusion model, \nwe can improve _local_, high-frequency details in generated images by improving the quality of the autoencoder. \nTo this end, we train the same autoencoder architecture used for the original Stable Diffusion at a larger batch-size (256 vs 9) \nand additionally track the weights with an exponential moving average (EMA). \nThe resulting autoencoder outperforms the original model in all evaluated reconstruction metrics, see the table below."
  },
  {
    "s": "mcro:sdxlvae",
    "p": "mcro:hasDataset",
    "o": "mcro:sdxlvae-DatasetInformationSection"
  },
  {
    "s": "mcro:sdxlvae-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:sdxlvae-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "COCO 2017"
  },
  {
    "s": "mcro:sdxlvae",
    "p": "mcro:hasEvaluation",
    "o": "mcro:sdxlvae-EvaluationInformationSection"
  },
  {
    "s": "mcro:sdxlvae-EvaluationInformationSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:gemma3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasCitation",
    "o": "mcro:gemma3-Citation"
  },
  {
    "s": "mcro:gemma3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gemma3-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasDataset",
    "o": "mcro:gemma3-Dataset"
  },
  {
    "s": "mcro:gemma3-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:gemma3-Dataset",
    "p": "prov:hasTextValue",
    "o": "These models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats."
  },
  {
    "s": "mcro:gemma3-Dataset",
    "p": "prov:hasTextValue",
    "o": "Here are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies]."
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gemma3-Architecture"
  },
  {
    "s": "mcro:gemma3-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gemma3-Architecture",
    "p": "prov:hasTextValue",
    "o": "Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability]."
  },
  {
    "s": "mcro:gemma3-Architecture",
    "p": "prov:hasTextValue",
    "o": "Training was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*"
  },
  {
    "s": "mcro:gemma3",
    "p": "mcro:hasUseCase",
    "o": "mcro:gemma3-UseCase"
  },
  {
    "s": "mcro:gemma3-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gemma3-UseCase",
    "p": "prov:hasTextValue",
    "o": "Open vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics."
  },
  {
    "s": "mcro:gemma3-UseCase",
    "p": "prov:hasTextValue",
    "o": "-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations."
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual",
    "p": "mcro:hasUseCase",
    "o": "mcro:jinaaijinarerankerv2basemultilingual-UseCaseInformationSection"
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The Jina Reranker v2 (`jina-reranker-v2-base-multilingual`) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy.\n\nCompared with the state-of-the-art reranker models, including the previous released `jina-reranker-v1-base-en`, the **Jina Reranker v2** model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks.\n\nThe `jina-reranker-v2-base-multilingual` model is capable of handling long texts with a context length of up to `1024` tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately.\n\nThe model is also equipped with a flash attention mechanism, which significantly improves the model's performance."
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual",
    "p": "mcro:hasLicense",
    "o": "mcro:jinaaijinarerankerv2basemultilingual-LicenseInformationSection"
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "_This model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please [contact us](https://jina.ai/contact-sales) for any further clarifications._"
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jinaaijinarerankerv2basemultilingual-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jinaaijinarerankerv2basemultilingual-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformer-based model"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2",
    "p": "mcro:hasModelParameter",
    "o": "mcro:crossencodermsmarcoTinyBERTL2v2-ModelParameter"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2",
    "p": "mcro:hasDataset",
    "o": "mcro:crossencodermsmarcoTinyBERTL2v2-Dataset"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2-Dataset",
    "p": "prov:hasTextValue",
    "o": "MS Marco Passage Ranking"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:crossencodermsmarcoTinyBERTL2v2-UseCase"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Information Retrieval"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:crossencodermsmarcoTinyBERTL2v2-QuantativeAnalysis"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:crossencodermsmarcoTinyBERTL2v2-ModelDetail"
  },
  {
    "s": "mcro:crossencodermsmarcoTinyBERTL2v2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli",
    "p": "mcro:hasUseCase",
    "o": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-UseCaseInformationSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody."
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli",
    "p": "mcro:hasDataset",
    "o": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-DatasetInformationSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model was created by fine-tuning the `facebook/wav2vec2-large-robust-ft-libri-960h` checkpoint on the [libritts](https://research.google/tools/datasets/libri-tts/) and [voxpopuli](https://github.com/facebookresearch/voxpopuli) datasets with a new vocabulary that includes punctuation."
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-PerformanceMetricInformationSection",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-PerformanceMetricInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model gets a respectable WER of 4.45% on the librispeech validation set. The baseline, `facebook/wav2vec2-large-robust-ft-libri-960h`, got 4.3%."
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli",
    "p": "mcro:hasLimitation",
    "o": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-LimitationInformationSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:jbetkerwav2vec2largerobustftlibrittsvoxpopuli-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Since the model was fine-tuned on clean audio, it is not well-suited for noisy audio like CommonVoice (though I may upload a checkpoint for that soon too). It still does pretty good, though."
  },
  {
    "s": "mcro:bigsciencebloomz560m",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bigsciencebloomz560m",
    "p": "mcro:hasCitation",
    "o": "mcro:bigsciencebloomz560m-Citation"
  },
  {
    "s": "mcro:bigsciencebloomz560m-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bigsciencebloomz560m-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{muennighoff2022crosslingual,\n  title={Crosslingual generalization through multitask finetuning},\n  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},\n  journal={arXiv preprint arXiv:2211.01786},\n  year={2022}\n}"
  },
  {
    "s": "mcro:bigsciencebloomz560m",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bigsciencebloomz560m-ModelArchitecture"
  },
  {
    "s": "mcro:bigsciencebloomz560m-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bigsciencebloomz560m-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Same as [bloom-560m](https://huggingface.co/bigscience/bloom-560m), also refer to the `config.json` file"
  },
  {
    "s": "mcro:bigsciencebloomz560m",
    "p": "mcro:hasUseCase",
    "o": "mcro:bigsciencebloomz560m-UseCase"
  },
  {
    "s": "mcro:bigsciencebloomz560m-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bigsciencebloomz560m-UseCase",
    "p": "prov:hasTextValue",
    "o": "We recommend using the model to perform tasks expressed in natural language. For example, given the prompt \"*Translate to English: Je t\u2019aime.*\", the model will most likely answer \"*I love you.*\"."
  },
  {
    "s": "mcro:spa-eng",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:spa-eng",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:spa-eng-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:spa-eng-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:spa-eng-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformer"
  },
  {
    "s": "mcro:spa-eng",
    "p": "mcro:hasDataset",
    "o": "mcro:spa-eng-DatasetInformationSection"
  },
  {
    "s": "mcro:spa-eng-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:spa-eng-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Spanish"
  },
  {
    "s": "mcro:spa-eng-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "English"
  },
  {
    "s": "mcro:Qwen1505BChat",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen1505BChat",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Qwen1505BChat-ModelDetail"
  },
  {
    "s": "mcro:Qwen1505BChat-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Qwen1505BChat-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "Qwen1.5 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes. For the beta version, temporarily we did not include GQA (except for 32B) and the mixture of SWA and full attention."
  },
  {
    "s": "mcro:Qwen1505BChat",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen1505BChat-Citation"
  },
  {
    "s": "mcro:Qwen1505BChat-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen1505BChat-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}"
  },
  {
    "s": "mcro:Qwen1505BChat",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen1505BChat-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen1505BChat-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen1505BChat-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data."
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish",
    "p": "mcro:hasModelDetail",
    "o": "mcro:wav2vec2xlsr300mcv7turkish-ModelDetail"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:wav2vec2xlsr300mcv7turkish-License"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wav2vec2xlsr300mcv7turkish-ModelArchitecture"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish",
    "p": "mcro:hasModelParameter",
    "o": "mcro:wav2vec2xlsr300mcv7turkish-ModelParameter"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-ModelParameter",
    "p": "mcro:hasTrainingData",
    "o": "mcro:wav2vec2xlsr300mcv7turkish-TrainingData"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mcv7turkish-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The following datasets were used for finetuning:\n - [Common Voice 7.0 TR](https://huggingface.co/datasets/mozilla-foundation/common_voice_7_0) All `validated` split except `test` split was used for training.\n - [MediaSpeech](https://www.openslr.org/108/)"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML4v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML4v2",
    "p": "mcro:hasDataset",
    "o": "mcro:crossencodermsmarcoMiniLML4v2-DatasetInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML4v2-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML4v2-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task."
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML4v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:crossencodermsmarcoMiniLML4v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML4v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:crossencodermsmarcoMiniLML4v2-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelDetailSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection1"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection2"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-UseCaseInformationSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k",
    "p": "mcro:hasDataset",
    "o": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-TrainingDataInformationSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection1",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2106-08254,\n  author    = {Hangbo Bao and\n               Li Dong and\n               Furu Wei},\n  title     = {BEiT: {BERT} Pre-Training of Image Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2106.08254},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2106.08254},\n  archivePrefix = {arXiv},\n  eprint    = {2106.08254},\n  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection2",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like)."
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BEiTbasesizedmodelfinetunedonImageNet22k-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=microsoft/beit) to look for\nfined-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:ProtGPT2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ProtGPT2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:ProtGPT2-ModelDetail"
  },
  {
    "s": "mcro:ProtGPT2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:ProtGPT2-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ProtGPT2-ModelArchitecture"
  },
  {
    "s": "mcro:ProtGPT2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ProtGPT2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "ProtGPT2 is based on the GPT2 Transformer architecture and contains 36 layers with a model dimensionality of 1280, totalling 738 million parameters."
  },
  {
    "s": "mcro:ProtGPT2-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:ProtGPT2-Dataset"
  },
  {
    "s": "mcro:ProtGPT2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:ProtGPT2-Dataset",
    "p": "prov:hasTextValue",
    "o": "UniRef50 (version 2021_04)"
  },
  {
    "s": "mcro:ProtGPT2-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "ProtGPT2 is a decoder-only transformer model pre-trained on the protein space, database UniRef50 (version 2021_04). The pre-training was done on the raw sequences without FASTA headers. Details of training and datasets can be found here: https://huggingface.co/datasets/nferruz/UR50_2021_04\n\nProtGPT2 was trained in a self-supervised fashion, i.e., the raw sequence data was used during training without including the annotation of sequences. In particular, ProtGPT2 was trained using a causal modelling objective, in which the model is trained to predict the next token (or, in this case, oligomer) in the sequence.\n By doing so, the model learns an internal representation of proteins and is able to <em>speak</em> the protein language."
  },
  {
    "s": "mcro:ProtGPT2",
    "p": "mcro:hasUseCase",
    "o": "mcro:ProtGPT2-UseCase"
  },
  {
    "s": "mcro:ProtGPT2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:ProtGPT2-UseCase",
    "p": "prov:hasTextValue",
    "o": "ProtGPT2 can be used for de novo protein design and engineering."
  },
  {
    "s": "mcro:gemma3model",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gemma3model",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gemma3model-ModelDetail"
  },
  {
    "s": "mcro:gemma3model-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gemma3model-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:gemma3model-Citation"
  },
  {
    "s": "mcro:gemma3model-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gemma3model-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}"
  },
  {
    "s": "mcro:gemma3model-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:gemma3model-Dataset"
  },
  {
    "s": "mcro:gemma3model-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:gemma3model-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:gemma3model-Architecture"
  },
  {
    "s": "mcro:gemma3model-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gemma3model-ModelDetail",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:gemma3model-UseCase"
  },
  {
    "s": "mcro:gemma3model-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gemma3model-UseCase",
    "p": "prov:hasTextValue",
    "o": "Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics."
  },
  {
    "s": "mcro:gemma3model-Dataset",
    "p": "prov:hasTextValue",
    "o": "These models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats."
  },
  {
    "s": "mcro:gemma3model-Architecture",
    "p": "prov:hasTextValue",
    "o": "Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*"
  },
  {
    "s": "mcro:all-distilroberta-v1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:all-distilroberta-v1",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:all-distilroberta-v1-UseCaseInformationSection"
  },
  {
    "s": "mcro:all-distilroberta-v1-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:all-distilroberta-v1-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 128 word pieces is truncated."
  },
  {
    "s": "mcro:all-distilroberta-v1",
    "p": "mcro:hasTrainingData",
    "o": "mcro:all-distilroberta-v1-TrainingDataInformationSection"
  },
  {
    "s": "mcro:all-distilroberta-v1-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:all-distilroberta-v1-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,124,818,467** |"
  },
  {
    "s": "mcro:all-distilroberta-v1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:all-distilroberta-v1-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:all-distilroberta-v1-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:all-distilroberta-v1-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "We used the pretrained [`distilroberta-base`](https://huggingface.co/distilroberta-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset."
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance on various downstream vision-language tasks. In particular, on the VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs. Notably, when further scaling the model, BridgeTower achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets."
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc",
    "p": "mcro:hasUseCase",
    "o": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-UseCaseInformationSection"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Here is how to use this model to perform contrastive learning between image and text pairs:\n\nfrom transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning\nimport requests\nfrom PIL import Image\nimport torch\n\nimage_urls = [\n    \"https://farm4.staticflickr.com/3395/3428278415_81c3e27f15_z.jpg\",\n \u00a0\u00a0 \"http://images.cocodataset.org/val2017/000000039769.jpg\"]\ntexts = [\n    \"two dogs in a car\",\n    \"two cats sleeping on a couch\"]\nimages = [Image.open(requests.get(url, stream=True).raw) for url in image_urls]\n\nprocessor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm\")\nmodel = BridgeTowerForContrastiveLearning.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n\ninputs\u00a0 = processor(images, texts, padding=True, return_tensors=\"pt\")\noutputs = model(**inputs)\n\ninputs\u00a0 = processor(images, texts[::-1], padding=True, return_tensors=\"pt\")\noutputs_swapped = model(**inputs)\n\nprint('Loss', outputs.loss.item())\n# Loss 0.00191505195107311\nprint('Loss with swapped images', outputs_swapped.loss.item())\n# Loss with swapped images 2.1259872913360596 \n\n\nHere is how to use this model to perform image and text matching\n\nfrom transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval\nimport requests\nfrom PIL import Image\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [\"An image of two cats chilling on a couch\", \"A football player scoring a goal\"]\n\nprocessor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-gaudi\")\nmodel = BridgeTowerForImageAndTextRetrieval.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-gaudi\")\n\n# forward pass\nscores = dict()\nfor text in texts:\n    # prepare inputs\n    encoding = processor(image, text, return_tensors=\"pt\")\n    outputs = model(**encoding)\n    scores[text] = outputs.logits[0,1].item()\n\n\nHere is how to use this model to perform masked language modeling:\n\nfrom transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000360943.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\ntext = \"a <mask> looking out of the window\"\n\nprocessor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-gaudi\")\nmodel = BridgeTowerForMaskedLM.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-gaudi\")\n\n# prepare inputs\nencoding = processor(image, text, return_tensors=\"pt\")\n\n# forward pass\noutputs = model(**encoding)\n\nresults = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())\n\nprint(results)\n#.a cat looking out of the window."
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc",
    "p": "mcro:hasTrainingData",
    "o": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-TrainingDataInformationSection"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "The BridgeTower model was pretrained on four public image-caption datasets:\n- [Conceptual Captions (CC3M)](https://ai.google.com/research/ConceptualCaptions/)\n- [Conceptual 12M (CC12M)](https://github.com/google-research-datasets/conceptual-12m)\n- [SBU Captions](https://www.cs.rice.edu/~vo9/sbucaptions/)\n- [MSCOCO Captions](https://arxiv.org/pdf/1504.00325.pdf)\n- [Visual Genome](https://visualgenome.org/)\n  \nThe total number of unique images in the combined data is around 14M."
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc",
    "p": "mcro:hasCitation",
    "o": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-CitationInformationSection"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BridgeTowerbridgetowerlargeitmmlmitc-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{xu2022bridge,\n  title={BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning},\n  author={Xu, Xiao and Wu, Chenfei and Rosenman, Shachar and Lal, Vasudev and Che, Wanxiang and Duan, Nan},\n  journal={arXiv preprint arXiv:2206.08657},\n  year={2022}\n}"
  },
  {
    "s": "mcro:Systranfasterwhispersmall",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Systranfasterwhispersmall",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Systranfasterwhispersmall-ModelArchitecture"
  },
  {
    "s": "mcro:Systranfasterwhispersmall-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Systranfasterwhispersmall-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "openai/whisper-small"
  },
  {
    "s": "mcro:hubertbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:hubertbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:hubertbase-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:hubertbase-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:hubertbase-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz."
  },
  {
    "s": "mcro:hubertbase",
    "p": "mcro:hasCitation",
    "o": "mcro:hubertbase-CitationInformationSection"
  },
  {
    "s": "mcro:hubertbase-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:hubertbase-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Paper"
  },
  {
    "s": "mcro:hubertbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:hubertbase-UseCaseInformationSection"
  },
  {
    "s": "mcro:hubertbase-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:hubertbase-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "See [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`."
  },
  {
    "s": "mcro:khawhitemangaocrbase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:khawhitemangaocrbase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:khawhitemangaocrbase-ModelArchitecture"
  },
  {
    "s": "mcro:khawhitemangaocrbase-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:khawhitemangaocrbase-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision Encoder Decoder framework"
  },
  {
    "s": "mcro:khawhitemangaocrbase",
    "p": "mcro:hasUseCase",
    "o": "mcro:khawhitemangaocrbase-UseCase"
  },
  {
    "s": "mcro:khawhitemangaocrbase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:khawhitemangaocrbase-UseCase",
    "p": "prov:hasTextValue",
    "o": "general purpose printed Japanese OCR"
  },
  {
    "s": "mcro:xclipbasesizedmodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:xclipbasesizedmodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:xclipbasesizedmodel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "X-CLIP is a minimal extension of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for general video-language understanding."
  },
  {
    "s": "mcro:xclipbasesizedmodel",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:xclipbasesizedmodel-UseCaseInformationSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for determining how well text goes with a given video."
  },
  {
    "s": "mcro:xclipbasesizedmodel",
    "p": "mcro:hasTrainingData",
    "o": "mcro:xclipbasesizedmodel-TrainingDataInformationSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-TrainingDataInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model was trained on [Kinetics-400](https://www.deepmind.com/open-source/kinetics)."
  },
  {
    "s": "mcro:xclipbasesizedmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:xclipbasesizedmodel-CitationInformationSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "It was introduced in the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Ni et al."
  },
  {
    "s": "mcro:xclipbasesizedmodel",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:xclipbasesizedmodel-QuantativeAnalysisSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-QuantativeAnalysisSection",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:xclipbasesizedmodel-QuantativeAnalysisSection",
    "p": "prov:hasTextValue",
    "o": "This model achieves a top-1 accuracy of 80.4% and a top-5 accuracy of 95.0%."
  },
  {
    "s": "mcro:ModelCardReport",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ModelCardReport",
    "p": "mcro:hasLicense",
    "o": "mcro:ModelCardReport-LicenseInformationSection"
  },
  {
    "s": "mcro:ModelCardReport-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:ModelCardReport",
    "p": "mcro:hasDataset",
    "o": "mcro:ModelCardReport-DatasetInformationSection"
  },
  {
    "s": "mcro:ModelCardReport-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:ModelCardReport",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ModelCardReport-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ModelCardReport-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ModelCardReport",
    "p": "mcro:hasCitation",
    "o": "mcro:ModelCardReport-CitationInformationSection"
  },
  {
    "s": "mcro:ModelCardReport-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ModelCardReport",
    "p": "mcro:hasUseCase",
    "o": "mcro:ModelCardReport-UseCaseInformationSection"
  },
  {
    "s": "mcro:ModelCardReport-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb0finetunedade20k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:segformerb0finetunedade20k",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:segformerb0finetunedade20k-ModelArchitecture"
  },
  {
    "s": "mcro:segformerb0finetunedade20k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:segformerb0finetunedade20k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset."
  },
  {
    "s": "mcro:segformerb0finetunedade20k",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:segformerb0finetunedade20k-IntendedUseCase"
  },
  {
    "s": "mcro:segformerb0finetunedade20k-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb0finetunedade20k-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:segformerb0finetunedade20k",
    "p": "mcro:hasLicense",
    "o": "mcro:segformerb0finetunedade20k-License"
  },
  {
    "s": "mcro:segformerb0finetunedade20k-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:segformerb0finetunedade20k-License",
    "p": "prov:hasTextValue",
    "o": "The license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE)."
  },
  {
    "s": "mcro:segformerb0finetunedade20k",
    "p": "mcro:hasCitation",
    "o": "mcro:segformerb0finetunedade20k-Citation"
  },
  {
    "s": "mcro:segformerb0finetunedade20k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:segformerb0finetunedade20k-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:USERbgem3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:USERbgem3",
    "p": "mcro:hasModelDetail",
    "o": "mcro:USERbgem3-ModelDetail"
  },
  {
    "s": "mcro:USERbgem3-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:USERbgem3-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:USERbgem3-Citation"
  },
  {
    "s": "mcro:USERbgem3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:USERbgem3-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepvk2024user,\n    title={USER: Universal Sentence Encoder for Russian},\n    author={Malashenko, Boris and  Zemerov, Anton and Spirin, Egor},\n    url={https://huggingface.co/datasets/deepvk/USER-base},\n    publisher={Hugging Face}\n    year={2024},\n}"
  },
  {
    "s": "mcro:USERbgem3-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:USERbgem3-Architecture"
  },
  {
    "s": "mcro:USERbgem3-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:USERbgem3-Architecture",
    "p": "prov:hasTextValue",
    "o": "sentence-transformer"
  },
  {
    "s": "mcro:USERbgem3",
    "p": "mcro:hasUseCase",
    "o": "mcro:USERbgem3-UseCase"
  },
  {
    "s": "mcro:USERbgem3-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:USERbgem3-UseCase",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:USERbgem3",
    "p": "mcro:hasLimitation",
    "o": "mcro:USERbgem3-Limitation"
  },
  {
    "s": "mcro:USERbgem3-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:USERbgem3-Limitation",
    "p": "prov:hasTextValue",
    "o": "We did not thoroughly evaluate the model's ability for sparse and multi-vec encoding."
  },
  {
    "s": "mcro:USERbgem3",
    "p": "mcro:hasTrainingData",
    "o": "mcro:USERbgem3-TrainingData"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "AllNLI"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "MIRACL"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "deepvk/ru-WANLI"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "deepvk/ru-HNP"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "MedNLI"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "RCB"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Terra"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Tapaco"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Lenta"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Mlsum"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Mr-TyDi"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Panorama"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "PravoIsrael"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Xlsum"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Fialka-v1"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "RussianKeywords"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Gazeta"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Gsm8k-ru"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "DSumRu"
  },
  {
    "s": "mcro:USERbgem3-TrainingData",
    "p": "prov:hasTextValue",
    "o": "SummDialogNews"
  },
  {
    "s": "mcro:detrresnet50",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:detrresnet50",
    "p": "mcro:hasModelDetail",
    "o": "mcro:detrresnet50-ModelDetail"
  },
  {
    "s": "mcro:detrresnet50-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:detrresnet50-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:detrresnet50-ModelArchitecture"
  },
  {
    "s": "mcro:detrresnet50-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:detrresnet50-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100."
  },
  {
    "s": "mcro:detrresnet50-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:detrresnet50-Citation"
  },
  {
    "s": "mcro:detrresnet50-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:detrresnet50-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2005-12872,\n  author    = {Nicolas Carion and\n               Francisco Massa and\n               Gabriel Synnaeve and\n               Nicolas Usunier and\n               Alexander Kirillov and\n               Sergey Zagoruyko},\n  title     = {End-to-End Object Detection with Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2005.12872},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2005.12872},\n  archivePrefix = {arXiv},\n  eprint    = {2005.12872},\n  timestamp = {Thu, 28 May 2020 17:38:09 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:detrresnet50",
    "p": "mcro:hasUseCase",
    "o": "mcro:detrresnet50-UseCase"
  },
  {
    "s": "mcro:detrresnet50-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:detrresnet50-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for object detection. See the [model hub](https://huggingface.co/models?search=facebook/detr) to look for all available DETR models."
  },
  {
    "s": "mcro:detrresnet50",
    "p": "mcro:hasTrainingData",
    "o": "mcro:detrresnet50-TrainingData"
  },
  {
    "s": "mcro:detrresnet50-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:detrresnet50-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The DETR model was trained on [COCO 2017 object detection](https://cocodataset.org/#download), a dataset consisting of 118k/5k annotated images for training/validation respectively."
  },
  {
    "s": "mcro:detrresnet50",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:detrresnet50-QuantativeAnalysis"
  },
  {
    "s": "mcro:detrresnet50-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:detrresnet50-QuantativeAnalysis",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:detrresnet50-PerformanceMetric"
  },
  {
    "s": "mcro:detrresnet50-PerformanceMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:detrresnet50-PerformanceMetric",
    "p": "prov:hasTextValue",
    "o": "This model achieves an AP (average precision) of **42.0** on COCO 2017 validation."
  },
  {
    "s": "mcro:Qwen25Omni",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen25Omni",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen25Omni-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen25Omni-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen25Omni-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Thinker-Talker architecture"
  },
  {
    "s": "mcro:Qwen25Omni",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen25Omni-UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen25Omni-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen25Omni-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner"
  },
  {
    "s": "mcro:Qwen25Omni",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen25Omni-CitationInformationSection"
  },
  {
    "s": "mcro:Qwen25Omni-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen25Omni-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{Qwen2.5-Omni,\n  title={Qwen2.5-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},\n  journal={arXiv preprint arXiv:2503.20215},\n  year={2025}\n}"
  },
  {
    "s": "mcro:bigvganv244khz128band512x",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bigvganv244khz128band512x",
    "p": "mcro:hasCitation",
    "o": "mcro:bigvganv244khz128band512x-CitationInformationSection"
  },
  {
    "s": "mcro:bigvganv244khz128band512x-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bigvganv244khz128band512x-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon"
  },
  {
    "s": "mcro:bigvganv244khz128band512x",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bigvganv244khz128band512x-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bigvganv244khz128band512x-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bigvganv244khz128band512x-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "BigVGAN"
  },
  {
    "s": "mcro:bigvganv244khz128band512x",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:bigvganv244khz128band512x-UseCaseInformationSection"
  },
  {
    "s": "mcro:bigvganv244khz128band512x-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bigvganv244khz128band512x-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Universal Neural Vocoder"
  },
  {
    "s": "mcro:bigvganv244khz128band512x",
    "p": "mcro:hasDataset",
    "o": "mcro:bigvganv244khz128band512x-DatasetInformationSection"
  },
  {
    "s": "mcro:bigvganv244khz128band512x-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bigvganv244khz128band512x-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Large-scale Compilation"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224",
    "p": "mcro:hasModelDetail",
    "o": "mcro:microsoftbeitlargepatch16224-ModelDetail"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftbeitlargepatch16224-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision Transformer (ViT)"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftbeitlargepatch16224-Citation"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-Citation",
    "p": "prov:hasTextValue",
    "o": "BEIT: BERT Pre-Training of Image Transformers"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:microsoftbeitlargepatch16224-IntendedUseCase"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "image classification"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224",
    "p": "mcro:hasTrainingData",
    "o": "mcro:microsoftbeitlargepatch16224-TrainingData"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-TrainingData",
    "p": "prov:hasTextValue",
    "o": "ImageNet-21k"
  },
  {
    "s": "mcro:microsoftbeitlargepatch16224-TrainingData",
    "p": "prov:hasTextValue",
    "o": "ImageNet"
  },
  {
    "s": "mcro:bert-base-uncased",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bert-base-uncased",
    "p": "mcro:hasLicense",
    "o": "mcro:bert-base-uncased-License"
  },
  {
    "s": "mcro:bert-base-uncased-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:bert-base-uncased-License",
    "p": "prov:hasTextValue",
    "o": "mit"
  },
  {
    "s": "mcro:bert-base-uncased",
    "p": "mcro:hasDataset",
    "o": "mcro:bert-base-uncased-Dataset"
  },
  {
    "s": "mcro:bert-base-uncased-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:bert-base-uncased-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet"
  },
  {
    "s": "mcro:bert-base-uncased",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:bert-base-uncased-Arch"
  },
  {
    "s": "mcro:bert-base-uncased-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:bert-base-uncased-Arch",
    "p": "prov:hasTextValue",
    "o": "CNN"
  },
  {
    "s": "mcro:bert-base-uncased",
    "p": "mcro:hasCitation",
    "o": "mcro:bert-base-uncased-Citation"
  },
  {
    "s": "mcro:bert-base-uncased-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:bert-base-uncased-Citation",
    "p": "prov:hasTextValue",
    "o": "citation text"
  },
  {
    "s": "mcro:bert-base-uncased",
    "p": "mcro:hasUseCase",
    "o": "mcro:bert-base-uncased-UseCase"
  },
  {
    "s": "mcro:bert-base-uncased-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bert-base-uncased-UseCase",
    "p": "prov:hasTextValue",
    "o": "Intended use case"
  },
  {
    "s": "mcro:gemma2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gemma2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gemma2-ModelDetail"
  },
  {
    "s": "mcro:gemma2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gemma2-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:gemma2-Citation"
  },
  {
    "s": "mcro:gemma2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gemma2-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{gemma_2024,\n    title={Gemma},\n    url={https://www.kaggle.com/m/3301},\n    DOI={10.34740/KAGGLE/M/3301},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2024}\n}"
  },
  {
    "s": "mcro:gemma2",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:gemma2-UseCase"
  },
  {
    "s": "mcro:gemma2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:gemma2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Open Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains."
  },
  {
    "s": "mcro:gemma2",
    "p": "mcro:hasDataset",
    "o": "mcro:gemma2-Dataset"
  },
  {
    "s": "mcro:gemma2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:gemma2-Dataset",
    "p": "prov:hasTextValue",
    "o": "These models were trained on a dataset of text data that includes a wide variety\nof sources."
  },
  {
    "s": "mcro:gemma2",
    "p": "mcro:hasLicense",
    "o": "mcro:gemma2-License"
  },
  {
    "s": "mcro:gemma2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:gemma2-License",
    "p": "prov:hasTextValue",
    "o": "Terms of Use: [Terms][terms]"
  },
  {
    "s": "mcro:gemma2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gemma2-Architecture"
  },
  {
    "s": "mcro:gemma2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gemma2-Architecture",
    "p": "prov:hasTextValue",
    "o": "Gemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models."
  },
  {
    "s": "mcro:StellaEn400Mv5",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:StellaEn400Mv5",
    "p": "mcro:hasCitation",
    "o": "mcro:StellaEn400Mv5-CitationInformationSection"
  },
  {
    "s": "mcro:StellaEn400Mv5-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:StellaEn400Mv5-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{zhang2025jasperstelladistillationsota,\n      title={Jasper and Stella: distillation of SOTA embedding models}, \n      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},\n      year={2025},\n      eprint={2412.19048},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2412.19048}, \n}"
  },
  {
    "s": "mcro:StellaEn400Mv5",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:StellaEn400Mv5-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:StellaEn400Mv5-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:StellaEn400Mv5-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The models are trained based on `Alibaba-NLP/gte-large-en-v1.5` and `Alibaba-NLP/gte-Qwen2-1.5B-instruct`."
  },
  {
    "s": "mcro:StellaEn400Mv5",
    "p": "mcro:hasUseCase",
    "o": "mcro:StellaEn400Mv5-UseCaseInformationSection"
  },
  {
    "s": "mcro:StellaEn400Mv5-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:StellaEn400Mv5-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "retrieve task and semantic textual similarity task"
  },
  {
    "s": "mcro:gemma2b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:gemma2b",
    "p": "mcro:hasModelDetail",
    "o": "mcro:gemma2b-ModelDetail"
  },
  {
    "s": "mcro:gemma2b-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:gemma2b-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:gemma2b-License"
  },
  {
    "s": "mcro:gemma2b-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:gemma2b-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:gemma2b-Citation"
  },
  {
    "s": "mcro:gemma2b-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:gemma2b-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:gemma2b-ModelArchitecture"
  },
  {
    "s": "mcro:gemma2b-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:gemma2b",
    "p": "mcro:hasModelParameter",
    "o": "mcro:gemma2b-ModelParameter"
  },
  {
    "s": "mcro:gemma2b-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:gemma2b",
    "p": "mcro:hasDataset",
    "o": "mcro:gemma2b-Dataset"
  },
  {
    "s": "mcro:gemma2b-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:gemma2b-Dataset",
    "p": "mcro:hasTrainingData",
    "o": "mcro:gemma2b-TrainingData"
  },
  {
    "s": "mcro:gemma2b-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:gemma2b",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:gemma2b-QuantativeAnalysis"
  },
  {
    "s": "mcro:gemma2b-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:gemma2b-QuantativeAnalysis",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:gemma2b-EvaluationData"
  },
  {
    "s": "mcro:gemma2b-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:gemma2b",
    "p": "mcro:hasConsideration",
    "o": "mcro:gemma2b-Consideration"
  },
  {
    "s": "mcro:gemma2b-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:gemma2b-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:gemma2b-Limitation"
  },
  {
    "s": "mcro:gemma2b-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:gemma2b-Consideration",
    "p": "mcro:hasUseCase",
    "o": "mcro:gemma2b-UseCase"
  },
  {
    "s": "mcro:gemma2b-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:eng-zho",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:eng-zho",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:eng-zho-ModelArchitecture"
  },
  {
    "s": "mcro:eng-zho-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:eng-zho-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformer"
  },
  {
    "s": "mcro:eng-zho",
    "p": "mcro:hasDataset",
    "o": "mcro:eng-zho-Dataset"
  },
  {
    "s": "mcro:eng-zho-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:eng-zho-Dataset",
    "p": "prov:hasTextValue",
    "o": "source group: English "
  },
  {
    "s": "mcro:eng-zho-Dataset",
    "p": "prov:hasTextValue",
    "o": "target group: Chinese "
  },
  {
    "s": "mcro:eng-zho-Dataset",
    "p": "prov:hasTextValue",
    "o": "pre-processing: normalization + SentencePiece (spm32k,spm32k)"
  },
  {
    "s": "mcro:eng-zho",
    "p": "mcro:hasUseCase",
    "o": "mcro:eng-zho-UseCase"
  },
  {
    "s": "mcro:eng-zho-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:eng-zho-UseCase",
    "p": "prov:hasTextValue",
    "o": "source language(s): eng"
  },
  {
    "s": "mcro:eng-zho-UseCase",
    "p": "prov:hasTextValue",
    "o": "target language(s): cjy_Hans cjy_Hant cmn cmn_Hans cmn_Hant gan lzh lzh_Hans nan wuu yue yue_Hans yue_Hant"
  },
  {
    "s": "mcro:trlinternaltestingtinyT5ForConditionalGeneration",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:trlinternaltestingtinyT5ForConditionalGeneration",
    "p": "mcro:hasModelDetail",
    "o": "mcro:trlinternaltestingtinyT5ForConditionalGeneration-ModelDetail"
  },
  {
    "s": "mcro:trlinternaltestingtinyT5ForConditionalGeneration-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:trlinternaltestingtinyT5ForConditionalGeneration-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library."
  },
  {
    "s": "mcro:whisper",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasModelDetail",
    "o": "mcro:whisper-ModelDetail"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:whisper-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:whisper-Citation"
  },
  {
    "s": "mcro:whisper-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:whisper-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasModelParameter",
    "o": "mcro:whisper-ModelParameter"
  },
  {
    "s": "mcro:whisper-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:whisper-ModelParameter",
    "p": "mcro:hasDataset",
    "o": "mcro:whisper-Dataset"
  },
  {
    "s": "mcro:whisper-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:whisper-Dataset",
    "p": "prov:hasTextValue",
    "o": "680k hours of labelled data"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisper-ModelArchitecture"
  },
  {
    "s": "mcro:whisper-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisper-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer based encoder-decoder model"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasUseCase",
    "o": "mcro:whisper-UseCase"
  },
  {
    "s": "mcro:whisper-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:whisper-UseCase",
    "p": "prov:hasTextValue",
    "o": "automatic speech recognition (ASR) and speech translation"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasTrainingData",
    "o": "mcro:whisper-TrainingData"
  },
  {
    "s": "mcro:whisper-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:whisper-TrainingData",
    "p": "prov:hasTextValue",
    "o": "680,000 hours of audio and the corresponding transcripts collected from the internet"
  },
  {
    "s": "mcro:whisper",
    "p": "mcro:hasConsideration",
    "o": "mcro:whisper-Consideration"
  },
  {
    "s": "mcro:whisper-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:whisper-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:whisper-Limitation"
  },
  {
    "s": "mcro:whisper-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:whisper-Consideration",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:whisper-EthicalConsideration"
  },
  {
    "s": "mcro:whisper-EthicalConsideration",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:whisper-EthicalConsideration",
    "p": "prov:hasTextValue",
    "o": "dual use concerns"
  },
  {
    "s": "mcro:GIT",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:GIT",
    "p": "mcro:hasModelDetail",
    "o": "mcro:GIT-ModelDetail"
  },
  {
    "s": "mcro:GIT-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:GIT-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:GIT-ModelArchitecture"
  },
  {
    "s": "mcro:GIT-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:GIT-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Transformer decoder conditioned on both CLIP image tokens and text tokens"
  },
  {
    "s": "mcro:GIT",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:GIT-UseCase"
  },
  {
    "s": "mcro:GIT-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:GIT-UseCase",
    "p": "prov:hasTextValue",
    "o": "image and video captioning"
  },
  {
    "s": "mcro:GIT-UseCase",
    "p": "prov:hasTextValue",
    "o": "visual question answering (VQA) on images and videos"
  },
  {
    "s": "mcro:GIT-UseCase",
    "p": "prov:hasTextValue",
    "o": "image classification"
  },
  {
    "s": "mcro:GIT",
    "p": "mcro:hasTrainingData",
    "o": "mcro:GIT-TrainingData"
  },
  {
    "s": "mcro:GIT-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:GIT-TrainingData",
    "p": "prov:hasTextValue",
    "o": "10 million image-text pairs"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese",
    "p": "mcro:hasModelDetail",
    "o": "mcro:TaiyiCLIPRoberta102MChinese-ModelDetail"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:TaiyiCLIPRoberta102MChinese-Architecture"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Architecture",
    "p": "prov:hasTextValue",
    "o": "CLIP (Roberta)"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:TaiyiCLIPRoberta102MChinese-Citation"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{fengshenbang,\n  author    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\n  title     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\n  journal   = {CoRR},\n  volume    = {abs/2209.02970},\n  year      = {2022}\n}"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:TaiyiCLIPRoberta102MChinese-Citation2"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{Fengshenbang-LM,\n  title={Fengshenbang-LM},\n  author={IDEA-CCNL},\n  year={2021},\n  howpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese",
    "p": "mcro:hasUseCase",
    "o": "mcro:TaiyiCLIPRoberta102MChinese-UseCase"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-UseCase",
    "p": "prov:hasTextValue",
    "o": "\u591a\u6a21\u6001 Multimodal"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese",
    "p": "mcro:hasDataset",
    "o": "mcro:TaiyiCLIPRoberta102MChinese-Dataset"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Dataset",
    "p": "prov:hasTextValue",
    "o": "Noah-Wukong"
  },
  {
    "s": "mcro:TaiyiCLIPRoberta102MChinese-Dataset",
    "p": "prov:hasTextValue",
    "o": "Zero"
  },
  {
    "s": "mcro:keremberkeyolov5nlicenseplate",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:keremberkeyolov5nlicenseplate",
    "p": "mcro:hasUseCase",
    "o": "mcro:keremberkeyolov5nlicenseplate-UseCase"
  },
  {
    "s": "mcro:keremberkeyolov5nlicenseplate-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:keremberkeyolov5nlicenseplate",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:keremberkeyolov5nlicenseplate-ModelArchitecture"
  },
  {
    "s": "mcro:keremberkeyolov5nlicenseplate-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:facebookcontriever",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:facebookcontriever",
    "p": "mcro:hasCitation",
    "o": "mcro:facebookcontriever-CitationInformationSection"
  },
  {
    "s": "mcro:facebookcontriever-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:facebookcontriever-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "This model has been trained without supervision following the approach described in [Towards Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118). The associated GitHub repository is available here https://github.com/facebookresearch/contriever."
  },
  {
    "s": "mcro:facebookcontriever",
    "p": "mcro:hasUseCase",
    "o": "mcro:facebookcontriever-UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookcontriever-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:facebookcontriever-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Using the model directly available in HuggingFace transformers requires to add a mean pooling operation to obtain a sentence embedding."
  },
  {
    "s": "mcro:madebyollinsdxlvaefp16fix",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:madebyollinsdxlvaefp16fix",
    "p": "mcro:hasModelDetail",
    "o": "mcro:madebyollinsdxlvaefp16fix-ModelDetail"
  },
  {
    "s": "mcro:madebyollinsdxlvaefp16fix-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:madebyollinsdxlvaefp16fix-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "SDXL-VAE-FP16-Fix is the [SDXL VAE](https://huggingface.co/stabilityai/sdxl-vae)*, but modified to run in fp16 precision without generating NaNs."
  },
  {
    "s": "mcro:WhisperbaseenmodelforCTranslate2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:WhisperbaseenmodelforCTranslate2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:WhisperbaseenmodelforCTranslate2-ModelArchitecture"
  },
  {
    "s": "mcro:WhisperbaseenmodelforCTranslate2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:metaLlama31Instruct-ModelDetail"
  },
  {
    "s": "mcro:metaLlama31Instruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:metaLlama31Instruct-License"
  },
  {
    "s": "mcro:metaLlama31Instruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-License",
    "p": "prov:hasTextValue",
    "o": "A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)"
  },
  {
    "s": "mcro:metaLlama31Instruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:metaLlama31Instruct-ModelArchitecture"
  },
  {
    "s": "mcro:metaLlama31Instruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:metaLlama31Instruct-IntendedUseCase"
  },
  {
    "s": "mcro:metaLlama31Instruct-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases."
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "mcro:hasOutOfScopeUseCase",
    "o": "mcro:metaLlama31Instruct-OutOfScopeUseCase"
  },
  {
    "s": "mcro:metaLlama31Instruct-OutOfScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutofScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:metaLlama31Instruct-OutOfScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**."
  },
  {
    "s": "mcro:metaLlama31Instruct",
    "p": "mcro:hasTrainingData",
    "o": "mcro:metaLlama31Instruct-TrainingData"
  },
  {
    "s": "mcro:metaLlama31Instruct-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:metaLlama31Instruct-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples."
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:ibmgranitegranite318binstruct-ModelDetail"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:ibmgranitegranite318binstruct-License"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:ibmgranitegranite318binstruct-Architecture"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-Architecture",
    "p": "prov:hasTextValue",
    "o": "Granite-3.1-8B-Instruct is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings."
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:ibmgranitegranite318binstruct-UseCase"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is designed to respond to general instructions and can be used to build AI assistants for multiple domains, including business applications."
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct",
    "p": "mcro:hasDataset",
    "o": "mcro:ibmgranitegranite318binstruct-Dataset"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:ibmgranitegranite318binstruct-Dataset",
    "p": "prov:hasTextValue",
    "o": "Overall, our SFT data is largely comprised of three key sources: (1) publicly available datasets with permissive license, (2) internal synthetic data targeting specific capabilities including long-context tasks, and (3) very small amounts of human-curated data. A detailed attribution of datasets can be found in the [Granite 3.0 Technical Report](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf), [Granite 3.1 Technical Report (coming soon)](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d), and [Accompanying Author List](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/author-ack.pdf)."
  },
  {
    "s": "mcro:opusmtenfr",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:opusmtenfr",
    "p": "mcro:hasDataset",
    "o": "mcro:opusmtenfr-DatasetInformationSection"
  },
  {
    "s": "mcro:opusmtenfr-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:opusmtenfr-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "opus"
  },
  {
    "s": "mcro:opusmtenfr",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:opusmtenfr-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:opusmtenfr-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:opusmtenfr-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformer-align"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250",
    "p": "mcro:hasModelDetail",
    "o": "mcro:czechwav2vec2xlsr300mcs250-ModelDetailSection"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:czechwav2vec2xlsr300mcs250-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "wav2vec2-xls-r-300m"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250-ModelDetailSection",
    "p": "mcro:hasDataset",
    "o": "mcro:czechwav2vec2xlsr300mcs250-DatasetInformationSection"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:czechwav2vec2xlsr300mcs250-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "common_voice 8.0"
  },
  {
    "s": "mcro:crossencodernlidebertav3base",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:crossencodernlidebertav3base",
    "p": "mcro:hasModelDetail",
    "o": "mcro:crossencodernlidebertav3base-ModelDetail"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:crossencodernlidebertav3base-ModelArchitecture"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "This model was trained using [SentenceTransformers](https://sbert.net) [Cross-Encoder](https://www.sbert.net/examples/applications/cross-encoder/README.html) class. This model is based on [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base)"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:crossencodernlidebertav3base-Dataset"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-Dataset",
    "p": "prov:hasTextValue",
    "o": "The model was trained on the [SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral."
  },
  {
    "s": "mcro:crossencodernlidebertav3base",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:crossencodernlidebertav3base-QuantativeAnalysis"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-QuantativeAnalysis",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:crossencodernlidebertav3base-PerformanceMetric1"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-PerformanceMetric1",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-PerformanceMetric1",
    "p": "prov:hasTextValue",
    "o": "- Accuracy on SNLI-test dataset: 92.38"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-QuantativeAnalysis",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:crossencodernlidebertav3base-PerformanceMetric2"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-PerformanceMetric2",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-PerformanceMetric2",
    "p": "prov:hasTextValue",
    "o": "- Accuracy on  MNLI mismatched set: 90.04"
  },
  {
    "s": "mcro:crossencodernlidebertav3base",
    "p": "mcro:hasUseCase",
    "o": "mcro:crossencodernlidebertav3base-UseCase"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:crossencodernlidebertav3base-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model can also be used for zero-shot-classification:"
  },
  {
    "s": "mcro:chronosboltmini",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronosboltmini",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronosboltmini-ModelArchitecture"
  },
  {
    "s": "mcro:chronosboltmini-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronosboltmini-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations."
  },
  {
    "s": "mcro:chronosboltmini",
    "p": "mcro:hasCitation",
    "o": "mcro:chronosboltmini-Citation"
  },
  {
    "s": "mcro:chronosboltmini-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronosboltmini-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronosboltmini",
    "p": "mcro:hasLicense",
    "o": "mcro:chronosboltmini-License"
  },
  {
    "s": "mcro:chronosboltmini-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronosboltmini-License",
    "p": "prov:hasTextValue",
    "o": "This project is licensed under the Apache-2.0 License."
  },
  {
    "s": "mcro:chronosboltmini",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronosboltmini-UseCase"
  },
  {
    "s": "mcro:chronosboltmini-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronosboltmini-UseCase",
    "p": "prov:hasTextValue",
    "o": "Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting."
  },
  {
    "s": "mcro:chronosboltmini",
    "p": "mcro:hasDataset",
    "o": "mcro:chronosboltmini-Dataset"
  },
  {
    "s": "mcro:chronosboltmini-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:chronosboltmini-Dataset",
    "p": "prov:hasTextValue",
    "o": "It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps\u2014a method known as direct multi-step forecasting."
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen25Coder7BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen25Coder7BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report},\n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:Qwen25Coder7BInstruct-UseCase"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Qwen25Coder7BInstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "Code Agents"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv",
    "p": "mcro:hasDataset",
    "o": "mcro:microsoftwavlmbaseplussv-DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)\n- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)\n- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftwavlmbaseplussv-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model was pretrained on 16kHz sampled speech audio with utterance and speaker contrastive loss."
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv",
    "p": "mcro:hasCitation",
    "o": "mcro:microsoftwavlmbaseplussv-CitationInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "[Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900)\n\nAuthors: Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei\n\n**Abstract**\n*Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.*"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftwavlmbaseplussv-UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Speaker Verification"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv",
    "p": "mcro:hasLicense",
    "o": "mcro:microsoftwavlmbaseplussv-LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftwavlmbaseplussv-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The official license can be found [here](https://github.com/microsoft/UniSpeech/blob/main/LICENSE)"
  },
  {
    "s": "mcro:indobenchmarkindobertbasep1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:indobenchmarkindobertbasep1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:indobenchmarkindobertbasep1-ModelArchitecture"
  },
  {
    "s": "mcro:indobenchmarkindobertbasep1-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:indobenchmarkindobertbasep1-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT model"
  },
  {
    "s": "mcro:indobenchmarkindobertbasep1",
    "p": "mcro:hasCitation",
    "o": "mcro:indobenchmarkindobertbasep1-Citation"
  },
  {
    "s": "mcro:indobenchmarkindobertbasep1-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:indobenchmarkindobertbasep1-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{wilie2020indonlu,\n  title={IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding},\n  author={Bryan Wilie and Karissa Vincentio and Genta Indra Winata and Samuel Cahyawijaya and X. Li and Zhi Yuan Lim and S. Soleman and R. Mahendra and Pascale Fung and Syafri Bahar and A. Purwarianti},\n  booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},\n  year={2020}\n}"
  },
  {
    "s": "mcro:modelcard123",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:modelcard123",
    "p": "mcro:hasLicense",
    "o": "mcro:modelcard123-License"
  },
  {
    "s": "mcro:modelcard123-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:modelcard123-License",
    "p": "prov:hasTextValue",
    "o": "mit"
  },
  {
    "s": "mcro:modelcard123",
    "p": "mcro:hasDataset",
    "o": "mcro:modelcard123-Dataset"
  },
  {
    "s": "mcro:modelcard123-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:modelcard123-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet"
  },
  {
    "s": "mcro:modelcard123",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:modelcard123-ModelArchitecture"
  },
  {
    "s": "mcro:modelcard123-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:modelcard123-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "CNN"
  },
  {
    "s": "mcro:modelcard123",
    "p": "mcro:hasCitation",
    "o": "mcro:modelcard123-Citation"
  },
  {
    "s": "mcro:modelcard123-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:modelcard123-Citation",
    "p": "prov:hasTextValue",
    "o": "Citation Text"
  },
  {
    "s": "mcro:modelcard123",
    "p": "mcro:hasUseCase",
    "o": "mcro:modelcard123-UseCase"
  },
  {
    "s": "mcro:modelcard123-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:modelcard123-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image classification"
  },
  {
    "s": "mcro:testmodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:testmodel",
    "p": "mcro:hasLicense",
    "o": "mcro:testmodel-LicenseInformationSection"
  },
  {
    "s": "mcro:testmodel",
    "p": "mcro:hasDataset",
    "o": "mcro:testmodel-DatasetInformationSection"
  },
  {
    "s": "mcro:testmodel",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:testmodel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:testmodel",
    "p": "mcro:hasCitation",
    "o": "mcro:testmodel-CitationInformationSection"
  },
  {
    "s": "mcro:testmodel",
    "p": "mcro:hasUseCase",
    "o": "mcro:testmodel-UseCaseInformationSection"
  },
  {
    "s": "mcro:testmodel-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:testmodel-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "mit"
  },
  {
    "s": "mcro:testmodel-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:testmodel-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "ImageNet"
  },
  {
    "s": "mcro:testmodel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:testmodel-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "CNN"
  },
  {
    "s": "mcro:testmodel-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:testmodel-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "test citation"
  },
  {
    "s": "mcro:testmodel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:testmodel-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "image classification"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev",
    "p": "mcro:hasModelDetail",
    "o": "mcro:blackforestlabsFLUX1Filldev-ModelDetail"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "`FLUX.1 Fill [dev]` is a 12 billion parameter rectified flow transformer capable of filling areas in existing images based on a text description.\nFor more information, please read our [blog post](https://blackforestlabs.ai/flux-1-tools/)."
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev",
    "p": "mcro:hasLicense",
    "o": "mcro:blackforestlabsFLUX1Filldev-License"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-License",
    "p": "prov:hasTextValue",
    "o": "Generated outputs can be used for personal, scientific, and commercial purposes as described in the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)."
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev",
    "p": "mcro:hasLimitation",
    "o": "mcro:blackforestlabsFLUX1Filldev-Limitation"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-Limitation",
    "p": "prov:hasTextValue",
    "o": "- This model is not intended or able to provide factual information.\n- As a statistical model this checkpoint might amplify existing societal biases.\n- The model may fail to generate output that matches the prompts.\n- Prompt following is heavily influenced by the prompting-style.\n- There may be slight-color shifts in areas that are not filled in\n- Filling in complex textures may produce lines at the edges of the filled-area."
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev",
    "p": "mcro:hasUseCase",
    "o": "mcro:blackforestlabsFLUX1Filldev-UseCase"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model and its derivatives may not be used\n\n- In any way that violates any applicable national, federal, state, local or international law or regulation.\n- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\n- To generate or disseminate verifiably false information and/or content with the purpose of harming others.\n- To generate or disseminate personal identifiable information that can be used to harm an individual.\n- To harass, abuse, threat"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev",
    "p": "mcro:hasOutofScopeUseCase",
    "o": "mcro:blackforestlabsFLUX1Filldev-OutofScopeUseCase"
  },
  {
    "s": "mcro:blackforestlabsFLUX1Filldev-OutofScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutofScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:t5large",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:t5large",
    "p": "mcro:hasModelDetail",
    "o": "mcro:t5large-ModelDetail"
  },
  {
    "s": "mcro:t5large-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:t5large-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:t5large-License"
  },
  {
    "s": "mcro:t5large-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:t5large-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:t5large-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:t5large-Citation"
  },
  {
    "s": "mcro:t5large-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:t5large-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:t5large-ModelArchitecture"
  },
  {
    "s": "mcro:t5large-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:t5large",
    "p": "mcro:hasUseCase",
    "o": "mcro:t5large-UseCase"
  },
  {
    "s": "mcro:t5large-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:t5large",
    "p": "mcro:hasConsideration",
    "o": "mcro:t5large-Consideration"
  },
  {
    "s": "mcro:t5large-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:t5large",
    "p": "mcro:hasModelParameter",
    "o": "mcro:t5large-ModelParameter"
  },
  {
    "s": "mcro:t5large-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:t5large",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:t5large-QuantativeAnalysis"
  },
  {
    "s": "mcro:t5large-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:t5large",
    "p": "mcro:hasDataset",
    "o": "mcro:t5large-Dataset"
  },
  {
    "s": "mcro:t5large-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr",
    "p": "mcro:hasModelDetail",
    "o": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-ModelDetail"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. The model was trained on Turkish machine translated versions of [NLI](https://huggingface.co/datasets/nli_tr) and [STS-b](https://huggingface.co/datasets/emrecan/stsb-mt-turkish) datasets, using example [training scripts]( https://github.com/UKPLab/sentence-transformers/tree/master/examples/training) from sentence-transformers GitHub repository."
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr",
    "p": "mcro:hasUseCase",
    "o": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-UseCase"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-UseCase",
    "p": "prov:hasTextValue",
    "o": "tasks like clustering or semantic search"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr",
    "p": "mcro:hasDataset",
    "o": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Dataset"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Dataset",
    "p": "prov:hasTextValue",
    "o": "NLI"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Dataset",
    "p": "prov:hasTextValue",
    "o": "STS-b"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Architecture"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-Architecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr",
    "p": "mcro:hasTrainingData",
    "o": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-TrainingData"
  },
  {
    "s": "mcro:emrecanbertbaseturkishcasedmeannlistsbtr-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen14B-License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-License",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen14B-Citation"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen14B-ModelArchitecture"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "MoE"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B",
    "p": "mcro:hasUseCase",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen14B-UseCase"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-UseCase",
    "p": "prov:hasTextValue",
    "o": "reasoning"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B",
    "p": "mcro:hasDataset",
    "o": "mcro:deepseekaiDeepSeekR1DistillQwen14B-Dataset"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1DistillQwen14B-Dataset",
    "p": "prov:hasTextValue",
    "o": "Qwen2.5 series"
  },
  {
    "s": "mcro:fasttextlanguageidentification",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:fasttextlanguageidentification",
    "p": "mcro:hasModelDetail",
    "o": "mcro:fasttextlanguageidentification-ModelDetail"
  },
  {
    "s": "mcro:fasttextlanguageidentification-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:fasttextlanguageidentification-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:fasttextlanguageidentification-License"
  },
  {
    "s": "mcro:fasttextlanguageidentification-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:fasttextlanguageidentification-License",
    "p": "prov:hasTextValue",
    "o": "The language identification model is distributed under the [*Creative Commons Attribution-NonCommercial 4.0 International Public License*](https://creativecommons.org/licenses/by-nc/4.0/)."
  },
  {
    "s": "mcro:fasttextlanguageidentification-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:fasttextlanguageidentification-Citation"
  },
  {
    "s": "mcro:fasttextlanguageidentification-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:fasttextlanguageidentification-Citation",
    "p": "prov:hasTextValue",
    "o": "Please cite [1] if using this code for learning word representations or [2] if using for text classification."
  },
  {
    "s": "mcro:fasttextlanguageidentification",
    "p": "mcro:hasUseCase",
    "o": "mcro:fasttextlanguageidentification-UseCase"
  },
  {
    "s": "mcro:fasttextlanguageidentification-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:fasttextlanguageidentification-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use pre-trained word vectors for text classification or language identification. See the [tutorials](https://fasttext.cc/docs/en/supervised-tutorial.html) and [resources](https://fasttext.cc/docs/en/english-vectors.html) on its official website to look for tasks that interest you."
  },
  {
    "s": "mcro:fasttextlanguageidentification",
    "p": "mcro:hasDataset",
    "o": "mcro:fasttextlanguageidentification-Dataset"
  },
  {
    "s": "mcro:fasttextlanguageidentification-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:fasttextlanguageidentification-Dataset",
    "p": "prov:hasTextValue",
    "o": "Pre-trained word vectors for 157 languages were trained on [Common Crawl](http://commoncrawl.org/) and [Wikipedia](https://www.wikipedia.org/) using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish."
  },
  {
    "s": "mcro:fasttextlanguageidentification",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:fasttextlanguageidentification-Architecture"
  },
  {
    "s": "mcro:fasttextlanguageidentification-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:fasttextlanguageidentification-Architecture",
    "p": "prov:hasTextValue",
    "o": "fastText is a library for efficient learning of word representations and sentence classification."
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens",
    "p": "mcro:hasUseCaseInformation",
    "o": "mcro:sentencetransformersrobertabasenlimeantokens-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "clustering or semantic search"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens",
    "p": "mcro:hasModelArchitectureInformation",
    "o": "mcro:sentencetransformersrobertabasenlimeantokens-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Transformer({'max_seq_length': 128, 'do_lower_case': True}) with Transformer model: RobertaModel"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens",
    "p": "mcro:hasCitationInformation",
    "o": "mcro:sentencetransformersrobertabasenlimeantokens-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Reimers, Nils and Gurevych, Iryna"
  },
  {
    "s": "mcro:sentencetransformersrobertabasenlimeantokens-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool",
    "p": "mcro:hasCitation",
    "o": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-CitationInformationSection"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{chalkidis-etal-2020-legal,\n    title = \"{LEGAL}-{BERT}: The Muppets straight out of Law School\",\n    author = \"Chalkidis, Ilias  and\n      Fergadiotis, Manos  and\n      Malakasiotis, Prodromos  and\n      Aletras, Nikolaos  and\n      Androutsopoulos, Ion\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    doi = \"10.18653/v1/2020.findings-emnlp.261\",\n    pages = \"2898--2904\"\n}"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "* We trained BERT using the official code provided in Google BERT's GitHub repository (https://github.com/google-research/bert).\n* We released a model similar to the English BERT-BASE model (12-layer, 768-hidden, 12-heads, 110M parameters).\n* We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4.\n* We were able to use a single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc), while also utilizing [GCP research credits](https://edu.google.com/programs/credits/research). Huge thanks to both Google programs for supporting us!\n* Part of LEGAL-BERT is a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool",
    "p": "mcro:hasDataset",
    "o": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-DatasetInformationSection"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "* 116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.\n    \n* 61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk).\n    \n* 19,867 cases from the European Court of Justice (ECJ), also available from EURLEX.\n    \n* 12,554 cases from HUDOC, the repository of the European Court of Human Rights (ECHR) (http://hudoc.echr.coe.int/eng).\n    \n* 164,141 cases from various courts across the USA, hosted in the Case Law Access Project portal (https://case.law).\n    \n* 76,366 US contracts from EDGAR, the database of US Securities and Exchange Commission (SECOM) (https://www.sec.gov/edgar.shtml)."
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool",
    "p": "mcro:hasUseCase",
    "o": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-UseCaseInformationSection"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:legalbertTheMuppetsStraightOutOfLawSchool-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available."
  },
  {
    "s": "mcro:JackFramllama68m",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:JackFramllama68m",
    "p": "mcro:hasModelDetail",
    "o": "mcro:JackFramllama68m-ModelDetail"
  },
  {
    "s": "mcro:JackFramllama68m-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:JackFramllama68m-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "This is a LLaMA-like model with only 68M parameters trained on Wikipedia and part of the C4-en and C4-realnewslike datasets.\n\nNo evaluation has been conducted yet, so use it with care.\n\nThe model is mainly developed as a base Small Speculative Model in the [SpecInfer](https://arxiv.org/abs/2305.09781) paper."
  },
  {
    "s": "mcro:JackFramllama68m-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:JackFramllama68m-Citation"
  },
  {
    "s": "mcro:JackFramllama68m-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:JackFramllama68m-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{miao2023specinfer,\n      title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification}, \n      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},\n      year={2023},\n      eprint={2305.09781},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:opusmtnlen",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:opusmtnlen",
    "p": "mcro:hasDataset",
    "o": "mcro:opusmtnlen-DatasetInformationSection"
  },
  {
    "s": "mcro:opusmtnlen-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:opusmtnlen-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "opus"
  },
  {
    "s": "mcro:opusmtnlen",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:opusmtnlen-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:opusmtnlen-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:opusmtnlen-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "transformer-align"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14",
    "p": "mcro:hasLicense",
    "o": "mcro:BRIA_Background_Removal_v14-License"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14-License",
    "p": "prov:hasTextValue",
    "o": "The model is released under a Creative Commons license for non-commercial use.\n  - Commercial use is subject to a commercial agreement with BRIA. To purchase a commercial license simply click "
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14",
    "p": "mcro:hasDataset",
    "o": "mcro:BRIA_Background_Removal_v14-Dataset"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14-Dataset",
    "p": "prov:hasTextValue",
    "o": "Bria-RMBG model was trained with over 12,000 high-quality, high-resolution, manually labeled (pixel-wise accuracy), fully licensed images.\nOur benchmark included balanced gender, balanced ethnicity, and people with different types of disabilities.\nFor clarity, we provide our data distribution according to different categories, demonstrating our model\u2019s versatility."
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BRIA_Background_Removal_v14-Architecture"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14-Architecture",
    "p": "prov:hasTextValue",
    "o": "RMBG v1.4 is developed on the [IS-Net](https://github.com/xuebinqin/DIS) enhanced with our unique training scheme and proprietary dataset. \nThese modifications significantly improve the model\u2019s accuracy and effectiveness in diverse image-processing scenarios."
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:BRIA_Background_Removal_v14-UseCase"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BRIA_Background_Removal_v14-UseCase",
    "p": "prov:hasTextValue",
    "o": "designed to effectively separate foreground from background in a range of\ncategories and image types. This model has been trained on a carefully selected dataset, which includes:\ngeneral stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale."
  },
  {
    "s": "mcro:InternVL22B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:InternVL22B",
    "p": "mcro:hasModelDetail",
    "o": "mcro:InternVL22B-ModelDetail"
  },
  {
    "s": "mcro:InternVL22B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:InternVL22B-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:InternVL22B-ModelArchitecture"
  },
  {
    "s": "mcro:InternVL22B-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:InternVL22B-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "InternViT-300M-448px, an MLP projector, and internlm2-chat-1_8b"
  },
  {
    "s": "mcro:InternVL22B",
    "p": "mcro:hasLicense",
    "o": "mcro:InternVL22B-License"
  },
  {
    "s": "mcro:InternVL22B-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:InternVL22B-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:InternVL22B",
    "p": "mcro:hasCitation",
    "o": "mcro:InternVL22B-Citation"
  },
  {
    "s": "mcro:InternVL22B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:InternVL22B-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{chen2024expanding,\n  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},\n  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},\n  journal={arXiv preprint arXiv:2412.05271},\n  year={2024}\n}\n@article{gao2024mini,\n  title={Mini-internvl: A flexible-transfer pocket multimodal model with 5\\% parameters and 90\\% performance},\n  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},\n  journal={arXiv preprint arXiv:2410.16261},\n  year={2024}\n}\n@article{chen2024far,\n  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},\n  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},\n  journal={arXiv preprint arXiv:2404.16821},\n  year={2024}\n}\n@inproceedings{chen2024internvl,\n  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},\n  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={24185--24198},\n  year={2024}\n}"
  },
  {
    "s": "mcro:whisperkit",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:whisperkit",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:whisperkit-ModelArchitecture"
  },
  {
    "s": "mcro:whisperkit-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:whisperkit-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "WhisperKit is an on-device speech recognition framework for Apple Silicon: https://github.com/argmaxinc/WhisperKit"
  },
  {
    "s": "mcro:aiforeversbertlargenluru",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:aiforeversbertlargenluru",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:aiforeversbertlargenluru-ModelArchitecture"
  },
  {
    "s": "mcro:aiforeversbertlargenluru-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:aiforeversbertlargenluru-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT large model (uncased) for Sentence Embeddings in Russian language."
  },
  {
    "s": "mcro:aiforeversbertlargenluru",
    "p": "mcro:hasUseCase",
    "o": "mcro:aiforeversbertlargenluru-UseCase"
  },
  {
    "s": "mcro:aiforeversbertlargenluru-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:aiforeversbertlargenluru-UseCase",
    "p": "prov:hasTextValue",
    "o": "compute sentence embeddings"
  },
  {
    "s": "mcro:flairnerfrench",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:flairnerfrench",
    "p": "mcro:hasModelDetail",
    "o": "mcro:flairnerfrench-ModelDetail"
  },
  {
    "s": "mcro:flairnerfrench-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:flairnerfrench",
    "p": "mcro:hasUseCase",
    "o": "mcro:flairnerfrench-UseCase"
  },
  {
    "s": "mcro:flairnerfrench-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:flairnerfrench",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:flairnerfrench-PerformanceMetric"
  },
  {
    "s": "mcro:flairnerfrench-PerformanceMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:flairnerfrench-PerformanceMetric",
    "p": "prov:hasTextValue",
    "o": "90,61"
  },
  {
    "s": "mcro:flairnerfrench-PerformanceMetric",
    "p": "obo:MCRO_0000051",
    "o": "WikiNER"
  },
  {
    "s": "mcro:flairnerfrench",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:flairnerfrench-ModelArchitecture"
  },
  {
    "s": "mcro:flairnerfrench-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:flairnerfrench-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "LSTM-CRF"
  },
  {
    "s": "mcro:flairnerfrench-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Flair embeddings"
  },
  {
    "s": "mcro:flairnerfrench",
    "p": "mcro:hasCitation",
    "o": "mcro:flairnerfrench-Citation"
  },
  {
    "s": "mcro:flairnerfrench-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:flairnerfrench-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}"
  },
  {
    "s": "mcro:trlinternaltestingtinyLlamaForCausalLM32",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:trlinternaltestingtinyLlamaForCausalLM32",
    "p": "mcro:hasModelDetail",
    "o": "mcro:trlinternaltestingtinyLlamaForCausalLM32-ModelDetail"
  },
  {
    "s": "mcro:trlinternaltestingtinyLlamaForCausalLM32-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:trlinternaltestingtinyLlamaForCausalLM32-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library."
  },
  {
    "s": "mcro:SmolDocling256Mpreview",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:SmolDocling256Mpreview",
    "p": "mcro:hasCitation",
    "o": "mcro:SmolDocling256Mpreview-CitationInformationSection"
  },
  {
    "s": "mcro:SmolDocling256Mpreview-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:SmolDocling256Mpreview-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{nassar2025smoldoclingultracompactvisionlanguagemodel,\n      title={SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion}, \n      author={Ahmed Nassar and Andres Marafioti and Matteo Omenetti and Maksym Lysak and Nikolaos Livathinos and Christoph Auer and Lucas Morin and Rafael Teixeira de Lima and Yusik Kim and A. Said Gurbuz and Michele Dolfi and Miquel Farr\u00e9 and Peter W. J. Staar},\n      year={2025},\n      eprint={2503.11576},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2503.11576}, \n}"
  },
  {
    "s": "mcro:SmolDocling256Mpreview",
    "p": "mcro:hasLicense",
    "o": "mcro:SmolDocling256Mpreview-LicenseInformationSection"
  },
  {
    "s": "mcro:SmolDocling256Mpreview-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:SmolDocling256Mpreview-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:SmolDocling256Mpreview",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:SmolDocling256Mpreview-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:SmolDocling256Mpreview-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:SmolDocling256Mpreview-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)"
  },
  {
    "s": "mcro:SmolDocling256Mpreview",
    "p": "mcro:hasUseCase",
    "o": "mcro:SmolDocling256Mpreview-UseCaseInformationSection"
  },
  {
    "s": "mcro:SmolDocling256Mpreview-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:SmolDocling256Mpreview-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "multimodal Image-Text-to-Text model designed for efficient document conversion"
  },
  {
    "s": "mcro:llamaGuard38B",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llamaGuard38B",
    "p": "mcro:hasModelDetails",
    "o": "mcro:llamaGuard38B-ModelDetail"
  },
  {
    "s": "mcro:llamaGuard38B-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llamaGuard38B-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM \u2013 it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n\nBelow is a response classification example for Llama Guard 3.\n\n\n  \n  \n\n\nIn order to produce classifier scores, we look at the probability for the first token, and use that as the \u201cunsafe\u201d class probability. We can then apply score thresholding to make binary decisions."
  },
  {
    "s": "mcro:llamaGuard38B",
    "p": "mcro:hasUseCase",
    "o": "mcro:llamaGuard38B-UseCase"
  },
  {
    "s": "mcro:llamaGuard38B-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llamaGuard38B-UseCase",
    "p": "prov:hasTextValue",
    "o": "As outlined in the Llama 3 paper, Llama Guard 3 provides industry leading system-level safety performance and is recommended to be deployed along with Llama 3.1. Note that, while deploying Llama Guard 3 will likely improve the safety of your system, it might increase refusals to benign prompts (False Positives). Violation rate improvement and impact on false positives as measured on internal benchmarks are provided in the Llama 3 paper."
  },
  {
    "s": "mcro:llamaGuard38B",
    "p": "mcro:hasLimitation",
    "o": "mcro:llamaGuard38B-Limitation"
  },
  {
    "s": "mcro:llamaGuard38B-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:llamaGuard38B-Limitation",
    "p": "prov:hasTextValue",
    "o": "There are some limitations associated with Llama Guard 3. First, Llama Guard 3 itself is an LLM fine-tuned on Llama 3.1. Thus, its performance (e.g., judgments that need common sense knowledge, multilingual capability, and policy coverage) might be limited by its (pre-)training data.\n\nSome hazard categories may require factual, up-to-date knowledge to be evaluated (for example, S5: Defamation, S8: Intellectual Property, and S13: Elections) . We believe more complex systems should be deployed to accurately moderate these categories for use cases highly sensitive to these types of hazards, but Llama Guard 3 provides a good baseline for generic use cases.\n\nLastly, as an LLM, Llama Guard 3 may be susceptible to adversarial attacks or prompt injection attacks that could bypass or alter its intended use. Please feel free to [report](https://github.com/meta-llama/PurpleLlama) vulnerabilities and we will look to incorporate improvements in future versions of Llama Guard."
  },
  {
    "s": "mcro:llamaGuard38B",
    "p": "mcro:hasCitation",
    "o": "mcro:llamaGuard38B-Citation"
  },
  {
    "s": "mcro:llamaGuard38B-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:llamaGuard38B-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{dubey2024llama3herdmodels,\n  title =         {The Llama 3 Herd of Models},\n  author =        {Llama Team, AI @ Meta},\n  year =          {2024},\n  eprint =        {2407.21783},\n  archivePrefix = {arXiv},\n  primaryClass =  {cs.AI},\n  url =           {https://arxiv.org/abs/2407.21783}\n}"
  },
  {
    "s": "mcro:CryptoBERT",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:CryptoBERT",
    "p": "mcro:hasCitation",
    "o": "mcro:CryptoBERT-CitationInformationSection"
  },
  {
    "s": "mcro:CryptoBERT-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:CryptoBERT-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "For academic reference, cite the following paper: https://ieeexplore.ieee.org/document/10223689"
  },
  {
    "s": "mcro:CryptoBERT",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:CryptoBERT-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:CryptoBERT-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:CryptoBERT-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "CryptoBERT is a pre-trained NLP model to analyse the language and sentiments of cryptocurrency-related social media posts and messages. It was built by further training the [vinai's bertweet-base](https://huggingface.co/vinai/bertweet-base) language model on the cryptocurrency domain, using a corpus of over 3.2M unique cryptocurrency-related social media posts. (A research paper with more details will follow soon.)"
  },
  {
    "s": "mcro:CryptoBERT",
    "p": "mcro:hasDataset",
    "o": "mcro:CryptoBERT-DatasetInformationSection"
  },
  {
    "s": "mcro:CryptoBERT-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:CryptoBERT-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "CryptoBERT's sentiment classification head was fine-tuned on a balanced dataset of 2M labelled StockTwits posts, sampled from [ElKulako/stocktwits-crypto](https://huggingface.co/datasets/ElKulako/stocktwits-crypto)."
  },
  {
    "s": "mcro:CryptoBERT",
    "p": "mcro:hasUseCase",
    "o": "mcro:CryptoBERT-UseCaseInformationSection"
  },
  {
    "s": "mcro:CryptoBERT-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:CryptoBERT-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "CryptoBERT is a pre-trained NLP model to analyse the language and sentiments of cryptocurrency-related social media posts and messages."
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka",
    "p": "mcro:hasModelDetail",
    "o": "mcro:rbhatia46financialragmatryoshka-ModelDetail"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:rbhatia46financialragmatryoshka-License"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-License",
    "p": "prov:hasTextValue",
    "o": "apache-2.0"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:rbhatia46financialragmatryoshka-Architecture"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-Architecture",
    "p": "prov:hasTextValue",
    "o": "Sentence Transformer"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka",
    "p": "mcro:hasUseCase",
    "o": "mcro:rbhatia46financialragmatryoshka-UseCase"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-UseCase",
    "p": "prov:hasTextValue",
    "o": "financial use-cases"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka",
    "p": "mcro:hasCitation",
    "o": "mcro:rbhatia46financialragmatryoshka-Citation"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{kusupati2024matryoshka,\n    title={Matryoshka Representation Learning}, \n    author={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},\n    year={2024},\n    eprint={2205.13147},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n"
  },
  {
    "s": "mcro:rbhatia46financialragmatryoshka-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{henderson2017efficient,\n    title={Efficient Natural Language Response Suggestion for Smart Reply}, \n    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\n    year={2017},\n    eprint={1705.00652},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n"
  },
  {
    "s": "mcro:Salesforceblipvqabase",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Salesforceblipvqabase",
    "p": "mcro:hasCitation",
    "o": "mcro:Salesforceblipvqabase-Citation"
  },
  {
    "s": "mcro:Salesforceblipvqabase-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqabase-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}"
  },
  {
    "s": "mcro:Salesforceblipvqabase",
    "p": "mcro:hasUseCase",
    "o": "mcro:Salesforceblipvqabase-UseCase"
  },
  {
    "s": "mcro:Salesforceblipvqabase-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqabase-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use this model for conditional and un-conditional image captioning"
  },
  {
    "s": "mcro:Salesforceblipvqabase",
    "p": "mcro:hasConsideration",
    "o": "mcro:Salesforceblipvqabase-Consideration"
  },
  {
    "s": "mcro:Salesforceblipvqabase-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqabase-Consideration",
    "p": "prov:hasTextValue",
    "o": "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people\u2019s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP."
  },
  {
    "s": "mcro:Salesforceblipvqabase",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Salesforceblipvqabase-Architecture"
  },
  {
    "s": "mcro:Salesforceblipvqabase-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqabase-Architecture",
    "p": "prov:hasTextValue",
    "o": "BLIP trained on visual question answering- base architecture (with ViT base backbone)."
  },
  {
    "s": "mcro:llavamodelcard",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llavamodelcard",
    "p": "mcro:hasModelDetail",
    "o": "mcro:llavamodelcard-ModelDetail"
  },
  {
    "s": "mcro:llavamodelcard-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:llavamodelcard-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.\nIt is an auto-regressive language model, based on the transformer architecture."
  },
  {
    "s": "mcro:llavamodelcard-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "LLaVA-v1.5-7B was trained in September 2023."
  },
  {
    "s": "mcro:llavamodelcard-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "https://llava-vl.github.io/"
  },
  {
    "s": "mcro:llavamodelcard",
    "p": "mcro:hasLicense",
    "o": "mcro:llavamodelcard-License"
  },
  {
    "s": "mcro:llavamodelcard-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llavamodelcard-License",
    "p": "prov:hasTextValue",
    "o": "Llama 2 is licensed under the LLAMA 2 Community License,\nCopyright (c) Meta Platforms, Inc. All Rights Reserved."
  },
  {
    "s": "mcro:llavamodelcard-License",
    "p": "prov:hasTextValue",
    "o": "https://github.com/haotian-liu/LLaVA/issues"
  },
  {
    "s": "mcro:llavamodelcard",
    "p": "mcro:hasUseCase",
    "o": "mcro:llavamodelcard-UseCase"
  },
  {
    "s": "mcro:llavamodelcard-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llavamodelcard-UseCase",
    "p": "prov:hasTextValue",
    "o": "The primary use of LLaVA is research on large multimodal models and chatbots."
  },
  {
    "s": "mcro:llavamodelcard-UseCase",
    "p": "prov:hasTextValue",
    "o": "The primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence."
  },
  {
    "s": "mcro:llavamodelcard",
    "p": "mcro:hasTrainingData",
    "o": "mcro:llavamodelcard-TrainingData"
  },
  {
    "s": "mcro:llavamodelcard-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:llavamodelcard-TrainingData",
    "p": "prov:hasTextValue",
    "o": "- 558K filtered image-text pairs from LAION/CC/SBU, captioned by BLIP.\n- 158K GPT-generated multimodal instruction-following data.\n- 450K academic-task-oriented VQA data mixture.\n- 40K ShareGPT data."
  },
  {
    "s": "mcro:llavamodelcard",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:llavamodelcard-EvaluationData"
  },
  {
    "s": "mcro:llavamodelcard-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:llavamodelcard-EvaluationData",
    "p": "prov:hasTextValue",
    "o": "A collection of 12 benchmarks, including 5 academic VQA benchmarks and 7 recent benchmarks specifically proposed for instruction-following LMMs."
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs",
    "p": "mcro:hasLicense",
    "o": "mcro:Snowflakesnowflakearcticembedxs-License"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-License",
    "p": "prov:hasTextValue",
    "o": "Arctic is licensed under the [Apache-2](https://www.apache.org/licenses/LICENSE-2.0). The released models can be used for commercial purposes free of charge."
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Snowflakesnowflakearcticembedxs-ModelDetail"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Snowflakesnowflakearcticembedxs-ModelArchitecture"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance."
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:Snowflakesnowflakearcticembedxs-IntendedUseCase"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The `snowflake-arctic-embedding` models achieve **state-of-the-art performance on the MTEB/BEIR leaderboard** for each of their size variants."
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs",
    "p": "mcro:hasDataset",
    "o": "mcro:Snowflakesnowflakearcticembedxs-Dataset"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Snowflakesnowflakearcticembedxs-Dataset",
    "p": "prov:hasTextValue",
    "o": "The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance."
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4",
    "p": "mcro:hasCitation",
    "o": "mcro:QwenQwen25Coder7BInstructGPTQInt4-Citation"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:QwenQwen25Coder7BInstructGPTQInt4-ModelArchitecture"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4",
    "p": "mcro:hasUseCase",
    "o": "mcro:QwenQwen25Coder7BInstructGPTQInt4-UseCase"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:QwenQwen25Coder7BInstructGPTQInt4-UseCase",
    "p": "prov:hasTextValue",
    "o": "Code Agents"
  },
  {
    "s": "mcro:canary1bflash",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:canary1bflash",
    "p": "mcro:hasLicense",
    "o": "mcro:canary1bflash-LicenseInformationSection"
  },
  {
    "s": "mcro:canary1bflash-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:canary1bflash-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "canary-1b-flash is released under the CC-BY-4.0 license. By using this model, you are agreeing to the terms and conditions of the license."
  },
  {
    "s": "mcro:canary1bflash",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:canary1bflash-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:canary1bflash-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:canary1bflash-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Canary is an encoder-decoder model with FastConformer [3] Encoder and Transformer Decoder [4]. With audio features extracted from the encoder, task tokens such as <target language>, <task>, <toggle timestamps> and <toggle PnC> are fed into the Transformer Decoder to trigger the text generation process. Canary uses a concatenated tokenizer [5] from individual SentencePiece [6] tokenizers of each language, which makes it easy to scale up to more languages. The canary-1b-flash model has 32 encoder layers and 4 decoder layers, leading to a total of 883M parameters. For more details about the architecture, please refer to [1]."
  },
  {
    "s": "mcro:canary1bflash",
    "p": "mcro:hasDataset",
    "o": "mcro:canary1bflash-DatasetInformationSection"
  },
  {
    "s": "mcro:canary1bflash-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:canary1bflash-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "The canary-1b-flash model is trained on a total of 85K hrs of speech data. It consists of 31K hrs of public data, 20K hrs collected by [Suno](https://suno.ai/), and 34K hrs of in-house data. \nThe datasets below include conversations, videos from the web and audiobook recordings.\n\n**Data Collection Method:**\n* Human <br>\n\n**Labeling Method:**\n* Hybrid: Human, Automated <br>\n\nThe constituents of public data are as follows. \n\n#### English (25.5k hours)\n- Librispeech 960 hours\n- Fisher Corpus\n- Switchboard-1 Dataset\n- WSJ-0 and WSJ-1\n- National Speech Corpus (Part 1, Part 6)\n- VCTK\n- VoxPopuli (EN)\n- Europarl-ASR (EN)\n- Multilingual Librispeech (MLS EN) - 2,000 hour subset\n- Mozilla Common Voice (v7.0)\n- People's Speech - 12,000 hour subset\n- Mozilla Common Voice (v11.0)  - 1,474 hour subset\n\n#### German (2.5k hours)\n- Mozilla Common Voice (v12.0)  - 800 hour subset\n- Multilingual Librispeech (MLS DE) - 1,500 hour subset\n- VoxPopuli (DE) - 200 hr subset\n\n#### Spanish (1.4k hours)\n- Mozilla Common Voice (v12.0)  - 395 hour subset\n- Multilingual Librispeech (MLS ES) - 780 hour subset\n- VoxPopuli (ES) - 108 hour subset\n- Fisher  - 141 hour subset\n\n#### French (1.8k hours)\n- Mozilla Common Voice (v12.0)  - 708 hour subset\n- Multilingual Librispeech (MLS FR) - 926 hour subset\n- VoxPopuli (FR) - 165 hour subset"
  },
  {
    "s": "mcro:canary1bflash",
    "p": "mcro:hasCitation",
    "o": "mcro:canary1bflash-CitationInformationSection"
  },
  {
    "s": "mcro:canary1bflash-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:canary1bflash-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "[1] [Training and Inference Efficiency of Encoder-Decoder Speech Models](https://arxiv.org/abs/2503.05931)\n\n[2] [Less is More: Accurate Speech Recognition & Translation without Web-Scale Data](https://www.isca-archive.org/interspeech_2024/puvvada24_interspeech.pdf)\n\n[3] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389701)\n\n[4] [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n\n[5] [Unified Model for Code-Switching Speech Recognition and Language Identification Based on Concatenated Tokenizer](https://aclanthology.org/2023.calcs-1.7.pdf)\n\n[6] [Google Sentencepiece Tokenizer](https://github.com/google/sentencepiece)\n\n[7] [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo)\n\n[8] [EMMeTT: Efficient Multimodal Machine Translation Training](https://arxiv.org/abs/2409.13523)\n\n[9] [Towards Measuring Fairness in AI: the Casual Conversations Dataset](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9634168)"
  },
  {
    "s": "mcro:canary1bflash",
    "p": "mcro:hasUseCase",
    "o": "mcro:canary1bflash-UseCaseInformationSection"
  },
  {
    "s": "mcro:canary1bflash-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:canary1bflash-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": null
  },
  {
    "s": "mcro:atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "A tiny version of https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium",
    "p": "mcro:hasModelDetail",
    "o": "mcro:stabilityaistablediffusion35medium-ModelDetail"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:stabilityaistablediffusion35medium-License"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-License",
    "p": "prov:hasTextValue",
    "o": "Free for research, non-commercial, and commercial use for organizations or individuals with less than $1M in total annual revenue. More details can be found in the Community License Agreement. Read more at https://stability.ai/license.\n- For individuals and organizations with annual revenue above $1M: please contact us to get an Enterprise License."
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:stabilityaistablediffusion35medium-ModelArchitecture"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "MMDiT-X text-to-image generative model"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium",
    "p": "mcro:hasUseCase",
    "o": "mcro:stabilityaistablediffusion35medium-UseCase"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-UseCase",
    "p": "prov:hasTextValue",
    "o": "Intended uses include the following:\n* Generation of artworks and use in design and other artistic processes.\n* Applications in educational or creative tools.\n* Research on generative models, including understanding the limitations of generative models.\n\nAll uses of the model must be in accordance with our Acceptable Use Policy."
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium",
    "p": "mcro:hasConsideration",
    "o": "mcro:stabilityaistablediffusion35medium-Consideration"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-Consideration",
    "p": "mcro:hasLimitation",
    "o": "mcro:stabilityaistablediffusion35medium-Limitation"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-Limitation",
    "p": "prov:hasTextValue",
    "o": "While this model can handle long prompts, you may observe artifacts on the edge of generations when T5 tokens go over 256. Pay attention to the token limits when using this model in your workflow, and shortern prompts if artifacts becomes too obvious.\n- The medium model has a different training data distribution than the large model, so it may not respond to the same prompt similarly.\n- We recommend sampling with **Skip Layer Guidance** for better structure and anatomy coherency."
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium",
    "p": "mcro:hasOutofScopeUseCase",
    "o": "mcro:stabilityaistablediffusion35medium-OutofScopeUseCase"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-OutofScopeUseCase",
    "p": "rdf:type",
    "o": "mcro:OutofScopeUseCaseSectionInformation"
  },
  {
    "s": "mcro:stabilityaistablediffusion35medium-OutofScopeUseCase",
    "p": "prov:hasTextValue",
    "o": "The model was not trained to be factual or true representations of people or events. As such, using the model to generate such content is out-of-scope of the abilities of this model."
  },
  {
    "s": "mcro:kobert",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:kobert",
    "p": "mcro:hasReference",
    "o": "mcro:kobert-ReferenceInformationSection"
  },
  {
    "s": "mcro:kobert-ReferenceInformationSection",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:kobert-ReferenceInformationSection",
    "p": "prov:hasTextValue",
    "o": "- https://github.com/SKTBrain/KoBERT"
  },
  {
    "s": "mcro:kobert",
    "p": "mcro:hasUseCase",
    "o": "mcro:kobert-UseCaseInformationSection"
  },
  {
    "s": "mcro:kobert-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:kobert-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "If you want to import KoBERT tokenizer with `AutoTokenizer`, you should give `trust_remote_code=True`."
  },
  {
    "s": "mcro:moondream2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:moondream2",
    "p": "mcro:hasUseCase",
    "o": "mcro:moondream2-UseCase"
  },
  {
    "s": "mcro:moondream2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:moondream2-UseCase",
    "p": "prov:hasTextValue",
    "o": "Moondream is a small vision language model designed to run efficiently everywhere."
  },
  {
    "s": "mcro:moondream2",
    "p": "mcro:hasVersion",
    "o": "mcro:moondream2-Version"
  },
  {
    "s": "mcro:moondream2-Version",
    "p": "rdf:type",
    "o": "mcro:VersionInformationSection"
  },
  {
    "s": "mcro:moondream2-Version",
    "p": "prov:hasTextValue",
    "o": "This repository contains the latest (**2025-04-14**) release of Moondream, as well as [historical releases](https://huggingface.co/vikhyatk/moondream2/blob/main/versions.txt). The model is updated frequently, so we recommend specifying a revision as shown below if you're using it in a production application."
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:microsoftPhi3mini128kinstruct-ModelDetail"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:microsoftPhi3mini128kinstruct-License"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-License",
    "p": "prov:hasTextValue",
    "o": "The model is licensed under the MIT license."
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftPhi3mini128kinstruct-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Phi-3 Mini-128K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines."
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct",
    "p": "mcro:hasDataset",
    "o": "mcro:microsoftPhi3mini128kinstruct-Dataset"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-Dataset",
    "p": "prov:hasTextValue",
    "o": "Our training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of 1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) Newly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness."
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftPhi3mini128kinstruct-UseCase"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi3mini128kinstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended for commercial and research use in English. The model provides uses for applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Architecture"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Architecture",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Dataset"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation1"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation1",
    "p": "prov:hasTextValue",
    "o": "@article{Woo2023ConvNeXtV2,\n  title={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},\n  author={Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon and Saining Xie},\n  year={2023},\n  journal={arXiv preprint arXiv:2301.00808},\n}"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation2"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-UseCase"
  },
  {
    "s": "mcro:convnextv2_nano.fcmae_ft_in22kin1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilwhisperdistillargev3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:distilwhisperdistillargev3",
    "p": "mcro:hasCitation",
    "o": "mcro:distilwhisperdistillargev3-Citation"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{gandhi2023distilwhisper,\n      title={Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling}, \n      author={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},\n      year={2023},\n      eprint={2311.00430},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:distilwhisperdistillargev3",
    "p": "mcro:hasLicense",
    "o": "mcro:distilwhisperdistillargev3-License"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-License",
    "p": "prov:hasTextValue",
    "o": "MIT license"
  },
  {
    "s": "mcro:distilwhisperdistillargev3",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:distilwhisperdistillargev3-Arch"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-Arch",
    "p": "prov:hasTextValue",
    "o": "encoder-decoder architecture"
  },
  {
    "s": "mcro:distilwhisperdistillargev3",
    "p": "mcro:hasDataset",
    "o": "mcro:distilwhisperdistillargev3-Dataset"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-Dataset",
    "p": "prov:hasTextValue",
    "o": "22,000 hours of audio data"
  },
  {
    "s": "mcro:distilwhisperdistillargev3",
    "p": "mcro:hasUseCase",
    "o": "mcro:distilwhisperdistillargev3-UseCase"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:distilwhisperdistillargev3-UseCase",
    "p": "prov:hasTextValue",
    "o": "drop-in replacement for Whisper large-v3 on English speech recognition"
  },
  {
    "s": "mcro:text-detection-model-for-surya",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:text-detection-model-for-surya",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:text-detection-model-for-surya-ModelArchitecture"
  },
  {
    "s": "mcro:text-detection-model-for-surya-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:text-detection-model-for-surya-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Text detection model"
  },
  {
    "s": "mcro:XenovaallMiniLML6v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:XenovaallMiniLML6v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:XenovaallMiniLML6v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:XenovaallMiniLML6v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:XenovaallMiniLML6v2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:XenovaallMiniLML6v2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:XenovaallMiniLML6v2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b",
    "p": "mcro:hasLicense",
    "o": "mcro:HuggingFaceM4idefics28b-License"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:HuggingFaceM4idefics28b-ModelArchitecture"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Multi-modal model (image+text)"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b",
    "p": "mcro:hasCitation",
    "o": "mcro:HuggingFaceM4idefics28b-Citation"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{laurencon2023obelics,\n      title={OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},\n      author={Hugo Lauren\u00e7on and Lucile Saulnier and L\u00e9o Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M. Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},\n      year={2023},\n      eprint={2306.16527},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{lauren\u00e7on2024matters,\n      title={What matters when building vision-language models?},\n      author={Hugo Lauren\u00e7on and L\u00e9o Tronchon and Matthieu Cord and Victor Sanh},\n      year={2024},\n      eprint={2405.02246},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b",
    "p": "mcro:hasDataset",
    "o": "mcro:HuggingFaceM4idefics28b-Dataset"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-Dataset",
    "p": "prov:hasTextValue",
    "o": "OBELICS"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b",
    "p": "mcro:hasUseCase",
    "o": "mcro:HuggingFaceM4idefics28b-UseCase"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:HuggingFaceM4idefics28b-UseCase",
    "p": "prov:hasTextValue",
    "o": "`idefics2-8b-base` and `idefics2-8b` can be used to perform inference on multimodal (image + text) tasks in which the input is composed of a text query along with one (or multiple) image(s). Text and images can be arbitrarily interleaved. That includes image captioning, visual question answering, etc."
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b",
    "p": "mcro:hasModelDetail",
    "o": "mcro:scb10xtyphoon21gemma312b-ModelDetail"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:scb10xtyphoon21gemma312b-License"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-License",
    "p": "prov:hasTextValue",
    "o": "Gemma License"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:scb10xtyphoon21gemma312b-ModelArchitecture"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "A 12B instruct decoder-only model based on Gemma3 architecture"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:scb10xtyphoon21gemma312b-IntendedUseCase"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-IntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "instructional model"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:scb10xtyphoon21gemma312b-Citation"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:scb10xtyphoon21gemma312b-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{typhoon2,\n      title={Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models}, \n      author={Kunat Pipatanakul and Potsawee Manakul and Natapong Nitarach and Warit Sirichotedumrong and Surapon Nonesung and Teetouch Jaknamon and Parinthapat Pengpun and Pittawat Taveekitworachai and Adisai Na-Thalang and Sittipong Sripaisarnmongkol and Krisanapong Jirayoot and Kasima Tharnpipitchai},\n      year={2024},\n      eprint={2412.13702},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.13702}, \n}"
  },
  {
    "s": "mcro:owlvitbasepatch32",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:owlvitbasepatch32",
    "p": "mcro:hasModelDetail",
    "o": "mcro:owlvitbasepatch32-ModelDetail"
  },
  {
    "s": "mcro:owlvitbasepatch32-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:owlvitbasepatch32-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:owlvitbasepatch32-Citation"
  },
  {
    "s": "mcro:owlvitbasepatch32-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:owlvitbasepatch32-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:owlvitbasepatch32-ModelArchitecture"
  },
  {
    "s": "mcro:owlvitbasepatch32-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:owlvitbasepatch32",
    "p": "mcro:hasDataset",
    "o": "mcro:owlvitbasepatch32-Dataset"
  },
  {
    "s": "mcro:owlvitbasepatch32-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:owlvitbasepatch32",
    "p": "mcro:hasUseCase",
    "o": "mcro:owlvitbasepatch32-UseCase"
  },
  {
    "s": "mcro:owlvitbasepatch32-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:owlvitbasepatch32-UseCase",
    "p": "mcro:hasPrimaryIntendedUseCase",
    "o": "mcro:owlvitbasepatch32-PrimaryIntendedUseCase"
  },
  {
    "s": "mcro:owlvitbasepatch32-PrimaryIntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:owlvitbasepatch32-PrimaryIntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training."
  },
  {
    "s": "mcro:owlvitbasepatch32-UseCase",
    "p": "mcro:hasUser",
    "o": "mcro:owlvitbasepatch32-User"
  },
  {
    "s": "mcro:owlvitbasepatch32-User",
    "p": "rdf:type",
    "o": "mcro:UserInformationSection"
  },
  {
    "s": "mcro:owlvitbasepatch32-User",
    "p": "mcro:hasPrimaryIntendedUser",
    "o": "mcro:owlvitbasepatch32-PrimaryIntendedUser"
  },
  {
    "s": "mcro:owlvitbasepatch32-PrimaryIntendedUser",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUserInformationSection"
  },
  {
    "s": "mcro:owlvitbasepatch32-PrimaryIntendedUser",
    "p": "prov:hasTextValue",
    "o": "The primary intended users of these models are AI researchers."
  },
  {
    "s": "mcro:owlvitbasepatch32-Dataset",
    "p": "mcro:hasTrainingData",
    "o": "mcro:owlvitbasepatch32-TrainingData"
  },
  {
    "s": "mcro:owlvitbasepatch32-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:owlvitbasepatch32-TrainingData",
    "p": "prov:hasTextValue",
    "o": "The CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)."
  },
  {
    "s": "mcro:owlvitbasepatch32-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{minderer2022simple,\n  title={Simple Open-Vocabulary Object Detection with Vision Transformers},\n  author={Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby},\n  journal={arXiv preprint arXiv:2205.06230},\n  year={2022},\n}"
  },
  {
    "s": "mcro:owlvitbasepatch32-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The model uses a CLIP backbone with a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective."
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53hungarian-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:opusmtruen",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:opusmtruen",
    "p": "mcro:hasModelDetail",
    "o": "mcro:opusmtruen-ModelDetail"
  },
  {
    "s": "mcro:opusmtruen-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:opusmtruen-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:opusmtruen-License"
  },
  {
    "s": "mcro:opusmtruen-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:opusmtruen-License",
    "p": "prov:hasTextValue",
    "o": "CC-BY-4.0"
  },
  {
    "s": "mcro:opusmtruen-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:opusmtruen-Citation"
  },
  {
    "s": "mcro:opusmtruen-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:opusmtruen",
    "p": "mcro:hasArchitecture",
    "o": "mcro:opusmtruen-Architecture"
  },
  {
    "s": "mcro:opusmtruen-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:opusmtruen-Architecture",
    "p": "prov:hasTextValue",
    "o": "Transformer-align"
  },
  {
    "s": "mcro:opusmtruen",
    "p": "mcro:hasUseCase",
    "o": "mcro:opusmtruen-UseCase"
  },
  {
    "s": "mcro:opusmtruen-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:opusmtruen-UseCase",
    "p": "prov:hasTextValue",
    "o": "This model can be used for translation and text-to-text generation."
  },
  {
    "s": "mcro:opusmtruen",
    "p": "mcro:hasTrainingData",
    "o": "mcro:opusmtruen-TrainingData"
  },
  {
    "s": "mcro:opusmtruen-TrainingData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:opusmtruen-TrainingData",
    "p": "prov:hasTextValue",
    "o": "* Dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)"
  },
  {
    "s": "mcro:opusmtruen",
    "p": "mcro:hasEvaluationData",
    "o": "mcro:opusmtruen-EvaluationData"
  },
  {
    "s": "mcro:opusmtruen-EvaluationData",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mhebrew",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wav2vec2xlsr300mhebrew",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wav2vec2xlsr300mhebrew-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mhebrew",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:wav2vec2xlsr300mhebrew-UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mhebrew",
    "p": "mcro:hasTrainingData",
    "o": "mcro:wav2vec2xlsr300mhebrew-TrainingDataInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mhebrew-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mhebrew-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2xlsr300mhebrew-TrainingDataInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:surya",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:surya",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:surya-ModelArchitecture"
  },
  {
    "s": "mcro:surya-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:surya-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "OCR model for [surya](https://www.github.com/VikParuchuri/surya)"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english",
    "p": "mcro:hasUseCase",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53english-UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english",
    "p": "mcro:hasModelDetail",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelDetail"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53english-Dataset"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelArchitecture"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:jonatasgrosmanwav2vec2largexlsr53english-Citation"
  },
  {
    "s": "mcro:jonatasgrosmanwav2vec2largexlsr53english-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt",
    "p": "mcro:hasCitation",
    "o": "mcro:mbartlarge50manytomanymmt-CitationInformationSection"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{tang2020multilingual,\n    title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},\n    author={Yuqing Tang and Chau Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},\n    year={2020},\n    eprint={2008.00401},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mbartlarge50manytomanymmt-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "mBART-50 many to many multilingual machine translation"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt",
    "p": "mcro:hasUseCase",
    "o": "mcro:mbartlarge50manytomanymmt-UseCaseInformationSection"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mbartlarge50manytomanymmt-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "multilingual machine translation"
  },
  {
    "s": "mcro:graphcodebert",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:graphcodebert",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:graphcodebert-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:graphcodebert-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:graphcodebert-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512."
  },
  {
    "s": "mcro:graphcodebert",
    "p": "mcro:hasDataset",
    "o": "mcro:graphcodebert-DatasetInformationSection"
  },
  {
    "s": "mcro:graphcodebert-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:graphcodebert-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages."
  },
  {
    "s": "mcro:graphcodebert",
    "p": "mcro:hasCitation",
    "o": "mcro:graphcodebert-CitationInformationSection"
  },
  {
    "s": "mcro:graphcodebert-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:graphcodebert-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "More details can be found in the [paper](https://arxiv.org/abs/2009.08366) by Guo et. al."
  },
  {
    "s": "mcro:TRELLISImageLarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:TRELLISImageLarge",
    "p": "mcro:hasCitation",
    "o": "mcro:TRELLISImageLarge-CitationInformationSection"
  },
  {
    "s": "mcro:TRELLISImageLarge-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:TRELLISImageLarge-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "Structured 3D Latents for Scalable and Versatile 3D Generation"
  },
  {
    "s": "mcro:TRELLISImageLarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:TRELLISImageLarge-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:TRELLISImageLarge-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:TRELLISImageLarge-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "large 3D genetive model"
  },
  {
    "s": "mcro:TRELLISImageLarge-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "TRELLIS"
  },
  {
    "s": "mcro:Dreamshaper8inpainting",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Dreamshaper8inpainting",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Dreamshaper8inpainting-ModelDetail"
  },
  {
    "s": "mcro:Dreamshaper8inpainting-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Dreamshaper8inpainting-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Dreamshaper8inpainting-ModelArchitecture"
  },
  {
    "s": "mcro:Dreamshaper8inpainting-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Dreamshaper8inpainting-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Stable Diffusion Inpainting model"
  },
  {
    "s": "mcro:Dreamshaper8inpainting-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:Dreamshaper8inpainting-Dataset"
  },
  {
    "s": "mcro:Dreamshaper8inpainting-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:Dreamshaper8inpainting-Dataset",
    "p": "prov:hasTextValue",
    "o": "runwayml/stable-diffusion-inpainting"
  },
  {
    "s": "mcro:Dreamshaper8inpainting",
    "p": "mcro:hasUseCase",
    "o": "mcro:Dreamshaper8inpainting-UseCase"
  },
  {
    "s": "mcro:Dreamshaper8inpainting-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:tiiuaefalconrw1b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:tiiuaefalconrw1b",
    "p": "mcro:hasLicense",
    "o": "mcro:tiiuaefalconrw1b-License"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:tiiuaefalconrw1b",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:tiiuaefalconrw1b-ModelArchitecture"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Causal decoder-only"
  },
  {
    "s": "mcro:tiiuaefalconrw1b",
    "p": "mcro:hasCitation",
    "o": "mcro:tiiuaefalconrw1b-Citation"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-Citation",
    "p": "prov:hasTextValue",
    "o": "Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay"
  },
  {
    "s": "mcro:tiiuaefalconrw1b",
    "p": "mcro:hasDataset",
    "o": "mcro:tiiuaefalconrw1b-Dataset"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-Dataset",
    "p": "prov:hasTextValue",
    "o": "RefinedWeb"
  },
  {
    "s": "mcro:tiiuaefalconrw1b",
    "p": "mcro:hasUseCase",
    "o": "mcro:tiiuaefalconrw1b-UseCase"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:tiiuaefalconrw1b-UseCase",
    "p": "prov:hasTextValue",
    "o": "Research on large language models, specifically the influence of adequately filtered and deduplicated web data on the properties of large language models (fairness, safety, limitations, capabilities, etc.)."
  },
  {
    "s": "mcro:alakxendermmsttsdivfinetunedmdm01",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:alakxendermmsttsdivfinetunedmdm01",
    "p": "mcro:hasModelDetail",
    "o": "mcro:alakxendermmsttsdivfinetunedmdm01-ModelDetail"
  },
  {
    "s": "mcro:alakxendermmsttsdivfinetunedmdm01-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:alakxendermmsttsdivfinetunedmdm01-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:alakxendermmsttsdivfinetunedmdm01-ModelArchitecture"
  },
  {
    "s": "mcro:alakxendermmsttsdivfinetunedmdm01-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:alakxendermmsttsdivfinetunedmdm01-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "MMS-TTS (VITS)"
  },
  {
    "s": "mcro:alakxendermmsttsdivfinetunedmdm01-ModelDetail",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:alakxendermmsttsdivfinetunedmdm01-IntendedUseCase"
  },
  {
    "s": "mcro:alakxendermmsttsdivfinetunedmdm01-IntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:BAAIbgesmallen",
    "p": "mcro:hasCitation",
    "o": "mcro:BAAIbgesmallen-CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen",
    "p": "mcro:hasLicense",
    "o": "mcro:BAAIbgesmallen-LicenseInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:BAAIbgesmallen-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen",
    "p": "mcro:hasDataset",
    "o": "mcro:BAAIbgesmallen-DatasetInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen",
    "p": "mcro:hasUseCase",
    "o": "mcro:BAAIbgesmallen-UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:BAAIbgesmallen-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:BAAIbgesmallen-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Transformer"
  },
  {
    "s": "mcro:BAAIbgesmallen-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "MTEB, C-MTEB"
  },
  {
    "s": "mcro:BAAIbgesmallen-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:BAAIbgesmallen-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Retrieval-augmented LLMs"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit",
    "p": "mcro:hasModelDetail",
    "o": "mcro:mlxcommunitygemma312bitqat4bit-ModelDetail"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mlxcommunitygemma312bitqat4bit-ModelArchitecture"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:mlxcommunitygemma312bitqat4bit-Citation"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:mlxcommunitygemma312bitqat4bit-License"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit",
    "p": "mcro:hasUseCase",
    "o": "mcro:mlxcommunitygemma312bitqat4bit-UseCase"
  },
  {
    "s": "mcro:mlxcommunitygemma312bitqat4bit-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:microsoftPhi35miniinstruct-ModelDetail"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:microsoftPhi35miniinstruct-UseCase"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features."
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct",
    "p": "mcro:hasModelParameter",
    "o": "mcro:microsoftPhi35miniinstruct-ModelParameter"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-ModelParameter",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-ModelParameter",
    "p": "mcro:hasDataset",
    "o": "mcro:microsoftPhi35miniinstruct-Dataset"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-ModelParameter",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:microsoftPhi35miniinstruct-ModelArchitecture"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "**Architecture:** Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini."
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct",
    "p": "mcro:hasLicense",
    "o": "mcro:microsoftPhi35miniinstruct-License"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:microsoftPhi35miniinstruct-License",
    "p": "prov:hasTextValue",
    "o": "The model is licensed under the [MIT license](./LICENSE)."
  },
  {
    "s": "mcro:Llama160m",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Llama160m",
    "p": "mcro:hasCitation",
    "o": "mcro:Llama160m-CitationInformationSection"
  },
  {
    "s": "mcro:Llama160m-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Llama160m-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{miao2023specinfer,\n      title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification}, \n      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},\n      year={2023},\n      eprint={2305.09781},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:Llama160m",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Llama160m-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Llama160m-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Llama160m-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "This is a LLaMA-like model with only 160M parameters trained on Wikipedia and part of the C4-en and C4-realnewslike datasets."
  },
  {
    "s": "mcro:Llama160m",
    "p": "mcro:hasUseCase",
    "o": "mcro:Llama160m-UseCaseInformationSection"
  },
  {
    "s": "mcro:Llama160m-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Llama160m-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is mainly developed as a base Small Speculative Model in the [SpecInfer](https://arxiv.org/abs/2305.09781) paper."
  },
  {
    "s": "mcro:emimodel",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:emimodel",
    "p": "mcro:hasLicense",
    "o": "mcro:emimodel-LicenseInformationSection"
  },
  {
    "s": "mcro:emimodel-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:emimodel-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "CreativeML Open RAIL++-M License"
  },
  {
    "s": "mcro:emimodel",
    "p": "mcro:hasModelDetail",
    "o": "mcro:emimodel-ModelDetailSection"
  },
  {
    "s": "mcro:emimodel-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:emimodel-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:emimodel-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:emimodel-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:emimodel-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Latent Diffusion Model"
  },
  {
    "s": "mcro:emimodel-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:emimodel-CitationInformationSection"
  },
  {
    "s": "mcro:emimodel-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:emimodel-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{podell2023sdxl,\n      title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis}, \n      author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas M\u00fcller and Joe Penna and Robin Rombach},\n      year={2023},\n      eprint={2307.01952},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:emimodel",
    "p": "mcro:hasUseCase",
    "o": "mcro:emimodel-UseCaseInformationSection"
  },
  {
    "s": "mcro:emimodel-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:emimodel-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "\u30a4\u30e9\u30b9\u30c8\u3084\u6f2b\u753b\u3001\u30a2\u30cb\u30e1\u306e\u4f5c\u753b\u88dc\u52a9"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llavahfllavav16mistral7bhf-ModelArchitecture"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "LLaVa-NeXT, leveraging [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) as LLM"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf",
    "p": "mcro:hasCitation",
    "o": "mcro:llavahfllavav16mistral7bhf-Citation"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf-Citation",
    "p": "prov:hasTextValue",
    "o": "The LLaVA-NeXT model was proposed in [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://llava-vl.github.io/blog/2024-01-30-llava-next/) by Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee. LLaVa-NeXT (also called LLaVa-1.6) improves upon [LLaVa-1.5](https://huggingface.co/transformers/main/model_doc/llava.html) by increasing the input image resolution and training on an improved visual instruction tuning dataset to improve OCR and common sense reasoning."
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf",
    "p": "mcro:hasUseCase",
    "o": "mcro:llavahfllavav16mistral7bhf-UseCase"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for tasks like image captioning, visual question answering, multimodal chatbot use cases. See the [model hub](https://huggingface.co/models?search=llava-hf) to look for\nother versions on a task that interests you."
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf",
    "p": "mcro:hasCitation",
    "o": "mcro:llavahfllavav16mistral7bhf-Citation2"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:llavahfllavav16mistral7bhf-Citation2",
    "p": "prov:hasTextValue",
    "o": "@misc{liu2023improved,\n      title={Improved Baselines with Visual Instruction Tuning}, \n      author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},\n      year={2023},\n      eprint={2310.03744},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:lmmslabLLaVAVideo7BQwen2-ModelDetail"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2",
    "p": "mcro:hasUseCase",
    "o": "mcro:lmmslabLLaVAVideo7BQwen2-UseCase"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2",
    "p": "mcro:hasLimitation",
    "o": "mcro:lmmslabLLaVAVideo7BQwen2-Limitation"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2",
    "p": "mcro:hasTrainingData",
    "o": "mcro:lmmslabLLaVAVideo7BQwen2-TrainingData"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-TrainingData",
    "p": "rdf:type",
    "o": "mcro:ModelParameterSection"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2",
    "p": "mcro:hasLicense",
    "o": "mcro:lmmslabLLaVAVideo7BQwen2-License"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2",
    "p": "mcro:hasCitation",
    "o": "mcro:lmmslabLLaVAVideo7BQwen2-Citation"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "The LLaVA-Video models are 7/72B parameter models trained on LLaVA-Video-178K and LLaVA-OneVision Dataset, based on Qwen2 language model with a context window of 32K tokens.\n\nThis model support at most 64 frames."
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model was trained on LLaVA-Video-178K and LLaVA-OneVision Dataset, having the ability to interact with images, multi-image and videos, but specific to videos."
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-TrainingData",
    "p": "prov:hasTextValue",
    "o": "- **Architecture:** SO400M + Qwen2\n- **Initialized Model:** lmms-lab/llava-onevision-qwen2-7b-si\n- **Data:** A mixture of 1.6M single-image/multi-image/video data, 1 epoch, full model\n- **Precision:** bfloat16"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-License",
    "p": "prov:hasTextValue",
    "o": null
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{zhang2024videoinstructiontuningsynthetic,\n    title={Video Instruction Tuning With Synthetic Data}, \n    author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},\n    year={2024},\n    eprint={2410.02713},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV},\n    url={https://arxiv.org/abs/2410.02713}, \n}"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:lmmslabLLaVAVideo7BQwen2-Architecture"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:lmmslabLLaVAVideo7BQwen2-Architecture",
    "p": "prov:hasTextValue",
    "o": "SO400M + Qwen2"
  },
  {
    "s": "mcro:mistralaiMistral7Bv01",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mistralaiMistral7Bv01",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mistralaiMistral7Bv01-ModelArchitecture"
  },
  {
    "s": "mcro:mistralaiMistral7Bv01-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7Bv01-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Mistral-7B-v0.1 is a transformer model, with the following architecture choices:\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer"
  },
  {
    "s": "mcro:mistralaiMistral7Bv01",
    "p": "mcro:hasConsideration",
    "o": "mcro:mistralaiMistral7Bv01-Consideration"
  },
  {
    "s": "mcro:mistralaiMistral7Bv01-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:mistralaiMistral7Bv01-Consideration",
    "p": "prov:hasTextValue",
    "o": "Mistral 7B is a pretrained base model and therefore does not have any moderation mechanisms."
  },
  {
    "s": "mcro:Auralis",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Auralis",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Auralis-ModelDetail"
  },
  {
    "s": "mcro:Auralis-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Auralis-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Auralis-ModelArchitecture"
  },
  {
    "s": "mcro:Auralis-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Auralis-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Based on [Coqui XTTS-v2](https://huggingface.co/coqui/XTTS-v2)"
  },
  {
    "s": "mcro:Auralis-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:Auralis-License"
  },
  {
    "s": "mcro:Auralis-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Auralis-License",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:Auralis",
    "p": "mcro:hasUseCase",
    "o": "mcro:Auralis-UseCase"
  },
  {
    "s": "mcro:Auralis-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Auralis-UseCase",
    "p": "prov:hasTextValue",
    "o": "Text-to-Speech (TTS) generation for real-world applications, including books, dialogues, and multilingual tasks."
  },
  {
    "s": "mcro:Auralis-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:Auralis-Citation"
  },
  {
    "s": "mcro:Auralis-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Auralis-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{auralis2024,\n  author = {AstraMind AI},\n  title = {Auralis: High-Performance Text-to-Speech Engine},\n  year = {2024},\n  url = {https://huggingface.co/AstraMindAI/auralis}\n}"
  },
  {
    "s": "mcro:mask2former",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mask2former",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mask2former-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mask2former-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mask2former-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone)."
  },
  {
    "s": "mcro:mask2former",
    "p": "mcro:hasCitation",
    "o": "mcro:mask2former-CitationInformationSection"
  },
  {
    "s": "mcro:mask2former-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mask2former-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/)."
  },
  {
    "s": "mcro:mask2former",
    "p": "mcro:hasUseCase",
    "o": "mcro:mask2former-UseCaseInformationSection"
  },
  {
    "s": "mcro:mask2former-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mask2former-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use this particular checkpoint for panoptic segmentation. See the [model hub](https://huggingface.co/models?search=mask2former) to look for other\nfined-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:ruri-small-v2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ruri-small-v2",
    "p": "mcro:hasUseCase",
    "o": "mcro:ruri-small-v2-UseCaseInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2",
    "p": "mcro:hasLicense",
    "o": "mcro:ruri-small-v2-LicenseInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "Apache 2.0"
  },
  {
    "s": "mcro:ruri-small-v2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:ruri-small-v2-ModelDetailSection"
  },
  {
    "s": "mcro:ruri-small-v2-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:ruri-small-v2-ModelDetailSection",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:ruri-small-v2-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Sentence Transformer"
  },
  {
    "s": "mcro:ruri-small-v2-ModelDetailSection",
    "p": "mcro:hasCitation",
    "o": "mcro:ruri-small-v2-CitationInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2",
    "p": "mcro:hasCitation",
    "o": "mcro:ruri-small-v2-ReferenceInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-ReferenceInformationSection",
    "p": "rdf:type",
    "o": "mcro:ReferenceInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-ReferenceInformationSection",
    "p": "prov:hasTextValue",
    "o": "https://arxiv.org/abs/2409.07737"
  },
  {
    "s": "mcro:ruri-small-v2",
    "p": "mcro:hasDataset",
    "o": "mcro:ruri-small-v2-DatasetInformationSection"
  },
  {
    "s": "mcro:ruri-small-v2-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft",
    "p": "mcro:hasCitation",
    "o": "mcro:wav2vec2largexlsr53espeakcvft-Citation"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft-Citation",
    "p": "prov:hasTextValue",
    "o": "Paper: Simple and Effective Zero-shot Cross-lingual Phoneme Recognition"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wav2vec2largexlsr53espeakcvft-ModelArchitecture"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Wav2Vec2"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft",
    "p": "mcro:hasDataset",
    "o": "mcro:wav2vec2largexlsr53espeakcvft-Dataset"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft-Dataset",
    "p": "prov:hasTextValue",
    "o": "CommonVoice"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft",
    "p": "mcro:hasUseCase",
    "o": "mcro:wav2vec2largexlsr53espeakcvft-UseCase"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53espeakcvft-UseCase",
    "p": "prov:hasTextValue",
    "o": "recognize phonetic labels in multiple languages"
  },
  {
    "s": "mcro:Yehorw2vxlsruk",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Yehorw2vxlsruk",
    "p": "mcro:hasCitation",
    "o": "mcro:Yehorw2vxlsruk-CitationInformationSection"
  },
  {
    "s": "mcro:Yehorw2vxlsruk-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Yehorw2vxlsruk-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc {\n\tsmoliakov_2025,\n\tauthor       = { {Smoliakov} },\n\ttitle        = { w2v-xls-r-uk (Revision 55b6dc0) },\n\tyear         = 2025,\n\turl          = { https://huggingface.co/Yehor/w2v-xls-r-uk },\n\tdoi          = { 10.57967/hf/4556 },\n\tpublisher    = { Hugging Face }\n}"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01",
    "p": "mcro:hasModelDetail",
    "o": "mcro:mistralaiMixtral8x7BInstructv01-ModelDetailSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-ModelDetailSection",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-ModelDetailSection",
    "p": "mcro:hasLicense",
    "o": "mcro:mistralaiMixtral8x7BInstructv01-LicenseSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-LicenseSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-ModelDetailSection",
    "p": "mcro:hasArchitecture",
    "o": "mcro:mistralaiMixtral8x7BInstructv01-ArchitectureSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-ArchitectureSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01",
    "p": "mcro:hasLimitation",
    "o": "mcro:mistralaiMixtral8x7BInstructv01-LimitationSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-LimitationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01",
    "p": "mcro:hasUseCase",
    "o": "mcro:mistralaiMixtral8x7BInstructv01-UseCaseSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-UseCaseSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01",
    "p": "mcro:hasConsideration",
    "o": "mcro:mistralaiMixtral8x7BInstructv01-ConsiderationSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-ConsiderationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:mistralaiMixtral8x7BInstructv01-ConsiderationSection",
    "p": "prov:hasTextValue",
    "o": "It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs."
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:llama32CollectionOfMultilingualLlLMs-Architecture"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs-Architecture",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety."
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs",
    "p": "mcro:hasLicense",
    "o": "mcro:llama32CollectionOfMultilingualLlLMs-License"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs-License",
    "p": "prov:hasTextValue",
    "o": "Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement)."
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs",
    "p": "mcro:hasUseCase",
    "o": "mcro:llama32CollectionOfMultilingualLlLMs-UseCase"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs-UseCase",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources."
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs",
    "p": "mcro:hasTrainingData",
    "o": "mcro:llama32CollectionOfMultilingualLlLMs-TrainingData"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:llama32CollectionOfMultilingualLlLMs-TrainingData",
    "p": "prov:hasTextValue",
    "o": "Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO)."
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sentencetransformersmsmarcodistilbertbasev4-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4",
    "p": "mcro:hasCitation",
    "o": "mcro:sentencetransformersmsmarcodistilbertbasev4-CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4",
    "p": "mcro:hasUseCase",
    "o": "mcro:sentencetransformersmsmarcodistilbertbasev4-UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sentencetransformersmsmarcodistilbertbasev4-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:FsoftAICpiiphi",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FsoftAICpiiphi",
    "p": "mcro:hasUseCase",
    "o": "mcro:FsoftAICpiiphi-UseCaseInformationSection"
  },
  {
    "s": "mcro:FsoftAICpiiphi-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:FsoftAICpiiphi-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The model is intended for PII detection in text documents to support tasks such as data anonymization, compliance, and security auditing."
  },
  {
    "s": "mcro:FsoftAICpiiphi",
    "p": "mcro:hasConsideration",
    "o": "mcro:FsoftAICpiiphi-ConsiderationInformationSection"
  },
  {
    "s": "mcro:FsoftAICpiiphi-ConsiderationInformationSection",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:FsoftAICpiiphi-ConsiderationInformationSection",
    "p": "prov:hasTextValue",
    "o": "* Not guaranteed to detect all forms of PII in every context.\n* May return false positives or omit contextually relevant information."
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256",
    "p": "mcro:hasModelDetail",
    "o": "mcro:timmViTB16SigLIPi18n256-ModelDetail"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "A SigLIP (Sigmoid loss for Language-Image Pre-training) model trained on WebLI."
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:timmViTB16SigLIPi18n256-ModelArchitecture"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Contrastive Image-Text, Zero-Shot Image Classification."
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:timmViTB16SigLIPi18n256-Dataset"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-Dataset",
    "p": "prov:hasTextValue",
    "o": "WebLI"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:timmViTB16SigLIPi18n256-Citation"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{zhai2023sigmoid,\n  title={Sigmoid loss for language image pre-training},\n  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2303.15343},\n  year={2023}\n}\n"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{big_vision,\n  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},\n  title = {Big Vision},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/google-research/big_vision}}\n}\n"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256",
    "p": "mcro:hasUseCase",
    "o": "mcro:timmViTB16SigLIPi18n256-UseCase"
  },
  {
    "s": "mcro:timmViTB16SigLIPi18n256-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:segformerb5finetunedade20k",
    "p": "mcro:hasModelArchitectureInformationSection",
    "o": "mcro:segformerb5finetunedade20k-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset."
  },
  {
    "s": "mcro:segformerb5finetunedade20k",
    "p": "mcro:hasUseCaseInformationSection",
    "o": "mcro:segformerb5finetunedade20k-UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you."
  },
  {
    "s": "mcro:segformerb5finetunedade20k",
    "p": "mcro:hasLicenseInformationSection",
    "o": "mcro:segformerb5finetunedade20k-LicenseInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "The license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE)."
  },
  {
    "s": "mcro:segformerb5finetunedade20k",
    "p": "mcro:hasCitationInformationSection",
    "o": "mcro:segformerb5finetunedade20k-CitationInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:segformerb5finetunedade20k-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4",
    "p": "mcro:hasModelDetail",
    "o": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-ModelDetail"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-License"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-Architecture"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-ModelDetail",
    "p": "mcro:hasUseCase",
    "o": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-UseCase"
  },
  {
    "s": "mcro:huggingquantsMetaLlama318BInstructAWQINT4-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF",
    "p": "mcro:hasCitation",
    "o": "mcro:unslothDeepSeekR10528Qwen38BGGUF-CitationInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF",
    "p": "mcro:hasLicense",
    "o": "mcro:unslothDeepSeekR10528Qwen38BGGUF-LicenseInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:unslothDeepSeekR10528Qwen38BGGUF-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:unslothDeepSeekR10528Qwen38BGGUF-UseCaseInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:unslothDeepSeekR10528Qwen38BGGUF-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B"
  },
  {
    "s": "mcro:bge-m3-onnx-o4",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:bge-m3-onnx-o4",
    "p": "mcro:hasUseCase",
    "o": "mcro:bge-m3-onnx-o4-UseCase"
  },
  {
    "s": "mcro:bge-m3-onnx-o4-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:bge-m3-onnx-o4-UseCase",
    "p": "prov:hasTextValue",
    "o": "- [x] Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\n- [x] Multi-Linguality: It can support more than **100** working languages.\n- [x] Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to **8192** tokens."
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k",
    "p": "mcro:hasModelDetail",
    "o": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelDetail"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelArchitecture"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Image classification / feature backbone"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:vitbasepatch8224augreg2in21kftin1k-Citation"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:vitbasepatch8224augreg2in21kftin1k-Dataset"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-Dataset",
    "p": "prov:hasTextValue",
    "o": "ImageNet-1k"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-ModelDetail",
    "p": "mcro:hasTrainingData",
    "o": "mcro:vitbasepatch8224augreg2in21kftin1k-TrainingData"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-TrainingData",
    "p": "prov:hasTextValue",
    "o": "ImageNet-21k"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k",
    "p": "mcro:hasUseCase",
    "o": "mcro:vitbasepatch8224augreg2in21kftin1k-UseCase"
  },
  {
    "s": "mcro:vitbasepatch8224augreg2in21kftin1k-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:MaziyarPanahiMistral7BInstructv03GGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:MaziyarPanahiMistral7BInstructv03GGUF",
    "p": "mcro:hasDescription",
    "o": "mcro:MaziyarPanahiMistral7BInstructv03GGUF-Description"
  },
  {
    "s": "mcro:MaziyarPanahiMistral7BInstructv03GGUF-Description",
    "p": "rdf:type",
    "o": "obo:IAO_0000314"
  },
  {
    "s": "mcro:MaziyarPanahiMistral7BInstructv03GGUF-Description",
    "p": "prov:hasTextValue",
    "o": "[MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF) contains GGUF format model files for [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)."
  },
  {
    "s": "mcro:IP-Adapter-FaceID",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:IP-Adapter-FaceID",
    "p": "mcro:hasModelDetail",
    "o": "mcro:IP-Adapter-FaceID-ModelDetail"
  },
  {
    "s": "mcro:IP-Adapter-FaceID-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:IP-Adapter-FaceID-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:IP-Adapter-FaceID-LicenseInformationSection"
  },
  {
    "s": "mcro:IP-Adapter-FaceID-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:IP-Adapter-FaceID-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "AS InsightFace pretrained models are available for non-commercial research purposes, IP-Adapter-FaceID models are released exclusively for research purposes and is not intended for commercial use."
  },
  {
    "s": "mcro:IP-Adapter-FaceID-ModelDetail",
    "p": "mcro:hasLimitation",
    "o": "mcro:IP-Adapter-FaceID-LimitationInformationSection"
  },
  {
    "s": "mcro:IP-Adapter-FaceID-LimitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:IP-Adapter-FaceID-LimitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "- The models do not achieve perfect photorealism and ID consistency.\n- The generalization of the models is limited due to limitations of the training data, base model and face recognition model."
  },
  {
    "s": "mcro:IP-Adapter-FaceID",
    "p": "mcro:hasUseCase",
    "o": "mcro:IP-Adapter-FaceID-UseCaseInformationSection"
  },
  {
    "s": "mcro:IP-Adapter-FaceID-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:IP-Adapter-FaceID-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "IP-Adapter-FaceID can generate various style images conditioned on a face with only text prompts."
  },
  {
    "s": "mcro:Llama3370BInstructAWQ",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Llama3370BInstructAWQ",
    "p": "mcro:hasLicense",
    "o": "mcro:Llama3370BInstructAWQ-License"
  },
  {
    "s": "mcro:Llama3370BInstructAWQ-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Llama3370BInstructAWQ-License",
    "p": "prov:hasTextValue",
    "o": "AWQ 4-bit quantized version"
  },
  {
    "s": "mcro:Llama3370BInstructAWQ",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Llama3370BInstructAWQ-ModelArchitecture"
  },
  {
    "s": "mcro:Llama3370BInstructAWQ-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Llama3370BInstructAWQ-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "`meta-llama/Llama-3.3-70B-Instruct`"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS",
    "p": "mcro:hasModelDetail",
    "o": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "Basic information about the model that includes licensing information, owner information, the architecture of the model (algorthim employed), references (cited papers), and versioning information."
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-License"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-Citation"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelArchitecture"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS",
    "p": "mcro:hasUseCase",
    "o": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-UseCase"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-UseCase",
    "p": "prov:hasTextValue",
    "o": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."
  },
  {
    "s": "mcro:snunlpKRSBERTV40KklueNLIaugSTS-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{kr-sbert,\n  author = {Park, Suzi and Hyopil Shin},\n  title = {KR-SBERT: A Pre-trained Korean-specific Sentence-BERT model},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/snunlp/KR-SBERT}}\n}"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:AlibabaNLPgteQwen27Binstruct-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Integration of bidirectional attention mechanisms, enriching its contextual understanding."
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Instruction tuning, applied solely on the query side for streamlined efficiency"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. This training leverages both weakly supervised and supervised data, ensuring the model's applicability across numerous languages and a wide array of downstream tasks."
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct",
    "p": "mcro:hasLicense",
    "o": "mcro:AlibabaNLPgteQwen27Binstruct-LicenseInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "MIT Licensed inference server"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct",
    "p": "mcro:hasCitation",
    "o": "mcro:AlibabaNLPgteQwen27Binstruct-CitationInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct",
    "p": "mcro:hasUseCase",
    "o": "mcro:AlibabaNLPgteQwen27Binstruct-UseCaseInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:AlibabaNLPgteQwen27Binstruct-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "General Text Embedding"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet",
    "p": "mcro:hasModelDetail",
    "o": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelDetail"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks."
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet",
    "p": "mcro:hasUseCase",
    "o": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-UseCase"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-UseCase",
    "p": "prov:hasTextValue",
    "o": "You can use the raw model for classifying audio into one of the AudioSet classes."
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Audio Spectrogram Transformer"
  },
  {
    "s": "mcro:owlv2",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:owlv2",
    "p": "mcro:hasModelDetail",
    "o": "mcro:owlv2-ModelDetails"
  },
  {
    "s": "mcro:owlv2-ModelDetails",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:owlv2-ModelDetails",
    "p": "prov:hasTextValue",
    "o": "The OWLv2 model (short for Open-World Localization) was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.\n\nThe model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."
  },
  {
    "s": "mcro:owlv2-ModelDetails",
    "p": "mcro:hasCitation",
    "o": "mcro:owlv2-CitationInformationSection"
  },
  {
    "s": "mcro:owlv2-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:owlv2",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:owlv2-ModelArchitecture"
  },
  {
    "s": "mcro:owlv2-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:owlv2-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The model uses a CLIP backbone with a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective."
  },
  {
    "s": "mcro:owlv2",
    "p": "mcro:hasUseCase",
    "o": "mcro:owlv2-UseCase"
  },
  {
    "s": "mcro:owlv2-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:owlv2-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training."
  },
  {
    "s": "mcro:owlv2-UseCase",
    "p": "mcro:hasPrimaryIntendedUseCase",
    "o": "mcro:owlv2-PrimaryIntendedUseCase"
  },
  {
    "s": "mcro:owlv2-PrimaryIntendedUseCase",
    "p": "rdf:type",
    "o": "mcro:PrimaryIntendedUseCaseInformationSection"
  },
  {
    "s": "mcro:owlv2-PrimaryIntendedUseCase",
    "p": "prov:hasTextValue",
    "o": "The primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models."
  },
  {
    "s": "mcro:owlv2",
    "p": "mcro:hasDataset",
    "o": "mcro:owlv2-Dataset"
  },
  {
    "s": "mcro:owlv2-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:owlv2-Dataset",
    "p": "prov:hasTextValue",
    "o": "The CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html).\n\n(to be updated for v2)"
  },
  {
    "s": "mcro:owlv2-ModelDetails",
    "p": "prov:hasTextValue",
    "o": "June 2023"
  },
  {
    "s": "mcro:owlv2-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@misc{minderer2023scaling,\n      title={Scaling Open-Vocabulary Object Detection}, \n      author={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},\n      year={2023},\n      eprint={2306.09683},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim",
    "p": "mcro:hasLicense",
    "o": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-LicenseInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "for research purpose only"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "Wav2Vec2-Large-Robust"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim",
    "p": "mcro:hasDataset",
    "o": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-DatasetInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "MSP-Podcast"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim",
    "p": "mcro:hasCitation",
    "o": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-CitationInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "doi:10.5281/zenodo.6221127"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim",
    "p": "mcro:hasUseCase",
    "o": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-UseCaseInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:audeeringwav2vec2largerobust12ftemotionmspdim-UseCaseInformationSection",
    "p": "prov:hasTextValue",
    "o": "dimensional speech emotion recognition"
  },
  {
    "s": "mcro:modelid",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:modelid",
    "p": "mcro:hasModelDetail",
    "o": "mcro:modelid-ModelDetail"
  },
  {
    "s": "mcro:modelid-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:modelid-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:modelid-Citation"
  },
  {
    "s": "mcro:modelid-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:modelid-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:modelid-License"
  },
  {
    "s": "mcro:modelid-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:modelid",
    "p": "mcro:hasUseCase",
    "o": "mcro:modelid-UseCase"
  },
  {
    "s": "mcro:modelid-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:modelid",
    "p": "mcro:hasTrainingData",
    "o": "mcro:modelid-TrainingData"
  },
  {
    "s": "mcro:modelid-TrainingData",
    "p": "rdf:type",
    "o": "mcro:TrainingDataInformationSection"
  },
  {
    "s": "mcro:modelid",
    "p": "mcro:hasEvaluation",
    "o": "mcro:modelid-Evaluation"
  },
  {
    "s": "mcro:modelid-Evaluation",
    "p": "rdf:type",
    "o": "mcro:EvaluationDataInformationSection"
  },
  {
    "s": "mcro:modelid",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:modelid-ModelArchitecture"
  },
  {
    "s": "mcro:modelid-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasCitation",
    "o": "mcro:FlagEmbedding-CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding",
    "p": "mcro:hasLicense",
    "o": "mcro:FlagEmbedding-LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:FlagEmbedding-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Qwen2505BInstruct-ModelDetail"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:Qwen2505BInstruct-Citation"
  },
  {
    "s": "mcro:Qwen2505BInstruct-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:Qwen2505BInstruct-ModelArchitecture"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Qwen2505BInstruct-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings"
  },
  {
    "s": "mcro:Qwen2505BInstruct",
    "p": "mcro:hasDataset",
    "o": "mcro:Qwen2505BInstruct-Dataset"
  },
  {
    "s": "mcro:Qwen2505BInstruct-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekR10528-License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekR10528-Citation"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:deepseekaiDeepSeekR10528-ModelArchitecture"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528."
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528",
    "p": "mcro:hasUseCase",
    "o": "mcro:deepseekaiDeepSeekR10528-UseCase"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR10528-UseCase",
    "p": "prov:hasTextValue",
    "o": "The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic."
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:chronosbolttiny-ModelArchitecture"
  },
  {
    "s": "mcro:chronosbolttiny-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "T5 encoder-decoder architecture"
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasCitation",
    "o": "mcro:chronosbolttiny-Citation"
  },
  {
    "s": "mcro:chronosbolttiny-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-Citation",
    "p": "prov:hasTextValue",
    "o": "@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}"
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasLicense",
    "o": "mcro:chronosbolttiny-License"
  },
  {
    "s": "mcro:chronosbolttiny-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-License",
    "p": "prov:hasTextValue",
    "o": "Apache-2.0 License"
  },
  {
    "s": "mcro:chronosbolttiny",
    "p": "mcro:hasUseCase",
    "o": "mcro:chronosbolttiny-UseCase"
  },
  {
    "s": "mcro:chronosbolttiny-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:chronosbolttiny-UseCase",
    "p": "prov:hasTextValue",
    "o": "zero-shot forecasting"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:rinnajapanesecloobvitb16-ModelArchitecture"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "The model was trained  a ViT-B/16 Transformer architecture as an image encoder and uses a 12-layer BERT as a text encoder. The image encoder was initialized from the [AugReg `vit-base-patch16-224` model](https://github.com/google-research/vision_transformer)."
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16",
    "p": "mcro:hasDataset",
    "o": "mcro:rinnajapanesecloobvitb16-Dataset"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-Dataset",
    "p": "prov:hasTextValue",
    "o": "The model was trained on [CC12M](https://github.com/google-research-datasets/conceptual-12m) translated the captions to Japanese."
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16",
    "p": "mcro:hasLicense",
    "o": "mcro:rinnajapanesecloobvitb16-License"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-License",
    "p": "prov:hasTextValue",
    "o": "[The Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16",
    "p": "mcro:hasCitation",
    "o": "mcro:rinnajapanesecloobvitb16-Citation1"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-Citation1",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-Citation1",
    "p": "prov:hasTextValue",
    "o": "@misc{rinna-japanese-cloob-vit-b-16,\n    title = {rinna/japanese-cloob-vit-b-16},\n    author = {Shing, Makoto and Zhao, Tianyu and Sawada, Kei},\n    url = {https://huggingface.co/rinna/japanese-cloob-vit-b-16}\n}"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16",
    "p": "mcro:hasCitation",
    "o": "mcro:rinnajapanesecloobvitb16-Citation2"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-Citation2",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:rinnajapanesecloobvitb16-Citation2",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{sawada2024release,\n    title = {Release of Pre-Trained Models for the {J}apanese Language},\n    author = {Sawada, Kei and Zhao, Tianyu and Shing, Makoto and Mitsui, Kentaro and Kaga, Akio and Hono, Yukiya and Wakatsuki, Toshiaki and Mitsuda, Koh},\n    booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},\n    month = {5},\n    year = {2024},\n    pages = {13898--13905},\n    url = {https://aclanthology.org/2024.lrec-main.1213},\n    note = {\\url{https://arxiv.org/abs/2404.01657}}\n}"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge",
    "p": "mcro:hasModelDetail",
    "o": "mcro:Salesforceblipvqacapfiltlarge-ModelDetail"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:Salesforceblipvqacapfiltlarge-Citation"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:Salesforceblipvqacapfiltlarge-License"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-ModelDetail",
    "p": "mcro:hasArchitecture",
    "o": "mcro:Salesforceblipvqacapfiltlarge-Arch"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-Arch",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-Arch",
    "p": "prov:hasTextValue",
    "o": "large architecture (with ViT large backbone)"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge",
    "p": "mcro:hasUseCase",
    "o": "mcro:Salesforceblipvqacapfiltlarge-UseCase"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-UseCase",
    "p": "prov:hasTextValue",
    "o": "conditional and un-conditional image captioning"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge",
    "p": "mcro:hasConsideration",
    "o": "mcro:Salesforceblipvqacapfiltlarge-Consideration"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-Consideration",
    "p": "rdf:type",
    "o": "mcro:ConsiderationInformationSection"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-Consideration",
    "p": "mcro:hasEthicalConsideration",
    "o": "mcro:Salesforceblipvqacapfiltlarge-EthicalConsideration"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-EthicalConsideration",
    "p": "rdf:type",
    "o": "mcro:EthicalConsiderationSection"
  },
  {
    "s": "mcro:Salesforceblipvqacapfiltlarge-EthicalConsideration",
    "p": "prov:hasTextValue",
    "o": "This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people\u2019s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP."
  },
  {
    "s": "mcro:WDViTLargeTaggerV3",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:WDViTLargeTaggerV3",
    "p": "mcro:hasDataset",
    "o": "mcro:WDViTLargeTaggerV3-DatasetInformationSection"
  },
  {
    "s": "mcro:WDViTLargeTaggerV3-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:WDViTLargeTaggerV3-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "Last image id: 7220105\nTrained on Danbooru images with IDs modulo 0000-0899.\nValidated on images with IDs modulo 0950-0999.\nImages with less than 10 general tags were filtered out.\nTags with less than 600 images were filtered out."
  },
  {
    "s": "mcro:WDViTLargeTaggerV3",
    "p": "mcro:hasPerformanceMetric",
    "o": "mcro:WDViTLargeTaggerV3-PerformanceMetric"
  },
  {
    "s": "mcro:WDViTLargeTaggerV3-PerformanceMetric",
    "p": "rdf:type",
    "o": "mcro:PerformanceMetricInformationSection"
  },
  {
    "s": "mcro:WDViTLargeTaggerV3-PerformanceMetric",
    "p": "prov:hasTextValue",
    "o": "`v1.0: P=R: threshold = 0.2606, F1 = 0.4674`"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h",
    "p": "mcro:hasCitation",
    "o": "mcro:wav2vec2largerobustftlibri960h-Citation"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h-Citation",
    "p": "prov:hasTextValue",
    "o": "Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, Michael Auli"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wav2vec2largerobustftlibri960h-Architecture"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h-Architecture",
    "p": "prov:hasTextValue",
    "o": "Wav2Vec2"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h",
    "p": "mcro:hasDataset",
    "o": "mcro:wav2vec2largerobustftlibri960h-Dataset"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h-Dataset",
    "p": "prov:hasTextValue",
    "o": "Librispeech"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h",
    "p": "mcro:hasUseCase",
    "o": "mcro:wav2vec2largerobustftlibri960h-UseCase"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2largerobustftlibri960h-UseCase",
    "p": "prov:hasTextValue",
    "o": "transcribe audio files"
  },
  {
    "s": "mcro:mradermacherDeepSeekV2LiteGGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mradermacherDeepSeekV2LiteGGUF",
    "p": "mcro:hasLicense",
    "o": "mcro:mradermacherDeepSeekV2LiteGGUF-License"
  },
  {
    "s": "mcro:mradermacherDeepSeekV2LiteGGUF-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:mradermacherDeepSeekV2LiteGGUF",
    "p": "mcro:hasQuantativeAnalysis",
    "o": "mcro:mradermacherDeepSeekV2LiteGGUF-QuantativeAnalysis"
  },
  {
    "s": "mcro:mradermacherDeepSeekV2LiteGGUF-QuantativeAnalysis",
    "p": "rdf:type",
    "o": "mcro:QuantativeAnalysisSection"
  },
  {
    "s": "mcro:sdxlturbo",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:sdxlturbo",
    "p": "mcro:hasModelDetail",
    "o": "mcro:sdxlturbo-ModelDetail"
  },
  {
    "s": "mcro:sdxlturbo-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:sdxlturbo-ModelDetail",
    "p": "prov:hasTextValue",
    "o": "SDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation."
  },
  {
    "s": "mcro:sdxlturbo-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:sdxlturbo-ModelArchitecture"
  },
  {
    "s": "mcro:sdxlturbo-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:sdxlturbo-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SDXL-Turbo is a distilled version of [SDXL 1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), trained for real-time synthesis."
  },
  {
    "s": "mcro:sdxlturbo",
    "p": "mcro:hasUseCase",
    "o": "mcro:sdxlturbo-UseCase"
  },
  {
    "s": "mcro:sdxlturbo-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:sdxlturbo-UseCase",
    "p": "prov:hasTextValue",
    "o": "The model is intended for both non-commercial and commercial usage."
  },
  {
    "s": "mcro:sdxlturbo",
    "p": "mcro:hasLimitation",
    "o": "mcro:sdxlturbo-Limitation"
  },
  {
    "s": "mcro:sdxlturbo-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:sdxlturbo-Limitation",
    "p": "prov:hasTextValue",
    "o": "- The generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy."
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken",
    "p": "mcro:hasDataset",
    "o": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Dataset"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Dataset",
    "p": "prov:hasTextValue",
    "o": "UMLS"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken",
    "p": "mcro:hasCitation",
    "o": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Citation"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{liu-etal-2021-self,\n    title = \"Self-Alignment Pretraining for Biomedical Entity Representations\",\n    author = \"Liu, Fangyu  and\n      Shareghi, Ehsan  and\n      Meng, Zaiqiao  and\n      Basaldella, Marco  and\n      Collier, Nigel\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2021.naacl-main.334\",\n    pages = \"4228--4238\",\n    abstract = \"Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.\",\n}\n"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-ModelArchitecture"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "SapBERT by [Liu et al. (2020)](https://arxiv.org/pdf/2010.11784.pdf). Trained with [UMLS](https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html) 2020AA (English only), using [microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) as the base model. Please use the mean-pooling of the output as the representation."
  },
  {
    "s": "mcro:germansentimentbert",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:germansentimentbert",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:germansentimentbert-ModelArchitecture"
  },
  {
    "s": "mcro:germansentimentbert-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:germansentimentbert-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Googles Bert architecture"
  },
  {
    "s": "mcro:germansentimentbert",
    "p": "mcro:hasDataset",
    "o": "mcro:germansentimentbert-Dataset"
  },
  {
    "s": "mcro:germansentimentbert-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:germansentimentbert-Dataset",
    "p": "prov:hasTextValue",
    "o": "1.834 million German-language samples"
  },
  {
    "s": "mcro:germansentimentbert",
    "p": "mcro:hasCitation",
    "o": "mcro:germansentimentbert-Citation"
  },
  {
    "s": "mcro:germansentimentbert-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:germansentimentbert-Citation",
    "p": "prov:hasTextValue",
    "o": "@InProceedings{guhr-EtAl:2020:LREC,\n  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  B\u00f6hme, Hans Joachim},\n  title     = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},\n  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},\n  month          = {May},\n  year           = {2020},\n  address        = {Marseille, France},\n  publisher      = {European Language Resources Association},\n  pages     = {1620--1625},\n  url       = {https://www.aclweb.org/anthology/2020.lrec-1.202}\n}"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization",
    "p": "mcro:hasModelDetail",
    "o": "mcro:FinetunedT5SmallTextSummarization-ModelDetail"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:FinetunedT5SmallTextSummarization-ModelArchitecture"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "T5 transformer model"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:FinetunedT5SmallTextSummarization-License"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization",
    "p": "mcro:hasUseCase",
    "o": "mcro:FinetunedT5SmallTextSummarization-UseCase"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-UseCase",
    "p": "prov:hasTextValue",
    "o": "Text Summarization"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization",
    "p": "mcro:hasDataset",
    "o": "mcro:FinetunedT5SmallTextSummarization-Dataset"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:FinetunedT5SmallTextSummarization-Dataset",
    "p": "prov:hasTextValue",
    "o": "a variety of documents and their corresponding human-generated summaries"
  },
  {
    "s": "mcro:Segformerb0finetunedade512512",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:Segformerb0finetunedade512512",
    "p": "mcro:hasUseCase",
    "o": "mcro:Segformerb0finetunedade512512-UseCase"
  },
  {
    "s": "mcro:Segformerb0finetunedade512512-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:Segformerb0finetunedade512512-UseCase",
    "p": "prov:hasTextValue",
    "o": "Image segmentation"
  },
  {
    "s": "mcro:MaziyarPanahiPhi35miniinstructGGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:MaziyarPanahiPhi35miniinstructGGUF",
    "p": "mcro:hasModelDetails",
    "o": "mcro:MaziyarPanahiPhi35miniinstructGGUF-ModelDetail"
  },
  {
    "s": "mcro:MaziyarPanahiPhi35miniinstructGGUF-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:MaziyarPanahiPhi35miniinstructGGUF-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:MaziyarPanahiPhi35miniinstructGGUF-License"
  },
  {
    "s": "mcro:MaziyarPanahiPhi35miniinstructGGUF-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:scibert",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:scibert",
    "p": "mcro:hasCitation",
    "o": "mcro:scibert-Citation"
  },
  {
    "s": "mcro:scibert-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:scibert-Citation",
    "p": "prov:hasTextValue",
    "o": "@inproceedings{beltagy-etal-2019-scibert,\n    title = \"SciBERT: A Pretrained Language Model for Scientific Text\",\n    author = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\n    booktitle = \"EMNLP\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/D19-1371\"\n}"
  },
  {
    "s": "mcro:scibert",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:scibert-ModelArchitecture"
  },
  {
    "s": "mcro:scibert-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:scibert-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "BERT model trained on scientific text"
  },
  {
    "s": "mcro:ocr-equation-images-and-text-to-latex",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:ocr-equation-images-and-text-to-latex",
    "p": "mcro:hasCitation",
    "o": "mcro:ocr-equation-images-and-text-to-latex-Citation"
  },
  {
    "s": "mcro:ocr-equation-images-and-text-to-latex-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:ocr-equation-images-and-text-to-latex-Citation",
    "p": "prov:hasTextValue",
    "o": "See [texify](https://github.com/VikParuchuri/texify)."
  },
  {
    "s": "mcro:openbmbMiniCPMo26",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:openbmbMiniCPMo26",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:openbmbMiniCPMo26-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:openbmbMiniCPMo26-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:openbmbMiniCPMo26",
    "p": "mcro:hasCitation",
    "o": "mcro:openbmbMiniCPMo26-CitationInformationSection"
  },
  {
    "s": "mcro:openbmbMiniCPMo26-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:openbmbMiniCPMo26",
    "p": "mcro:hasUseCase",
    "o": "mcro:openbmbMiniCPMo26-UseCaseInformationSection"
  },
  {
    "s": "mcro:openbmbMiniCPMo26-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:wav2vec2largexlsr53",
    "p": "mcro:hasModelDetail",
    "o": "mcro:wav2vec2largexlsr53-ModelDetail"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-ModelDetail",
    "p": "mcro:hasCitation",
    "o": "mcro:wav2vec2largexlsr53-Citation"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:wav2vec2largexlsr53-Architecture"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-Architecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:wav2vec2largexlsr53-License"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53",
    "p": "mcro:hasUseCase",
    "o": "mcro:wav2vec2largexlsr53-UseCase"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:wav2vec2largexlsr53-Citation",
    "p": "prov:hasTextValue",
    "o": "[Paper](https://arxiv.org/abs/2006.13979)"
  },
  {
    "s": "mcro:openvla7b",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:openvla7b",
    "p": "mcro:hasModelDetail",
    "o": "mcro:openvla7b-ModelDetail"
  },
  {
    "s": "mcro:openvla7b-ModelDetail",
    "p": "rdf:type",
    "o": "mcro:ModelDetailSection"
  },
  {
    "s": "mcro:openvla7b-ModelDetail",
    "p": "mcro:hasLicense",
    "o": "mcro:openvla7b-License"
  },
  {
    "s": "mcro:openvla7b-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:openvla7b-License",
    "p": "prov:hasTextValue",
    "o": "MIT"
  },
  {
    "s": "mcro:openvla7b-ModelDetail",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:openvla7b-ModelArchitecture"
  },
  {
    "s": "mcro:openvla7b-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:openvla7b-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "Vision-language-action (language, image => robot actions)"
  },
  {
    "s": "mcro:openvla7b-ModelDetail",
    "p": "mcro:hasDataset",
    "o": "mcro:openvla7b-Dataset"
  },
  {
    "s": "mcro:openvla7b-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:openvla7b-Dataset",
    "p": "prov:hasTextValue",
    "o": "[Open X-Embodiment](https://robotics-transformer-x.github.io/) -- specific component datasets can be found [here](https://github.com/openvla/openvla)."
  },
  {
    "s": "mcro:openvla7b",
    "p": "mcro:hasCitation",
    "o": "mcro:openvla7b-Citation"
  },
  {
    "s": "mcro:openvla7b-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:openvla7b-Citation",
    "p": "prov:hasTextValue",
    "o": "[OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)"
  },
  {
    "s": "mcro:openvla7b",
    "p": "mcro:hasUseCase",
    "o": "mcro:openvla7b-UseCase"
  },
  {
    "s": "mcro:openvla7b-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:openvla7b-UseCase",
    "p": "prov:hasTextValue",
    "o": "OpenVLA models take a language instruction and a camera image of a robot workspace as input, and predict (normalized) robot actions consisting of 7-DoF end-effector deltas\nof the form (x, y, z, roll, pitch, yaw, gripper). To execute on an actual robot platform, actions need to be *un-normalized* subject to statistics computed on a per-robot,\nper-dataset basis. See [our repository](https://github.com/openvla/openvla) for more information.\n\nOpenVLA models can be used zero-shot to control robots for specific combinations of embodiments and domains seen in the Open-X pretraining mixture (e.g., for \n[BridgeV2 environments with a Widow-X robot](https://rail-berkeley.github.io/bridgedata/)). They can also be efficiently *fine-tuned* for new tasks and robot setups\ngiven minimal demonstration data; [see here](https://github.com/openvla/openvla/blob/main/scripts/finetune.py)."
  },
  {
    "s": "mcro:openvla7b-UseCase",
    "p": "mcro:hasLimitation",
    "o": "mcro:openvla7b-Limitation"
  },
  {
    "s": "mcro:openvla7b-Limitation",
    "p": "rdf:type",
    "o": "mcro:LimitationInformationSection"
  },
  {
    "s": "mcro:openvla7b-UseCase",
    "p": "prov:hasTextValue",
    "o": "**Out-of-Scope:** OpenVLA models do not zero-shot generalize to new (unseen) robot embodiments, or setups that are not represented in the pretraining mix; in these cases,\nwe suggest collecting a dataset of demonstrations on the desired setup, and fine-tuning OpenVLA models instead."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasLicense",
    "o": "mcro:TheBlokephi2GGUF-License"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-License",
    "p": "prov:hasTextValue",
    "o": "The model is licensed under the [microsoft-research-license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE)."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:TheBlokephi2GGUF-ModelArchitecture"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "a Transformer-based model with next-word prediction objective"
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasCitation",
    "o": "mcro:TheBlokephi2GGUF-Citation"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Citation",
    "p": "prov:hasTextValue",
    "o": "Microsoft's Phi 2"
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasUseCase",
    "o": "mcro:TheBlokephi2GGUF-UseCase"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-UseCase",
    "p": "prov:hasTextValue",
    "o": "Phi-2 is intended for research purposes only. Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format."
  },
  {
    "s": "mcro:TheBlokephi2GGUF",
    "p": "mcro:hasDataset",
    "o": "mcro:TheBlokephi2GGUF-Dataset"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Dataset",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:TheBlokephi2GGUF-Dataset",
    "p": "prov:hasTextValue",
    "o": "250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4."
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasLicense",
    "o": "mcro:deepseekaiDeepSeekR1-License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-License",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-License",
    "p": "prov:hasTextValue",
    "o": "MIT License"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasCitation",
    "o": "mcro:deepseekaiDeepSeekR1-Citation"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-Citation",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-Citation",
    "p": "prov:hasTextValue",
    "o": "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:deepseekaiDeepSeekR1-ModelArchitecture"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "DeepSeek-V3"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1",
    "p": "mcro:hasIntendedUseCase",
    "o": "mcro:deepseekaiDeepSeekR1-UseCase"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-UseCase",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:deepseekaiDeepSeekR1-UseCase",
    "p": "prov:hasTextValue",
    "o": "reasoning"
  },
  {
    "s": "mcro:TheBlokeMistral7BInstructv01GGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:TheBlokeMistral7BInstructv01GGUF",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:TheBlokeMistral7BInstructv01GGUF-ModelArchitecture"
  },
  {
    "s": "mcro:TheBlokeMistral7BInstructv01GGUF-ModelArchitecture",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:TheBlokeMistral7BInstructv01GGUF-ModelArchitecture",
    "p": "prov:hasTextValue",
    "o": "This instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF",
    "p": "mcro:hasCitation",
    "o": "mcro:MaziyarPanahigemma31bitGGUF-CitationInformationSection"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "[MaziyarPanahi/gemma-3-1b-it-GGUF](https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF) contains GGUF format model files for [google/gemma-3-1b-it](https://huggingface.co/google/gemma-3-1b-it)."
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:MaziyarPanahigemma31bitGGUF-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "GGUF"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF",
    "p": "mcro:hasLicense",
    "o": "mcro:MaziyarPanahigemma31bitGGUF-LicenseInformationSection"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF-LicenseInformationSection",
    "p": "rdf:type",
    "o": "mcro:LicenseInformationSection"
  },
  {
    "s": "mcro:MaziyarPanahigemma31bitGGUF-LicenseInformationSection",
    "p": "prov:hasTextValue",
    "o": null
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge",
    "p": "rdf:type",
    "o": "mcro:Model"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge",
    "p": "mcro:hasUseCase",
    "o": "mcro:mmlwretrievalrobertalarge-UseCaseInformationSection"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge-UseCaseInformationSection",
    "p": "rdf:type",
    "o": "mcro:UseCaseInformationSection"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge",
    "p": "mcro:hasCitation",
    "o": "mcro:mmlwretrievalrobertalarge-CitationInformationSection"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge-CitationInformationSection",
    "p": "rdf:type",
    "o": "mcro:CitationInformationSection"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge-CitationInformationSection",
    "p": "prov:hasTextValue",
    "o": "@article{dadas2024pirb,\n  title={{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods}, \n  author={S\u0142awomir Dadas and Micha\u0142 Pere\u0142kiewicz and Rafa\u0142 Po\u015bwiata},\n  year={2024},\n  eprint={2402.13350},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge",
    "p": "mcro:hasModelArchitecture",
    "o": "mcro:mmlwretrievalrobertalarge-ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge-ModelArchitectureInformationSection",
    "p": "rdf:type",
    "o": "mcro:ModelArchitectureInformationSection"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge-ModelArchitectureInformationSection",
    "p": "prov:hasTextValue",
    "o": "MMLW (musz\u0119 mie\u0107 lepsz\u0105 wiadomo\u015b\u0107) are neural text encoders for Polish.\nThis model is optimized for information retrieval tasks. It can transform queries and passages to 1024 dimensional vectors. \nThe model was developed using a two-step procedure: \n- In the first step, it was initialized with Polish RoBERTa checkpoint, and then trained with [multilingual knowledge distillation method](https://aclanthology.org/2020.emnlp-main.365/) on a diverse corpus of 60 million Polish-English text pairs. We utilised [English FlagEmbeddings (BGE)](https://huggingface.co/BAAI/bge-large-en) as teacher models for distillation. \n- The second step involved fine-tuning the obtained models with contrastrive loss on [Polish MS MARCO](https://huggingface.co/datasets/clarin-knext/msmarco-pl) training split. In order to improve the efficiency of contrastive training, we used large batch sizes - 1152 for small, 768 for base, and 288 for large models. Fine-tuning was conducted on a cluster of 12 A100 GPUs."
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge",
    "p": "mcro:hasDataset",
    "o": "mcro:mmlwretrievalrobertalarge-DatasetInformationSection"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge-DatasetInformationSection",
    "p": "rdf:type",
    "o": "mcro:DatasetInformationSection"
  },
  {
    "s": "mcro:mmlwretrievalrobertalarge-DatasetInformationSection",
    "p": "prov:hasTextValue",
    "o": "[Polish MS MARCO](https://huggingface.co/datasets/clarin-knext/msmarco-pl)"
  }
]