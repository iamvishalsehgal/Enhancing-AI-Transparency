@prefix mcro: <http://purl.obolibrary.org/obo/MCRO_> .
@prefix prov1: <https://www.w3.org/ns/prov#> .

mcro:Falconsainsfwimagedetection a mcro:Model ;
    mcro:hasDataset mcro:Falconsainsfwimagedetection-DatasetInformationSection ;
    mcro:hasLimitations mcro:Falconsainsfwimagedetection-LimitationsInformationSection ;
    mcro:hasMetrics mcro:Falconsainsfwimagedetection-MetricsInformationSection ;
    mcro:hasModelArchitecture mcro:Falconsainsfwimagedetection-ModelArchitectureInformationSection ;
    mcro:hasModelDetail mcro:Falconsainsfwimagedetection-ModelDetailSection ;
    mcro:hasReference mcro:Falconsainsfwimagedetection-ReferenceInformationSection ;
    mcro:hasTrainingData mcro:Falconsainsfwimagedetection-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:Falconsainsfwimagedetection-UseCaseInformationSection .

mcro:YOLOv8DetectionModel a mcro:Model ;
    mcro:hasDataset mcro:YOLOv8DetectionModel-DatasetInformationSection ;
    mcro:hasLimitations mcro:YOLOv8DetectionModel-LimitationsInformationSection ;
    mcro:hasMetrics mcro:YOLOv8DetectionModel-MetricsInformationSection ;
    mcro:hasUseCase mcro:YOLOv8DetectionModel-UseCaseInformationSection .

mcro:allMiniLML6v2 a mcro:Model ;
    mcro:hasModelDetail mcro:allMiniLML6v2-ModelDetailSection .

mcro:allmpnetbasev2 a mcro:Model ;
    mcro:hasDataset mcro:AllNLIDataset,
        mcro:AltlexDataset,
        mcro:COCOImageCaptionsDataset,
        mcro:CodeSearchDataset,
        mcro:Eli5Dataset,
        mcro:Flickr30kDataset,
        mcro:GOOAQDataset,
        mcro:MSMARCOTripletsDataset,
        mcro:NaturalQuestionsNQDataset,
        mcro:PAQDataset,
        mcro:QuoraQuestionTripletsDataset,
        mcro:RedditCommentsDataset,
        mcro:S2ORCCitationPairsAbstractsDataset,
        mcro:S2ORCCitationPairsTitlesDataset,
        mcro:S2ORCTitleAbstractDataset,
        mcro:SPECTERCitationTripletsDataset,
        mcro:SQuAD20Dataset,
        mcro:SearchQADataset,
        mcro:SentenceCompressionDataset,
        mcro:SimpleWikipediaDataset,
        mcro:StackExchangeDuplicateQuestionsBodiesDataset,
        mcro:StackExchangeDuplicateQuestionsTitlesBodiesDataset,
        mcro:StackExchangeDuplicateQuestionsTitlesDataset,
        mcro:StackExchangeTitleAnswerPairsDataset,
        mcro:StackExchangeTitleBodyAnswerPairsDataset,
        mcro:StackExchangeTitleBodyPairsDataset,
        mcro:TriviaQADataset,
        mcro:WikiAnswersDataset,
        mcro:WikihowDataset,
        mcro:YahooAnswersQuestionAnswerDataset,
        mcro:YahooAnswersTitleAnswerDataset,
        mcro:YahooAnswersTitleQuestionDataset ;
    mcro:hasModelArchitecture mcro:allmpnetbasev2-ModelArchitectureInformationSection ;
    mcro:hasModelDetail mcro:allmpnetbasev2-ModelDetailSection ;
    mcro:hasTrainingData mcro:allmpnetbasev2-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:allmpnetbasev2-UseCaseInformationSection .

mcro:bertbaseuncased a mcro:Model ;
    mcro:hasLimitations mcro:bertbaseuncased-LimitationsInformationSection ;
    mcro:hasModelDetail mcro:bertbaseuncased-ModelDetailSection ;
    mcro:hasReference mcro:bertbaseuncased-ReferenceInformationSection ;
    mcro:hasTrainingData mcro:bertbaseuncased-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:bertbaseuncased-UseCaseInformationSection .

mcro:clip a mcro:Model ;
    mcro:hasLimitation mcro:clip-LimitationsInformationSection ;
    mcro:hasModelDetail mcro:clip-ModelDetailSection ;
    mcro:hasPerformanceMetric mcro:clip-PerformanceMetricInformationSection ;
    mcro:hasReference mcro:clip-ReferenceInformationSection ;
    mcro:hasTrainingData mcro:clip-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:clip-UseCaseInformationSection .

mcro:clip-AccuracyInformationSection a mcro:AccuracyInformationSection ;
    prov1:hasTextValue "Accuracy: Not specified" .

mcro:clip-AreaUnderTheCurveInformationSection a mcro:AreaUnderTheCurveInformationSection ;
    prov1:hasTextValue "Area Under the Curve: Not specified" .

mcro:clip-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Citation text: Not specified" .

mcro:clip-ConsiderationInformationSection a mcro:ConsiderationInformationSection ;
    prov1:hasTextValue "Considerations: Not specified" .

mcro:clip-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Datasets: YFCC100M" .

mcro:clip-EthicalConsiderationSection a mcro:EthicalConsiderationSection ;
    prov1:hasTextValue "Ethical consideration: Not specified" .

mcro:clip-EvaluationDataInformationSection a mcro:EvaluationDataInformationSection ;
    prov1:hasTextValue "Evaluation Data: Not specified" .

mcro:clip-F1ScoreInformationSection a mcro:F1ScoreInformationSection ;
    prov1:hasTextValue "F1-Score Information: Not specified" .

mcro:clip-FalseDiscoveryRateInformationSection a mcro:FalseDiscoveryRateInformationSection ;
    prov1:hasTextValue "False Discovery Rate: Not specified" .

mcro:clip-FalseOmmissionRateSectionInformation a mcro:FalseOmmissionRateSectionInformation ;
    prov1:hasTextValue "False Ommission Rate Information: Not specified" .

mcro:clip-FalsePositiveRateInformationSection a mcro:FalsePositiveRateInformationSection ;
    prov1:hasTextValue "False Positive Rate Information: Not specified" .

mcro:clip-LicenseInformationSection a mcro:LicenseInformationSection ;
    prov1:hasTextValue "License: Not specified" .

mcro:clip-LossInformationSection a mcro:LossInformationSection ;
    prov1:hasTextValue "Loss: Not specified" .

mcro:clip-MitigationStrategySection a mcro:MitigationStrategySection ;
    prov1:hasTextValue "Mitigation strategy: Not specified" .

mcro:clip-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Model architecture: The base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss;The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer." .

mcro:clip-OutofScopeUseCaseInformationSection a mcro:OutofScopeUseCaseInformationSection ;
    prov1:hasTextValue "Out-of-scope use cases: Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIPâ€™s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases." .

mcro:clip-PercisionRecallCurveInformationSection a mcro:PercisionRecallCurveInformationSection ;
    prov1:hasTextValue "Percision-Recall Curve Information: Not specified" .

mcro:clip-PrimaryIntendedUseCaseInformationSection a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue "Primary intended use: Not specified" .

mcro:clip-PrimaryIntendedUserInformationSection a mcro:PrimaryIntendedUserInformationSection ;
    prov1:hasTextValue "Primary Intended User: The primary intended users of these models are AI researchers. We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models." .

mcro:clip-QuantativeAnalysisSection a mcro:QuantativeAnalysisSection ;
    prov1:hasTextValue "Quantitative Analysis: We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets: - Food101 - CIFAR10   - CIFAR100   - Birdsnap - SUN397 - Stanford Cars - FGVC Aircraft - VOC2007 - DTD - Oxford-IIIT Pet dataset - Caltech101 - Flowers102 - MNIST   - SVHN  - IIIT5K   - Hateful Memes   - SST-2 - UCF101 - Kinetics700 - Country211 - CLEVR Counting - KITTI Distance - STL-10 - RareAct - Flickr30 - MSCOCO - ImageNet - ImageNet-A - ImageNet-R - ImageNet Sketch - ObjectNet (ImageNet Overlap) - Youtube-BB - ImageNet-Vid" .

mcro:clip-RiskInformationSection a mcro:RiskInformationSection ;
    prov1:hasTextValue "Risks: Not specified" .

mcro:clip-TradeoffInformationSection a mcro:TradeoffInformationSection ;
    prov1:hasTextValue "Trade-off information: Not specified" .

mcro:clip-UserInformationSection a mcro:UserInformationSection ;
    prov1:hasTextValue "User Information: Not specified" .

mcro:clip-VersionInformationSection a mcro:VersionInformationSection ;
    prov1:hasTextValue "Version: Not specified" .

mcro:clipModel a mcro:Model ;
    mcro:hasDataset mcro:clipModel-DatasetInformationSection ;
    mcro:hasLimitation mcro:clipModel-LimitationsSection ;
    mcro:hasMetrics mcro:clipModel-MetricsInformationSection ;
    mcro:hasModelDetail mcro:clipModel-ModelDetailSection ;
    mcro:hasTrainingData mcro:clipModel-TrainingDataSection ;
    mcro:hasUseCase mcro:clipModel-UseCaseSection .

mcro:dima806fairfaceageimagedetection a mcro:Model ;
    mcro:hasAccuracy mcro:dima806fairfaceageimagedetection-AccuracyInformationSection ;
    mcro:hasMetrics mcro:dima806fairfaceageimagedetection-MetricsInformationSection ;
    mcro:hasReference mcro:dima806fairfaceageimagedetection-ReferenceInformationSection .

mcro:mobilenetv3small100lambin1k a mcro:Model ;
    mcro:hasCitation mcro:mobilenetv3small100lambin1k-CitationInformationSection2 ;
    mcro:hasDataset mcro:mobilenetv3small100lambin1k-DatasetInformationSection ;
    mcro:hasModelArchitecture mcro:mobilenetv3small100lambin1k-ModelArchitectureInformationSection ;
    mcro:hasModelDetail mcro:mobilenetv3small100lambin1k-ModelDetailSection ;
    mcro:hasReference mcro:mobilenetv3small100lambin1k-ReferenceInformationSection ;
    mcro:hasTrainingData mcro:mobilenetv3small100lambin1k-TrainingDataInformationSection ;
    mcro:hasUseCase mcro:mobilenetv3small100lambin1k-UseCaseInformationSection .

mcro:AllNLIDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: AllNLI ([SNLI] and [MultiNLI])" .

mcro:AltlexDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Altlex" .

mcro:COCOImageCaptionsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: COCO Image captions" .

mcro:CodeSearchDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Code Search" .

mcro:Eli5Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Eli5" .

mcro:Falconsainsfwimagedetection-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Datasets: ImageNet-21k, proprietary dataset" .

mcro:Falconsainsfwimagedetection-LimitationsInformationSection a mcro:LimitationsInformationSection ;
    prov1:hasTextValue "Limitations: Specialized Task Fine-Tuning - While the model is adept at NSFW image classification, its performance may vary when applied to other tasks.; Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results." .

mcro:Falconsainsfwimagedetection-MetricsInformationSection a mcro:MetricsInformationSection ;
    prov1:hasTextValue "Training Stats: eval_loss: 0.07463177293539047, eval_accuracy: 0.980375" .

mcro:Falconsainsfwimagedetection-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Model architecture: Vision Transformer (ViT)" .

mcro:Falconsainsfwimagedetection-ModelDetailSection a mcro:ModelDetailSection ;
    prov1:hasTextValue "Model Description: The Fine-Tuned Vision Transformer (ViT) is a variant of the transformer encoder architecture, similar to BERT, that has been adapted for image classification tasks. This specific model, named \"google/vit-base-patch16-224-in21k,\" is pre-trained on a substantial collection of images in a supervised manner, leveraging the ImageNet-21k dataset. The images in the pre-training dataset are resized to a resolution of 224x224 pixels, making it suitable for a wide range of image recognition tasks.;Hyperparameter Settings: The model was fine-tuned with a judiciously chosen batch size of 16. To facilitate this fine-tuning process, a learning rate of 5e-5 was employed.;Training Process: This training phase was executed using a proprietary dataset containing an extensive collection of 80,000 images, each characterized by a substantial degree of variability. The dataset was thoughtfully curated to include two distinct classes, namely \"normal\" and \"nsfw.\" This diversity allowed the model to grasp nuanced visual patterns, equipping it with the competence to accurately differentiate between safe and explicit content." .

mcro:Falconsainsfwimagedetection-ReferenceInformationSection a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "References: Hugging Face Model Hub - [https://huggingface.co/models]; Vision Transformer (ViT) Paper - [https://arxiv.org/abs/2010.11929]; ImageNet-21k Dataset - [http://www.image-net.org/]" .

mcro:Falconsainsfwimagedetection-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Training Data: The model's training data includes a proprietary dataset comprising approximately 80,000 images. This dataset encompasses a significant amount of variability and consists of two distinct classes: \"normal\" and \"nsfw.\" The training process on this data aimed to equip the model with the ability to distinguish between safe and explicit content effectively." .

mcro:Falconsainsfwimagedetection-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Intended Uses: NSFW Image Classification - The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications." .

mcro:Flickr30kDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Flickr 30k" .

mcro:GOOAQDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: GOOAQ: Open Question Answering with Diverse Answer Types" .

mcro:MSMARCOTripletsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: MS MARCO triplets" .

mcro:NaturalQuestionsNQDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Natural Questions (NQ)" .

mcro:PAQDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: PAQ (Question, Answer) pairs" .

mcro:QuoraQuestionTripletsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Quora Question Triplets" .

mcro:RedditCommentsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Reddit comments (2015-2018)" .

mcro:S2ORCCitationPairsAbstractsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: S2ORC Citation pairs (Abstracts)" .

mcro:S2ORCCitationPairsTitlesDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: S2ORC Citation pairs (Titles)" .

mcro:S2ORCTitleAbstractDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: S2ORC (Title, Abstract)" .

mcro:SPECTERCitationTripletsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: SPECTER citation triplets" .

mcro:SQuAD20Dataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: SQuAD2.0" .

mcro:SearchQADataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: SearchQA" .

mcro:SentenceCompressionDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Sentence Compression" .

mcro:SimpleWikipediaDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Simple Wikipedia" .

mcro:StackExchangeDuplicateQuestionsBodiesDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Stack Exchange Duplicate questions (bodies)" .

mcro:StackExchangeDuplicateQuestionsTitlesBodiesDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Stack Exchange Duplicate questions (titles+bodies)" .

mcro:StackExchangeDuplicateQuestionsTitlesDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Stack Exchange Duplicate questions (titles)" .

mcro:StackExchangeTitleAnswerPairsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Stack Exchange (Title, Answer) pairs" .

mcro:StackExchangeTitleBodyAnswerPairsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Stack Exchange (Title+Body, Answer) pairs" .

mcro:StackExchangeTitleBodyPairsDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Stack Exchange (Title, Body) pairs" .

mcro:TriviaQADataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: TriviaQA" .

mcro:WikiAnswersDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: WikiAnswers Duplicate question pairs" .

mcro:WikihowDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Wikihow" .

mcro:YOLOv8DetectionModel-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Face Datasets: Anime Face CreateML, xml2txt, AN, wider face; Hand Datasets: AnHDet, hand-detection-fuao9; Person Datasets: coco2017, AniSeg, skytnt/anime-segmentation; deepfashion2 Dataset: deepfashion2" .

mcro:YOLOv8DetectionModel-LimitationsInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Unsafe files: Since `getattr` is classified as a dangerous pickle function, any segmentation model that uses it is classified as unsafe. All models were created and saved using the official ultralytics library, so it's okay to use files downloaded from a trusted source. See also: https://huggingface.co/docs/hub/security-pickle" .

mcro:YOLOv8DetectionModel-MetricsInformationSection a mcro:MetricsInformationSection ;
    prov1:hasTextValue "Model Performance: face_yolov8n.pt: mAP 50 = 0.660, mAP 50-95 = 0.366; face_yolov8n_v2.pt: mAP 50 = 0.669, mAP 50-95 = 0.372; face_yolov8s.pt: mAP 50 = 0.713, mAP 50-95 = 0.404; face_yolov8m.pt: mAP 50 = 0.737, mAP 50-95 = 0.424; face_yolov9c.pt: mAP 50 = 0.748, mAP 50-95 = 0.433; hand_yolov8n.pt: mAP 50 = 0.767, mAP 50-95 = 0.505; hand_yolov8s.pt: mAP 50 = 0.794, mAP 50-95 = 0.527; hand_yolov9c.pt: mAP 50 = 0.810, mAP 50-95 = 0.550; person_yolov8n-seg.pt: mAP 50 (bbox) = 0.782, mAP 50 (mask) = 0.761, mAP 50-95 (bbox) = 0.555, mAP 50-95 (mask) = 0.460; person_yolov8s-seg.pt: mAP 50 (bbox) = 0.824, mAP 50 (mask) = 0.809, mAP 50-95 (bbox) = 0.605, mAP 50-95 (mask) = 0.508; person_yolov8m-seg.pt: mAP 50 (bbox) = 0.849, mAP 50 (mask) = 0.831, mAP 50-95 (bbox) = 0.636, mAP 50-95 (mask) = 0.533; deepfashion2_yolov8s-seg.pt: mAP 50 (bbox) = 0.849, mAP 50 (mask) = 0.840, mAP 50-95 (bbox) = 0.763, mAP 50-95 (mask) = 0.675" .

mcro:YOLOv8DetectionModel-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Usage: See model page" .

mcro:YahooAnswersQuestionAnswerDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Yahoo Answers (Question, Answer)" .

mcro:YahooAnswersTitleAnswerDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Yahoo Answers (Title, Answer)" .

mcro:YahooAnswersTitleQuestionDataset a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: Yahoo Answers (Title, Question)" .

mcro:allMiniLML6v2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Model architecture: We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a 1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset." .

mcro:allMiniLML6v2-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasIntendedUseCase mcro:allMiniLML6v2-UseCaseInformationSection ;
    mcro:hasModelArchitecture mcro:allMiniLML6v2-ModelArchitectureInformationSection ;
    mcro:hasTrainingData mcro:allMiniLML6v2-TrainingDataInformationSection ;
    prov1:hasTextValue "Model detail: This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:allMiniLML6v2-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Training data: We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences. We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file." .

mcro:allMiniLML6v2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Intended use case: Our model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks. By default, input text longer than 256 word pieces is truncated." .

mcro:allmpnetbasev2-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Model architecture: We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a 1B sentence pairs dataset." .

mcro:allmpnetbasev2-ModelDetailSection a mcro:ModelDetailSection ;
    prov1:hasTextValue "Model detail: This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search." .

mcro:allmpnetbasev2-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue """Training data: We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.
We sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.;Dataset: [Reddit comments (2015-2018)], Number of training tuples: 726,484,430;Dataset: [S2ORC] Citation pairs (Abstracts), Number of training tuples: 116,288,806;Dataset: [WikiAnswers] Duplicate question pairs, Number of training tuples: 77,427,422;Dataset: [PAQ] (Question, Answer) pairs, Number of training tuples: 64,371,441;Dataset: [S2ORC] Citation pairs (Titles), Number of training tuples: 52,603,982;Dataset: [S2ORC] (Title, Abstract), Number of training tuples: 41,769,185;Dataset: [Stack Exchange] (Title, Body) pairs, Number of training tuples: 25,316,456;Dataset: [Stack Exchange] (Title+Body, Answer) pairs, Number of training tuples: 21,396,559;Dataset: [Stack Exchange] (Title, Answer) pairs, Number of training tuples: 21,396,559;Dataset: [MS MARCO] triplets, Number of training tuples: 9,144,553;Dataset: [GOOAQ: Open Question Answering with Diverse Answer Types], Number of training tuples: 3,012,496;Dataset: [Yahoo Answers] (Title, Answer), Number of training tuples: 1,198,260;Dataset: [Code Search], Number of training tuples: 1,151,414;Dataset: [COCO] Image captions, Number of training tuples: 828,395;Dataset: [SPECTER] citation triplets, Number of training tuples: 684,100;Dataset: [Yahoo Answers] (Question, Answer), Number of training tuples: 681,164;Dataset: [Yahoo Answers] (Title, Question), Number of training tuples: 659,896;Dataset: [SearchQA], Number of training tuples: 582,261;Dataset: [Eli5], Number of training tuples: 325,475;Dataset: [Flickr 30k], Number of training tuples: 317,695;Dataset: [Stack Exchange] Duplicate questions (titles), Number of training tuples: 304,525;Dataset: AllNLI ([SNLI] and [MultiNLI], Number of training tuples: 277,230;Dataset: [Stack Exchange] Duplicate questions (bodies), Number of training tuples: 250,519;Dataset: [Stack Exchange] Duplicate questions (titles+bodies), Number of training tuples: 250,460;Dataset: [Sentence Compression], Number of training tuples: 180,000;Dataset: [Wikihow], Number of training tuples: 128,542;Dataset: [Altlex], Number of training tuples: 112,696;Dataset: [Quora Question Triplets], Number of training tuples: 103,663;Dataset: [Simple Wikipedia], Number of training tuples: 102,225;Dataset: [Natural Questions (NQ)], Number of training tuples: 100,231;Dataset: [SQuAD2.0], Number of training tuples: 87,599;Dataset: [TriviaQA], Number of training tuples: 73,346;Total tuples: 1,170,060,424""" .

mcro:allmpnetbasev2-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue """Intended use case: Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.

By default, input text longer than 384 word pieces is truncated.""" .

mcro:bertbaseuncased-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Citation: this paper(https://arxiv.org/abs/1810.04805); this repository(https://github.com/google-research/bert)" .

mcro:bertbaseuncased-LimitationsInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Limitations: this model can have biased predictions; This bias will also affect all fine-tuned versions of this model." .

mcro:bertbaseuncased-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Model architecture: transformers" .

mcro:bertbaseuncased-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:bertbaseuncased-CitationInformationSection ;
    mcro:hasModelArchitecture mcro:bertbaseuncased-ModelArchitectureInformationSection ;
    prov1:hasTextValue "Model detail: BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion;This model is uncased: it does not make a difference between english and English" .

mcro:bertbaseuncased-ReferenceInformationSection a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "Reference: See model page" .

mcro:bertbaseuncased-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Training data: BookCorpus; English Wikipedia" .

mcro:bertbaseuncased-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Intended use case: masked language modeling; next sentence prediction; fine-tuned on a downstream task; sequence classification, token classification or question answering" .

mcro:clip-LimitationsInformationSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Limitations: CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance." .

mcro:clip-ModelDetailSection a mcro:ModelDetailSection ;
    prov1:hasTextValue "Model detail: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context theyâ€™re being deployed within." .

mcro:clip-PerformanceMetricInformationSection a mcro:PerformanceMetricInformationSection ;
    prov1:hasTextValue "Performance Metric Information: See Quantative Analysis Section" .

mcro:clip-ReferenceInformationSection a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "Reference: - [Blog Post](https://openai.com/blog/clip/) - [CLIP Paper](https://arxiv.org/abs/2103.00020)" .

mcro:clip-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Training data: The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users." .

mcro:clip-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Intended use case: The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis." .

mcro:clipModel-ArchitectureSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Model architecture: ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder; These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss; The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer." .

mcro:clipModel-CitationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Citation: Blog Post - https://openai.com/blog/clip/; CLIP Paper - https://arxiv.org/abs/2103.00020" .

mcro:clipModel-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Datasets: Food101, CIFAR10, CIFAR100, Birdsnap, SUN397, Stanford Cars, FGVC Aircraft, VOC2007, DTD, Oxford-IIIT Pet dataset, Caltech101, Flowers102, MNIST, SVHN, IIIT5K, Hateful Memes, SST-2, UCF101, Kinetics700, Country211, CLEVR Counting, KITTI Distance, STL-10, RareAct, Flickr30, MSCOCO, ImageNet, ImageNet-A, ImageNet-R, ImageNet Sketch, ObjectNet (ImageNet Overlap), Youtube-BB, ImageNet-Vid, Fairface" .

mcro:clipModel-LimitationsSection a mcro:LimitationInformationSection ;
    prov1:hasTextValue "Limitations: CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance." .

mcro:clipModel-MetricsInformationSection a mcro:MetricsInformationSection ;
    prov1:hasTextValue "Performance Metrics: accuracy >96% across all races for gender classification with â€˜Middle Easternâ€™ having the highest accuracy (98.4%) and â€˜Whiteâ€™ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification." .

mcro:clipModel-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:clipModel-CitationSection ;
    mcro:hasModelArchitecture mcro:clipModel-ArchitectureSection ;
    prov1:hasTextValue "Model detail: The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context theyâ€™re being deployed within." .

mcro:clipModel-OutOfScopeUseCaseSection a mcro:OutofScopeUseCaseSectionInformation ;
    prov1:hasTextValue "Out-of-Scope Use Cases: Any deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIPâ€™s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. Certain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use. Since the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases." .

mcro:clipModel-PrimaryIntendedUseSection a mcro:PrimaryIntendedUseCaseInformationSection ;
    prov1:hasTextValue "Primary intended uses: The primary intended users of these models are AI researchers; We primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models." .

mcro:clipModel-TrainingDataSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Training data: The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users." .

mcro:clipModel-UseCaseSection a mcro:UseCaseInformationSection ;
    mcro:hasOutOfScopeUseCase mcro:clipModel-OutOfScopeUseCaseSection ;
    mcro:hasPrimaryIntendedUse mcro:clipModel-PrimaryIntendedUseSection ;
    prov1:hasTextValue "Intended Use: The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis." .

mcro:dima806fairfaceageimagedetection-AccuracyInformationSection a mcro:AccuracyInformationSection ;
    prov1:hasTextValue "Accuracy: Detects age group with about 59% accuracy based on an image." .

mcro:dima806fairfaceageimagedetection-MetricsInformationSection a mcro:MetricsInformationSection ;
    prov1:hasTextValue "Classification report: precision, recall, f1-score, support for age groups 0-2, 3-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, more than 70. accuracy: 0.5892, macro avg: precision 0.5971, recall 0.5303, f1-score 0.5459, weighted avg: precision 0.5863, recall 0.5892, f1-score 0.5844" .

mcro:dima806fairfaceageimagedetection-ReferenceInformationSection a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "Reference: See https://www.kaggle.com/code/dima806/age-group-image-classification-vit for details." .

mcro:mobilenetv3small100lambin1k-CitationInformationSection a mcro:CitationInformationSection ;
    prov1:hasTextValue "Papers: Searching for MobileNetV3: https://arxiv.org/abs/1905.02244; Original: https://github.com/huggingface/pytorch-image-models" .

mcro:mobilenetv3small100lambin1k-CitationInformationSection2 a mcro:CitationInformationSection ;
    prov1:hasTextValue "Citation: @misc{rw2019timm, author = {Ross Wightman}, title = {PyTorch Image Models}, year = {2019}, publisher = {GitHub}, journal = {GitHub repository}, doi = {10.5281/zenodo.4414861}, howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}} }; @inproceedings{howard2019searching, title={Searching for mobilenetv3}, author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others}, booktitle={Proceedings of the IEEE/CVF international conference on computer vision}, pages={1314--1324}, year={2019} }" .

mcro:mobilenetv3small100lambin1k-DatasetInformationSection a mcro:DatasetInformationSection ;
    prov1:hasTextValue "Dataset name: ImageNet-1k" .

mcro:mobilenetv3small100lambin1k-ModelArchitectureInformationSection a mcro:ModelArchitectureInformationSection ;
    prov1:hasTextValue "Model architecture: MobileNet-v3 image classification model" .

mcro:mobilenetv3small100lambin1k-ModelDetailSection a mcro:ModelDetailSection ;
    mcro:hasCitation mcro:mobilenetv3small100lambin1k-CitationInformationSection ;
    prov1:hasTextValue "Model type: Image classification / feature backbone; Model Stats: Params (M): 2.5, GMACs: 0.1, Activations (M): 1.4, Image size: 224 x 224; Dataset: ImageNet-1k" .

mcro:mobilenetv3small100lambin1k-ReferenceInformationSection a mcro:ReferenceInformationSection ;
    prov1:hasTextValue "Reference: Explore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results)." .

mcro:mobilenetv3small100lambin1k-TrainingDataInformationSection a mcro:TrainingDataInformationSection ;
    prov1:hasTextValue "Training data: Trained on ImageNet-1k in `timm` using recipe template described below.; Recipe details: A LAMB optimizer recipe that is similar to [ResNet Strikes Back](https://arxiv.org/abs/2110.00476) `A2` but 50% longer with EMA weight averaging, no CutMix; RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging; Step (exponential decay w/ staircase) LR schedule with warmup" .

mcro:mobilenetv3small100lambin1k-UseCaseInformationSection a mcro:UseCaseInformationSection ;
    prov1:hasTextValue "Intended use case: Image Classification, Feature Map Extraction, Image Embeddings" .

