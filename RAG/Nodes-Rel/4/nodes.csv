:ID,:LABEL,properties
mcro_convnextv2_nano.fcmae_ft_in22kin1k-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}'}"
mcro_flan-t5-large,mcro_Model,{}
mcro_flan-t5-large-ModelDetail,mcro_ModelDetailSection,{}
mcro_Qwen3-8B,mcro_Model,{}
mcro_intfloate5mistral7binstruct,mcro_Model,{}
mcro_intfloate5mistral7binstruct-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': ""Using this model for inputs longer than 4096 tokens is not recommended.\n\nThis model's multilingual capability is still inferior to [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) for some cases.""}"
mcro_distilrobertabase-Dataset,mcro_DatasetInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53chinesezhcn,mcro_Model,{}
mcro_EleutherAIpythia70mdeduped-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'the Pile after the dataset has been globally deduplicated'}
mcro_example_model-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'mit'}
mcro_FacebookAIxlmrobertalarge-UseCase,mcro_UseCaseInformationSection,{}
mcro_pyannotespeakerdiarization31,mcro_Model,{}
mcro_pyannotespeakerdiarization31-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_stabilityaisdturbo,mcro_Model,{}
mcro_stabilityaisdturbo-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'Faces and people in general may not be generated properly.; The generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.; The model cannot render legible text.; The quality and prompt alignment is lower than that of [SDXL-Turbo](https://huggingface.co/stabilityai/sdxl-turbo).'}"
mcro_BRIA_Background_Removal_v14-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Bria-RMBG model was trained with over 12,000 high-quality, high-resolution, manually labeled (pixel-wise accuracy), fully licensed images.\nOur benchmark included balanced gender, balanced ethnicity, and people with different types of disabilities.\nFor clarity, we provide our data distribution according to different categories, demonstrating our model’s versatility.'}"
mcro_emimodel-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Latent Diffusion Model'}
mcro_vitbasepatch8224augreg2in21kftin1k,mcro_Model,{}
mcro_sentencetransformersdistilusebasemultilingualcasedv2,mcro_Model,{}
mcro_sentencetransformersdistilusebasemultilingualcasedv2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_clipvitlargepatch14,mcro_Model,{}
mcro_clipvitlargepatch14-Consideration,mcro_ConsiderationInformationSection,{}
mcro_ModelCardReport-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_MaziyarPanahiPhi35miniinstructGGUF,mcro_Model,{}
mcro_MaziyarPanahiPhi35miniinstructGGUF-ModelDetail,mcro_ModelDetailSection,{}
mcro_sentencetransformersdistilusebasemultilingualcasedv1,mcro_Model,{}
mcro_microsoftmdebertav3base-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'This model was trained using the 2.5T CC100 data as XLM-R.'}
mcro_flan-t5-large-UseCase,mcro_UseCaseInformationSection,{}
mcro_resnet18a1in1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_resnet18a1in1k-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'ReLU activations; 1x1 convolution shortcut downsample; single layer 7x7 convolution with pooling'}
mcro_USERbgem3,mcro_Model,{}
mcro_RoBERTaBaseModel-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'OpenWebText; CC-News; English Wikipedia; Stories; BookCorpus'}
mcro_distilbertbasemultilingualcased-ModelDetail,mcro_ModelDetailSection,{}
mcro_distilbertbasemultilingualcased-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}'}"
mcro_germansentimentbert-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@InProceedings{guhr-EtAl:2020:LREC,\n  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  Böhme, Hans Joachim},\n  title     = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},\n  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},\n  month          = {May},\n  year           = {2020},\n  address        = {Marseille, France},\n  publisher      = {European Language Resources Association},\n  pages     = {1620--1625},\n  url       = {https://www.aclweb.org/anthology/2020.lrec-1.202}\n}'}"
mcro_distilbertbasemultilingualcased-TrainingParameter,mcro_ModelParameterSection,"{'prov_hasTextValue': '- The model was pretrained with the supervision of [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) on the concatenation of Wikipedia in 104 different languages\n- The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters.\n- Further information about the training procedure and data is included in the [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) model card.'}"
mcro_sentencetransformersLaBSE-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'It can be used to map 109 languages to a shared vector space.'}
mcro_chronos-t5-small,mcro_Model,{}
mcro_tabletransformerfinetunedfortablestructurerecognition,mcro_Model,{}
mcro_tabletransformerfinetunedfortablestructurerecognition-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'You can use the raw model for detecting the structure (like rows, columns) in tables. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer) for more info.'}"
mcro_Qwen3-4B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Causal Language Models'}
mcro_parakeettdt06bv2-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '**Architecture Type**:\n\nFastConformer-TDT\n\n**Network Architecture**:\n\n* This model was developed based on [FastConformer encoder](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) architecture[1] and TDT decoder[2]\n* This model has 600 million model parameters.'}
mcro_stabilityaisdturbo-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Generation of artworks and use in design and other artistic processes.; Research on real-time applications of generative models.; Research on generative models.; Safe deployment of models which have the potential to generate harmful content.'}
mcro_crossencodernlidebertav3base,mcro_Model,{}
mcro_crossencodernlidebertav3base-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model can also be used for zero-shot-classification:'}
mcro_ibmresearchMoLFormerXLboth10pct-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'This model is not intended for molecule generation. It is also not tested for molecules larger than ~200 atoms (i.e., macromolecules). Furthermore, using invalid or noncanonical SMILES may result in worse performance.'}"
mcro_testModel-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CNN'}
mcro_ai4bharatindictrans2indicen1B,mcro_Model,{}
mcro_ai4bharatindictrans2indicen1B-UseCase,mcro_UseCaseInformationSection,{}
mcro_Falconsainsfwimagedetection,mcro_Model,{}
mcro_Falconsainsfwimagedetection-ModelDetail,mcro_ModelDetailSection,{}
mcro_trpakovvitfaceexpression-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Vision Transformer (ViT)'}
mcro_byt5small-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'ByT5: Towards a token-free future with pre-trained byte-to-byte models'}
mcro_RobertaLargeMnli,mcro_Model,{}
mcro_RobertaLargeMnli-EvaluationData,mcro_DatasetInformationSection,{}
mcro_huggingquantsMetaLlama318BInstructGPTQINT4-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Meta Llama 3.1'}
mcro_XuhuiToxDectrobertalarge,mcro_Model,{}
mcro_XuhuiToxDectrobertalarge-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Roberta-large'}
mcro_microsoftFlorence2large,mcro_Model,{}
mcro_microsoftFlorence2large-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks. Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model.""}"
mcro_rubertbasecased-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'RuBERT (Russian, cased, 12‑layer, 768‑hidden, 12‑heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT‑base as an initialization for RuBERT[1].\n\n08.11.2021: upload model with MLM and NSP heads'}"
mcro_sentencetransformersdistilusebasemultilingualcasedv1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})""}"
mcro_tabletransformerfinetunedfortabledetection-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'detecting tables in documents'}
mcro_efficientnet_b3.ra2_in1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_efficientnet_b3.ra2_in1k-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'EfficientNet'}
mcro_Distilbertbasemultilingualcasednerhrl-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Europeana Newspapers; conll 2003; Latvian NER; conll 2002; ANERcorp; Paramopama + Second Harem; MSRA; Italian I-CAB'}
mcro_deepseekaiDeepSeekV3-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Innovative Load Balancing Strategy and Training Objective\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration.'}"
mcro_wavlmlarge-ModelDetail,mcro_ModelDetailSection,{}
mcro_mistralaiMixtral8x7BInstructv01-ConsiderationSection,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': ""It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.""}"
mcro_speechbrainspkrececapavoxceleb-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'attentive statistical pooling; convolutional and residual blocks; Additive Margin Softmax Loss; ECAPA-TDNN'}
mcro_audeeringwav2vec2largerobust12ftemotionmspdim-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'dimensional speech emotion recognition'}
mcro_mmlwretrievalrobertalarge-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_intfloatmultilinguale5small-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Multilingual E5 Text Embeddings: A Technical Report'}
mcro_llavamodelcard-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'https://llava-vl.github.io/; LLaVA-v1.5-7B was trained in September 2023.; LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.\nIt is an auto-regressive language model, based on the transformer architecture.'}"
mcro_intfloatmultilinguale5largeinstruct,mcro_Model,{}
mcro_intfloatmultilinguale5largeinstruct-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': 'Long texts will be truncated to at most 512 tokens.'}
mcro_whisper-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_robertBase-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'RoTex; Oscar; RoWiki'}
mcro_jonatasgrosmanwav2vec2largexlsr53polish,mcro_Model,{}
mcro_whisper,mcro_Model,{}
mcro_whisper-ModelDetail,mcro_ModelDetailSection,{}
mcro_microsoftmdebertav3base-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'mDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.\nThe mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R.'}
mcro_multilinguale5large-TrainingDetails,mcro_DatasetInformationSection,"{'prov_hasTextValue': '**Initialization**: [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)\n\n**First stage**: contrastive pre-training with weak supervision\n\n| Dataset                                                                                                | Weak supervision                      | # of text pairs |\n|--------------------------------------------------------------------------------------------------------|---------------------------------------|-----------------|\n| Filtered [mC4](https://huggingface.co/datasets/mc4)                                                    | (title, page content)                 | 1B              |\n| [CC News](https://huggingface.co/datasets/intfloat/multilingual_cc_news)                               | (title, news content)                 | 400M            |\n| [NLLB](https://huggingface.co/datasets/allenai/nllb)                                                   | translation pairs                     | 2.4B            |\n| [Wikipedia](https://huggingface.co/datasets/intfloat/wikipedia)                                        | (hierarchical section title, passage) | 150M            |\n| Filtered [Reddit](https://www.reddit.com/)                                                             | (comment, response)                   | 800M            |\n| [S2ORC](https://github.com/allenai/s2orc)                                                              | (title, abstract) and citation pairs  | 100M            |\n| [Stackexchange](https://stackexchange.com/)                                                            | (question, answer)                    | 50M             |\n| [xP3](https://huggingface.co/datasets/bigscience/xP3)                                                  | (input prompt, response)              | 80M             |\n| [Miscellaneous unsupervised SBERT data](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | -                                     | 10M             |\n\n**Second stage**: supervised fine-tuning\n\n| Dataset                                                                                | Language     | # of text pairs |\n|----------------------------------------------------------------------------------------|--------------|-----------------|\n| [MS MARCO](https://microsoft.github.io/msmarco/)                                       | English      | 500k            |\n| [NQ](https://github.com/facebookresearch/DPR)                                          | English      | 70k             |\n| [Trivia QA](https://github.com/facebookresearch/DPR)                                   | English      | 60k             |\n| [NLI from SimCSE](https://github.com/princeton-nlp/SimCSE)                             | English      | <300k           |\n| [ELI5](https://huggingface.co/datasets/eli5)                                           | English      | 500k            |\n| [DuReader Retrieval](https://github.com/baidu/DuReader/tree/master/DuReader-Retrieval) | Chinese      | 86k             |\n| [KILT Fever](https://huggingface.co/datasets/kilt_tasks)                               | English      | 70k             |\n| [KILT HotpotQA](https://huggingface.co/datasets/kilt_tasks)                            | English      | 70k             |\n| [SQuAD](https://huggingface.co/datasets/squad)                                         | English      | 87k             |\n| [Quora](https://huggingface.co/datasets/quora)                                         | English      | 150k            |\n| [Mr. TyDi](https://huggingface.co/datasets/castorini/mr-tydi)                                                                           | 11 languages | 50k             |\n| [MIRACL](https://huggingface.co/datasets/miracl/miracl)                                                                             | 16 languages | 40k             |\n\nFor all labeled datasets, we only use its training set for fine-tuning.\n\nFor other training details, please refer to our paper at [https://arxiv.org/pdf/2402.05672](https://arxiv.org/pdf/2402.05672).'}"
mcro_bertbaseuncased,mcro_Model,{}
mcro_bertbaseuncased-ModelDetail,mcro_ModelDetailSection,{}
mcro_BAAIbgebaseen,mcro_Model,{}
mcro_BAAIbgebaseen-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.'}
mcro_rbhatia46financialragmatryoshka-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'financial use-cases'}
mcro_QwenQwen2VL7BInstruct,mcro_Model,{}
mcro_QwenQwen2VL7BInstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_Xlmrobertalargenerspanish-Performance,mcro_PerformanceMetricInformationSection,{'prov_hasTextValue': 'F1-score of 89.17'}
mcro_ibmresearchMoLFormerXLboth10pct-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Molecules were canonicalized with RDKit prior to training and isomeric information was removed. Also, molecules longer than 202 tokens were dropped.'}"
mcro_wavlmlarge-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing'}
mcro_flan-t5-base-Citation,mcro_CitationInformationSection,{}
mcro_Auralis-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Text-to-Speech (TTS) generation for real-world applications, including books, dialogues, and multilingual tasks.'}"
mcro_resnet18a1in1k,mcro_Model,{}
mcro_potion-base-8M-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@software{minishlab2024model2vec,\n  authors = {Stephan Tulkens, Thomas van Dongen},\n  title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n  year = {2024},\n  url = {https://github.com/MinishLab/model2vec},\n}'}"
mcro_Whisperlargev2modelforCTranslate2,mcro_Model,{}
mcro_Whisperlargev2modelforCTranslate2-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.'}
mcro_intfloatmultilinguale5small-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': 'Long texts will be truncated to at most 512 tokens.'}
mcro_gpt2,mcro_Model,{}
mcro_gpt2-Consideration,mcro_ConsiderationInformationSection,{}
mcro_blackforestlabsFLUX1dev-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""The model and its derivatives may not be used\n\n- In any way that violates any applicable national, federal, state, local or international law or regulation.\n- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\n- To generate or disseminate verifiably false information and/or content with the purpose of harming others.\n- To generate or disseminate personal identifiable information that can be used to harm an individual.\n- To harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\n- To create non-consensual nudity or illegal pornographic content.\n- For fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\n- Generating or facilitating large-scale disinformation campaigns.""}"
mcro_AlibabaNLPgtebaseenv15,mcro_Model,{}
mcro_sentencetransformersbertbasenlimeantokens-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_Auralis-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{auralis2024,\n  author = {AstraMind AI},\n  title = {Auralis: High-Performance Text-to-Speech Engine},\n  year = {2024},\n  url = {https://huggingface.co/AstraMindAI/auralis}\n}'}"
mcro_indonesianrobertabaseposptagger-License,mcro_LicenseInformationSection,{}
mcro_chronosboltbase,mcro_Model,{}
mcro_distilbertbasemultilingualcased-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{'prov_hasTextValue': 'The model developers report the following accuracy results for DistilmBERT (see [GitHub Repo](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)): \n\n> Here are the results on the test sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):\n\n| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |\n| :---:                        | :---:   | :---:   | :---:   | :---:  | :---:   | :---:|\n| mBERT base cased (computed)  | 82.1    | 74.6    | 69.1    | 72.3   | 66.4    | 58.5 |\n| mBERT base uncased (reported)| 81.4    | 74.3    | 63.8    | 70.5   | 62.1    | 58.3 |\n| DistilmBERT                  | 78.2    | 69.1    | 64.0    | 66.3   | 59.1    | 54.7 |'}
mcro_phobertpretrainedlanguagemodels-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'PhoBERT pre-training approach is based on [RoBERTa](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)  which optimizes the [BERT](https://github.com/google-research/bert) pre-training procedure for more robust performance.'}
mcro_IP-Adapter-FaceID-LimitationInformationSection,mcro_LimitationInformationSection,"{'prov_hasTextValue': '- The models do not achieve perfect photorealism and ID consistency.\n- The generalization of the models is limited due to limitations of the training data, base model and face recognition model.'}"
mcro_whisper-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_sdxlvae,mcro_Model,{}
mcro_sdxlvae-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'SDXL is a latent diffusion model, where the diffusion operates in a pretrained, \nlearned (and fixed) latent space of an autoencoder. \nWhile the bulk of the semantic composition is done by the latent diffusion model, \nwe can improve _local_, high-frequency details in generated images by improving the quality of the autoencoder. \nTo this end, we train the same autoencoder architecture used for the original Stable Diffusion at a larger batch-size (256 vs 9) \nand additionally track the weights with an exponential moving average (EMA). \nThe resulting autoencoder outperforms the original model in all evaluated reconstruction metrics, see the table below.'}"
mcro_bertbaseNER-EvalResults,mcro_QuantativeAnalysisSection,{}
mcro_cambridgeltlSapBERTfromPubMedBERTfulltext-ModelDetail,mcro_ModelDetailSection,{}
mcro_cambridgeltlSapBERTfromPubMedBERTfulltext-Citation,mcro_CitationInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53portuguese,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2largexlsr53portuguese-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{grosman2021xlsr53-large-portuguese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ortuguese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese}},\n  year={2021}\n}'}"
mcro_huggingquantsMetaLlama318BInstructGPTQINT4-ModelDetail,mcro_ModelDetailSection,{}
mcro_BAAIbgererankerlarge-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_ProtGPT2,mcro_Model,{}
mcro_SmolLM2-Limitations,mcro_LimitationInformationSection,{}
mcro_eng-zho,mcro_Model,{}
mcro_eng-zho-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformer'}
mcro_speechbrainspkrececapavoxceleb-ModelDetail,mcro_ModelDetailSection,{}
mcro_trpakovvitfaceexpression-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'FER2013'}
mcro_microsoftPhi35visioninstruct-ModelDetailSection,mcro_ModelDetailSection,"{'prov_hasTextValue': 'Phi-3.5-vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\n🏡 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n📰 [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\n📖 [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\n👩\u200d🍳 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n🖥️ [Try It](https://aka.ms/try-phi3.5vision) <br>\n\n**Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)'}"
mcro_CLIPViTB32LAION2B-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'OpenCLIP software; OpenAI CLIP paper'}
mcro_CryptoBERT,mcro_Model,{}
mcro_CryptoBERT-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'For academic reference, cite the following paper: https://ieeexplore.ieee.org/document/10223689'}"
mcro_jonatasgrosmanwav2vec2largexlsr53russian-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech recognition in Russian'}
mcro_Salesforceblipimagecaptioningbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'base architecture (with ViT base backbone)'}
mcro_BRIA_Background_Removal_v14-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The model is released under a Creative Commons license for non-commercial use.\n  - Commercial use is subject to a commercial agreement with BRIA. To purchase a commercial license simply click'}
mcro_hubertbase,mcro_Model,{}
mcro_hubertbase-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.'}
mcro_modelid-ModelDetail,mcro_ModelDetailSection,{}
mcro_xclipbasesizedmodel,mcro_Model,{}
mcro_ESM2,mcro_Model,{}
mcro_ESM2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'state-of-the-art protein model trained on a masked language modelling objective'}
mcro_bertmultilingualbasemodelcased-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_deepseekaiDeepSeekV3-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Pre-Training: Towards Ultimate Training Efficiency\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.'}"
mcro_lmmslabLLaVAVideo7BQwen2,mcro_Model,{}
mcro_lmmslabLLaVAVideo7BQwen2-Limitation,mcro_LimitationInformationSection,{}
mcro_mxbai-rerank-xsmall-v1-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_trlinternaltestingtinyLlamaForCausalLM32-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library.'}
mcro_layoutlmv3-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.'}"
mcro_Whisperlargev3model-ModelDetail,mcro_ModelDetailSection,{}
mcro_Whisperlargev3model-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Whisper large-v3'}
mcro_Qwen2.5-1.5B,mcro_Model,{}
mcro_Qwen2.5-1.5B-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_stablediffusionv15modelcard,mcro_Model,{}
mcro_stablediffusionv15modelcard-Consideration,mcro_ConsiderationInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53english-Dataset,mcro_DatasetInformationSection,{}
mcro_bigvganv244khz128band512x-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BigVGAN'}
mcro_flan-t5-large-EvaluationData,mcro_EvaluationDataInformationSection,{}
mcro_t5small-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Language model'}
mcro_microsoftphi2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-based model with next-word prediction objective'}
mcro_example_model-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'example use case'}
mcro_Whisperlargev2modelforCTranslate2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'openai/whisper-large-v2'}
mcro_indonesianrobertabaseposptagger-UseCase,mcro_UseCaseInformationSection,{}
mcro_mxbai-rerank-xsmall-v1,mcro_Model,{}
mcro_mxbai-rerank-xsmall-v1-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}'}"
mcro_scibert,mcro_Model,{}
mcro_scibert-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{beltagy-etal-2019-scibert,\n    title = ""SciBERT: A Pretrained Language Model for Scientific Text"",\n    author = ""Beltagy, Iz  and Lo, Kyle  and Cohan, Arman"",\n    booktitle = ""EMNLP"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/D19-1371""\n}'}"
mcro_Inteldptlarge-EthicalConsideration,mcro_EthicalConsiderationSection,{'prov_hasTextValue': 'The training data come from multiple image datasets compiled together.'}
mcro_metaLlama3-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.'}"
mcro_InternVL22B-ModelDetail,mcro_ModelDetailSection,{}
mcro_InternVL22B-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'InternViT-300M-448px, an MLP projector, and internlm2-chat-1_8b'}"
mcro_parakeettdt06bv2-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'This model was trained using the NeMo toolkit [3], following the strategies below:\n\n- Initialized from a FastConformer SSL checkpoint that was pretrained with a wav2vec method on the LibriLight dataset[7].\n- Trained for 150,000 steps on 64 A100 GPUs.\n- Dataset corpora were balanced using a temperature sampling value of 0.5.\n- Stage 2 fine-tuning was performed for 2,500 steps on 4 A100 GPUs using approximately 500 hours of high-quality, human-transcribed data of NeMo ASR Set 3.0.\n\nTraining was conducted using this [example script](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py) and [TDT configuration](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/fastconformer/hybrid_transducer_ctc/fastconformer_hybrid_tdt_ctc_bpe.yaml).\n\nThe tokenizer was constructed from the training set transcripts using this [script](https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py).'}"
mcro_owlv2,mcro_Model,{}
mcro_owlv2-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The model uses a CLIP backbone with a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective.'}"
mcro_chronosbolttiny-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'zero-shot forecasting; Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting.'}
mcro_EmergentMethodsglinermediumnewsv21-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'microsoft/deberta'}
mcro_facebookmask2formerswintinycocoinstance-Architecture,mcro_ModelArchitectureInformationSection,{}
mcro_facebookhubertlargels960ft-UseCase,mcro_UseCaseInformationSection,{}
mcro_phobertpretrainedlanguagemodels,mcro_Model,{}
mcro_googleelectrabasediscriminator-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'For a detailed description and experimental results, please refer to our paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB).'}"
mcro_MyAwesomeModel-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet'}
mcro_bertbasechinese,mcro_Model,{}
mcro_bertbasechinese-License,mcro_LicenseInformationSection,{'prov_hasTextValue': '[More Information needed]'}
mcro_albertbasev2-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'ALBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion.'}
mcro_TahaDouajidetrdoctabledetection-ModelDetail,mcro_ModelDetailSection,{}
mcro_AlibabaNLPgteQwen27Binstruct,mcro_Model,{}
mcro_AlibabaNLPgteQwen27Binstruct-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT Licensed inference server'}
mcro_jonatasgrosmanwav2vec2xlsr1bportuguese-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech recognition in Portuguese'}
mcro_USERbgem3-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepvk2024user,\n    title={USER: Universal Sentence Encoder for Russian},\n    author={Malashenko, Boris and  Zemerov, Anton and Spirin, Egor},\n    url={https://huggingface.co/datasets/deepvk/USER-base},\n    publisher={Hugging Face}\n    year={2024},\n}'}"
mcro_ibmresearchMoLFormerXLboth10pct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'MoLFormer is a large-scale chemical language model designed with the intention of learning a model trained on small molecules which are represented as SMILES strings. MoLFormer leverges masked language modeling and employs a linear attention Transformer combined with rotary embeddings.'}
mcro_NepaliBERT-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This transformer model can be used for any NLP tasks related to Devenagari language. At the time of training, this is the state of the art model developed \nfor Devanagari dataset. Intrinsic evaluation with Perplexity of 8.56 achieves this state of the art whereas extrinsit evaluation done on sentiment analysis of Nepali tweets outperformed other existing \nmasked language models on Nepali dataset.'}"
mcro_WDViTLargeTaggerV3,mcro_Model,{}
mcro_WDViTLargeTaggerV3-PerformanceMetric,mcro_PerformanceMetricInformationSection,"{'prov_hasTextValue': '`v1.0: P=R: threshold = 0.2606, F1 = 0.4674`'}"
mcro_gemma2-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'These models were trained on a dataset of text data that includes a wide variety\nof sources.'}
mcro_detrresnet50-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-2005-12872,\n  author    = {Nicolas Carion and\n               Francisco Massa and\n               Gabriel Synnaeve and\n               Nicolas Usunier and\n               Alexander Kirillov and\n               Sergey Zagoruyko},\n  title     = {End-to-End Object Detection with Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2005.12872},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2005.12872},\n  archivePrefix = {arXiv},\n  eprint    = {2005.12872},\n  timestamp = {Thu, 28 May 2020 17:38:09 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_whisperlargev3-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'The large-v3 checkpoint is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected using Whisper large-v2.'}
mcro_gpt2medium-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-based language model'}
mcro_bertbaseNER-TrainingData,mcro_DatasetInformationSection,{'prov_hasTextValue': 'CoNLL-2003 Named Entity Recognition'}
mcro_mbartlarge50manytomanymmt-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{tang2020multilingual,\n    title={Multilingual Translation with Extensible Multilingual Pretraining and Finetuning},\n    author={Yuqing Tang and Chau Tran and Xian Li and Peng-Jen Chen and Naman Goyal and Vishrav Chaudhary and Jiatao Gu and Angela Fan},\n    year={2020},\n    eprint={2008.00401},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}'}"
mcro_nvidiatitanetlargeenus-LicenseInformationSection,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'License to use this model is covered by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). By downloading the public and release version of the model, you accept the terms and conditions of the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license.'}"
mcro_owlvitbasepatch32-PrimaryIntendedUseCase,mcro_PrimaryIntendedUseCaseInformationSection,"{'prov_hasTextValue': 'The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training.'}"
mcro_FacebookAIxlmrobertalarge,mcro_Model,{}
mcro_TheBlokeMistral7BInstructv01GGUF,mcro_Model,{}
mcro_llama32CollectionOfMultilingualLargeLanguageModelsLlms-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).'}"
mcro_yuvalkirstainPickScorev1,mcro_Model,{}
mcro_yuvalkirstainPickScorev1-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'Pick-a-Pic dataset'}
mcro_Qwen2532BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_whisperlargev3-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Whisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model.'}"
mcro_gemma2,mcro_Model,{}
mcro_gemma2-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Gemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.'}"
mcro_StellaEn400Mv5-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'retrieve task and semantic textual similarity task'}
mcro_metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm,mcro_Model,{}
mcro_metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelDetail,mcro_ModelDetailSection,{}
mcro_opensearchprojectopensearchneuralsparseencodingdocv2distill-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'learned sparse retrieval'}
mcro_BEiTbasesizedmodelfinetunedonImageNet22k-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like).'}"
mcro_Qwen2505B,mcro_Model,{}
mcro_Qwen2505B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_gemma2b-ModelParameter,mcro_ModelParameterSection,{}
mcro_openaiwhisperlargev3turbo-ModelDetail,mcro_ModelDetailSection,{}
mcro_openaiwhisperlargev3turbo-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_Xenovabgebaseenv15,mcro_Model,{}
mcro_Xenovabgebaseenv15-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_testmodel,mcro_Model,{}
mcro_testmodel-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'mit'}
mcro_clipvitlargepatch14-QuantitativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_clipvitlargepatch14-PerformanceMetric,mcro_PerformanceMetricInformationSection,{}
mcro_efficientnet_b0.ra_in1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_efficientnet_b0.ra_in1k-Citation1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{tan2019efficientnet,\n  title={Efficientnet: Rethinking model scaling for convolutional neural networks},\n  author={Tan, Mingxing and Le, Quoc},\n  booktitle={International conference on machine learning},\n  pages={6105--6114},\n  year={2019},\n  organization={PMLR}\n}'}"
mcro_deepseekaiDeepSeekV30324-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'This repository and the model weights are licensed under the [MIT License](LICENSE).'}
mcro_rubertbasecasedsentimentrusentiment-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'RuSentiment'}
mcro_whisper-baseen-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Robust Speech Recognition via Large-Scale Weak Supervision'}
mcro_moondream2,mcro_Model,{}
mcro_moondream2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Moondream is a small vision language model designed to run efficiently everywhere.'}
mcro_stablediffusionv15modelcard-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@InProceedings{Rombach_2022_CVPR,\n author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\""orn},\n title = {High-Resolution Image Synthesis With Latent Diffusion Models},\n booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n month = {June},\n year = {2022},\n pages = {10684-10695}\n}'}"
mcro_whisperbase,mcro_Model,{}
mcro_whisperbase-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'ASR solution for developers, especially for English speech recognition'}"
mcro_Protbertmodel,mcro_Model,{}
mcro_Protbertmodel-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{}
mcro_HuggingFaceM4idefics28b-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_ChatGLM2-6B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': ""@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}""}"
mcro_flan-t5-base-Consideration,mcro_ConsiderationInformationSection,{}
mcro_tiiuaefalconrw1b,mcro_Model,{}
mcro_tiiuaefalconrw1b-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Research on large language models, specifically the influence of adequately filtered and deduplicated web data on the properties of large language models (fairness, safety, limitations, capabilities, etc.).'}"
mcro_patrickjohncyhfashionclip,mcro_Model,{}
mcro_patrickjohncyhfashionclip-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts.'}
mcro_llama32Collection-TrainingDataInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).'}"
mcro_crossencodermsmarcoMiniLML4v2,mcro_Model,{}
mcro_debertaV3Small,mcro_Model,{}
mcro_debertaV3Small-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}; @misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_IP-Adapter-FaceID-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'IP-Adapter-FaceID can generate various style images conditioned on a face with only text prompts.'}
mcro_llavahfllavaonevisionqwen205bovhf-UseCase,mcro_UseCaseInformationSection,{}
mcro_t53b,mcro_Model,{}
mcro_t53b-ModelDetail,mcro_ModelDetailSection,{}
mcro_deepseekaiDeepSeekR1-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_fasttextlanguageidentification,mcro_Model,{}
mcro_multiqaminiLML6cosv1-TrainingDataInformationSection,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'We use the concatenation from multiple datasets to fine-tune our model. In total we have about 215M (question, answer) pairs.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.'}"
mcro_unslothDeepSeekR10528Qwen38BGGUF-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528'}"
mcro_kokoro82M-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'open-weight TTS model with 82 million parameters'}
mcro_bertbasemodelcased-Architecture,mcro_ModelArchitectureInformationSection,{}
mcro_sentencetransformersmultiqampnetbasedotv1-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_Qwen257B-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias'}"
mcro_distilbertbasemodeluncased,mcro_Model,{}
mcro_distilbertbasemodeluncased-ModelDetail,mcro_ModelDetailSection,{}
mcro_chronos-t5-small-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'This project is licensed under the Apache-2.0 License.'}
mcro_detoxify,mcro_Model,{}
mcro_detoxify-ModelArchitectureInformationSection_Pytorch_Lightning,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Pytorch Lightning'}
mcro_deepseekaiDeepSeekR10528-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic.'}"
mcro_Stablediffusionsafetychecker-TechnicalSpecification,mcro_ModelParameterSection,{}
mcro_upskyybgem3korean-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_upskyybgem3korean-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Sentence Transformer'}
mcro_BAAIbgebaseen-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning.'}
mcro_allMiniLML6v2,mcro_Model,{}
mcro_wav2vec2largexlsrhindi-QuantativeAnalysisSection,mcro_QuantativeAnalysisSection,{}
mcro_kobert,mcro_Model,{}
mcro_kobert-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'If you want to import KoBERT tokenizer with `AutoTokenizer`, you should give `trust_remote_code=True`.'}"
mcro_clip-vit-large-patch14-336-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'More information needed'}
mcro_clipvitlargepatch14-UseCase,mcro_UseCaseInformationSection,{}
mcro_clipvitlargepatch14-OutOfScopeUseCase,mcro_OutOfScopeUseCaseSectionInformation,{'prov_hasTextValue': 'Any deployed use case of the model - whether commercial or not - is currently out of scope.'}
mcro_cognitivecomputationsdolphin291yi1534b-TrainingData,mcro_DatasetInformationSection,{'prov_hasTextValue': '/workspace/datasets/dolphin-2.9/dolphin201-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/dolphin-coder-translate-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/dolphin-coder-codegen-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/m-a-p_Code-Feedback-sharegpt-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/m-a-p_CodeFeedback-Filtered-Instruction-sharegpt-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/not_samantha_norefusals.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/Orca-Math-resort-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/agent_instruct_react_unfiltered.jsonl\n    type: sharegpt  \n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_instruct_j1s1_3k_unfiltered.jsonl\n    type: sharegpt  \n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_negative_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_react_10p_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_tflan_cot_30p_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/openhermes200k_unfiltered.jsonl\n    type: sharegpt \n    conversation: chatml'}
mcro_efficientnet_b3.ra2_in1k-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_visiontransformerbasesizedmodel-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for image classification.; You can use the raw model for feature extraction. See the [model hub](https://huggingface.co/models?search=facebook/dinov2) to look for\nfine-tuned versions on a task that interests you.'}
mcro_facebookmusicgenmedium-OutOfScopeUseCase,mcro_OutOfScopeUseCaseSectionInformation,"{'prov_hasTextValue': 'The model should not be used on downstream applications without further risk evaluation and mitigation. The model should not be used to intentionally create or disseminate music pieces that create hostile or alienating environments for people. This includes generating music that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.'}"
mcro_distilrobertabase-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_facebookesm2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'suitable for fine-tuning on a wide range of tasks that take protein sequences as input'}
mcro_fasttextlanguageidentification-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use pre-trained word vectors for text classification or language identification. See the [tutorials](https://fasttext.cc/docs/en/supervised-tutorial.html) and [resources](https://fasttext.cc/docs/en/english-vectors.html) on its official website to look for tasks that interest you.'}
mcro_bertmultilingualbasemodeluncased-Citation,mcro_CitationInformationSection,{}
mcro_intfloatmultilinguale5base,mcro_Model,{}
mcro_intfloatmultilinguale5base-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}'}"
mcro_clipvitlargepatch14-PrimaryIntendedUseCase,mcro_PrimaryIntendedUseCaseInformationSection,{'prov_hasTextValue': 'research'}
mcro_deepseekaiDeepSeekR1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'MoE'}
mcro_Qwen2505B-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_vocos,mcro_Model,{}
mcro_facebookhubertlargels960ft-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_whisperbase-ModelDetail,mcro_ModelDetailSection,{}
mcro_whisperbase-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Robust Speech Recognition via Large-Scale Weak Supervision'}
mcro_pyannotespeakerdiarization30-License,mcro_LicenseInformationSection,{}
mcro_mlxcommunitygemma312bitqat4bit-ModelDetail,mcro_ModelDetailSection,{}
mcro_NepaliBERT,mcro_Model,{}
mcro_byt5small,mcro_Model,{}
mcro_byt5small-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'ByT5 works especially well on noisy text data'}
mcro_HuggingFaceM4idefics28b,mcro_Model,{}
mcro_llama2,mcro_Model,{}
mcro_llama2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.'}"
mcro_bigvganv222khz80band256x-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon'}"
mcro_sentencetransformersdistilbertbasenlimeantokens-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_microsoftPhi35miniinstruct-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.'}"
mcro_kluerobertabase-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': '_NOTE:_ Use `BertTokenizer` instead of RobertaTokenizer. (`AutoTokenizer` will load `BertTokenizer`)'}
mcro_sdxlinpainting01-ModelDetail,mcro_ModelDetailSection,{}
mcro_nvidiatitanetlargeenus,mcro_Model,{}
mcro_flairnerenglishlarge-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Based on document-level XLM-R embeddings and FLERT'}
mcro_jhartmannemotionenglishdistilrobertabase-Reference,mcro_ReferenceInformationSection,"{'prov_hasTextValue': 'Jochen Hartmann, ""Emotion English DistilRoBERTa-base"". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.'}"
mcro_Bertbasemultilingualcasednerhrl-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'NER'}
mcro_trocrsmalliam,mcro_Model,{}
mcro_t5small,mcro_Model,{}
mcro_t5small-TrainingData,mcro_DatasetInformationSection,{}
mcro_parakeet_rnnt_06b-Reference,mcro_ReferenceInformationSection,{'prov_hasTextValue': '[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)\n\n[2] [Google Sentencepiece Tokenizer](https://github.com/google/sentencepiece)\n\n[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)\n\n[4] [Suno.ai](https://suno.ai/)\n\n[5] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)'}
mcro_metaLlama3-ModelDetail,mcro_ModelDetailSection,{}
mcro_metaLlama3-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_trpakovvitfaceexpression-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': 'Data Bias'}
mcro_granitetimeseriesttmr2model-TrainingDataInformation,mcro_TrainingDataInformationSection,{}
mcro_Qwen257BInstruct1M,mcro_Model,{}
mcro_Qwen257BInstruct1M-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias'}"
mcro_efficientnet_b3.ra2_in1k-Citation3,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}'}"
mcro_resnet18a1in1k-Citation1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}'}"
mcro_distilwhisperdistillargev3,mcro_Model,{}
mcro_distilwhisperdistillargev3-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{gandhi2023distilwhisper,\n      title={Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling}, \n      author={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},\n      year={2023},\n      eprint={2311.00430},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_DFN5BCLIPViTH14378-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This section details whether the model was developed with general or specific tasks in mind (e.g., plant recognition worldwide or in the Pacific Northwest). The use cases may be as broadly or narrowly defined as the developers intend. For example, if the model was built simply to label images, then this task should be indicated as the primary intended use case.'}"
mcro_mobilenetv3small100lambin1k-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image Classification'}
mcro_pyannotespeakerdiarization31-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{Bredin23,\n  author={Hervé Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}; @inproceedings{Plaquet23,\n  author={Alexis Plaquet and Hervé Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}'}"
mcro_mistralaiMistral7BInstructv03-OwnerInformation,mcro_OwnerInformationSection,"{'prov_hasTextValue': 'Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall'}"
mcro_microsoftPhi4multimodalinstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Phi-4-multimodal-instruct has 5.6B parameters and is a multimodal transformer model.'}
mcro_Qwen2.5-VL-32B-Instruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}'}"
mcro_deepseekaiDeepSeekR1,mcro_Model,{}
mcro_deepseekaiDeepSeekR1-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_huggingquantsMetaLlama318BInstructGPTQINT4-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'multilingual dialogue use cases'}
mcro_whisperkit-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'WhisperKit is an on-device speech recognition framework for Apple Silicon: https://github.com/argmaxinc/WhisperKit'}
mcro_DeBERTaDecodingenhancedBERTwithDisentangledAttention-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder'}
mcro_gte-large,mcro_Model,{}
mcro_gte-large-ModelDetail,mcro_ModelDetailSection,{}
mcro_Xlmrobertalargenerspanish-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'XLM-Roberta-large'}
mcro_granitetimeseriesttmr2model,mcro_Model,{}
mcro_granitetimeseriesttmr2model-ModelDetail,mcro_ModelDetailSection,{}
mcro_Xenovabgebaseenv15-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_Supabasegtesmall-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'semantic textual similarity; information retrieval; text reranking'}
mcro_granitetimeseriesttmr1-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_pyannotespeakerdiarization30,mcro_Model,{}
mcro_pyannotespeakerdiarization30-UseCase,mcro_UseCaseInformationSection,{}
mcro_tabletransformerfinetunedfortabledetection,mcro_Model,{}
mcro_tabletransformerfinetunedfortabledetection-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'PubTables1M'}
mcro_Qwen2505BInstruct-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training.'}"
mcro_XTTS-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip.'}
mcro_bertbasemodelcased-License,mcro_LicenseInformationSection,{}
mcro_facebookopt125m-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents: \n\n  - BookCorpus, which consists of more than 10K unpublished books,\n  - CC-Stories, which contains a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas,\n  - The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included. \n  - Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021)\n  - CCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n\nThe final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally\nto each dataset’s size in the pretraining corpus. \n\nThe dataset might contains offensive content as parts of the dataset are a subset of\npublic Common Crawl data, along with a subset of public Reddit data, which could contain sentences\nthat, if viewed directly, can be insulting, threatening, or might otherwise cause anxiety.'}"
mcro_vitbasepatch32clip448laion2bftin12kin1k-UseCase,mcro_UseCaseInformationSection,{}
mcro_microsoftbeitlargepatch16224,mcro_Model,{}
mcro_microsoftbeitlargepatch16224-ModelDetail,mcro_ModelDetailSection,{}
mcro_roberta-baseforextractiveqa,mcro_Model,{}
mcro_roberta-baseforextractiveqa-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Extractive QA'}
mcro_granitetimeseriesttmr1-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53dutch-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{grosman2021xlsr53-large-dutch,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {D}utch},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-dutch}},\n  year={2021}\n}'}"
mcro_wav2vec2largexlsr53-ModelDetail,mcro_ModelDetailSection,{}
mcro_wav2vec2largexlsr53-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '[Paper](https://arxiv.org/abs/2006.13979)'}
mcro_owlv2-ModelDetails,mcro_ModelDetailSection,"{'prov_hasTextValue': 'June 2023; The OWLv2 model (short for Open-World Localization) was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.\n\nThe model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.'}"
mcro_MERTv195M,mcro_Model,{}
mcro_MERTv195M-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'music generation'}
mcro_unslothDeepSeekR10528Qwen38BGGUF-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}'}"
mcro_MaziyarPanahiMistral7BInstructv03GGUF-Description,obo_http_//purl.obolibrary.org/obo/IAO_0000314,{'prov_hasTextValue': '[MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF) contains GGUF format model files for [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3).'}
mcro_speechbrainspkrececapavoxceleb-Citation-DBLP,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{DBLP:conf/interspeech/DesplanquesTD20,\n  author    = {Brecht Desplanques and\n               Jenthe Thienpondt and\n               Kris Demuynck},\n  editor    = {Helen Meng and\n               Bo Xu and\n               Thomas Fang Zheng},\n  title     = {{ECAPA-TDNN:} Emphasized Channel Attention, Propagation and Aggregation\n               in {TDNN} Based Speaker Verification},\n  booktitle = {Interspeech 2020},\n  pages     = {3830--3834},\n  publisher = {{ISCA}},\n  year      = {2020},\n}'}"
mcro_QwenQwen2VL7BInstruct-License,mcro_LicenseInformationSection,{}
mcro_whisper-baseen-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer based encoder-decoder model'}
mcro_AlibabaNLPgteQwen27Binstruct-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'General Text Embedding'}
mcro_QwenQwen25VL3BInstruct,mcro_Model,{}
mcro_QwenQwen25VL3BInstruct-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'reasoning; tool use; phone use; computer use; visual agent'}
mcro_QwenQwen2VL7BInstruct-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'SoTA understanding of images of various resolution & ratio; Understanding videos of 20min+; Agent that can operate your mobiles, robots, etc.; Multilingual Support'}"
mcro_metallamaLlama323BInstruct-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).'}"
mcro_ibmresearchMoLFormerXLboth10pct-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'We trained MoLFormer-XL on a combination of molecules from the ZINC15 and PubChem datasets. This repository contains the version trained on 10% ZINC + 10% PubChem.'}
mcro_speechbrainemotionrecognitionwav2vec2IEMOCAP,mcro_Model,{}
mcro_owlvitbasepatch32-Dataset,mcro_DatasetInformationSection,{}
mcro_owlvitbasepatch32-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html).'}"
mcro_QwenQwen2VL7BInstruct-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': ""Limited Capacity for Complex Instruction: When faced with intricate multi-step instructions, the model's understanding and execution capabilities require enhancement.; Weak Spatial Reasoning Skills: Especially in 3D spaces, the model's inference of object positional relationships is inadequate, making it difficult to precisely judge the relative positions of objects.; Data timeliness: Our image dataset is updated until June 2023, and information subsequent to this date may not be covered.; Insufficient Counting Accuracy: Particularly in complex scenes, the accuracy of object counting is not high, necessitating further improvements.; Constraints in Individuals and Intellectual Property (IP): The model's capacity to recognize specific individuals or IPs is limited, potentially failing to comprehensively cover all well-known personalities or brands.; Lack of Audio Support: The current model does not comprehend audio information within videos.""}"
mcro_allrobertalargev1,mcro_Model,{}
mcro_allrobertalargev1-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.'}"
mcro_DFN5BCLIPViTH14378-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'DFN-5b'}
mcro_Qwen306B-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': '28; 0.6B; Pretraining & Post-training; 16 for Q and 8 for KV; 0.44B; 32,768; Causal Language Models'}"
mcro_gemma2b-Consideration,mcro_ConsiderationInformationSection,{}
mcro_gemma2b-Limitation,mcro_LimitationInformationSection,{}
mcro_opusmtruen,mcro_Model,{}
mcro_NLLB200,mcro_Model,{}
mcro_NLLB200-PerfMetric,mcro_PerformanceMetricInformationSection,"{'prov_hasTextValue': '• Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.'}"
mcro_distilbertbasemultilingualcasedsentimentsstudent-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'sentiment analysis'}
mcro_cointegratedruberttiny2-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The model should be used as is to produce sentence embeddings (e.g. for KNN classification of short texts) or fine-tuned for a downstream task.'}
mcro_distilgpt2,mcro_Model,{}
mcro_distilgpt2-UseCase,mcro_UseCaseInformationSection,{}
mcro_resnet18a1in1k-UseCase,mcro_UseCaseInformationSection,{}
mcro_chronosbolttiny,mcro_Model,{}
mcro_chronosbolttiny-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}'}"
mcro_t5large-ModelParameter,mcro_ModelParameterSection,{}
mcro_llavahfllavav16mistral7bhf-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{liu2023improved,\n      title={Improved Baselines with Visual Instruction Tuning}, \n      author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},\n      year={2023},\n      eprint={2310.03744},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_tsmatzxlmrobertanerjapanese-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{}
mcro_Stablediffusionv14ModelCard-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@InProceedings{Rombach_2022_CVPR,\n author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\""orn},\n title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n month     = {June},\n year      = {2022},\n pages     = {10684-10695}\n    }'}"
mcro_clipvitlargepatch14-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'ViT-L/14 Transformer'}
mcro_debertaV3Small-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '98M parameters in the Embedding layer; vocabulary containing 128K tokens; 44M backbone parameters; hidden size of 768; 6 layers'}
mcro_facebookencodec24khz-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_Qwen3-32B,mcro_Model,{}
mcro_Qwen3-32B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}'}"
mcro_QwenQwen25VL3BInstruct-Citation3,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Qwen-VL,'}"
mcro_NepaliBERT-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'This model is a fine-tuned version of [Bert Base Uncased](https://huggingface.co/bert-base-uncased) on dataset composed of different news scrapped from nepali news portals comprising of 4.6 GB of textual data.\nTHe training corpus is developed using 85467 news scrapped from different job portals. This is a preliminary dataset \nfor the experimentation. THe corpus size is about 4.3 GB of textual data. Similary evaluation data contains few news articles about 12 mb of textual data.\nTrained on about 4.6 GB of Nepali text corpus collected from various sources\nThese data were collected from nepali news site, OSCAR nepali corpus'}"
mcro_resnet50v1.5-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for image classification'}
mcro_chronos-t5-base-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}'}"
mcro_openaiclip,mcro_Model,{}
mcro_openaiclip-Dataset,mcro_DatasetInformationSection,{}
mcro_huggingquantsMetaLlama318BInstructGPTQINT4,mcro_Model,{}
mcro_visiontransformerbase-sizedmodel-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.'}"
mcro_metaLlama31Instruct-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.'}"
mcro_ViTSO400M14SigLIP384-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Contrastive Image-Text, Zero-Shot Image Classification'}"
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelDetail,mcro_ModelDetailSection,{}
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Citation,mcro_CitationInformationSection,{}
mcro_Salesforceblip2opt27b-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters).\nIt was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository.\n\nDisclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nBLIP-2 consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model.\n\nThe authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen\nwhile training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of ""query tokens"" to query embeddings,\nwhich bridge the gap between the embedding space of the image encoder and the large language model.\n\nThe goal for the model is simply to predict the next text token, giving the query embeddings and the previous text.\n\nThis allows the model to be used for tasks like:\n\n- image captioning\n- visual question answering (VQA)\n- chat-like conversations by feeding the image and the previous conversation as prompt to the model'}"
mcro_flan-t5-large-Consideration,mcro_ConsiderationInformationSection,{}
mcro_trlinternaltestingtinyT5ForConditionalGeneration-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library.'}
mcro_MaziyarPanahigemma31bitGGUF-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'None'}
mcro_CryptoBERT-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': ""CryptoBERT's sentiment classification head was fine-tuned on a balanced dataset of 2M labelled StockTwits posts, sampled from [ElKulako/stocktwits-crypto](https://huggingface.co/datasets/ElKulako/stocktwits-crypto).""}"
mcro_WDViTLargeTaggerV3-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Last image id: 7220105\nTrained on Danbooru images with IDs modulo 0000-0899.\nValidated on images with IDs modulo 0950-0999.\nImages with less than 10 general tags were filtered out.\nTags with less than 600 images were filtered out.'}
mcro_allmpnetbasev2-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.'}"
mcro_bigvganv222khz80band256x-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Large-scale Compilation'}
mcro_unslothQwen2505BInstructbnb4bit,mcro_Model,{}
mcro_unslothQwen2505BInstructbnb4bit-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_sentencetransformersparaphrasemultilingualmpnetbasev2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_ModelCardReport,mcro_Model,{}
mcro_ModelCardReport-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_rinnajapanesecloobvitb16,mcro_Model,{}
mcro_googlest5v11-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Pre-trained on C4 only without mixing in the downstream tasks.'}
mcro_prajjwal1bertmedium,mcro_Model,{}
mcro_prajjwal1bertmedium-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'These models are supposed to be trained on a downstream task.'}
mcro_sentencetransformersbertbasenlimeantokens,mcro_Model,{}
mcro_sentencetransformersbertbasenlimeantokens-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_bertmultilingualbasemodelcased,mcro_Model,{}
mcro_FlagEmbedding,mcro_Model,{}
mcro_FlagEmbedding-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning.'}
mcro_Llama3370BInstructAWQ,mcro_Model,{}
mcro_BAAIbgelargeen-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_mimi-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'audio neural codec'}
mcro_autogluonchronosboltbase,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2largexlsr53hungarian-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_segformerb5finetunedade20k-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE).'}
mcro_stablediffusionv2-1,mcro_Model,{}
mcro_stablediffusionv2-1-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'LAION-5B and subsets'}
mcro_mobilevitsmall-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The MobileViT model was pretrained on ImageNet-1k, a dataset consisting of 1 million images and 1,000 classes.'}"
mcro_CLIPViTB32LAION2B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CLIP ViT-B/32'}
mcro_whisper-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}'}"
mcro_USERbgem3-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'deepvk/ru-HNP; DSumRu; Terra; Gsm8k-ru; RussianKeywords; deepvk/ru-WANLI; Mr-TyDi; Fialka-v1; AllNLI; MIRACL; Panorama; Lenta; Xlsum; PravoIsrael; MedNLI; Mlsum; RCB; Gazeta; SummDialogNews; Tapaco'}
mcro_lmmslabLLaVAVideo7BQwen2-TrainingData,mcro_ModelParameterSection,"{'prov_hasTextValue': '- **Architecture:** SO400M + Qwen2\n- **Initialized Model:** lmms-lab/llava-onevision-qwen2-7b-si\n- **Data:** A mixture of 1.6M single-image/multi-image/video data, 1 epoch, full model\n- **Precision:** bfloat16'}"
mcro_facebookhubertlargels960ft,mcro_Model,{}
mcro_facebookhubertlargels960ft-ModelDetail,mcro_ModelDetailSection,{}
mcro_gte-base-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}'}"
mcro_modelcard123,mcro_Model,{}
mcro_modelcard123-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'mit'}
mcro_distilgpt2-ModelDetail,mcro_ModelDetailSection,{}
mcro_wavlmlarge,mcro_Model,{}
mcro_wavlmlarge-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This is an English pre-trained speech model that has to be fine-tuned on a downstream task like speech recognition or audio classification before it can be \nused in inference.'}
mcro_deepseekaiDeepSeekV30324-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}'}"
mcro_googleelectrasmalldiscriminator,mcro_Model,{}
mcro_googleelectrasmalldiscriminator-UseCase,mcro_UseCaseInformationSection,{}
mcro_QwenQwen25VL7BInstruct,mcro_Model,{}
mcro_QwenQwen25VL7BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '* Dynamic Resolution and Frame Rate Training for Video Understanding'}
mcro_SmolLM2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'text rewriting, summarization and function calling'}"
mcro_LTXVideo-ModelDetail,mcro_ModelDetailSection,{}
mcro_example_model-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'example citation'}
mcro_doclingmodels,mcro_Model,{}
mcro_Qwen25Omni,mcro_Model,{}
mcro_Qwen25Omni-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner'}"
mcro_bartlargesizedmodelfinetunedoncnndailymail-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use this model for text summarization.'}
mcro_microsoftdebertaxlargemnli-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention'}
mcro_ESM2-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'accompanying paper'}
mcro_microsoftbeitlargepatch16224-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Vision Transformer (ViT)'}
mcro_googlegemma31bit,mcro_Model,{}
mcro_googlegemma31bit-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}'}"
mcro_distilrobertabase-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_prajjwal1berttiny-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The following model is a Pytorch pre-trained model obtained from converting Tensorflow checkpoint found in the [official Google BERT repository](https://github.com/google-research/bert).\n\nThis is one of the smaller pre-trained BERT variants, together with [bert-mini](https://huggingface.co/prajjwal1/bert-mini) [bert-small](https://huggingface.co/prajjwal1/bert-small) and [bert-medium](https://huggingface.co/prajjwal1/bert-medium). They were introduced in the study `Well-Read Students Learn Better: On the Importance of Pre-training Compact Models` ([arxiv](https://arxiv.org/abs/1908.08962)), and ported to HF for the study `Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics` ([arXiv](https://arxiv.org/abs/2110.01518)). These models are supposed to be trained on a downstream task.'}"
mcro_Qwen257B,mcro_Model,{}
mcro_googlegemma31bit-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'These models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.'}"
mcro_intfloatmultilinguale5base-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_emilyalsentzerBioClinicalBERT,mcro_Model,{}
mcro_intfloatmultilinguale5base-BenchmarkResultsSection,mcro_QuantativeAnalysisSection,{}
mcro_naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelDetail,mcro_ModelDetailSection,{}
mcro_naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_parakeettdt06bv2-Reference,mcro_ReferenceInformationSection,"{'prov_hasTextValue': '[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)\n\n[2] [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations](https://arxiv.org/abs/2304.06795)\n\n[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)\n\n[4] [Youtube-commons: A massive open corpus for conversational and multimodal data](https://huggingface.co/blog/Pclanglais/youtube-commons)\n\n[5] [Yodas: Youtube-oriented dataset for audio and speech](https://arxiv.org/abs/2406.00899)\n\n[6] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n\n[7] [MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages](https://arxiv.org/abs/2410.01036)\n\n[8] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/pdf/2505.13404)'}"
mcro_microsoftPhi3mini128kinstruct-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The model is licensed under the MIT license.'}
mcro_Protbertmodel-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_kokoro82M,mcro_Model,{}
mcro_swin_transformer_v2_tiny_sized_model,mcro_Model,{}
mcro_swin_transformer_v2_tiny_sized_model-ModelDetail,mcro_ModelDetailSection,{}
mcro_nomicainomicembedtextv1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_stabilityaisdturbo-OutOfScopeUse,mcro_OutOfScopeUseCaseSectionInformation,"{'prov_hasTextValue': 'The model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model.'}"
mcro_segformerb1finetunedade20k-ModelDetail,mcro_ModelDetailSection,{}
mcro_segformerb1finetunedade20k-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.'}"
mcro_crossencodermsmarcoMiniLML4v2-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.'}
mcro_prithividaparrotparaphraseronT5,mcro_Model,{}
mcro_prithividaparrotparaphraseronT5-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models'}
mcro_whisper-baseen-ModelDetail,mcro_ModelDetailSection,{}
mcro_avsolatorioGISTsmallEmbeddingv0,mcro_Model,{}
mcro_Segformerb0finetunedade512512,mcro_Model,{}
mcro_Segformerb0finetunedade512512-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image segmentation'}
mcro_YOLOv8DetectionModel-PerformanceMetricInformationSection,mcro_PerformanceMetricInformationSection,{}
mcro_microsoftwavlmbaseplussv-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': '- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)\n- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)\n- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)'}"
mcro_jhartmannemotionenglishdistilrobertabase-ModelDetail,mcro_ModelDetailSection,{}
mcro_metaLlama31Instruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_Qwen306B,mcro_Model,{}
mcro_AlibabaNLPgtebaseenv15-Dataset,mcro_DatasetInformationSection,{}
mcro_Falconsainsfwimagedetection-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru. 2019. Model Cards for Model Reporting. In FAT* ’19: Conference on Fairness, Accountability, and Transparency, January 29–31, 2019, Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3287560. 3287596'}"
mcro_opusmtruen-ModelDetail,mcro_ModelDetailSection,{}
mcro_IDEAResearchgroundingdinobase-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang'}"
mcro_ibmgranitegranite318binstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_kokoro82M-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache-licensed weights'}
mcro_EmergentMethodsglinermediumnewsv21-ModelDetail,mcro_ModelDetailSection,{}
mcro_appleOpenELM11BInstruct-Dataset,mcro_DatasetInformationSection,{}
mcro_sentencetransformersparaphraseMiniLML3v2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_jonatasgrosmanwav2vec2largexlsr53chinesezhcn-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Chinese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [ST-CMDS](http://www.openslr.org/38/).\nWhen using this model, make sure that your speech input is sampled at 16kHz.'}"
mcro_aiforeversbertlargenluru,mcro_Model,{}
mcro_aiforeversbertlargenluru-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'compute sentence embeddings'}
mcro_jonatasgrosmanwav2vec2largexlsr53japanese,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2largexlsr53japanese-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Japanese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [JSUT](https://sites.google.com/site/shinnosuketakamichi/publication/jsut).'}"
mcro_crossencodermsmarcoMiniLML12v2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)'}"
mcro_conjunctsditre15,mcro_Model,{}
mcro_conjunctsditre15-EvaluationData,mcro_EvaluationDataInformationSection,{}
mcro_microsoftmdebertav3base-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Fine-tuning on NLU tasks\n\nWe present the dev results on XNLI with zero-shot cross-lingual transfer setting, i.e. training with English data only, test on other languages.\n\n| Model        |avg | en |  fr| es  | de  | el  | bg  | ru  |tr   |ar   |vi   | th  | zh | hi  | sw  | ur  |\n|--------------| ----|----|----|---- |--   |--   |--   | --  |--   |--   |--   | --  | -- | --  | --  | --  |\n| XLM-R-base   |76.2 |85.8|79.7|80.7 |78.7 |77.5 |79.6 |78.1 |74.2 |73.8 |76.5 |74.6 |76.7| 72.4| 66.5| 68.3|\n| mDeBERTa-base|**79.8**+/-0.2|**88.2**|**82.6**|**84.4** |**82.7** |**82.3** |**82.4** |**80.8** |**79.5** |**78.5** |**78.1** |**76.4** |**79.5**| **75.9**| **73.9**| **72.4**|'}"
mcro_googlegemma31bit-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Open vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.'}"
mcro_stablediffusionv15modelcard-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': '- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n and is not fit for product use without additional safety mechanisms and\n considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.'}"
mcro_ibmresearchMoLFormerXLboth10pct,mcro_Model,{}
mcro_ibmresearchMoLFormerXLboth10pct-ModelDetail,mcro_ModelDetailSection,{}
mcro_resnet50v1.5-ModelDetail,mcro_ModelDetailSection,{}
mcro_LTXVideo-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-video and image-to-video generation model'}
mcro_QdrantallminiLML6v2withattentions,mcro_Model,{}
mcro_QdrantallminiLML6v2withattentions-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model is intended to be used for [BM42 searches](https://qdrant.tech/articles/bm42/).'}
mcro_clipvitlargepatch14-SensitiveDataInformation,mcro_SensitiveDataInformationSection,{}
mcro_MyAwesomeModel-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Reference Paper'}
mcro_Qwen25Coder7BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report},\n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_T5basesummarizationclaimextractor-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Extraction of atomic claims from summaries.\nServe as a component in pipelines for factuality evaluation of summaries.'}
mcro_mlxcommunitygemma312bitqat4bit-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_LTXVideo-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'text-to-video as well as image+text-to-video usecases'}
mcro_answerdotaiModernBERTbase-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'We release the ModernBERT model architectures, model weights, training codebase under the Apache 2.0 license.'}"
mcro_sentencetransformersdistilusebasemultilingualcasedv1-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_swin_transformer_v2_tiny_sized_model-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-2111-09883,\n  author    = {Ze Liu and\n               Han Hu and\n               Yutong Lin and\n               Zhuliang Yao and\n               Zhenda Xie and\n               Yixuan Wei and\n               Jia Ning and\n               Yue Cao and\n               Zheng Zhang and\n               Li Dong and\n               Furu Wei and\n               Baining Guo},\n  title     = {Swin Transformer {V2:} Scaling Up Capacity and Resolution},\n  journal   = {CoRR},\n  volume    = {abs/2111.09883},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2111.09883},\n  eprinttype = {arXiv},\n  eprint    = {2111.09883},\n  timestamp = {Thu, 02 Dec 2021 15:54:22 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09883.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_kresnikwav2vec2largexlsrkorean-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'kresnik/zeroth_korean'}
mcro_prajjwal1berttiny-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bhargava2021generalization,\n      title={Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics}, \n      author={Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},\n      year={2021},\n      eprint={2110.01518},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{DBLP:journals/corr/abs-1908-08962,\n  author    = {Iulia Turc and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {Well-Read Students Learn Better: The Impact of Student Initialization\n               on Knowledge Distillation},\n  journal   = {CoRR},\n  volume    = {abs/1908.08962},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1908.08962},\n  eprinttype = {arXiv},\n  eprint    = {1908.08962},\n  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_QwenQwen2515BInstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_phi4-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': '14B parameters, dense decoder-only Transformer model'}"
mcro_aiforeversbertlargenluru-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT large model (uncased) for Sentence Embeddings in Russian language.'}
mcro_facebookmask2formerswintinycocoinstance-ModelDetail,mcro_ModelDetailSection,{}
mcro_fasttextlanguageidentification-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The language identification model is distributed under the [*Creative Commons Attribution-NonCommercial 4.0 International Public License*](https://creativecommons.org/licenses/by-nc/4.0/).'}
mcro_jonatasgrosmanwav2vec2largexlsr53portuguese-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)'}
mcro_Bertbasemultilingualcasednerhrl,mcro_Model,{}
mcro_Bertbasemultilingualcasednerhrl-TrainingData,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Latvian NER; conll 2002; ANERcorp; Paramopama + Second Harem; Europeana Newspapers; conll 2003; MSRA; Italian I-CAB'}
mcro_patrickjohncyhfashionclip-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., “stripes”, “long sleeves”, “Armani”) and _short description_ (“80s styled t-shirt”)) available in the Farfetch dataset.'}"
mcro_mixedbreadaimxbaiembedlargev1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""Here, we provide several ways to produce sentence embeddings. Please note that you have to provide the prompt `Represent this sentence for searching relevant passages:` for query if you want to use it for retrieval. Besides that you don't need any prompt. Our model also supports [Matryoshka Representation Learning and binary quantization](https://www.mixedbread.ai/blog/binary-mrl).""}"
mcro_flairnerfrench-ModelDetail,mcro_ModelDetailSection,{}
mcro_chronos-t5-small-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters.'}"
mcro_XTTS,mcro_Model,{}
mcro_pyannotesegmentation30,mcro_Model,{}
mcro_pyannotesegmentation30-ModelDetail,mcro_ModelDetailSection,{}
mcro_microsoftdebertav3base-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}'}"
mcro_rorshark-vit-base,mcro_Model,{}
mcro_rorshark-vit-base-Dataset,mcro_DatasetInformationSection,{}
mcro_BilateralReferenceForHighResolutionDichotomousImageSegmentation-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{zheng2024birefnet,\n  title={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},\n  author={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},\n  journal={CAAI Artificial Intelligence Research},\n  volume = {3},\n  pages = {9150038},\n  year={2024}\n}'}"
mcro_WhisperbaseenmodelforCTranslate2,mcro_Model,{}
mcro_gpt2medium,mcro_Model,{}
mcro_gpt2medium-UseCase,mcro_UseCaseInformationSection,{}
mcro_bartbase-ModelDetail,mcro_ModelDetailSection,{}
mcro_bartbase-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_audeeringwav2vec2largerobust12ftemotionmspdim-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'doi:10.5281/zenodo.6221127'}
mcro_debertav3basetasksourcenli-Owner,mcro_OwnerInformationSection,{'prov_hasTextValue': 'damien.sileo@inria.fr'}
mcro_clipvitlargepatch14-ModelDetail,mcro_ModelDetailSection,{}
mcro_FsoftAICpiiphi,mcro_Model,{}
mcro_FsoftAICpiiphi-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model is intended for PII detection in text documents to support tasks such as data anonymization, compliance, and security auditing.'}"
mcro_layoutlmbaseuncased-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{xu2019layoutlm,\n    title={LayoutLM: Pre-training of Text and Layout for Document Image Understanding},\n    author={Yiheng Xu and Minghao Li and Lei Cui and Shaohan Huang and Furu Wei and Ming Zhou},\n    year={2019},\n    eprint={1912.13318},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}'}"
mcro_Salesforceblipimagecaptioninglarge,mcro_Model,{}
mcro_Salesforceblipimagecaptioninglarge-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'COCO dataset'}
mcro_yiyanghkustfinberttone,mcro_Model,{}
mcro_yiyanghkustfinberttone-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT model'}
mcro_gte-large-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens.'}"
mcro_facebookopt125m-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\nIn addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling). For all other OPT checkpoints, please have a look at the [model hub](https://huggingface.co/models?filter=opt).'}"
mcro_speechbrainspkrececapavoxceleb,mcro_Model,{}
mcro_speechbrainspkrececapavoxceleb-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speaker verification; extract speaker embeddings'}
mcro_openaiclip-Consideration,mcro_ConsiderationInformationSection,{}
mcro_Qwen2505BInstruct-ModelDetailSection,mcro_ModelDetailSection,"{'prov_hasTextValue': 'This repo contains an **unmodified version** of the instruction-tuned 0.5B Qwen2.5 model, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens'}"
mcro_HuggingFaceH4zephyr7bbeta-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'the model can be used for chat'}
mcro_USERbgem3-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_e5largev2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model has 24 layers and the embedding size is 1024.'}
mcro_emilyalsentzerBioClinicalBERT-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This model card describes the Bio+Clinical BERT model, which was initialized from [BioBERT](https://arxiv.org/abs/1901.08746) & trained on all MIMIC notes.'}"
mcro_colbertv2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'ColBERT is a _fast_ and _accurate_ retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.'}"
mcro_pyannotespeakerdiarization31-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_deepseekaiDeepSeekV3-ModelDetail,mcro_ModelDetailSection,{}
mcro_deepseekaiDeepSeekV3-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}'}"
mcro_BAAIbgereRankerBase-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_distilbertbasemodeluncased-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}'}"
mcro_Salesforceblip2opt27b-EthicalConsideration,mcro_EthicalConsiderationSection,"{'prov_hasTextValue': 'This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people’s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.'}"
mcro_vocos-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{siuzdak2023vocos,\n  title={Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis},\n  author={Siuzdak, Hubert},\n  journal={arXiv preprint arXiv:2306.00814},\n  year={2023}\n}'}"
mcro_gtelargeenv15,mcro_Model,{}
mcro_doclingmodels-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'RT-DETR'}
mcro_flan-t5-base,mcro_Model,{}
mcro_flan-t5-base-ModelDetail,mcro_ModelDetailSection,{}
mcro_wav2vec2largexlsr53espeakcvft,mcro_Model,{}
mcro_wav2vec2largexlsr53espeakcvft-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'CommonVoice'}
mcro_pyannotewespeakervoxcelebresnet34LM-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{Wang2023,\n  title={Wespeaker: A research and production oriented speaker embedding learning toolkit},\n  author={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},\n  booktitle={ICASSP 2023, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={1--5},\n  year={2023},\n  organization={IEEE}\n}; @inproceedings{Bredin23,\n  author={Hervé Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n  pages={1983--1987},\n  doi={10.21437/Interspeech.2023-105}\n}'}"
mcro_chronosboltmini-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps—a method known as direct multi-step forecasting.'}"
mcro_nvidiatitanetlargeenus-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': '[1] [TitaNet: Neural Model for Speaker Representation with 1D Depth-wise Separable convolutions and global context](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746806) \n[2] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)'}
mcro_sentencetransformersparaphrasemultilingualmpnetbasev2-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_bigvganv244khz128band512x-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon'}"
mcro_metallamaLlama3211BVisionInstruct-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.'}"
mcro_wavlmbaseplus-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': '- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)\n- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)\n- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)'}"
mcro_mimi-Consideration,mcro_ConsiderationInformationSection,{}
mcro_layoutlmbaseuncased-ModelDetail,mcro_ModelDetailSection,{}
mcro_TahaDouajidetrdoctabledetection,mcro_Model,{}
mcro_TahaDouajidetrdoctabledetection-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_Qwen25Coder7BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias'}"
mcro_Salesforceblipimagecaptioninglarge-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'ViT large backbone'}
mcro_bertbasemodelcased-EvaluationData,mcro_EvaluationDataInformationSection,{}
mcro_sentencetransformersparaphrasempnetbasev2-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_patrickjohncyhfashionclip-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@Article{Chia2022,\n    title=""Contrastive language and vision learning of general fashion concepts"",\n    author=""Chia, Patrick John\n            and Attanasio, Giuseppe\n            and Bianchi, Federico\n            and Terragni, Silvia\n            and Magalh{\\~a}es, Ana Rita\n            and Goncalves, Diogo\n            and Greco, Ciro\n            and Tagliabue, Jacopo"",\n    journal=""Scientific Reports"",\n    year=""2022"",\n    month=""Nov"",\n    day=""08"",\n    volume=""12"",\n    number=""1"",\n    abstract=""The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from general and transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model adapted for the fashion industry. We demonstrate the effectiveness of the representations learned by FashionCLIP with extensive tests across a variety of tasks, datasets and generalization probes. We argue that adaptations of large pre-trained models such as CLIP offer new perspectives in terms of scalability and sustainability for certain types of players in the industry. Finally, we detail the costs and environmental impact of training, and release the model weights and code as open source contribution to the community."",\n    issn=""2045-2322"",\n    doi=""10.1038/s41598-022-23052-9"",\n    url=""https://doi.org/10.1038/s41598-022-23052-9""\n}'}"
mcro_nomicainomicembedtextv15,mcro_Model,{}
mcro_nomicainomicembedtextv15-Arch,mcro_ModelArchitectureInformationSection,{}
mcro_llama2-ModelDetail,mcro_ModelDetailSection,{}
mcro_llama2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '""Llama-2: Open Foundation and Fine-tuned Chat Models""(arxiv.org/abs/2307.09288)'}"
mcro_trocrsmalliam-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{li2021trocr,\n      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n      year={2021},\n      eprint={2109.10282},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli,mcro_Model,{}
mcro_RobertaLargeMnli-UseCase,mcro_UseCaseInformationSection,{}
mcro_siglipso400mpatch14384,mcro_Model,{}
mcro_siglipso400mpatch14384-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'WebLI dataset'}
mcro_HuggingFaceH4zephyr7bbeta,mcro_Model,{}
mcro_JackFramllama68m-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'This is a LLaMA-like model with only 68M parameters trained on Wikipedia and part of the C4-en and C4-realnewslike datasets.\n\nNo evaluation has been conducted yet, so use it with care.\n\nThe model is mainly developed as a base Small Speculative Model in the [SpecInfer](https://arxiv.org/abs/2305.09781) paper.'}"
mcro_autogluonchronosboltbase-ModelParameter,mcro_ModelParameterSection,{}
mcro_InternVL22B,mcro_Model,{}
mcro_InternVL22B-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_Snowflakesnowflakearcticembedxs,mcro_Model,{}
mcro_owlvitbasepatch32-User,mcro_UserInformationSection,{}
mcro_t5base,mcro_Model,{}
mcro_t5base-TrainingData,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The model is pre-trained on the [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), which was developed and released in the context of the same [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) as T5.'}"
mcro_jbetkerwav2vec2largerobustftlibrittsvoxpopuli-LimitationInformationSection,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'Since the model was fine-tuned on clean audio, it is not well-suited for noisy audio like CommonVoice (though I may upload a checkpoint for that soon too). It still does pretty good, though.'}"
mcro_sdxl10base,mcro_Model,{}
mcro_sdxl10base-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'research purposes only'}
mcro_mimi-UseCase,mcro_UseCaseInformationSection,{}
mcro_bertbaseNER-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Named Entity Recognition'}
mcro_mistralaiMistral7BInstructv03-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.'}
mcro_potion-base-8M,mcro_Model,{}
mcro_potion-base-8M-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'It is a distilled version of the [baai/bge-base-en-v1.5](https://huggingface.co/baai/bge-base-en-v1.5) Sentence Transformer. It uses static embeddings, allowing text embeddings to be computed orders of magnitude faster on both GPU and CPU.'}"
mcro_playgroundaiplaygroundv251024pxaesthetic-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{li2024playground,\n      title={Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation}, \n      author={Daiqing Li and Aleks Kamko and Ehsan Akhgari and Ali Sabet and Linmiao Xu and Suhail Doshi},\n      year={2024},\n      eprint={2402.17245},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_facebookencodec24khz,mcro_Model,{}
mcro_facebookencodec24khz-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_rubertbasecasedsentimentrusentiment-UseCase,mcro_UseCaseInformationSection,{}
mcro_opusmtnlen-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformer-align'}
mcro_metallamaLlama31,mcro_Model,{}
mcro_T5basesummarizationclaimextractor,mcro_Model,{}
mcro_deepseekaiDeepSeekV3,mcro_Model,{}
mcro_Stablediffusionv14ModelCard-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_Stablediffusionv14ModelCard-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The CreativeML OpenRAIL M license'}
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. ‘Less Annotating, More Classifying – Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI’. Preprint, June. Open Science Framework. https://osf.io/74b8k.'}"
mcro_xclipbasesizedmodel-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'X-CLIP is a minimal extension of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for general video-language understanding.'}
mcro_appleOpenELM11BInstruct,mcro_Model,{}
mcro_bertbaseNER-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT model'}
mcro_phi4-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT'}
mcro_TahaDouajidetrdoctabledetection-Consideration,mcro_ConsiderationInformationSection,{}
mcro_sdxl10base-ModelDetail,mcro_ModelDetailSection,{}
mcro_sdxl10base-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-image generative model'}
mcro_Salesforceblipimagecaptioninglarge-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': 'This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people’s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.'}"
mcro_TaiyiCLIPRoberta102MChinese-ModelDetail,mcro_ModelDetailSection,{}
mcro_TaiyiCLIPRoberta102MChinese-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{Fengshenbang-LM,\n  title={Fengshenbang-LM},\n  author={IDEA-CCNL},\n  year={2021},\n  howpublished={\\url{https://github.com/IDEA-CCNL/Fengshenbang-LM}},\n}'}"
mcro_allrobertalargev1-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.'}
mcro_xlmrobertabaselanguagedetection-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'language detector, i.e. for sequence classification tasks'}"
mcro_crossencodermsmarcoMiniLML6v2-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Cross-Encoder for MS Marco'}
mcro_stablediffusionv2-1-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-image generation model'}
mcro_flan-t5-large-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_googleflant5small-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_Qwen3-14B,mcro_Model,{}
mcro_Qwen3-14B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Causal Language Models'}
mcro_modelcard123-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CNN'}
mcro_rubertbasecased,mcro_Model,{}
mcro_rubertbasecased-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '[1]: Kuratov, Y., Arkhipov, M. (2019). Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language. arXiv preprint [arXiv:1905.07213](https://arxiv.org/abs/1905.07213).'}"
mcro_rbhatia46financialragmatryoshka,mcro_Model,{}
mcro_microsoftbeitlargepatch16224-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'ImageNet-21k; ImageNet'}
mcro_wav2vec2largexlsrhindi,mcro_Model,{}
mcro_wav2vec2largexlsrhindi-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_chronosboltsmall-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'nearly 100 billion time series observations'}
mcro_vitbasepatch8224augreg2in21kftin1k-UseCase,mcro_UseCaseInformationSection,{}
mcro_vitbasepatch32clip448laion2bftin12kin1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_audeeringwav2vec2largerobust12ftemotionmspdim,mcro_Model,{}
mcro_audeeringwav2vec2largerobust12ftemotionmspdim-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'for research purpose only'}
mcro_distilgpt2-Architecture,mcro_ModelArchitectureInformationSection,{}
mcro_FlagEmbedding-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_mimi,mcro_Model,{}
mcro_mimi-ModelDetail,mcro_ModelDetailSection,{}
mcro_mobilenetv3small100lambin1k-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{howard2019searching,\n  title={Searching for mobilenetv3},\n  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},\n  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},\n  pages={1314--1324},\n  year={2019}\n}'}"
mcro_HuggingFaceM4idefics28b-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{laurencon2023obelics,\n      title={OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},\n      author={Hugo Laurençon and Lucile Saulnier and Léo Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M. Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},\n      year={2023},\n      eprint={2306.16527},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{laurençon2024matters,\n      title={What matters when building vision-language models?},\n      author={Hugo Laurençon and Léo Tronchon and Matthieu Cord and Victor Sanh},\n      year={2024},\n      eprint={2405.02246},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_metallamaLlama3370BInstruct,mcro_Model,{}
mcro_metallamaLlama3370BInstruct-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.'}"
mcro_bertmultilingualbasemodeluncased,mcro_Model,{}
mcro_bertmultilingualbasemodeluncased-Dataset,mcro_DatasetInformationSection,{}
mcro_layoutlmbaseuncased,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2largexlsr53polish-Dataset,mcro_DatasetInformationSection,{}
mcro_depthanythingDepthAnythingV2Smallhf,mcro_Model,{}
mcro_DeBERTaDecodingenhancedBERTwithDisentangledAttention,mcro_Model,{}
mcro_multiqaminiLML6cosv1-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Our model is intented to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.'}
mcro_prajjwal1bertmedium-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Config of this model:\n- `prajjwal1/bert-medium` (L=8, H=512) [Model Link](https://huggingface.co/prajjwal1/bert-medium)'}"
mcro_mistralaiMistral7BInstructv02-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': ""The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.""}"
mcro_t5large,mcro_Model,{}
mcro_kresnikwav2vec2largexlsrkorean-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_bigsciencebloomz560m,mcro_Model,{}
mcro_bigsciencebloomz560m-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Same as [bloom-560m](https://huggingface.co/bigscience/bloom-560m), also refer to the `config.json` file'}"
mcro_mimi-Citation,mcro_CitationInformationSection,{}
mcro_nvidiatitanetlargeenus-LimitationInformationSection,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'This model is trained on both telephonic and non-telephonic speech from voxceleb datasets, Fisher and switch board. If your domain of data differs from trained data or doesnot show relatively good performance consider finetuning for that speech domain.'}"
mcro_whisper-Consideration,mcro_ConsiderationInformationSection,{'prov_hasTextValue': 'the models combine trying to predict the next word in audio with trying to transcribe the audio itself.'}
mcro_gemma3model-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google\'s commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google\'s latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *""the \'single\ncontroller\' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.""*'}"
mcro_arabertv1andv2-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""based on Google's BERT architechture""}"
mcro_jonatasgrosmanwav2vec2largexlsr53dutch-Dataset2,mcro_DatasetInformationSection,{'prov_hasTextValue': 'CSS10'}
mcro_Supabasegtesmall-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT framework'}
mcro_distilbertbasemultilingualcased-EnvironmentalImpact,mcro_ConsiderationInformationSection,{'prov_hasTextValue': 'Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed'}
mcro_opensearchprojectopensearchneuralsparseencodingdocv2distill,mcro_Model,{}
mcro_opensearchprojectopensearchneuralsparseencodingdocv2distill-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers'}
mcro_cognitivecomputationsdolphin291yi1534b-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling'}"
mcro_SmolDocling256Mpreview-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_playgroundaiplaygroundv251024pxaesthetic,mcro_Model,{}
mcro_playgroundaiplaygroundv251024pxaesthetic-ModelDetail,mcro_ModelDetailSection,{}
mcro_spa-eng-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformer'}
mcro_Qwen2.5-VL-32B-Instruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_visiontransformerbase-sizedmodel-hybrid-TrainingProcedure,mcro_ModelParameterSection,{}
mcro_Salesforceblipvqabase-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use this model for conditional and un-conditional image captioning'}
mcro_XenovaallMiniLML6v2,mcro_Model,{}
mcro_wav2vec2largerobustftlibri960h,mcro_Model,{}
mcro_ibmgranitegranite318binstruct-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model is designed to respond to general instructions and can be used to build AI assistants for multiple domains, including business applications.'}"
mcro_testmodel-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet'}
mcro_visiontransformerbasesizedmodel,mcro_Model,{}
mcro_visiontransformerbasesizedmodel-CitationInformationSection3,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}'}"
mcro_jonatasgrosmanwav2vec2largexlsr53arabic-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Common Voice 6.1'}
mcro_hubertbase-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'Paper'}
mcro_indonesianrobertabaseposptagger-Dataset,mcro_DatasetInformationSection,{}
mcro_distilbartcnn126,mcro_Model,{}
mcro_distilbartcnn126-QuantitativeAnalysisSection,mcro_QuantativeAnalysisSection,{'prov_hasTextValue': '| Model Name                 |   MM Params |   Inference Time (MS) |   Speedup |   Rouge 2 |   Rouge-L |\n|:---------------------------|------------:|----------------------:|----------:|----------:|----------:|\n| distilbart-xsum-12-1       |         222 |                    90 |      2.54 |     18.31 |     33.37 |\n| distilbart-xsum-6-6        |         230 |                   132 |      1.73 |     20.92 |     35.73 |\n| distilbart-xsum-12-3       |         255 |                   106 |      2.16 |     21.37 |     36.39 |\n| distilbart-xsum-9-6        |         268 |                   136 |      1.68 |     21.72 |     36.61 |\n| bart-large-xsum (baseline) |         406 |                   229 |      1    |     21.85 |     36.50 |\n| distilbart-xsum-12-6       |         306 |                   137 |      1.68 |     22.12 |     36.99 |\n| bart-large-cnn (baseline)  |         406 |                   381 |      1    |     21.06 |     30.63 |\n| distilbart-12-3-cnn        |         255 |                   214 |      1.78 |     20.57 |     30.00 |\n| distilbart-12-6-cnn        |         306 |                   307 |      1.24 |     21.26 |     30.59 |\n| distilbart-6-6-cnn         |         230 |                   182 |      2.09 |     20.17 |     29.70 |'}
mcro_facebookopt125m,mcro_Model,{}
mcro_facebookopt125m-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': ""As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of\nunfiltered content from the internet, which is far from neutral the model is strongly biased : \n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models. \n\nThis bias will also affect all fine-tuned versions of this model.""}"
mcro_avsolatorioGISTsmallEmbeddingv0-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{solatorio2024gistembed,\n    title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},\n    author={Aivin V. Solatorio},\n    journal={arXiv preprint arXiv:2402.16829},\n    year={2024},\n    URL={https://arxiv.org/abs/2402.16829}\n    eprint={2402.16829},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}'}"
mcro_chronosbolttiny-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache-2.0 License; This project is licensed under the Apache-2.0 License.'}
mcro_sat-3l-sm-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': '[`Segment any Text` paper](arxiv.org/abs/2406.16678)'}
mcro_petalsteamStableBeluga2-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{touvron2023llama,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},\n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_openvla7b-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '[OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)'}
mcro_tiiuaefalconrw1b-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Causal decoder-only'}
mcro_mistralaiMistral7BInstructv02,mcro_Model,{}
mcro_tabletransformerpretrainedfortablestructurerecognition-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The Table Transformer is equivalent to [DETR](https://huggingface.co/docs/transformers/model_doc/detr), a Transformer-based object detection model. Note that the authors decided to use the ""normalize before"" setting of DETR, which means that layernorm is applied before self- and cross-attention.'}"
mcro_multilinguale5large,mcro_Model,{}
mcro_MyAwesomeModel,mcro_Model,{}
mcro_whisper-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer based encoder-decoder model'}
mcro_JackFramllama68m-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{miao2023specinfer,\n      title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification}, \n      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},\n      year={2023},\n      eprint={2305.09781},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_ViTSO400M14SigLIP384-ModelDetail,mcro_ModelDetailSection,{}
mcro_ViTSO400M14SigLIP384-Citation1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{zhai2023sigmoid,\n  title={Sigmoid loss for language image pre-training},\n  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2303.15343},\n  year={2023}\n}'}"
mcro_DeBERTaDecodingenhancedBERTwithDisentangledAttention-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}'}"
mcro_roberta-baseforextractiveqa-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_jbetkerwav2vec2largerobustftlibrittsvoxpopuli-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'This model was created by fine-tuning the `facebook/wav2vec2-large-robust-ft-libri-960h` checkpoint on the [libritts](https://research.google/tools/datasets/libri-tts/) and [voxpopuli](https://github.com/facebookresearch/voxpopuli) datasets with a new vocabulary that includes punctuation.'}
mcro_mlxcommunitygemma312bitqat4bit-UseCase,mcro_UseCaseInformationSection,{}
mcro_keremberkeyolov5nlicenseplate,mcro_Model,{}
mcro_keremberkeyolov5nlicenseplate-UseCase,mcro_UseCaseInformationSection,{}
mcro_detoxify-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'mit'}
mcro_Falconsainsfwimagedetection-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Fine-Tuned Vision Transformer (ViT)'}
mcro_jonatasgrosmanwav2vec2largexlsr53english-Citation,mcro_CitationInformationSection,{}
mcro_chronosboltsmall-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'zero-shot forecasting'}
mcro_BiomedParse-ModelDetailSection,mcro_ModelDetailSection,"{'prov_hasTextValue': 'Basic information about the model that includes licensing information, owner information, the architecture of the model (algorthim employed), references (cited papers), and versioning information.'}"
mcro_crossencodermsmarcoTinyBERTL2v2-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'MS Marco Passage Ranking'}
mcro_modelcard123-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Citation Text'}
mcro_Qwen2VL2BInstruct-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '@article{Qwen-VL; @article{Qwen2VL'}
mcro_Protbertmodel-ModelDetail,mcro_ModelDetailSection,{}
mcro_Protbertmodel-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_phi4-ModelDetail,mcro_ModelDetailSection,{}
mcro_intfloatmultilinguale5base-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{}
mcro_EmergentMethodsglinermediumnewsv21,mcro_Model,{}
mcro_EmergentMethodsglinermediumnewsv21-TrainingData,mcro_DatasetInformationSection,{'prov_hasTextValue': 'AskNews-NER-v0'}
mcro_CryptoBERT-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'CryptoBERT is a pre-trained NLP model to analyse the language and sentiments of cryptocurrency-related social media posts and messages.'}
mcro_answerdotaiModernBERTbase-ModelDetail,mcro_ModelDetailSection,{}
mcro_ruri-small-v2-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_ruri-small-v2-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_llama32Collection,mcro_Model,{}
mcro_llama32Collection-LicenseInformationSection,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).'}"
mcro_autogluonchronosboltbase-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting.'}
mcro_tiiuaefalconrw1b-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_Qwen3-8B-ModelDetail,mcro_ModelDetailSection,{}
mcro_Qwen3-8B-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'general-purpose dialogue'}
mcro_BAAIbgesmallen,mcro_Model,{}
mcro_BAAIbgesmallen-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer'}
mcro_hallucinationevaluationmodel,mcro_Model,{}
mcro_bartlargesizedmodelfinetunedoncnndailymail-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_fasttextlanguageidentification-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Pre-trained word vectors for 157 languages were trained on [Common Crawl](http://commoncrawl.org/) and [Wikipedia](https://www.wikipedia.org/) using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.'}"
mcro_QwenQwen25VL7BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5-VL,'}"
mcro_resnet50.a1_in1k-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'Image size: train = 224 x 224, test = 288 x 288; Params (M): 25.6; Activations (M): 11.1; Image classification / feature backbone; GMACs: 4.1'}"
mcro_clipvitlargepatch14-EthicalConsideration,mcro_EthicalConsiderationSection,{'prov_hasTextValue': 'bias and fairness'}
mcro_FlagEmbedding-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'FlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Dense Retrieval**: [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB); Retrieval-augmented LLMs'}"
mcro_CLIPViTB32LAION2B,mcro_Model,{}
mcro_CLIPViTB32LAION2B-ModelDetail,mcro_ModelDetailSection,{}
mcro_ProtGPT2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'ProtGPT2 can be used for de novo protein design and engineering.'}
mcro_facebookmask2formerswintinycocoinstance-License,mcro_LicenseInformationSection,{}
mcro_QwenQwen2VL7BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Naive Dynamic Resolution; Multimodal Rotary Position Embedding (M-ROPE)'}
mcro_stablediffusionv15modelcard-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-image generation model'}
mcro_clip-EthicalConsideration,mcro_EthicalConsiderationSection,"{'prov_hasTextValue': 'We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with ‘Middle Eastern’ having the highest accuracy (98.4%) and ‘White’ having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.'}"
mcro_e5smallv2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Use \\""query: \\"" and \\""passage: \\"" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\\n\\n- Use \\""query: \\"" prefix for symmetric tasks such as semantic similarity, paraphrase retrieval.\\n\\n- Use \\""query: \\"" prefix if you want to use embeddings as features, such as linear probing classification, clustering.'}"
mcro_scibert-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT model trained on scientific text'}
mcro_microsoftPhi3mini4kinstruct-Benchmark,mcro_QuantativeAnalysisSection,"{'prov_hasTextValue': ""We report the results under completion format for Phi-3-Mini-4K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT3.5-Turbo-1106.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k–shot examples is listed per-benchmark. \n\n| Category | Benchmark | Phi-3-Mini-4K-Ins | Gemma-7B | Mistral-7b | Mixtral-8x7b | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |\n|:----------|:-----------|:-------------------|:----------|:------------|:--------------|:----------------|:-------------------|\n| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.0 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |\n| | MMLU <br>5-shot | 70.9 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |\n| | BigBench Hard CoT<br>3-shot| 73.5 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |\n| Language Understanding | ANLI <br>7-shot | 53.6 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |\n| | HellaSwag <br>5-shot| 75.3 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |\n| Reasoning | ARC Challenge <br>10-shot | 86.3 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |\n| | BoolQ <br>0-shot | 78.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |\n| | MedQA <br>2-shot| 56.5 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |\n| | OpenBookQA <br>10-shot| 82.2 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |\n| | PIQA <br>5-shot| 83.5 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |\n| | GPQA <br>0-shot| 30.6 | 2.9 | 15 | 6.9 | 32.4 | 30.8 |\n| | Social IQA <br>5-shot| 77.6 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |\n| | TruthfulQA (MC2) <br>10-shot| 64.7 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |\n| | WinoGrande <br>5-shot| 71.6 | 55.6 | 54.2 | 62 | 65 | 68.8 |\n| Factual Knowledge | TriviaQA <br>5-shot| 61.4 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |\n| Math | GSM8K CoT <br>8-shot| 85.7 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |\n| Code Generation | HumanEval <br>0-shot| 57.3 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |\n| | MBPP <br>3-shot| 69.8 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |\n| **Average** | | **67.6** | **56.0** | **56.4** | **64.4** | **65.5** | **70.4** |\n\n\nWe take a closer look at different categories across 100 public benchmark datasets at the table below: \n\n| Category | Phi-3-Mini-4K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |\n|:----------|:------------------------|:----------|:------------|:--------------|:---------------------|:---------------|\n| Popular aggregated benchmark | 61.1 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |\n| Reasoning | 70.8 | 60.3 | 62.8 | 68.1 | 69.6 | 71.8 |\n| Language understanding | 60.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |\n| Code generation | 60.7 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |\n| Math | 50.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |\n| Factual knowledge | 38.4 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |\n| Multilingual | 56.7 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |\n| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |\n\n\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine.""}"
mcro_wav2vec2base,mcro_Model,{}
mcro_openvla7b-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '**Out-of-Scope:** OpenVLA models do not zero-shot generalize to new (unseen) robot embodiments, or setups that are not represented in the pretraining mix; in these cases,\nwe suggest collecting a dataset of demonstrations on the desired setup, and fine-tuning OpenVLA models instead.; OpenVLA models take a language instruction and a camera image of a robot workspace as input, and predict (normalized) robot actions consisting of 7-DoF end-effector deltas\nof the form (x, y, z, roll, pitch, yaw, gripper). To execute on an actual robot platform, actions need to be *un-normalized* subject to statistics computed on a per-robot,\nper-dataset basis. See [our repository](https://github.com/openvla/openvla) for more information.\n\nOpenVLA models can be used zero-shot to control robots for specific combinations of embodiments and domains seen in the Open-X pretraining mixture (e.g., for \n[BridgeV2 environments with a Widow-X robot](https://rail-berkeley.github.io/bridgedata/)). They can also be efficiently *fine-tuned* for new tasks and robot setups\ngiven minimal demonstration data; [see here](https://github.com/openvla/openvla/blob/main/scripts/finetune.py).'}"
mcro_RobertaLargeMnli-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-based language model'}
mcro_wav2vec2largexlsr53espeakcvft-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Paper: Simple and Effective Zero-shot Cross-lingual Phoneme Recognition'}
mcro_lmmslabLLaVAVideo7BQwen2-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'The LLaVA-Video models are 7/72B parameter models trained on LLaVA-Video-178K and LLaVA-OneVision Dataset, based on Qwen2 language model with a context window of 32K tokens.\n\nThis model support at most 64 frames.'}"
mcro_openaiclip-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_Dreamshaper8inpainting,mcro_Model,{}
mcro_Dreamshaper8inpainting-ModelDetail,mcro_ModelDetailSection,{}
mcro_detrresnet50-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100.'}"
mcro_Salesforceblipvqacapfiltlarge,mcro_Model,{}
mcro_Salesforceblipvqacapfiltlarge-Consideration,mcro_ConsiderationInformationSection,{}
mcro_Llama160m-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This is a LLaMA-like model with only 160M parameters trained on Wikipedia and part of the C4-en and C4-realnewslike datasets.'}
mcro_pyannotewespeakervoxcelebresnet34LM,mcro_Model,{}
mcro_pyannotewespeakervoxcelebresnet34LM-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_llamaGuard38B,mcro_Model,{}
mcro_obideidrobertai2b2-ModelDetail,mcro_ModelDetailSection,{}
mcro_obideidrobertai2b2-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '* A RoBERTa'}
mcro_stablediffusionv15modelcard-ModelDetail,mcro_ModelDetailSection,{}
mcro_stablediffusionv15modelcard-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The CreativeML OpenRAIL M license'}
mcro_crossencodernlidebertav3base-ModelDetail,mcro_ModelDetailSection,{}
mcro_sentencetransformersparaphraseMiniLML3v2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_bertbasemultilingualuncasedsentiment-QuantativeAnalysis,mcro_QuantativeAnalysisSection,"{'prov_hasTextValue': 'The fine-tuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:'}"
mcro_gpt2-TrainingProcedure,mcro_ModelParameterSection,{}
mcro_Stablediffusionsafetychecker,mcro_Model,{}
mcro_MyAwesomeModel-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image Classification'}
mcro_chronos-t5-base,mcro_Model,{}
mcro_chronos-t5-base-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters.'}"
mcro_jonatasgrosmanwav2vec2largexlsr53polish-ModelDetail,mcro_ModelDetailSection,{}
mcro_doclingmodels-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'identify the structure of the table, starting from an image of a table'}"
mcro_whisper-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': '680,000 hours of audio and the corresponding transcripts collected from the internet; The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet.'}"
mcro_stablediffusioninpaintingmodelcard-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'LAION-2B (en) and subsets thereof'}
mcro_mobilevitsmall,mcro_Model,{}
mcro_doclingmodels-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'DocLayNet'}
mcro_vitmatte-model-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for image matting. See the [model hub](https://huggingface.co/models?search=vitmatte) to look for other\nfine-tuned versions that may interest you.'}
mcro_QwenQwen25Coder7BInstructGPTQInt4-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_flairnerfrench,mcro_Model,{}
mcro_t53b-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Colossal Clean Crawled Corpus (C4)'}
mcro_owlvitbasepatch32-PrimaryIntendedUser,mcro_PrimaryIntendedUserInformationSection,{'prov_hasTextValue': 'The primary intended users of these models are AI researchers.'}
mcro_llama32CollectionOfMultilingualLlLMs-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).'}"
mcro_czechwav2vec2xlsr300mcs250,mcro_Model,{}
mcro_czechwav2vec2xlsr300mcs250-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_parakeet_rnnt_06b,mcro_Model,{}
mcro_parakeet_rnnt_06b-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'License to use this model is covered by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). By downloading the public and release version of the model, you accept the terms and conditions of the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license.'}"
mcro_MaziyarPanahigemma31bitGGUF-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': '[MaziyarPanahi/gemma-3-1b-it-GGUF](https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF) contains GGUF format model files for [google/gemma-3-1b-it](https://huggingface.co/google/gemma-3-1b-it).'}
mcro_bartbase-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_obideidrobertai2b2-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': '* The I2B2 2014'}
mcro_facebookencodec24khz-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_sentencetransformersrobertabasenlimeantokens-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks; Reimers, Nils and Gurevych, Iryna'}"
mcro_microsoftPhi3mini4kinstruct-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-mini-4k/resolve/main/LICENSE).'}
mcro_blackforestlabsFLUX1Filldev,mcro_Model,{}
mcro_blackforestlabsFLUX1Filldev-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': '- This model is not intended or able to provide factual information.\n- As a statistical model this checkpoint might amplify existing societal biases.\n- The model may fail to generate output that matches the prompts.\n- Prompt following is heavily influenced by the prompting-style.\n- There may be slight-color shifts in areas that are not filled in\n- Filling in complex textures may produce lines at the edges of the filled-area.'}
mcro_distilbertdistilbertbaseuncasedfinetunedsst2englis-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'Stanford Sentiment Treebank'}
mcro_owlvitbasepatch32-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{minderer2022simple,\n  title={Simple Open-Vocabulary Object Detection with Vision Transformers},\n  author={Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby},\n  journal={arXiv preprint arXiv:2205.06230},\n  year={2022},\n}'}"
mcro_InternVL378B,mcro_Model,{}
mcro_InternVL378B-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'an advanced multimodal large language model (MLLM) series that demonstrates superior overall performance'}
mcro_HuggingFaceH4zephyr7bbeta-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.'}"
mcro_QwenQwen25VL7BInstruct-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.'}"
mcro_microsoftPhi4multimodalinstruct,mcro_Model,{}
mcro_microsoftPhi4multimodalinstruct-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The model is intended for broad multilingual and multimodal commercial and research use.'}
mcro_BAAIbgererankerlarge-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'multilingual pair data'}
mcro_deepseekaiDeepSeekR1-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_Snowflakesnowflakearcticembedxs-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The `snowflake-arctic-embedding` models achieve **state-of-the-art performance on the MTEB/BEIR leaderboard** for each of their size variants.'}
mcro_granitetimeseriesttmr2model-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{ekambaram2024tinytimemixersttms,\n      title={Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series},\n      author={Vijay Ekambaram and Arindam Jati and Pankaj Dayama and Sumanta Mukherjee and Nam H. Nguyen and Wesley M. Gifford and Chandra Reddy and Jayant Kalagnanam},\n      booktitle={Advances in Neural Information Processing Systems (NeurIPS 2024)},\n      year={2024},\n}'}"
mcro_SmolLM2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer decoder'}
mcro_intfloate5mistral7binstruct-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\n\nThis model is initialized from [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\nand fine-tuned on a mixture of multilingual datasets.\nAs a result, it has some multilingual capability.\nHowever, since Mistral-7B-v0.1 is mainly trained on English data, we recommend using this model for English only.\nFor multilingual use cases, please refer to [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large).'}"
mcro_efficientnet_b3.ra2_in1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_microsoftFlorence2base,mcro_Model,{}
mcro_Florence-2Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}'}"
mcro_jonatasgrosmanwav2vec2largexlsr53arabic-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{grosman2021xlsr53-large-arabic,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {A}rabic},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-arabic}},\n  year={2021}\n}'}"
mcro_microsoftdebertaxlargemnli,mcro_Model,{}
mcro_spladecocondenserensembledistil,mcro_Model,{}
mcro_NLLB200-Tradeoff,mcro_TradeoffInformationSection,"{'prov_hasTextValue': '• Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.; Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.'}"
mcro_blackforestlabsFLUX1schnell-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'apache-2.0'}
mcro_ibmresearchMoLFormerXLboth10pct-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'You can use the model for masked language modeling, but it is mainly intended to be used as a feature extractor or to be fine-tuned for a prediction task. The ""frozen"" model embeddings may be used for similarity measurements, visualization, or training predictor models. The model may also be fine-tuned for sequence classification tasks (e.g., solubility, toxicity, etc.).'}"
mcro_blackforestlabsFLUX1Filldev-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': '`FLUX.1 Fill [dev]` is a 12 billion parameter rectified flow transformer capable of filling areas in existing images based on a text description.\nFor more information, please read our [blog post](https://blackforestlabs.ai/flux-1-tools/).'}"
mcro_Falconsainsfwimagedetection-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '- **NSFW Image Classification**: The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications.\n- **Specialized Task Fine-Tuning**: While the model is adept at NSFW image classification, its performance may vary when applied to other tasks.\n- Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.'}"
mcro_ProtGPT2-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'ProtGPT2 is a decoder-only transformer model pre-trained on the protein space, database UniRef50 (version 2021_04). The pre-training was done on the raw sequences without FASTA headers. Details of training and datasets can be found here: https://huggingface.co/datasets/nferruz/UR50_2021_04\n\nProtGPT2 was trained in a self-supervised fashion, i.e., the raw sequence data was used during training without including the annotation of sequences. In particular, ProtGPT2 was trained using a causal modelling objective, in which the model is trained to predict the next token (or, in this case, oligomer) in the sequence.\n By doing so, the model learns an internal representation of proteins and is able to <em>speak</em> the protein language.'}"
mcro_mobilevitsmall-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'MobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers.'}"
mcro_chronosboltsmall,mcro_Model,{}
mcro_chronosboltsmall-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'T5 encoder-decoder architecture'}
mcro_canary1bflash,mcro_Model,{}
mcro_emimodel,mcro_Model,{}
mcro_emimodel-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_mistralaiMistral7BInstructv03,mcro_Model,{}
mcro_toxigen,mcro_Model,{}
mcro_toxigen-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model comes from the paper [ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509) and can be used to detect implicit hate speech.'}
mcro_metallamaLlama3370BInstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_metallamaLlama3370BInstruct-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'A custom commercial license, the Llama 3.3 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE)'}"
mcro_vitlargepatch14reg4dinov2lvd142m-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_depthanythingDepthAnythingV2Smallhf-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for tasks like zero-shot depth estimation. See the [model hub](https://huggingface.co/models?search=depth-anything) to look for\nother versions on a task that interests you.'}
mcro_vitbasepatch8224augreg2in21kftin1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_vitbasepatch8224augreg2in21kftin1k-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'ImageNet-21k'}
mcro_metallamaLlama323BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_ruri-small-v2,mcro_Model,{}
mcro_ruri-small-v2-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_metaLlama32Collection,mcro_Model,{}
mcro_metaLlama32Collection-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_longformerbase4096,mcro_Model,{}
mcro_longformerbase4096-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Longformer is a transformer model for long documents. `longformer-base-4096` is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. \n \nLongformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.'}"
mcro_rinnajapanesecloobvitb16-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{sawada2024release,\n    title = {Release of Pre-Trained Models for the {J}apanese Language},\n    author = {Sawada, Kei and Zhao, Tianyu and Shing, Makoto and Mitsui, Kentaro and Kaga, Akio and Hono, Yukiya and Wakatsuki, Toshiaki and Mitsuda, Koh},\n    booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},\n    month = {5},\n    year = {2024},\n    pages = {13898--13905},\n    url = {https://aclanthology.org/2024.lrec-main.1213},\n    note = {\\url{https://arxiv.org/abs/2404.01657}}\n}'}"
mcro_UFNLPgatortronbase,mcro_Model,{}
mcro_metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)'}"
mcro_metaLlama31Instruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_deberta-v3-base-prompt-injection-v2-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache License 2.0'}
mcro_distilwhisperdistillargev3-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'drop-in replacement for Whisper large-v3 on English speech recognition'}
mcro_example_model,mcro_Model,{}
mcro_appleOpenELM11BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_blackforestlabsFLUX1schnell,mcro_Model,{}
mcro_blackforestlabsFLUX1schnell-OutOfScopeUse,mcro_OutofScopeUseCaseSectionInformation,{}
mcro_xclipbasesizedmodel-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for determining how well text goes with a given video.'}
mcro_infoxlm-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{chi-etal-2021-infoxlm,\n  title = ""{I}nfo{XLM}: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"",\n  author={Chi, Zewen and Dong, Li and Wei, Furu and Yang, Nan and Singhal, Saksham and Wang, Wenhui and Song, Xia and Mao, Xian-Ling and Huang, Heyan and Zhou, Ming},\n  booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",\n  month = jun,\n  year = ""2021"",\n  address = ""Online"",\n  publisher = ""Association for Computational Linguistics"",\n  url = ""https://aclanthology.org/2021.naacl-main.280"",\n  doi = ""10.18653/v1/2021.naacl-main.280"",\n  pages = ""3576--3588"",}'}"
mcro_chronos-t5-small-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Chronos is a family of **pretrained time series forecasting models** based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes.'}"
mcro_upskyybgem3korean-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_trocrbasesroie-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.'}"
mcro_BAAIbgem3,mcro_Model,{}
mcro_RoBERTaBaseModel,mcro_Model,{}
mcro_RoBERTaBaseModel-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'masked language modeling; fine-tuned on a downstream task'}
mcro_whisper-baseen-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': '680,000 hours of audio'}"
mcro_microsoftcodebertbasemlm-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'masked-language-modeling'}
mcro_llavahfllavaonevisionqwen205bovhf,mcro_Model,{}
mcro_llavahfllavaonevisionqwen205bovhf-ModelDetail,mcro_ModelDetailSection,{}
mcro_playgroundaiplaygroundv251024pxaesthetic-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-image generative model'}
mcro_facebookmmslid256-ModelDetail,mcro_ModelDetailSection,{}
mcro_facebookmmslid256-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Wav2Vec2 architecture'}
mcro_canary1bflash-LicenseInformationSection,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'canary-1b-flash is released under the CC-BY-4.0 license. By using this model, you are agreeing to the terms and conditions of the license.'}"
mcro_microsoftPhi3mini128kinstruct-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model is intended for commercial and research use in English. The model provides uses for applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)'}"
mcro_resnet50.a1_in1k-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'ReLU activations; 1x1 convolution shortcut downsample; single layer 7x7 convolution with pooling'}
mcro_jhartmannemotionenglishdistilrobertabase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{hartmann2022emotionenglish,\n  author={Hartmann, Jochen},\n  title={Emotion English DistilRoBERTa-base},\n  year={2022},\n  howpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},\n}'}"
mcro_longformerbase4096-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Beltagy2020Longformer,\n  title={Longformer: The Long-Document Transformer},\n  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},\n  journal={arXiv:2004.05150},\n  year={2020},\n}'}"
mcro_QwenQwen25VL3BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5-VL,'}"
mcro_Xlmrobertalargenerspanish,mcro_Model,{}
mcro_metallamaLlama3211BVisionInstruct-IntendedUse,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.2-Vision is intended for commercial and research use. Instruction tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pretrained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2-Vision’s ability to take images and text as inputs, additional use cases could include:\n\n1. Visual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it.   \n2. Document Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image.  \n3. Image Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story.  \n4. Image-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words.  \n5. Visual Grounding: Visual grounding is like connecting the dots between what we see and say. It’s about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions.'}"
mcro_distilbertbasemultilingualcasedsentimentsstudent-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'distilbert-base-multilingual-cased'}
mcro_facebookmusicgenmedium-IntendedUseCase,mcro_UseCaseInformationSection,{}
mcro_facebookmusicgenmedium-PrimaryIntendedUseCase,mcro_PrimaryIntendedUseCaseInformationSection,"{'prov_hasTextValue': 'The primary use of MusicGen is research on AI-based music generation, including:\n\n- Research efforts, such as probing and better understanding the limitations of generative models to further improve the state of science\n- Generation of music guided by text or melody to understand current abilities of generative AI models by machine learning amateurs'}"
mcro_autogluonchronosboltbase-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_crossencodermsmarcoTinyBERTL2v2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Information Retrieval'}
mcro_lengyue233contentvecbest-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_depthanythingDepthAnythingV2Smallhf-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': ""@misc{yang2024depth,\n      title={Depth Anything V2}, \n      author={Lihe Yang and Bingyi Kang and Zilong Huang and Zhen Zhao and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},\n      year={2024},\n      eprint={2406.09414},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}\n}""}"
mcro_OrengutengLlama38BLexiUncensored,mcro_Model,{}
mcro_OrengutengLlama38BLexiUncensored-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{'prov_hasTextValue': '|             Metric              |Value|\n|---------------------------------|----:|\n|Avg.                             |66.18|\n|AI2 Reasoning Challenge (25-Shot)|59.56|\n|HellaSwag (10-Shot)              |77.88|\n|MMLU (5-Shot)                    |67.68|\n|TruthfulQA (0-shot)              |47.72|\n|Winogrande (5-shot)              |75.85|\n|GSM8k (5-shot)                   |68.39|'}
mcro_Inteldptlarge,mcro_Model,{}
mcro_Inteldptlarge-Tradeoff,mcro_Trade-offInformationSection,"{'prov_hasTextValue': 'Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. There are no additional caveats or recommendations for this model.'}"
mcro_Qwen257BInstruct,mcro_Model,{}
mcro_lmmslabLLaVAVideo7BQwen2-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'None'}
mcro_sentencetransformersdistilbertbasenlimeantokens-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_sdxlvae-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'COCO 2017'}
mcro_flairnerfrench-UseCase,mcro_UseCaseInformationSection,{}
mcro_indonesianrobertabaseposptagger,mcro_Model,{}
mcro_indonesianrobertabaseposptagger-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_opusmtnlen,mcro_Model,{}
mcro_TaiyiCLIPRoberta102MChinese-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': '多模态 Multimodal'}
mcro_deepseekaiDeepSeekV30324-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This model supports features such as function calling, JSON output, and FIM completion.'}"
mcro_rbhatia46financialragmatryoshka-ModelDetail,mcro_ModelDetailSection,{}
mcro_Salesforceblipvqacapfiltlarge-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'large architecture (with ViT large backbone)'}
mcro_HuggingFaceH4zephyr7bbeta-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'UltraChat'}
mcro_crossencodermsmarcoMiniLML6v2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order.'}"
mcro_USERbgem3-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'sentence-transformer'}
mcro_gte-small-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'text reranking; semantic textual similarity; information retrieval'}
mcro_deepseekaiDeepSeekR1DistillLlama8B,mcro_Model,{}
mcro_deepseekaiDeepSeekR1DistillLlama8B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}'}"
mcro_gte-large-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_mistralaiMistral7BInstructv02-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'For full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).'}
mcro_owlv2-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'The OWLv2 model (short for Open-World Localization) was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.\n\nThe model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.'}"
mcro_clip-vit-large-patch14-336,mcro_Model,{}
mcro_BiomedParse,mcro_Model,{}
mcro_IDEAResearchgroundingdinobase,mcro_Model,{}
mcro_IDEAResearchgroundingdinobase-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'zero-shot object detection (the task of detecting things in an image out-of-the-box without labeled data)'}
mcro_sdxlinpainting01,mcro_Model,{}
mcro_sdxlinpainting01-intendeduse,mcro_PrimaryIntendedUseCaseInformationSection,{'prov_hasTextValue': 'research purposes only'}
mcro_indobenchmarkindobertbasep1,mcro_Model,{}
mcro_indobenchmarkindobertbasep1-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT model'}
mcro_yuvalkirstainPickScorev1-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'model evaluation; image ranking; human preference prediction; general scoring function'}
mcro_bartlargemnli-ModelDetail,mcro_ModelDetailSection,{}
mcro_bartlargemnli-Citation,mcro_CitationInformationSection,{}
mcro_robertBase-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT'}
mcro_metallamaLlama3370BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_FlagEmbedding-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.; MIT License'}
mcro_naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-License,mcro_LicenseInformationSection,{}
mcro_intfloatmultilinguale5largeinstruct-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Below are examples to encode queries and passages from the MS-MARCO passage ranking dataset.'}
mcro_deberta-v3-base-prompt-injection-v2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model classifies inputs into benign (`0`) and injection-detected (`1`).'}
mcro_ESMFold-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""If you're interested in using ESMFold in practice, please check out the associated [tutorial notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb).""}"
mcro_siglip2so400mpatch16naflex-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{tschannen2025siglip2multilingualvisionlanguage,\n      title={SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features}, \n      author={Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier Hénaff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai},\n      year={2025},\n      eprint={2502.14786},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2502.14786}, \n}'}"
mcro_obideidrobertai2b2,mcro_Model,{}
mcro_bertbaseuncased-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': 'Even if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:'}"
mcro_potion-base-8M-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'It is designed for applications where computational resources are limited or where real-time performance is critical.'}
mcro_googlegemma31bit-Arch,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Gemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.'}"
mcro_fnetbasemodel-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': ""@article{DBLP:journals/corr/abs-2105-03824,\n  author    = {James Lee{-}Thorp and\n               Joshua Ainslie and\n               Ilya Eckstein and\n               Santiago Onta{\\~{n}}{\\'{o}}n},\n  title     = {FNet: Mixing Tokens with Fourier Transforms},\n  journal   = {CoRR},\n  volume    = {abs/2105.03824},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.03824},\n  archivePrefix = {arXiv},\n  eprint    = {2105.03824},\n  timestamp = {Fri, 14 May 2021 12:13:30 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-03824.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}""}"
mcro_germanbert,mcro_Model,{}
mcro_germanbert-Dataset,mcro_DatasetInformationSection,{}
mcro_TaiyiCLIPRoberta102MChinese-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CLIP (Roberta)'}
mcro_banglat5_banglaparaphrase-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_microsoftbeitlargepatch16224-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'BEIT: BERT Pre-Training of Image Transformers'}
mcro_mistralaiMistral7Bv01-Consideration,mcro_ConsiderationInformationSection,{'prov_hasTextValue': 'Mistral 7B is a pretrained base model and therefore does not have any moderation mechanisms.'}
mcro_pyannotespeakerdiarization30-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'AVA-AVD; REPERE; MSDWild; VoxConverse; DIHARD; AMI; AISHELL; Ego4D; AliMeeting'}
mcro_czechwav2vec2xlsr300mcs250-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'wav2vec2-xls-r-300m'}
mcro_multilinguale5large-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with ""query: "" or ""passage: "", even for non-English texts.\n# For tasks other than retrieval, you can simply use the ""query: "" prefix.\ninput_texts = [\'query: how much protein should a female eat\',\n               \'query: 南瓜的家常做法\',\n               ""passage: As a general guideline, the CDC\'s average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you\'ll need to increase that if you\'re expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day."",\n               ""passage: 1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅""]\n\ntokenizer = AutoTokenizer.from_pretrained(\'intfloat/multilingual-e5-large\')\nmodel = AutoModel.from_pretrained(\'intfloat/multilingual-e5-large\')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors=\'pt\')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict[\'attention_mask\'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())'}"
mcro_jonatasgrosmanwav2vec2largexlsr53russian,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2largexlsr53russian-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Russian using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [CSS10](https://github.com/Kyubyong/css10).'}
mcro_chronosbolttiny-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'It chunks the historical time series context into patches of multiple observations'}
mcro_visiontransformerbase-sizedmodel-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.'}
mcro_Auralis,mcro_Model,{}
mcro_ruri-small-v2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Sentence Transformer'}
mcro_rbhatia46financialragmatryoshka-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Sentence Transformer'}
mcro_all-distilroberta-v1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'We used the pretrained [`distilroberta-base`](https://huggingface.co/distilroberta-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.'}"
mcro_ViTSO400M14SigLIP384,mcro_Model,{}
mcro_hallucinationevaluationmodel-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'AggreFact; RAGTruth'}
mcro_granitetimeseriesttmr1-ModelDetail,mcro_ModelDetailSection,{}
mcro_granitetimeseriesttmr1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Dataset,mcro_DatasetInformationSection,{}
mcro_googlest5v11-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'no parameter sharing between embedding and classifier layer; GEGLU activation in feed-forward hidden layer, rather than ReLU'}"
mcro_segformerb1finetunedade20k,mcro_Model,{}
mcro_GIT-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer decoder conditioned on both CLIP image tokens and text tokens'}
mcro_Qwen2505BInstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_Qwen2505BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_sentencetransformersstsbxlmrmultilingual,mcro_Model,{}
mcro_sentencetransformersstsbxlmrmultilingual-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_rbhatia46financialragmatryoshka-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{henderson2017efficient,\n    title={Efficient Natural Language Response Suggestion for Smart Reply}, \n    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\n    year={2017},\n    eprint={1705.00652},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}; @inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://arxiv.org/abs/1908.10084"",\n}; @misc{kusupati2024matryoshka,\n    title={Matryoshka Representation Learning}, \n    author={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},\n    year={2024},\n    eprint={2205.13147},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}'}"
mcro_sentencetransformersparaphraseMiniLML3v2-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_Qwen2515BInstruct,mcro_Model,{}
mcro_Qwen2515BInstruct-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training.'}"
mcro_timmViTB16SigLIPi18n256-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'WebLI'}
mcro_emrecanbertbaseturkishcasedmeannlistsbtr,mcro_Model,{}
mcro_emrecanbertbaseturkishcasedmeannlistsbtr-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'SentenceTransformer'}
mcro_QwenQwen2505BInstruct,mcro_Model,{}
mcro_QwenQwen2505BInstruct-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_metaLlama32Collection-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_FinBERT,mcro_Model,{}
mcro_FinBERT-ModelDetail,mcro_ModelDetailSection,{}
mcro_jhartmannemotionenglishdistilrobertabase-UseCase,mcro_UseCaseInformationSection,{}
mcro_whisper-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer based encoder-decoder model'}
mcro_mistralaiMistral7BInstructv02-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'In order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.'}"
mcro_RoBERTaLargeModel-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion.'}
mcro_owlv2-PrimaryIntendedUseCase,mcro_PrimaryIntendedUseCaseInformationSection,"{'prov_hasTextValue': 'The primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.'}"
mcro_hubertsiuzdaksnac24khz-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech synthesis'}
mcro_flan-t5-base-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_AudioSpectrogramTransformerfinetunedonAudioSet-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Audio Spectrogram Transformer; The Audio Spectrogram Transformer is equivalent to [ViT](https://huggingface.co/docs/transformers/model_doc/vit), but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.'}"
mcro_vitlargepatch14reg4dinov2lvd142m,mcro_Model,{}
mcro_vitlargepatch14reg4dinov2lvd142m-ModelDetail,mcro_ModelDetailSection,{}
mcro_Qwen257BInstruct-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias'}"
mcro_sdxl10refinermodel-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'research purposes only'}
mcro_legalbertTheMuppetsStraightOutOfLawSchool-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{chalkidis-etal-2020-legal,\n    title = ""{LEGAL}-{BERT}: The Muppets straight out of Law School"",\n    author = ""Chalkidis, Ilias  and\n      Fergadiotis, Manos  and\n      Malakasiotis, Prodromos  and\n      Aletras, Nikolaos  and\n      Androutsopoulos, Ion"",\n    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2020"",\n    month = nov,\n    year = ""2020"",\n    address = ""Online"",\n    publisher = ""Association for Computational Linguistics"",\n    doi = ""10.18653/v1/2020.findings-emnlp.261"",\n    pages = ""2898--2904""\n}'}"
mcro_siglip2so400mpatch16naflex-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'You can use the raw model for tasks like zero-shot image classification and\nimage-text retrieval, or as a vision encoder for VLMs (and other vision tasks).'}"
mcro_timmViTB16SigLIPi18n256-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{zhai2023sigmoid,\n  title={Sigmoid loss for language image pre-training},\n  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2303.15343},\n  year={2023}\n}; @misc{big_vision,\n  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},\n  title = {Big Vision},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/google-research/big_vision}}\n}'}"
mcro_facebookesm2t363BUR50D-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'It is suitable for fine-tuning on a wide range of tasks that take protein sequences as input.'}
mcro_sentencetransformersparaphraseMiniLML6v2-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_jonatasgrosmanwav2vec2largexlsr53chinesezhcn-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{grosman2021xlsr53-large-chinese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {C}hinese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn}},\n  year={2021}\n}'}"
mcro_myshellaiMeloTTS-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'MeloTTS is a **high-quality multi-lingual** text-to-speech library'}
mcro_rubertbasecasedsentimentrusentiment,mcro_Model,{}
mcro_wavlmbaseplus-ModelDetail,mcro_ModelDetailSection,{}
mcro_facebookwav2vec2base960h-Citation,mcro_CitationInformationSection,{}
mcro_resnet18a1in1k-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}'}"
mcro_parakeettdt06bv2,mcro_Model,{}
mcro_sentencetransformersLaBSE-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Have a look at [LaBSE](https://tfhub.dev/google/LaBSE/1) for the respective publication that describes LaBSE.'}
mcro_metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_FLD-5B,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'FLD-5B dataset, containing 5.4 billion annotations across 126 million images'}"
mcro_sdxl10refinermodel,mcro_Model,{}
mcro_codebertbase,mcro_Model,{}
mcro_codebertbase-Reference2,mcro_ReferenceInformationSection,"{'prov_hasTextValue': ""1. [CodeBERT trained with Masked LM objective](https://huggingface.co/microsoft/codebert-base-mlm) (suitable for code completion)\n2. 🤗 [Hugging Face's CodeBERTa](https://huggingface.co/huggingface/CodeBERTa-small-v1) (small size, 6 layers)""}"
mcro_WhereIsAIUAELargeV1-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT'}
mcro_flairnerfrench-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}'}"
mcro_metallamaLlama3211BVisionInstruct,mcro_Model,{}
mcro_openvla7b-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': '[Open X-Embodiment](https://robotics-transformer-x.github.io/) -- specific component datasets can be found [here](https://github.com/openvla/openvla).'}
mcro_debertav3basetasksourcenli-ModelDetail,mcro_ModelDetailSection,{}
mcro_distilrobertabase,mcro_Model,{}
mcro_distilrobertabase-ModelParameter,mcro_ModelParameterSection,{}
mcro_deberta-v3-base-prompt-injection-v2,mcro_Model,{}
mcro_deberta-v3-base-prompt-injection-v2-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': '`deberta-v3-base-prompt-injection-v2` is highly accurate in identifying prompt injections in English.\nIt does not detect jailbreak attacks or handle non-English prompts, which may limit its applicability in diverse linguistic environments or against advanced adversarial techniques.\n\nAdditionally, we do not recommend using this scanner for system prompts, as it produces false-positives.'}"
mcro_robertBase-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'masked language modeling (MLM); next sentence prediction (NSP)'}
mcro_metaLlama31Instruct,mcro_Model,{}
mcro_sentencetransformersLaBSE,mcro_Model,{}
mcro_Systranfasterwhispersmall-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'openai/whisper-small'}
mcro_whisperbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer based encoder-decoder model'}
mcro_variousmodels-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Do not use it in production.'}
mcro_jonatasgrosmanwav2vec2xlsr1bportuguese,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2xlsr1bportuguese-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'train and validation splits of [Common Voice 8.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_8_0), [CORAA](https://github.com/nilc-nlp/CORAA), [Multilingual TEDx](http://www.openslr.org/100), and [Multilingual LibriSpeech](https://www.openslr.org/94/).'}"
mcro_jonatasgrosmanwav2vec2largexlsr53english,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2largexlsr53english-ModelDetail,mcro_ModelDetailSection,{}
mcro_upskyybgem3korean-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_testModel-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'citation'}
mcro_deepseekaiDeepSeekV30324,mcro_Model,{}
mcro_NepaliBERT-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Pretraining done on bert base architecture.'}
mcro_wavlmbaseplus-License,mcro_LicenseInformationSection,{}
mcro_byt5small-ModelDetail,mcro_ModelDetailSection,{}
mcro_obideidrobertai2b2-ModelParameter,mcro_ModelParameterSection,{'prov_hasTextValue': '* Training details:'}
mcro_cambridgeltlSapBERTfromPubMedBERTfulltext,mcro_Model,{}
mcro_cambridgeltlSapBERTfromPubMedBERTfulltext-UseCase,mcro_UseCaseInformationSection,{}
mcro_llamaGuard38B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{dubey2024llama3herdmodels,\n  title =         {The Llama 3 Herd of Models},\n  author =        {Llama Team, AI @ Meta},\n  year =          {2024},\n  eprint =        {2407.21783},\n  archivePrefix = {arXiv},\n  primaryClass =  {cs.AI},\n  url =           {https://arxiv.org/abs/2407.21783}\n}'}"
mcro_HelsinkiNLPopusmtzhen-Citation,mcro_CitationInformationSection,{}
mcro_arabertv1andv2,mcro_Model,{}
mcro_arabertv1andv2-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'OSCAR unshuffled and filtered.\n- [Arabic Wikipedia dump](https://archive.org/details/arwiki-20190201) from 2020/09/01\n- [The 1.5B words Arabic Corpus](https://www.semanticscholar.org/paper/1.5-billion-words-Arabic-Corpus-El-Khair/f3eeef4afb81223df96575adadf808fe7fe440b4)\n- [The OSIAN Corpus](https://www.aclweb.org/anthology/W19-4619)\n- Assafir news articles.'}
mcro_sentencetransformersdistilusebasemultilingualcasedv2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_gemma2b-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_gemma2b-EvaluationData,mcro_EvaluationDataInformationSection,{}
mcro_Salesforceblipvqabase,mcro_Model,{}
mcro_Salesforceblipvqabase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}'}"
mcro_Qwen1505BChat-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data.'}"
mcro_M2M100418M,mcro_Model,{}
mcro_M2M100418M-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{fan2020englishcentric,\n      title={Beyond English-Centric Multilingual Machine Translation}, \n      author={Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Edouard Grave and Michael Auli and Armand Joulin},\n      year={2020},\n      eprint={2010.11125},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_CLIPViTbigG14LAION2B-TrainingData,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/). \nFine-tuning was also partially done on LAION-A, a 900M subset of LAION-2B filtered with aesthetic V2 4.5+ and phash deduplicated.\n\n**IMPORTANT NOTE:** The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a “safe” subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress.'}"
mcro_LTXVideo,mcro_Model,{}
mcro_LTXVideo-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'The model works best on resolutions under 720 x 1280 and number of frames below 257.; The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames.; Prompts should be in English. The more elaborate the better. Good prompt looks like `The turquoise waves crash against the dark, jagged rocks of the shore, sending white foam spraying into the air. The scene is dominated by the stark contrast between the bright blue water and the dark, almost black rocks. The water is a clear, turquoise color, and the waves are capped with white foam. The rocks are dark and jagged, and they are covered in patches of green moss. The shore is lined with lush green vegetation, including trees and bushes. In the background, there are rolling hills covered in dense forest. The sky is cloudy, and the light is dim.`'}"
mcro_segformerb5finetunedade20k-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.'}"
mcro_RoBERTaLargeModel,mcro_Model,{}
mcro_RoBERTaLargeModel-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.'}"
mcro_AlibabaNLPgtebaseenv15-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_chronos-t5-tiny-CitationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'If you find Chronos models useful for your research, please consider citing the associated [paper](https://arxiv.org/abs/2403.07815):\n\n@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}'}"
mcro_alakxendermmsttsdivfinetunedmdm01-ModelDetail,mcro_ModelDetailSection,{}
mcro_intfloatmultilinguale5small-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Use ""query: "" prefix for symmetric tasks such as semantic similarity, bitext mining, paraphrase retrieval.'}"
mcro_mobilenetv3small100lambin1k-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_TaiyiCLIPRoberta102MChinese-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{fengshenbang,\n  author    = {Jiaxing Zhang and Ruyi Gan and Junjie Wang and Yuxiang Zhang and Lin Zhang and Ping Yang and Xinyu Gao and Ziwei Wu and Xiaoqun Dong and Junqing He and Jianheng Zhuo and Qi Yang and Yongfeng Huang and Xiayu Li and Yanghan Wu and Junyu Lu and Xinyu Zhu and Weifeng Chen and Ting Han and Kunhao Pan and Rui Wang and Hao Wang and Xiaojun Wu and Zhongshen Zeng and Chongpei Chen},\n  title     = {Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence},\n  journal   = {CoRR},\n  volume    = {abs/2209.02970},\n  year      = {2022}\n}'}"
mcro_openvla7b-Limitation,mcro_LimitationInformationSection,{}
mcro_parakeettdt06bv2-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The model was trained on the Granary dataset[8], consisting of approximately 120,000 hours of English speech data:\n\n- 10,000 hours from human-transcribed NeMo ASR Set 3.0, including:\n  - LibriSpeech (960 hours)\n  - Fisher Corpus\n  - National Speech Corpus Part 1\n  - VCTK\n  - VoxPopuli (English)\n  - Europarl-ASR (English)\n  - Multilingual LibriSpeech (MLS English) – 2,000-hour subset\n  - Mozilla Common Voice (v7.0)\n  - AMI\n\n- 110,000 hours of pseudo-labeled data from:\n  - YTC (YouTube-Commons) dataset[4]\n  - YODAS dataset [5]\n  - Librilight [7]\n\nAll transcriptions preserve punctuation and capitalization. The Granary dataset[8] will be made publicly available after presentation at Interspeech 2025.\n\n**Data Collection Method by dataset**\n\n* Hybrid: Automated, Human\n\n**Labeling Method by dataset**\n\n* Hybrid: Synthetic, Human\n\n**Properties:**\n\n* Noise robust data from various sources\n* Single channel, 16kHz sampled data'}"
mcro_jonatasgrosmanwav2vec2largexlsr53arabic-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Fine-tuned XLSR-53 large model'}
mcro_crossencodermsmarcoMiniLML6v2,mcro_Model,{}
mcro_visiontransformerbase-sizedmodel-hybrid,mcro_Model,{}
mcro_TaiyiCLIPRoberta102MChinese-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Noah-Wukong; Zero'}
mcro_QwenQwen2505BInstruct-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_detrresnet50-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_detrresnet50-PerformanceMetric,mcro_PerformanceMetricInformationSection,{'prov_hasTextValue': 'This model achieves an AP (average precision) of **42.0** on COCO 2017 validation.'}
mcro_DFN5BCLIPViTH14378-ModelDetail,mcro_ModelDetailSection,{}
mcro_DFN5BCLIPViTH14378-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Contrastive Image-Text, Zero-Shot Image Classification.'}"
mcro_LTXVideo-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'license'}
mcro_clipvitlargepatch14-Citation,mcro_CitationInformationSection,{}
mcro_openvla7b-ModelDetail,mcro_ModelDetailSection,{}
mcro_TahaDouajidetrdoctabledetection-UseCase,mcro_UseCaseInformationSection,{}
mcro_TahaDouajidetrdoctabledetection-PrimaryIntendedUseCase,mcro_PrimaryIntendedUseCaseInformationSection,{}
mcro_sentencetransformersparaphrasexlmrmultilingualv1-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}
mcro_XuhuiToxDectrobertalarge-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'tweets'}
mcro_resnet50v1.5-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'ResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections.'}
mcro_facebookesm2t363BUR50D-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'ESM-2 is a state-of-the-art protein model trained on a masked language modelling objective.'}
mcro_mimi-OutOfScopeUseCase,mcro_OutOfScopeUseCaseSectionInformation,{}
mcro_gtelargeenv15-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformer++'}
mcro_vocos-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The code in this repository is released under the MIT license.'}
mcro_Salesforcecodet5basemultisum-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'for code summarization'}
mcro_ruri-small-v2-ReferenceInformationSection,mcro_ReferenceInformationSection,{'prov_hasTextValue': 'https://arxiv.org/abs/2409.07737'}
mcro_Qwen306B-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'translation; multi-turn dialogues; creative writing; efficient, general-purpose dialogue; complex logical reasoning, math, and coding; instruction following; role-playing; multilingual instruction following; precise integration with external tools in both thinking and unthinking modes'}"
mcro_doclingmodels-Reference,mcro_ReferenceInformationSection,"{'prov_hasTextValue': '@techreport{Docling,\n  author = {Deep Search Team},\n  month = {8},\n  title = {{Docling Technical Report}},\n  url={https://arxiv.org/abs/2408.09869},\n  eprint={2408.09869},\n  doi = ""10.48550/arXiv.2408.09869"",\n  version = {1.0.0},\n  year = {2024}\n}\n\n@article{doclaynet2022,\n  title = {DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis},  \n  doi = {10.1145/3534678.353904},\n  url = {https://arxiv.org/abs/2206.01062},\n  author = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter W J},\n  year = {2022}\n}\n\n@InProceedings{TableFormer2022,\n    author    = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},\n    title     = {TableFormer: Table Structure Understanding With Transformers},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2022},\n    pages     = {4614-4623},\n    doi = {https://doi.org/10.1109/CVPR52688.2022.00457}\n}'}"
mcro_AlibabaNLPgteQwen27Binstruct-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}'}"
mcro_sdxlinpainting01-Bias,mcro_ConsiderationInformationSection,{}
mcro_Auralis-ModelDetail,mcro_ModelDetailSection,{}
mcro_facebookdinov2large-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Vision Transformer (ViT)'}
mcro_HelsinkiNLPopusmtzhen-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CC-BY-4.0'}
mcro_Stablediffusionsafetychecker-ModelDetail,mcro_ModelDetailSection,{}
mcro_sentencetransformersdistilbertbasenlimeantokens,mcro_Model,{}
mcro_kobert-ReferenceInformationSection,mcro_ReferenceInformationSection,{'prov_hasTextValue': '- https://github.com/SKTBrain/KoBERT'}
mcro_llavahfllava157bhf-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Llama 2 is licensed under the LLAMA 2 Community License,\nCopyright (c) Meta Platforms, Inc. All Rights Reserved.'}"
mcro_ko-sroberta-multitask,mcro_Model,{}
mcro_ko-sroberta-multitask-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: RobertaModel   (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False}))""}"
mcro_granitetimeseriesttmr2model-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_Stablediffusionv14ModelCard-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The model is intended for research purposes only.'}
mcro_czechwav2vec2xlsr300mcs250-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'common_voice 8.0'}
mcro_AlibabaNLPgtebaseenv15-UseCase,mcro_UseCaseInformationSection,{}
mcro_graphcodebert,mcro_Model,{}
mcro_graphcodebert-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'More details can be found in the [paper](https://arxiv.org/abs/2009.08366) by Guo et. al.'}
mcro_distilwhisperdistillargev3-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': '22,000 hours of audio data'}"
mcro_RoBERTaLargeModel-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.""}"
mcro_jonatasgrosmanwav2vec2largexlsr53english-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_siglip2so400mpatch16naflex,mcro_Model,{}
mcro_segformerb5finetunedade20k-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you.'}
mcro_BAAIbgererankerv2m3-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_conjunctsditre15-ModelDetail,mcro_ModelDetailSection,{}
mcro_nomicainomicembedtextv1,mcro_Model,{}
mcro_nomicainomicembedtextv1-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_FinBERT-Citation3,mcro_CitationInformationSection,{'prov_hasTextValue': '[blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.'}
mcro_debertav3basetasksourcenli-License,mcro_LicenseInformationSection,{}
mcro_gpt2medium-ModelDetail,mcro_ModelDetailSection,{}
mcro_gpt2medium-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{radford2019language,\n  title={Language models are unsupervised multitask learners},\n  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},\n  journal={OpenAI blog},\n  volume={1},\n  number={8},\n  pages={9},\n  year={2019}\n}'}"
mcro_trpakovvitfaceexpression,mcro_Model,{}
mcro_trpakovvitfaceexpression-LimitationGeneralization,mcro_LimitationInformationSection,{'prov_hasTextValue': 'Generalization'}
mcro_Qwen2VL2BInstruct-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': 'Limited Capacity for Complex Instruction; Insufficient Counting Accuracy; Constraints in Individuals and Intellectual Property (IP); Lack of Audio Support; Weak Spatial Reasoning Skills; Data timeliness'}
mcro_granitetimeseriesttmr1-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{}
mcro_gemma3-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google\'s commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google\'s latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *""the \'single\ncontroller\' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.""*'}"
mcro_banglat5_banglaparaphrase,mcro_Model,{}
mcro_banglat5_banglaparaphrase-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_trlinternaltestingtinyQwen2ForCausalLM25,mcro_Model,{}
mcro_bert-base-uncased-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Intended use case'}
mcro_BAAIbgesmallen-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_whisper-baseen,mcro_Model,{}
mcro_nguyenvulebinhwav2vec2basevi,mcro_Model,{}
mcro_nguyenvulebinhwav2vec2basevi-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Our self-supervised model is pre-trained on a massive audio set of 13k hours of Vietnamese youtube audio, which includes:\n  - Clean audio\n  - Noise audio\n  - Conversation\n  - Multi-gender and dialects'}"
mcro_infoxlm,mcro_Model,{}
mcro_ocr-equation-images-and-text-to-latex,mcro_Model,{}
mcro_roberta-baseforextractiveqa-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'SQuAD 2.0'}
mcro_blackforestlabsFLUX1Filldev-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model and its derivatives may not be used\n\n- In any way that violates any applicable national, federal, state, local or international law or regulation.\n- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\n- To generate or disseminate verifiably false information and/or content with the purpose of harming others.\n- To generate or disseminate personal identifiable information that can be used to harm an individual.\n- To harass, abuse, threat'}"
mcro_bartlargemnli,mcro_Model,{}
mcro_bartlargemnli-UseCase,mcro_UseCaseInformationSection,{}
mcro_SmolDocling256Mpreview,mcro_Model,{}
mcro_SmolDocling256Mpreview-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)'}
mcro_facebookmusicgenmedium-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{copet2023simple,\n      title={Simple and Controllable Music Generation}, \n      author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre Défossez},\n      year={2023},\n      eprint={2306.05284},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD}\n}'}"
mcro_BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}'}"
mcro_distilrobertabase-ModelDetail,mcro_ModelDetailSection,{}
mcro_metaLlama31Instruct-OutOfScopeUseCase,mcro_OutofScopeUseCaseSectionInformation,{'prov_hasTextValue': 'Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.'}
mcro_owlv2-PrimaryUse,mcro_PrimaryIntendedUseCaseInformationSection,"{'prov_hasTextValue': 'The primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.'}"
mcro_myshellaiMeloTTS-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@software{zhao2024melo,\n  author={Zhao, Wenliang and Yu, Xumin and Qin, Zengyi},\n  title = {MeloTTS: High-quality Multi-lingual Multi-accent Text-to-Speech},\n  url = {https://github.com/myshell-ai/MeloTTS},\n  year = {2023}\n}'}"
mcro_llavamodelcard-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'https://github.com/haotian-liu/LLaVA/issues; Llama 2 is licensed under the LLAMA 2 Community License,\nCopyright (c) Meta Platforms, Inc. All Rights Reserved.'}"
mcro_timmViTB16SigLIPi18n256-UseCase,mcro_UseCaseInformationSection,{}
mcro_facebookhubertlargels960ft-Citation,mcro_CitationInformationSection,{}
mcro_chronos-t5-tiny-LicenseSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'This project is licensed under the Apache-2.0 License.'}
mcro_Distilbertbasemultilingualcasednerhrl-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'distilbert-base-multilingual-cased'}
mcro_wav2vec2xlsr300mcv7turkish-ModelParameter,mcro_ModelParameterSection,{}
mcro_EleutherAIpythia70mdeduped-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-based Language Model'}
mcro_wav2vec2-baseforemotionrecognition-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""This is a ported version of \n[S3PRL's Wav2Vec2 for the SUPERB Emotion Recognition task](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/emotion).\n\nThe base model is [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base), which is pretrained on 16kHz \nsampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n\nFor more information refer to [SUPERB: Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051)""}"
mcro_sdxlinpainting01-UseCase,mcro_UseCaseInformationSection,{}
mcro_yuvalkirstainPickScorev1-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{Kirstain2023PickaPicAO,\n  title={Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation},\n  author={Yuval Kirstain and Adam Polyak and Uriel Singer and Shahbuland Matiana and Joe Penna and Omer Levy},\n  year={2023}\n}'}"
mcro_CLIPViTH14LAION2B-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model.'}"
mcro_googlest5v11-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task'}"
mcro_appleOpenELM11BInstruct-UseCase,mcro_UseCaseInformationSection,{}
mcro_trlinternaltestingtinyQwen2ForCausalLM25-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library.'}
mcro_Salesforceblipvqacapfiltlarge-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'conditional and un-conditional image captioning'}
mcro_TaiyiCLIPRoberta102MChinese,mcro_Model,{}
mcro_lftwr4target-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{vidgen2021lftw,\n  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},\n  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},\n  booktitle={ACL},\n  year={2021}\n}'}"
mcro_microsoftmdebertav3base-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}'}"
mcro_jinaaijinaembeddingsv3,mcro_Model,{}
mcro_jinaaijinaembeddingsv3-LicenseInformationSection,mcro_LicenseInformationSection,"{'prov_hasTextValue': '`jina-embeddings-v3` is listed on AWS & Azure. If you need to use it beyond those platforms or on-premises within your company, note that the models is licensed under CC BY-NC 4.0. For commercial usage inquiries, feel free to [contact us](https://jina.ai/contact-sales/).'}"
mcro_Marqo-FashionSigLIP-ModelDetail,mcro_ModelDetailSection,{}
mcro_Marqo-FashionSigLIP-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_debertav3basetasksourcenli-Arch,mcro_ModelArchitectureInformationSection,{}
mcro_Qwen2VL2BInstruct-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Agent that can operate your mobiles, robots, etc.; Multilingual Support; SoTA understanding of images of various resolution & ratio; Understanding videos of 20min+'}"
mcro_timmViTB16SigLIPi18n256-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'A SigLIP (Sigmoid loss for Language-Image Pre-training) model trained on WebLI.'}
mcro_timmViTB16SigLIPi18n256-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Contrastive Image-Text, Zero-Shot Image Classification.'}"
mcro_t5large-UseCase,mcro_UseCaseInformationSection,{}
mcro_alakxendermmsttsdivfinetunedmdm01-IntendedUseCase,mcro_UseCaseInformationSection,{}
mcro_unslothQwen2505BInstructbnb4bit-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'instruction-tuned 0.5B Qwen2.5 model'}
mcro_metallamaMetaLlama38B-Benchmark,mcro_QuantativeAnalysisSection,{}
mcro_openvla7b,mcro_Model,{}
mcro_CLIPViTbigG14LAION2B-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{Radford2021LearningTV,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n  booktitle={ICML},\n  year={2021}\n}'}"
mcro_Salesforceblip2opt27b-ModelDetail,mcro_ModelDetailSection,{}
mcro_Salesforceblip2opt27b-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for conditional text generation given an image and optional text. See the model hub to look for\nfine-tuned versions on a task that interests you.'}
mcro_whisperbase-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': '680,000 hours of audio and the corresponding transcripts collected from the internet'}"
mcro_EleutherAIpythia70mdeduped-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'https://arxiv.org/pdf/2304.01373.pdf'}
mcro_googleflant5small-ModelDetail,mcro_ModelDetailSection,{}
mcro_llavamodelcard-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The primary use of LLaVA is research on large multimodal models and chatbots.; The primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence.'}"
mcro_clip-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{'prov_hasTextValue': 'We have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid'}
mcro_convnextv2_nano.fcmae_ft_in22kin1k,mcro_Model,{}
mcro_convnextv2_nano.fcmae_ft_in22kin1k-UseCase,mcro_UseCaseInformationSection,{}
mcro_distilrobertabase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-based language model'}
mcro_googleflant5small-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_t5small-ModelDetail,mcro_ModelDetailSection,{}
mcro_t5small-Citation,mcro_CitationInformationSection,{}
mcro_deepseekaiDeepSeekR1DistillLlama8B-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Reasoning'}
mcro_BAAIbgereRankerBase,mcro_Model,{}
mcro_BAAIbgereRankerBase-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_flairnerfrench-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Flair embeddings; LSTM-CRF'}
mcro_BAAIbgelargeen-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'FlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.'}
mcro_unslothDeepSeekR10528Qwen38BGGUF-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_xlmrobertabaselanguagedetection-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Unsupervised Cross-lingual Representation Learning at Scale'}
mcro_Salesforceblipvqacapfiltlarge-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}'}"
mcro_Qwen257BInstruct-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'instruction-tuned 7B Qwen2.5 model'}
mcro_MyAwesomeModel-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CNN'}
mcro_sentencetransformersparaphrasexlmrmultilingualv1,mcro_Model,{}
mcro_sentencetransformersparaphrasexlmrmultilingualv1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_wav2vec2xlsr300mhebrew-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_Falconsainsfwimagedetection-Reference,mcro_ReferenceInformationSection,{'prov_hasTextValue': '- [Hugging Face Model Hub](https://huggingface.co/models)\n- [Vision Transformer (ViT) Paper](https://arxiv.org/abs/2010.11929)\n- [ImageNet-21k Dataset](http://www.image-net.org/)'}
mcro_AlibabaNLPgtebaseenv15-ModelDetail,mcro_ModelDetailSection,{}
mcro_DFN5BCLIPViTH14378-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{fang2023data,\n  title={Data Filtering Networks},\n  author={Fang, Alex and Jose, Albin Madappally and Jain, Amit and Schmidt, Ludwig and Toshev, Alexander and Shankar, Vaishaal},\n  journal={arXiv preprint arXiv:2309.17425},\n  year={2023}\n}'}"
mcro_jinaaijinarerankerv2basemultilingual,mcro_Model,{}
mcro_jinaaijinarerankerv2basemultilingual-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""The Jina Reranker v2 (`jina-reranker-v2-base-multilingual`) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy.\n\nCompared with the state-of-the-art reranker models, including the previous released `jina-reranker-v1-base-en`, the **Jina Reranker v2** model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks.\n\nThe `jina-reranker-v2-base-multilingual` model is capable of handling long texts with a context length of up to `1024` tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately.\n\nThe model is also equipped with a flash attention mechanism, which significantly improves the model's performance.""}"
mcro_openaiclip-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_oliverguhrfullstoppunctuationmultilanglarge-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'xlm-roberta-base'}
mcro_Inteldptlarge-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The dataset is called MIX 6, and contains around 1.4M images. The model was initialized with ImageNet-pretrained weights.'}"
mcro_StanfordAIMIstanforddeidentifierbase,mcro_Model,{}
mcro_codebertbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model is initialized with Roberta-base and trained with MLM+RTD objective (cf. the paper).'}
mcro_bigvganv244khz128band512x-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Universal Neural Vocoder'}
mcro_blackforestlabsFLUX1dev,mcro_Model,{}
mcro_blackforestlabsFLUX1dev-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'This model falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).'}
mcro_stablediffusioninpaintingmodelcard,mcro_Model,{}
mcro_stablediffusioninpaintingmodelcard-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-image generation model'}
mcro_unslothDeepSeekR10528Qwen38BGGUF,mcro_Model,{}
mcro_canary1bflash-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Canary is an encoder-decoder model with FastConformer [3] Encoder and Transformer Decoder [4]. With audio features extracted from the encoder, task tokens such as <target language>, <task>, <toggle timestamps> and <toggle PnC> are fed into the Transformer Decoder to trigger the text generation process. Canary uses a concatenated tokenizer [5] from individual SentencePiece [6] tokenizers of each language, which makes it easy to scale up to more languages. The canary-1b-flash model has 32 encoder layers and 4 decoder layers, leading to a total of 883M parameters. For more details about the architecture, please refer to [1].'}"
mcro_jonatasgrosmanwav2vec2largexlsr53polish-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_microsoftFlorence2large-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This model is capable of performing different tasks through changing the prompts. Tasks include Caption, Detailed Caption, More Detailed Caption, Caption to Phrase Grounding, Object Detection, Dense Region Caption, Region proposal, OCR, OCR with Region, Output confidence score with Object Detection'}"
mcro_robertBase-ModelDetail,mcro_ModelDetailSection,{}
mcro_robertBase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{masala2020robert,\n  title={RoBERT--A Romanian BERT Model},\n  author={Masala, Mihai and Ruseti, Stefan and Dascalu, Mihai},\n  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},\n  pages={6626--6637},\n  year={2020}\n}'}"
mcro_sentencetransformersparaphrasempnetbasev2,mcro_Model,{}
mcro_sentencetransformersparaphrasempnetbasev2-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering; semantic search'}
mcro_openaiwhisperlargev3turbo-Citation,mcro_CitationInformationSection,{}
mcro_mobilenetv3small100lambin1k-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'https://github.com/huggingface/pytorch-image-models'}
mcro_metaLlama3,mcro_Model,{}
mcro_metaLlama3-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}'}"
mcro_wav2vec2largexlsr53espeakcvft-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'recognize phonetic labels in multiple languages'}
mcro_Bertbasemultilingualcasednerhrl-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'bert-base-multilingual-cased'}
mcro_sentencetransformersparaphraseMiniLML3v2,mcro_Model,{}
mcro_metaLlama32Collection-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).'}"
mcro_llama32Collection-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.'}"
mcro_wav2vec2xlsr300mcv7turkish,mcro_Model,{}
mcro_Qwen2532BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias'}"
mcro_gte-base,mcro_Model,{}
mcro_gte-base-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT framework'}
mcro_mobilenetv3small100lambin1k-UseCase2,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Feature Map Extraction'}
mcro_microsoftdebertav3large,mcro_Model,{}
mcro_microsoftdebertav3large-Citation,mcro_CitationInformationSection,{}
mcro_UFNLPgatortronbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT architecure'}
mcro_sentencetransformersparaphrasempnetbasev2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_englishnerinflairfastmodel-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'English NER'}
mcro_Snowflakesnowflakearcticembedxs-ModelDetail,mcro_ModelDetailSection,{}
mcro_OrengutengLlama38BLexiUncensored-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'META LLAMA 3 COMMUNITY LICENSE AGREEMENT'}
mcro_fasttextlanguageidentification-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'fastText is a library for efficient learning of word representations and sentence classification.'}
mcro_granitetimeseriesttmr1,mcro_Model,{}
mcro_googleelectrabasediscriminator,mcro_Model,{}
mcro_crossencodermsmarcoMiniLML12v2,mcro_Model,{}
mcro_crossencodermsmarcoMiniLML12v2-Performance,mcro_PerformanceMetricInformationSection,{}
mcro_intfloate5mistral7binstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model has 32 layers and the embedding size is 4096.'}
mcro_mixedbreadaimxbaiembedlargev1,mcro_Model,{}
mcro_gemma3-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google\'s commitments to operate sustainably][sustainability].; Training was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google\'s latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *""the \'single\ncontroller\' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.""*'}"
mcro_Qwen257B-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Causal Language Models'}
mcro_oliverguhrfullstoppunctuationmultilanglarge-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': 'confusion matrix t/p 0 . , ? - : 0 1.0 0.0 0.0 0.0 0.0 0.0 . 0.0 1.0 0.0 0.0 0.0 0.0 , 0.1 0.0 0.9 0.0 0.0 0.0 ? 0.0 0.1 0.0 0.8 0.0 0.0 - 0.1 0.1 0.5 0.0 0.3 0.0 : 0.0 0.3 0.1 0.0 0.0 0.5'}"
mcro_detoxify-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Toxic comment classification'}
mcro_variousmodels,mcro_Model,{}
mcro_chronos-t5-tiny,mcro_Model,{}
mcro_chronosboltbase-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'time series forecasting'}
mcro_BAAIbgem3-ModelDetail,mcro_ModelDetailSection,{}
mcro_llama2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.'}
mcro_petalsteamStableBeluga2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Start chatting with `Stable Beluga 2` using the following code snippet:'}
mcro_detoxify-UseCaseInformationSection_content_moderators,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'aid content moderators in flagging out harmful content quicker'}
mcro_gemma3,mcro_Model,{}
mcro_Supabasegtesmall-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'General Text Embeddings (GTE) model.\n\nThe GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including [GTE-large](https://huggingface.co/thenlper/gte-large), [GTE-base](https://huggingface.co/thenlper/gte-base), and [GTE-small](https://huggingface.co/thenlper/gte-small). The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including **information retrieval**, **semantic textual similarity**, **text reranking**, etc.'}"
mcro_visiontransformerbase-sizedmodel,mcro_Model,{}
mcro_rinnajapanesecloobvitb16-Citation1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{rinna-japanese-cloob-vit-b-16,\n    title = {rinna/japanese-cloob-vit-b-16},\n    author = {Shing, Makoto and Zhao, Tianyu and Sawada, Kei},\n    url = {https://huggingface.co/rinna/japanese-cloob-vit-b-16}\n}'}"
mcro_opensearchprojectopensearchneuralsparseencodingdocv2distill-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache v2.0 License'}
mcro_prajjwal1berttiny,mcro_Model,{}
mcro_microsoftPhi3mini4kinstruct-TrainingDataInformationSection,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Our training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).'}"
mcro_stablediffusionv2-1-ModelDetail,mcro_ModelDetailSection,{}
mcro_NLLB200-EvalData,mcro_EvaluationDataInformationSection,{'prov_hasTextValue': '- Datasets: Flores-200 dataset is described in Section 4\n- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\n- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200.'}
mcro_deberta-v3-base-prompt-injection-v2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deberta-v3-base-prompt-injection-v2,\n  author = {ProtectAI.com},\n  title = {Fine-Tuned DeBERTa-v3-base for Prompt Injection Detection},\n  year = {2024},\n  publisher = {HuggingFace},\n  url = {https://huggingface.co/ProtectAI/deberta-v3-base-prompt-injection-v2},\n}'}"
mcro_microsoftdebertav3base,mcro_Model,{}
mcro_microsoftdebertav3base-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.'}
mcro_vit_base_patch16_224.augreg2_in21k_ft_in1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_intfloatmultilinguale5largeinstruct-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': '**Initialization**: [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)\n\n**First stage**: contrastive pre-training with 1 billion weakly supervised text pairs.\n\n**Second stage**: fine-tuning on datasets from the [E5-mistral](https://arxiv.org/abs/2401.00368) paper.'}
mcro_Inteldptlarge-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. \nIt was introduced in the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ranftl et al. (2021) and first released in [this repository](https://github.com/isl-org/DPT). \nDPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dpt_architecture.jpg)\n\nThe model card has been written in combination by the Hugging Face team and Intel.'}
mcro_scb10xtyphoon21gemma312b-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Gemma License'}
mcro_prithividaparrotparaphraseronT5-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'prithivida/parrot_paraphraser_on_T5'}
mcro_tsmatzxlmrobertanerjapanese,mcro_Model,{}
mcro_Whispertiny,mcro_Model,{}
mcro_multiqaminiLML6cosv1,mcro_Model,{}
mcro_CLIPViTbigG14LAION2B-Citation4,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{cherti2022reproducible,\n  title={Reproducible scaling laws for contrastive language-image learning},\n  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n  journal={arXiv preprint arXiv:2212.07143},\n  year={2022}\n}'}"
mcro_chronosbolttiny-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations.; T5 encoder-decoder architecture'}
mcro_autogluonchronosboltbase-ModelDetail,mcro_ModelDetailSection,{}
mcro_Whispertiny-ModelDetail,mcro_ModelDetailSection,{}
mcro_edgenextsmallusiin1k-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{https://doi.org/10.48550/arxiv.2204.03475,\n  doi = {10.48550/ARXIV.2204.03475},  \n  url = {https://arxiv.org/abs/2204.03475},  \n  author = {Ridnik, Tal and Lawen, Hussam and Ben-Baruch, Emanuel and Noy, Asaf},  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},  \n  title = {Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results},  \n  publisher = {arXiv},  \n  year = {2022},  \n}'}"
mcro_HelsinkiNLPopusmtzhen,mcro_Model,{}
mcro_HelsinkiNLPopusmtzhen-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_surya-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'OCR model for [surya](https://www.github.com/VikParuchuri/surya)'}
mcro_fnetbasemodel-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{'prov_hasTextValue': 'FNet-base was fine-tuned and evaluated on the validation data of the [GLUE benchamrk](https://huggingface.co/datasets/glue).'}
mcro_testmodel-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'image classification'}
mcro_trocrbasesroie-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{li2021trocr,\n      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n      year={2021},\n      eprint={2109.10282},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_text-detection-model-for-surya-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Text detection model'}
mcro_jonatasgrosmanwav2vec2largexlsr53persian,mcro_Model,{}
mcro_Qwen257BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias'}"
mcro_BEiTbasesizedmodelfinetunedonImageNet22k,mcro_Model,{}
mcro_BEiTbasesizedmodelfinetunedonImageNet22k-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=microsoft/beit) to look for\nfined-tuned versions on a task that interests you.'}
mcro_metallamaLlama3370BInstruct-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases.'}"
mcro_xtunerllavallama38bv11transformers,mcro_Model,{}
mcro_xtunerllavallama38bv11transformers-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{2023xtuner,\n    title={XTuner: A Toolkit for Efficiently Fine-tuning LLM},\n    author={XTuner Contributors},\n    howpublished = {\\url{https://github.com/InternLM/xtuner}},\n    year={2023}\n}'}"
mcro_scb10xtyphoon21gemma312b-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{typhoon2,\n      title={Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models}, \n      author={Kunat Pipatanakul and Potsawee Manakul and Natapong Nitarach and Warit Sirichotedumrong and Surapon Nonesung and Teetouch Jaknamon and Parinthapat Pengpun and Pittawat Taveekitworachai and Adisai Na-Thalang and Sittipong Sripaisarnmongkol and Krisanapong Jirayoot and Kasima Tharnpipitchai},\n      year={2024},\n      eprint={2412.13702},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.13702}, \n}'}"
mcro_englishnerinflairfastmodel-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}'}"
mcro_jonatasgrosmanwav2vec2largexlsr53hungarian,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2largexlsr53hungarian-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_whisper-Performance,mcro_QuantativeAnalysisSection,"{'prov_hasTextValue': 'improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level.'}"
mcro_trocrsmalliam-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you.'}
mcro_Protbertmodel-EvaluationDataInformationSection,mcro_EvaluationDataInformationSection,{}
mcro_microsoftPhi4multimodalinstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_microsoftPhi4multimodalinstruct-TrainingData,mcro_DatasetInformationSection,"{'prov_hasTextValue': '5T tokens, 2.3M speech hours, and 1.1T image-text tokens'}"
mcro_t5base-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'T5-Base is the checkpoint with 220 million parameters.'}
mcro_t5base-Citation,mcro_CitationInformationSection,{}
mcro_owlvitbasepatch32-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The model uses a CLIP backbone with a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective.'}"
mcro_CLIPViTH14LAION2B-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).'}
mcro_Supabasegtesmall-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens.'}"
mcro_stablediffusioninpaintingmodelcard-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The CreativeML OpenRAIL M license'}
mcro_cognitivecomputationsdolphin291yi1534b,mcro_Model,{}
mcro_cognitivecomputationsdolphin291yi1534b-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'apache 2.0 license'}
mcro_bert-base-uncased-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'mit'}
mcro_gemma3model-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}'}"
mcro_FinetunedT5SmallTextSummarization-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Text Summarization'}
mcro_allmpnetbasev2-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.'}
mcro_BiomedParse-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Zhao, T., Gu, Y., Yang, J. et al. A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nat Methods 22, 166–176 (2025). https://doi.org/10.1038/s41592-024-02499-w\n\n@article{zhao2025foundation,\n  title={A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities},\n  author={Zhao, Theodore and Gu, Yu and Yang, Jianwei and Usuyama, Naoto and Lee, Ho Hin and Kiblawi, Sid and Naumann, Tristan and Gao, Jianfeng and Crabtree, Angela and Abel, Jacob and others},\n  journal={Nature methods},\n  volume={22},\n  number={1},\n  pages={166--176},\n  year={2025},\n  publisher={Nature Publishing Group US New York}\n}'}"
mcro_xtunerllavallama38bv11transformers-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'llava-llama-3-8b-v1_1-hf is a LLaVA model fine-tuned from [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and [CLIP-ViT-Large-patch14-336](https://huggingface.co/openai/clip-vit-large-patch14-336) with [ShareGPT4V-PT](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V) and [InternVL-SFT](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets) by [XTuner](https://github.com/InternLM/xtuner).\n\n**Note: This model is in HuggingFace LLaVA format.**\n\nResources:\n\n- GitHub: [xtuner](https://github.com/InternLM/xtuner)\n- Official LLaVA format model: [xtuner/llava-llama-3-8b-v1_1-hf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-hf)\n- XTuner LLaVA format model: [xtuner/llava-llama-3-8b-v1_1](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1)\n- GGUF format model: [xtuner/llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf)'}
mcro_colbertv2,mcro_Model,{}
mcro_tiiuaefalconrw1b-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'RefinedWeb'}
mcro_sentencetransformersrobertabasenlimeantokens-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_scb10xtyphoon21gemma312b-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'instructional model'}
mcro_vikplayoutsegmenter-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Based on layoutlmv3.'}
mcro_GIT-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'visual question answering (VQA) on images and videos; image classification; image and video captioning'}
mcro_stabilityaisdturbo-ModelDetail,mcro_ModelDetailSection,{}
mcro_FacebookAIxlmrobertabase-UseCase,mcro_UseCaseInformationSection,{}
mcro_testmodel-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CNN'}
mcro_e5-base-v2,mcro_Model,{}
mcro_e5-base-v2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '[Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/pdf/2212.03533.pdf).\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022'}"
mcro_rinnajapanesecloobvitb16-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The model was trained  a ViT-B/16 Transformer architecture as an image encoder and uses a 12-layer BERT as a text encoder. The image encoder was initialized from the [AugReg `vit-base-patch16-224` model](https://github.com/google-research/vision_transformer).'}
mcro_llamaGuard38B-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'There are some limitations associated with Llama Guard 3. First, Llama Guard 3 itself is an LLM fine-tuned on Llama 3.1. Thus, its performance (e.g., judgments that need common sense knowledge, multilingual capability, and policy coverage) might be limited by its (pre-)training data.\n\nSome hazard categories may require factual, up-to-date knowledge to be evaluated (for example, S5: Defamation, S8: Intellectual Property, and S13: Elections) . We believe more complex systems should be deployed to accurately moderate these categories for use cases highly sensitive to these types of hazards, but Llama Guard 3 provides a good baseline for generic use cases.\n\nLastly, as an LLM, Llama Guard 3 may be susceptible to adversarial attacks or prompt injection attacks that could bypass or alter its intended use. Please feel free to [report](https://github.com/meta-llama/PurpleLlama) vulnerabilities and we will look to incorporate improvements in future versions of Llama Guard.'}"
mcro_sdxl10refinermodel-ModelDetail,mcro_ModelDetailSection,{}
mcro_sdxl10refinermodel-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-image generative model'}
mcro_parakeet_rnnt_06b-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': '`parakeet-rnnt-0.6b` is an ASR model that transcribes speech in lower case English alphabet. This model is jointly developed by [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) and [Suno.ai](https://www.suno.ai/) teams.\nIt is an XL version of FastConformer Transducer [1] (around 600M parameters) model.\nSee the [model architecture](#model-architecture) section and [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) for complete architecture details.'}
mcro_metallamaLlama323BInstruct,mcro_Model,{}
mcro_metallamaLlama323BInstruct-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.'}"
mcro_whisper-baseen-License,mcro_LicenseInformationSection,{}
mcro_yuvalkirstainPickScorev1-ModelDetail,mcro_ModelDetailSection,{}
mcro_yuvalkirstainPickScorev1-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CLIP-H'}
mcro_ocr-equation-images-and-text-to-latex-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'See [texify](https://github.com/VikParuchuri/texify).'}
mcro_parakeettdt06bv2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms.'}"
mcro_segformerb0finetunedade20k-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you.'}
mcro_StellaEn400Mv5,mcro_Model,{}
mcro_StellaEn400Mv5-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The models are trained based on `Alibaba-NLP/gte-large-en-v1.5` and `Alibaba-NLP/gte-Qwen2-1.5B-instruct`.'}
mcro_layoutlmbaseuncased-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'document image understanding and information extraction tasks, such as form understanding and receipt understanding'}"
mcro_petalsteamStableBeluga2,mcro_Model,{}
mcro_petalsteamStableBeluga2-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': '`Stable Beluga 2` is a Llama2 70B model finetuned on an Orca style Dataset'}
mcro_bertMiniatures,mcro_Model,{}
mcro_bertMiniatures-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'standard BERT recipe (including model architecture and training objective)'}
mcro_stablediffusionv2-1-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CreativeML Open RAIL++-M License'}
mcro_mmlwretrievalrobertalarge-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'MMLW (muszę mieć lepszą wiadomość) are neural text encoders for Polish.\nThis model is optimized for information retrieval tasks. It can transform queries and passages to 1024 dimensional vectors. \nThe model was developed using a two-step procedure: \n- In the first step, it was initialized with Polish RoBERTa checkpoint, and then trained with [multilingual knowledge distillation method](https://aclanthology.org/2020.emnlp-main.365/) on a diverse corpus of 60 million Polish-English text pairs. We utilised [English FlagEmbeddings (BGE)](https://huggingface.co/BAAI/bge-large-en) as teacher models for distillation. \n- The second step involved fine-tuning the obtained models with contrastrive loss on [Polish MS MARCO](https://huggingface.co/datasets/clarin-knext/msmarco-pl) training split. In order to improve the efficiency of contrastive training, we used large batch sizes - 1152 for small, 768 for base, and 288 for large models. Fine-tuning was conducted on a cluster of 12 A100 GPUs.'}"
mcro_huggingquantsMetaLlama318BInstructAWQINT4-Architecture,mcro_ModelArchitectureInformationSection,{}
mcro_scb10xtyphoon21gemma312b,mcro_Model,{}
mcro_scb10xtyphoon21gemma312b-ModelDetail,mcro_ModelDetailSection,{}
mcro_bertbaseuncased-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion.'}
mcro_facebookmmslid256-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech language identification (LID)'}
mcro_chronosboltmini,mcro_Model,{}
mcro_chronosboltmini-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations.'}
mcro_hubertbase-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'See [this blog](https://huggingface.co/blog/fine-tune-wav2vec2-english) for more information on how to fine-tune the model. Note that the class `Wav2Vec2ForCTC` has to be replaced by `HubertForCTC`.'}
mcro_TRELLISImageLarge,mcro_Model,{}
mcro_TRELLISImageLarge-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'Structured 3D Latents for Scalable and Versatile 3D Generation'}
mcro_mobilenetv3small100lambin1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_mistralaiMixtral8x7BInstructv01,mcro_Model,{}
mcro_mobilevitsmall-ModelDetail,mcro_ModelDetailSection,{}
mcro_gpt2-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'all the web pages from outbound links on Reddit which received at least 3 karma'}
mcro_stabilityaistablediffusion35medium-ModelDetail,mcro_ModelDetailSection,{}
mcro_faceparsing-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-based semantic segmentation image model'}
mcro_siglipso400mpatch14384-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'zero-shot image classification and image-text retrieval'}
mcro_parakeettdt06bv2-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'GOVERNING TERMS: Use of this model is governed by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode.en) license.'}
mcro_Qwen2505BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_ibmresearchMoLFormerXLboth10pct-Citation1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{10.1038/s42256-022-00580-7,\n  year = {2022},\n  title = {{Large-scale chemical language representations capture molecular structure and   properties}},\n  author = {Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and   Mroueh, Youssef and Das, Payel},\n  journal = {Nature Machine Intelligence},\n  doi = {10.1038/s42256-022-00580-7},\n  pages = {1256--1264},\n  number = {12},\n  volume = {4}\n}'}"
mcro_allMiniLML12v2-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.'}"
mcro_EleutherAIpythia70mdeduped-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'research on the behavior, functionality, and limitations of large language models'}"
mcro_playgroundaiplaygroundv251024pxaesthetic-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Playground v2.5 Community License'}
mcro_myshellaiMeloTTS,mcro_Model,{}
mcro_myshellaiMeloTTS-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'MeloTTS is a **high-quality multi-lingual** text-to-speech library by [MIT](https://www.mit.edu/) and [MyShell.ai](https://myshell.ai).'}
mcro_TinyLlama11BChatv10,mcro_Model,{}
mcro_TinyLlama11BChatv10-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 2'}
mcro_facebookmmslid256-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CC-BY-NC 4.0 license'}
mcro_bertbasemultilingualuncasedsentiment-ModelDetail,mcro_ModelDetailSection,{}
mcro_bertbasemultilingualuncasedsentiment-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Here is the number of product reviews we used for finetuning the model:'}
mcro_convnextv2_nano.fcmae_ft_in22kin1k-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_opensearchprojectopensearchneuralsparseencodingdocv2distill-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'gooaq_pairs; yahoo_answers_question_answer; searchQA_top5_snippets; stackexchange_duplicate_questions_body_body; wikihow; squad_pairs; stackexchange_duplicate_questions_title-body_title-body; WikiAnswers; MS MARCO; S2ORC_title_abstract; eli5_question_answer; yahoo_answers_title_question; stackexchange_duplicate_questions_title_title; yahoo_answers_title_answer'}
mcro_germansentimentbert,mcro_Model,{}
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelParameter,mcro_ModelParameterSection,{}
mcro_sarvamaisarvamm-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'text-only language model built on Mistral-Small'}
mcro_DeBERTaDecodingenhancedBERTwithDisentangledAttention-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Fine-tuning on NLU tasks'}
mcro_jonatasgrosmanwav2vec2largexlsr53japanese-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech recognition in Japanese'}
mcro_TahaDouajidetrdoctabledetection-OutOfScopeUseCase,mcro_OutOfScopeUseCaseSectionInformation,{}
mcro_AudioSpectrogramTransformerfinetunedonAudioSet-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.'}"
mcro_detoxify-DatasetInformationSection_Multilingual,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Multilingual toxic comment classification'}
mcro_googlest5v11-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer'}
mcro_wavlmbaseplus-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""[Microsoft's WavLM](https://github.com/microsoft/unilm/tree/master/wavlm)""}"
mcro_BridgeTowerbridgetowerlargeitmmlmitc,mcro_Model,{}
mcro_BridgeTowerbridgetowerlargeitmmlmitc-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Here is how to use this model to perform contrastive learning between image and text pairs:\n\nfrom transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning\nimport requests\nfrom PIL import Image\nimport torch\n\nimage_urls = [\n    ""https://farm4.staticflickr.com/3395/3428278415_81c3e27f15_z.jpg"",\n \xa0\xa0 ""http://images.cocodataset.org/val2017/000000039769.jpg""]\ntexts = [\n    ""two dogs in a car"",\n    ""two cats sleeping on a couch""]\nimages = [Image.open(requests.get(url, stream=True).raw) for url in image_urls]\n\nprocessor = BridgeTowerProcessor.from_pretrained(""BridgeTower/bridgetower-large-itm-mlm"")\nmodel = BridgeTowerForContrastiveLearning.from_pretrained(""BridgeTower/bridgetower-large-itm-mlm-itc"")\n\ninputs\xa0 = processor(images, texts, padding=True, return_tensors=""pt"")\noutputs = model(**inputs)\n\ninputs\xa0 = processor(images, texts[::-1], padding=True, return_tensors=""pt"")\noutputs_swapped = model(**inputs)\n\nprint(\'Loss\', outputs.loss.item())\n# Loss 0.00191505195107311\nprint(\'Loss with swapped images\', outputs_swapped.loss.item())\n# Loss with swapped images 2.1259872913360596 \n\n\nHere is how to use this model to perform image and text matching\n\nfrom transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval\nimport requests\nfrom PIL import Image\n\nurl = ""http://images.cocodataset.org/val2017/000000039769.jpg""\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [""An image of two cats chilling on a couch"", ""A football player scoring a goal""]\n\nprocessor = BridgeTowerProcessor.from_pretrained(""BridgeTower/bridgetower-large-itm-mlm-gaudi"")\nmodel = BridgeTowerForImageAndTextRetrieval.from_pretrained(""BridgeTower/bridgetower-large-itm-mlm-gaudi"")\n\n# forward pass\nscores = dict()\nfor text in texts:\n    # prepare inputs\n    encoding = processor(image, text, return_tensors=""pt"")\n    outputs = model(**encoding)\n    scores[text] = outputs.logits[0,1].item()\n\n\nHere is how to use this model to perform masked language modeling:\n\nfrom transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM\nfrom PIL import Image\nimport requests\n\nurl = ""http://images.cocodataset.org/val2017/000000360943.jpg""\nimage = Image.open(requests.get(url, stream=True).raw).convert(""RGB"")\ntext = ""a <mask> looking out of the window""\n\nprocessor = BridgeTowerProcessor.from_pretrained(""BridgeTower/bridgetower-large-itm-mlm-gaudi"")\nmodel = BridgeTowerForMaskedLM.from_pretrained(""BridgeTower/bridgetower-large-itm-mlm-gaudi"")\n\n# prepare inputs\nencoding = processor(image, text, return_tensors=""pt"")\n\n# forward pass\noutputs = model(**encoding)\n\nresults = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())\n\nprint(results)\n#.a cat looking out of the window.'}"
mcro_segformerb2finetunedforclothessegmentation-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'SegFormer model'}
mcro_facebookcontriever-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Using the model directly available in HuggingFace transformers requires to add a mean pooling operation to obtain a sentence embedding.'}
mcro_sdxlinpainting01-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-image generative model'}
mcro_BAAIbgererankerv2m3,mcro_Model,{}
mcro_BAAIbgererankerv2m3-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'reranker uses question and document as input and directly output similarity instead of embedding.\nYou can get a relevance score by inputting query and passage to the reranker.\nAnd the score can be mapped to a float value in [0,1] by sigmoid function.'}"
mcro_emimodel-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'イラストや漫画、アニメの作画補助'}
mcro_bertmultilingualbasemodeluncased-UseCase,mcro_UseCaseInformationSection,{}
mcro_CLIPViTH14LAION2B-Evaluation,mcro_QuantativeAnalysisSection,{'prov_hasTextValue': 'The model achieves a 78.0 zero-shot top-1 accuracy on ImageNet-1k.'}
mcro_all-distilroberta-v1-TrainingDataInformationSection,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,124,818,467** |'}"
mcro_gte-small,mcro_Model,{}
mcro_gte-small-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}'}"
mcro_toxigen-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar.'}"
mcro_FinBERT-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'analyze sentiment of financial text'}
mcro_vit_base_patch16_224.augreg2_in21k_ft_in1k,mcro_Model,{}
mcro_vit_base_patch16_224.augreg2_in21k_ft_in1k-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image Classification; Image Embeddings'}
mcro_sentencetransformersparaphrasemultilingualMiniLML12v2,mcro_Model,{}
mcro_sentencetransformersparaphrasemultilingualMiniLML12v2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_twitterrobertabasesentiment-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'roBERTa-base'}
mcro_t5base-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.'}"
mcro_wav2vec2base-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech recognition'}
mcro_Snowflakesnowflakearcticembedm-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance.'}
mcro_Snowflakesnowflakearcticembedm-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance. First, the models are trained with large batches of query-document pairs where negatives are derived in-batch—pretraining leverages about 400m samples of a mix of public datasets and proprietary web search data. Following pretraining models are further optimized with long training on a smaller dataset (about 1m samples) of triplets of query, positive document, and negative document derived from hard harmful mining. Mining of the negatives and data curation is crucial to retrieval accuracy.'}"
mcro_ESMFold-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'ESMFold is a state-of-the-art end-to-end protein folding model based on an ESM-2 backbone. It does not require any lookup or MSA step, and therefore does not require any external databases to be present in order to make predictions. As a result, inference time is very significantly faster than AlphaFold2.'}"
mcro_gemma3model-ModelDetail,mcro_ModelDetailSection,{}
mcro_openbmbMiniCPMo26,mcro_Model,{}
mcro_openbmbMiniCPMo26-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_chronosboltmini-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}'}"
mcro_BridgeTowerbridgetowerlargeitmmlmitc-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{xu2022bridge,\n  title={BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning},\n  author={Xu, Xiao and Wu, Chenfei and Rosenman, Shachar and Lal, Vasudev and Che, Wanxiang and Duan, Nan},\n  journal={arXiv preprint arXiv:2206.08657},\n  year={2022}\n}'}"
mcro_bertlargemodeluncased,mcro_Model,{}
mcro_bertlargemodeluncased-ModelDetail,mcro_ModelDetailSection,{}
mcro_multiqaminiLML6cosv1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for **semantic search**. It has been trained on 215M (question, answer) pairs from diverse sources.'}"
mcro_tsmatzxlmrobertanerjapanese-ModelParameterSection,mcro_ModelParameterSection,{}
mcro_metaLlama32Collection-LicenseInformationSection,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).'}"
mcro_convnextv2_nano.fcmae_ft_in22kin1k-Citation1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Woo2023ConvNeXtV2,\n  title={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},\n  author={Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon and Saining Xie},\n  year={2023},\n  journal={arXiv preprint arXiv:2301.00808},\n}'}"
mcro_microsoftPhi35miniinstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '**Architecture:** Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini.'}
mcro_chronos-t5-base-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Chronos is a family of **pretrained time series forecasting models** based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes.'}"
mcro_answerdotaiModernBERTbase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{modernbert,\n      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, \n      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\n      year={2024},\n      eprint={2412.13663},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.13663}, \n}'}"
mcro_NLLB200-IntendedUse,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '- Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\n- Primary intended users: Primary users are researchers and machine translation research community.\n- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations.'}"
mcro_SmolDocling256Mpreview-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'multimodal Image-Text-to-Text model designed for efficient document conversion'}
mcro_resnet18a1in1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_detoxify-UseCaseInformationSection_fine_tuning,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'fine-tuning'}
mcro_edgenextsmallusiin1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_edgenextsmallusiin1k-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{Maaz2022EdgeNeXt,\n  title={EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications},\n    author={Muhammad Maaz and Abdelrahman Shaker and Hisham Cholakkal and Salman Khan and Syed Waqas Zamir and Rao Muhammad Anwer and Fahad Shahbaz Khan},\n  booktitle={International Workshop on Computational Aspects of Deep Learning at 17th European Conference on Computer Vision (CADL2022)},\n  year={2022},\n  organization={Springer}\n}'}"
mcro_segformerb1finetunedade20k-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_mixedbreadaimxbaiembedlargev1-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@online{emb2024mxbai,\n  title={Open Source Strikes Bread - New Fluffy Embeddings Model},\n  author={Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-embed-large-v1},\n}\n\n@article{li2023angle,\n  title={AnglE-optimized Text Embeddings},\n  author={Li, Xianming and Li, Jing},\n  journal={arXiv preprint arXiv:2309.12871},\n  year={2023}\n}'}"
mcro_kresnikwav2vec2largexlsrkorean,mcro_Model,{}
mcro_bertbaseuncased-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_facebookencodec24khz-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{défossez2022high,\n      title={High Fidelity Neural Audio Compression}, \n      author={Alexandre Défossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},\n      year={2022},\n      eprint={2210.13438},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS}\n}'}"
mcro_snunlpKRSBERTV40KklueNLIaugSTS-License,mcro_LicenseInformationSection,{}
mcro_mask2former,mcro_Model,{}
mcro_mask2former-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use this particular checkpoint for panoptic segmentation. See the [model hub](https://huggingface.co/models?search=mask2former) to look for other\nfined-tuned versions on a task that interests you.'}
mcro_USERbgem3-ModelDetail,mcro_ModelDetailSection,{}
mcro_prajjwal1bertmedium-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bhargava2021generalization,\n      title={Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics}, \n      author={Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},\n      year={2021},\n      eprint={2110.01518},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{DBLP:journals/corr/abs-1908-08962,\n  author    = {Iulia Turc and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {Well-Read Students Learn Better: The Impact of Student Initialization\n               on Knowledge Distillation},\n  journal   = {CoRR},\n  volume    = {abs/1908.08962},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1908.08962},\n  eprinttype = {arXiv},\n  eprint    = {1908.08962},\n  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_BRIA_Background_Removal_v14-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'RMBG v1.4 is developed on the [IS-Net](https://github.com/xuebinqin/DIS) enhanced with our unique training scheme and proprietary dataset. \nThese modifications significantly improve the model’s accuracy and effectiveness in diverse image-processing scenarios.'}
mcro_bertbaseNER-ModelDetail,mcro_ModelDetailSection,{}
mcro_facebookmusicgenmedium-PrimaryIntendedUser,mcro_PrimaryIntendedUserInformationSection,"{'prov_hasTextValue': 'The primary intended users of the model are researchers in audio, machine learning and artificial intelligence, as well as amateur seeking to better understand those models.'}"
mcro_stabilityaistablediffusion35medium-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Intended uses include the following:\n* Generation of artworks and use in design and other artistic processes.\n* Applications in educational or creative tools.\n* Research on generative models, including understanding the limitations of generative models.\n\nAll uses of the model must be in accordance with our Acceptable Use Policy.'}"
mcro_Qwen2505BInstruct-Dataset,mcro_DatasetInformationSection,{}
mcro_visiontransformerbase-sizedmodel-hybrid-TrainingData,mcro_DatasetInformationSection,{}
mcro_legalbertTheMuppetsStraightOutOfLawSchool-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': '* 116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.\n    \n* 61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk).\n    \n* 19,867 cases from the European Court of Justice (ECJ), also available from EURLEX.\n    \n* 12,554 cases from HUDOC, the repository of the European Court of Human Rights (ECHR) (http://hudoc.echr.coe.int/eng).\n    \n* 164,141 cases from various courts across the USA, hosted in the Case Law Access Project portal (https://case.law).\n    \n* 76,366 US contracts from EDGAR, the database of US Securities and Exchange Commission (SECOM) (https://www.sec.gov/edgar.shtml).'}"
mcro_lmmslabLLaVAVideo7BQwen2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model was trained on LLaVA-Video-178K and LLaVA-OneVision Dataset, having the ability to interact with images, multi-image and videos, but specific to videos.'}"
mcro_microsoftwavlmbaseplussv-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The official license can be found [here](https://github.com/microsoft/UniSpeech/blob/main/LICENSE)'}
mcro_YOLOv8DetectionModel,mcro_Model,{}
mcro_YOLOv8DetectionModel-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_googleelectrabasediscriminator-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': '**ELECTRA** is a new method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish ""real"" input tokens vs ""fake"" input tokens generated by another neural network, similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.'}"
mcro_whisper-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'automatic speech recognition (ASR) and speech translation'}
mcro_Qwen3-32B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Causal Language Models'}
mcro_t5base-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Language model'}
mcro_t5small-EvaluationData,mcro_DatasetInformationSection,{}
mcro_distilbertbasemodeluncased-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers).'}"
mcro_CLIPViTH14LAION2B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CLIP ViT-H/14'}
mcro_detoxify-DatasetInformationSection_Unintended_Bias,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Unintended Bias in Toxic comments'}
mcro_metaLlama31Instruct-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)'}"
mcro_LTXVideo-Consideration,mcro_ConsiderationInformationSection,{'prov_hasTextValue': 'Prompt following is heavily influenced by the prompting-style.; The model may fail to generate videos that matches the prompts perfectly.; As a statistical model this checkpoint might amplify existing societal biases.; This model is not intended or able to provide factual information.'}
mcro_llama4Scout17B16EInstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_llama4Scout17B16EInstruct-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'A custom commercial license, the Llama 4 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE)'}"
mcro_oliverguhrfullstoppunctuationmultilanglarge,mcro_Model,{}
mcro_oliverguhrfullstoppunctuationmultilanglarge-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian'}"
mcro_AudioSpectrogramTransformerfinetunedonAudioSet,mcro_Model,{}
mcro_Qwen257BInstruct1M-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5-1m,\n    title = {Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens},\n    url = {https://qwenlm.github.io/blog/qwen2.5-1m/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{qwen2.5,\n      title={Qwen2.5-1M Technical Report},\n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\n      journal={arXiv preprint arXiv:2501.15383},\n      year={2025}\n}'}"
mcro_mistralaiMixtral8x7BInstructv01-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_visiontransformerbase-sizedmodel-hybrid-IntendedUseCase,mcro_UseCaseInformationSection,{}
mcro_facebookmusicgenmedium,mcro_Model,{}
mcro_wav2vec2largerobustftlibri960h-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Librispeech'}
mcro_naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-Consideration,mcro_ConsiderationInformationSection,{}
mcro_deepseekaiDeepSeekR1DistillQwen14B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'MoE'}
mcro_BiomedParse-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '""This section should allow readers to quickly grasp what the model should and should not be used for, and why it was created. It can also help frame the statistical analysis presented in the rest of the card, including a short description of the user(s), use-case(s), and context(s) for which the model was originally developed.""'}"
mcro_resnet50v1.5,mcro_Model,{}
mcro_Salesforceblipvqabase-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': 'This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people’s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.'}"
mcro_gemma3model-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'These models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.'}"
mcro_unik3d-ReferenceInformationSection,mcro_ReferenceInformationSection,{'prov_hasTextValue': 'https://github.com/lpiccinelli-eth/UniK3D; https://huggingface.co/docs/huggingface_hub/package_reference/mixins#huggingface_hub.PyTorchModelHubMixin'}
mcro_xlmrobertabaselanguagedetection-ModelDetail,mcro_ModelDetailSection,{}
mcro_Falconsainsfwimagedetection-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'unknown'}
mcro_AlibabaNLPgtebaseenv15-Citation,mcro_CitationInformationSection,{}
mcro_mistralaiMistral7BInstructv02-Owner,mcro_OwnerInformationSection,"{'prov_hasTextValue': 'The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.'}"
mcro_eng-zho-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'source language(s): eng; target language(s): cjy_Hans cjy_Hant cmn cmn_Hans cmn_Hant gan lzh lzh_Hans nan wuu yue yue_Hans yue_Hant'}
mcro_mistralaiMistral7BInstructv03-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Large Language Model (LLM)'}
mcro_hallucinationevaluationmodel-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'detecting hallucinations in LLMs'}
mcro_bertmultilingualbasemodeluncased-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_NusaBertnerv13-ModelDetail,mcro_ModelDetailSection,{}
mcro_NusaBertnerv13-DatasetInformation,mcro_DatasetInformationSection,{}
mcro_fnetbasemodel-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'FNet is a transformers model with attention replaced with fourier transforms.'}
mcro_mimi-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_BEiTbasesizedmodelfinetunedonImageNet22k-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{}
mcro_metallamaLlama323BInstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_tabletransformerpretrainedfortablestructurerecognition-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'It was introduced in the paper [Aligning benchmark datasets for table structure recognition](https://arxiv.org/abs/2303.00716) by Smock et al. and first released in [this repository](https://github.com/microsoft/table-transformer).'}
mcro_Snowflakesnowflakearcticembedxs-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Arctic is licensed under the [Apache-2](https://www.apache.org/licenses/LICENSE-2.0). The released models can be used for commercial purposes free of charge.'}
mcro_gemma2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{gemma_2024,\n    title={Gemma},\n    url={https://www.kaggle.com/m/3301},\n    DOI={10.34740/KAGGLE/M/3301},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2024}\n}'}"
mcro_t53b-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_microsoftphi2-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The model is licensed under the MIT license'}
mcro_autogluonchronosboltbase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}'}"
mcro_M2M100418M-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model that can directly translate between the 9,900 directions of 100 languages.\nTo translate into a target language, the target language id is forced as the first generated token.\nTo force the target language id as the first generated token, pass the `forced_bos_token_id` parameter to the `generate` method.'}"
mcro_robertBase,mcro_Model,{}
mcro_Qwen2532BInstruct-UseCase,mcro_UseCaseInformationSection,{}
mcro_RobertaLargeMnli-ModelDetail,mcro_ModelDetailSection,{}
mcro_mobilebertuncased-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.'}"
mcro_crossencodernlidebertav3base-PerformanceMetric2,mcro_PerformanceMetricInformationSection,{'prov_hasTextValue': '- Accuracy on  MNLI mismatched set: 90.04'}
mcro_t5large-Consideration,mcro_ConsiderationInformationSection,{}
mcro_clip-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)'}
mcro_distilbertbasemodeluncased-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.""}"
mcro_jonatasgrosmanwav2vec2largexlsr53russian-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{grosman2021xlsr53-large-russian,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {R}ussian},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian}},\n  year={2021}\n}'}"
mcro_sdxlinpainting01-outofscopeuse,mcro_OutOfScopeUseCaseSectionInformation,{'prov_hasTextValue': 'not trained to be factual or true representations of people or events'}
mcro_Qwen3-32B-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'general-purpose dialogue'}
mcro_appleOpenELM11BInstruct-License,mcro_LicenseInformationSection,{}
mcro_doclingmodels-ModelDetail,mcro_ModelDetailSection,{}
mcro_twitterxlmrobertabasesentiment-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_twitterxlmrobertabasesentiment-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-UseCase,mcro_UseCaseInformationSection,{}
mcro_InternVL378B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{chen2024expanding,\n  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},\n  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},\n  journal={arXiv preprint arXiv:2412.05271},\n  year={2024}\n}\n@article{wang2024mpo,\n  title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization},\n  author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and Dai, Jifeng},\n  journal={arXiv preprint arXiv:2411.10442},\n  year={2024}\n}\n@article{chen2024far,\n  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},\n  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},\n  journal={arXiv preprint arXiv:2404.16821},\n  year={2024}\n}\n@inproceedings{chen2024internvl,\n  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},\n  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={24185--24198},\n  year={2024}\n}'}"
mcro_sentencetransformersparaphrasemultilingualmpnetbasev2,mcro_Model,{}
mcro_twitterxlmrobertabasesentiment,mcro_Model,{}
mcro_RoBERTaBaseModel-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformers model'}
mcro_naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B,mcro_Model,{}
mcro_naverhyperclovaxHyperCLOVAXSEEDVisionInstruct3B-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_appleOpenELM11BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': ""@article{mehtaOpenELMEfficientLanguage2024,\n\ttitle = {{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}},\n\tshorttitle = {{OpenELM}},\n\turl = {https://arxiv.org/abs/2404.14619v1},\n\tlanguage = {en},\n\turldate = {2024-04-24},\n\tjournal = {arXiv.org},\n\tauthor = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},\n\tmonth = apr,\n\tyear = {2024},\n}\n\n@inproceedings{mehta2022cvnets, \n     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, \n     title = {CVNets: High Performance Library for Computer Vision}, \n     year = {2022}, \n     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, \n     series = {MM '22} \n}""}"
mcro_bertbasemodelcased,mcro_Model,{}
mcro_mbartlarge50manytomanymmt-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'multilingual machine translation'}
mcro_canary1bflash-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': '[1] [Training and Inference Efficiency of Encoder-Decoder Speech Models](https://arxiv.org/abs/2503.05931)\n\n[2] [Less is More: Accurate Speech Recognition & Translation without Web-Scale Data](https://www.isca-archive.org/interspeech_2024/puvvada24_interspeech.pdf)\n\n[3] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389701)\n\n[4] [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n\n[5] [Unified Model for Code-Switching Speech Recognition and Language Identification Based on Concatenated Tokenizer](https://aclanthology.org/2023.calcs-1.7.pdf)\n\n[6] [Google Sentencepiece Tokenizer](https://github.com/google/sentencepiece)\n\n[7] [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo)\n\n[8] [EMMeTT: Efficient Multimodal Machine Translation Training](https://arxiv.org/abs/2409.13523)\n\n[9] [Towards Measuring Fairness in AI: the Casual Conversations Dataset](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9634168)'}
mcro_arabertv1andv2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{antoun2020arabert,\n  title={AraBERT: Transformer-based Model for Arabic Language Understanding},\n  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},\n  booktitle={LREC 2020 Workshop Language Resources and Evaluation Conference 11--16 May 2020},\n  pages={9}\n}'}"
mcro_rubertbasecasedsentimentrusentiment-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'A. Rogers A. Romanov A. Rumshisky S. Volkova M. Gronas A. Gribov RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian. Proceedings of COLING 2018.'}
mcro_bertmultilingualbasemodeluncased-ModelDetail,mcro_ModelDetailSection,{}
mcro_Salesforceblipvqacapfiltlarge-License,mcro_LicenseInformationSection,{}
mcro_kluerobertabase-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{park2021klue,\n      title={KLUE: Korean Language Understanding Evaluation},\n      author={Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n      year={2021},\n      eprint={2105.09680},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_openvla7b-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Vision-language-action (language, image => robot actions)'}"
mcro_visiontransformerbasesizedmodel-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision}, \n      author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_Salesforcecodet5basemultisum-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'We employ the filtered version of CodeSearchNet data [[Husain et al., 2019](https://arxiv.org/abs/1909.09436)]\nfrom [CodeXGLUE](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text) benchmark for fine-tuning on\ncode summarization. The data is tokenized with our pre-trained code-specific BPE (Byte-Pair Encoding) tokenizer. One can\nprepare text (or code) for the model using RobertaTokenizer with the vocab files from [codet5-base](https://huggingface.co/Salesforce/codet5-base).'}"
mcro_sentencetransformersrobertabasenlimeantokens,mcro_Model,{}
mcro_sentencetransformersrobertabasenlimeantokens-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False}); Transformer({'max_seq_length': 128, 'do_lower_case': True}) with Transformer model: RobertaModel""}"
mcro_segformerb0finetunedade20k-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_Supabasegtesmall,mcro_Model,{}
mcro_chronos-t5-tiny-UseCaseSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'To perform inference with Chronos models, install the package in the GitHub [companion repo](https://github.com/amazon-science/chronos-forecasting) by running:\n\npip install git+https://github.com/amazon-science/chronos-forecasting.git\n\nA minimal example showing how to perform inference using Chronos models:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom chronos import ChronosPipeline\n\npipeline = ChronosPipeline.from_pretrained(\n  ""amazon/chronos-t5-tiny"",\n  device_map=""cuda"",\n  torch_dtype=torch.bfloat16,\n)\n\ndf = pd.read_csv(""https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv"")\n\n# context must be either a 1D tensor, a list of 1D tensors,\n# or a left-padded 2D tensor with batch as the first dimension\ncontext = torch.tensor(df[""#Passengers""])\nprediction_length = 12\nforecast = pipeline.predict(context, prediction_length)  # shape [num_series, num_samples, prediction_length]\n\n# visualize the forecast\nforecast_index = range(len(df), len(df) + prediction_length)\nlow, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n\nplt.figure(figsize=(8, 4))\nplt.plot(df[""#Passengers""], color=""royalblue"", label=""historical data"")\nplt.plot(forecast_index, median, color=""tomato"", label=""median forecast"")\nplt.fill_between(forecast_index, low, high, color=""tomato"", alpha=0.3, label=""80% prediction interval"")\nplt.legend()\nplt.grid()\nplt.show()'}"
mcro_speechbrainemotionrecognitionwav2vec2IEMOCAP-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{speechbrain,\n  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n  year={2021},\n  eprint={2106.04624},\n  archivePrefix={arXiv},\n  primaryClass={eess.AS},\n  note={arXiv:2106.04624}\n}'}"
mcro_ChatGLM2-6B,mcro_Model,{}
mcro_ChatGLM2-6B-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': '1.4T 中英标识符'}
mcro_debertaV3Small-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': '160GB data'}
mcro_oliverguhrfullstoppunctuationmultilanglarge-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'This multilanguage model was trained on the [Europarl Dataset](https://huggingface.co/datasets/wmt/europarl) provided by the [SEPP-NLG Shared Task](https://sites.google.com/view/sentence-segmentation). *Please note that this dataset consists of political speeches. Therefore the model might perform differently on texts from other domains.*'}
mcro_bigvganv222khz80band256x-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BigVGAN'}
mcro_BAAIbgererankerlarge-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'cross-encoder'}
mcro_Dreamshaper8inpainting-UseCase,mcro_UseCaseInformationSection,{}
mcro_HelsinkiNLPopusmtzhen-ModelDetail,mcro_ModelDetailSection,{}
mcro_FacebookAIxlmrobertabase-Limitation,mcro_LimitationInformationSection,{}
mcro_WhereIsAIUAELargeV1-Citation,mcro_CitationInformationSection,{}
mcro_EleutherAIpythia70mdeduped-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_Salesforceblipvqabase-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BLIP trained on visual question answering- base architecture (with ViT base backbone).'}
mcro_allMiniLML12v2,mcro_Model,{}
mcro_allMiniLML12v2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}
mcro_microsoftPhi3mini4kinstruct,mcro_Model,{}
mcro_wav2vec2largexlsr53-License,mcro_LicenseInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53polish-License,mcro_LicenseInformationSection,{}
mcro_swin_transformer_v2_tiny_sized_model-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Swin Transformer v2'}
mcro_convnextv2_nano.fcmae_ft_in22kin1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_NepaliBERT-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'S. Pudasaini, S. Shakya, A. Tamang, S. Adhikari, S. Thapa and S. Lamichhane, ""NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus,"" 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), Kirtipur, Nepal, 2023, pp. 325-330, doi: 10.1109/I-SMAC58438.2023.10290690.\n\n@INPROCEEDINGS{10290690,\n  author={Pudasaini, Shushanta and Shakya, Subarna and Tamang, Aakash and Adhikari, Sajjan and Thapa, Sunil and Lamichhane, Sagar},\n  booktitle={2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, \n  title={NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus}, \n  year={2023},\n  volume={},\n  number={},\n  pages={325-330},\n  doi={10.1109/I-SMAC58438.2023.10290690}}'}"
mcro_sentencetransformersstsbrobertabase-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_clip-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.'}"
mcro_audeeringwav2vec2largerobust12ftemotionmspdim-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Wav2Vec2-Large-Robust'}
mcro_llama32CollectionOfMultilingualLargeLanguageModelsLlms,mcro_Model,{}
mcro_layoutlmbaseuncased-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'IIT-CDIP Test Collection 1.0'}
mcro_emilyalsentzerBioClinicalBERT-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '[Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) (NAACL Clinical NLP Workshop 2019)'}
mcro_opensearchprojectopensearchneuralsparseencodingdocv2distill-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'sparse vectors'}
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit,mcro_Model,{}
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelDetail,mcro_ModelDetailSection,{}
mcro_BAAIbgererankerv2m3-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'reranker'}
mcro_debertav3basetasksourcenli,mcro_Model,{}
mcro_e5-base-v2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model has 12 layers and the embedding size is 768.'}
mcro_metallamaLlama31-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.'}"
mcro_microsoftPhi3mini4kinstruct-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require \n1) memory/compute constrained environments; \n2) latency bound scenarios; \n3) strong reasoning (especially math and logic). \n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**'}"
mcro_wav2vec2base-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""Facebook's Wav2Vec2""}"
mcro_deepseekaiDeepSeekR1-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}'}"
mcro_metallamaLlama3211BVisionInstruct-OutOfScope,mcro_OutOfScopeUseCaseSectionInformation,{'prov_hasTextValue': 'Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.'}
mcro_Qwen317B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Causal Language Models'}
mcro_WhereIsAIUAELargeV1,mcro_Model,{}
mcro_microsoftPhi35visioninstruct,mcro_Model,{}
mcro_microsoftPhi35visioninstruct-TrainingDataInformationSection,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Our training data includes a wide variety of sources, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data and code;\n2) selected high-quality image-text interleave data;\n3) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides, newly created multi-image and video data, e.g., short video clips/pair of two similar images;\n4) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).'}"
mcro_germansentimentbert-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': '1.834 million German-language samples'}
mcro_detrresnet50-ModelDetail,mcro_ModelDetailSection,{}
mcro_parakeet_rnnt_06b-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'FastConformer [1] is an optimized version of the Conformer model with 8x depthwise-separable convolutional downsampling. The model is trained in a multitask setup with a Transducer decoder (RNNT) loss. You may find more information on the details of FastConformer here: [Fast-Conformer Model](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer).'}
mcro_intfloate5mistral7binstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{wang2023improving,\n  title={Improving Text Embeddings with Large Language Models},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2401.00368},\n  year={2023}\n}\n\n@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}'}"
mcro_FinetunedT5SmallTextSummarization-License,mcro_LicenseInformationSection,{}
mcro_QwenQwen25Coder7BInstructGPTQInt4-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Code Agents'}
mcro_pyannotespeakerdiarization30-ModelDetail,mcro_ModelDetailSection,{}
mcro_segformerb5finetunedade20k-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_owlv2-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{minderer2023scaling,\n      title={Scaling Open-Vocabulary Object Detection}, \n      author={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},\n      year={2023},\n      eprint={2306.09683},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_visiontransformerbasesizedmodel-TrainingDataInformationSection,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The ViT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes.'}"
mcro_BEiTbasesizedmodelfinetunedonImageNet22k-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_vikplayoutsegmenter,mcro_Model,{}
mcro_distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelDetail,mcro_ModelDetailSection,{}
mcro_distilbertdistilbertbaseuncasedfinetunedsst2englis-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache-2.0'}
mcro_FinBERT-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT language model'}
mcro_snunlpKRSBERTV40KklueNLIaugSTS-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'Basic information about the model that includes licensing information, owner information, the architecture of the model (algorthim employed), references (cited papers), and versioning information.'}"
mcro_snunlpKRSBERTV40KklueNLIaugSTS-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{kr-sbert,\n  author = {Park, Suzi and Hyopil Shin},\n  title = {KR-SBERT: A Pre-trained Korean-specific Sentence-BERT model},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/snunlp/KR-SBERT}}\n}'}"
mcro_StanfordAIMIstanforddeidentifierbase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{10.1093/jamia/ocac219,\n    author = {Chambon, Pierre J and Wu, Christopher and Steinkamp, Jackson M and Adleberg, Jason and Cook, Tessa S and Langlotz, Curtis P},\n    title = ""{Automated deidentification of radiology reports combining transformer and “hide in plain sight” rule-based methods}"",\n    journal = {Journal of the American Medical Informatics Association},\n    year = {2022},\n    month = {11},\n    abstract = ""{To develop an automated deidentification pipeline for radiology reports that detect protected health information (PHI) entities and replaces them with realistic surrogates “hiding in plain sight.”In this retrospective study, 999 chest X-ray and CT reports collected between November 2019 and November 2020 were annotated for PHI at the token level and combined with 3001 X-rays and 2193 medical notes previously labeled, forming a large multi-institutional and cross-domain dataset of 6193 documents. Two radiology test sets, from a known and a new institution, as well as i2b2 2006 and 2014 test sets, served as an evaluation set to estimate model performance and to compare it with previously released deidentification tools. Several PHI detection models were developed based on different training datasets, fine-tuning approaches and data augmentation techniques, and a synthetic PHI generation algorithm. These models were compared using metrics such as precision, recall and F1 score, as well as paired samples Wilcoxon tests.Our best PHI detection model achieves 97.9 F1 score on radiology reports from a known institution, 99.6 from a new institution, 99.5 on i2b2 2006, and 98.9 on i2b2 2014. On reports from a known institution, it achieves 99.1 recall of detecting the core of each PHI span.Our model outperforms all deidentifiers it was compared to on all test sets as well as human labelers on i2b2 2014 data. It enables accurate and automatic deidentification of radiology reports.A transformer-based deidentification pipeline can achieve state-of-the-art performance for deidentifying radiology reports and other medical documents.}"",\n    issn = {1527-974X},\n    doi = {10.1093/jamia/ocac219},\n    url = {https://doi.org/10.1093/jamia/ocac219},\n    note = {ocac219},\n    eprint = {https://academic.oup.com/jamia/advance-article-pdf/doi/10.1093/jamia/ocac219/47220191/ocac219.pdf},\n}'}"
mcro_emilyalsentzerBioClinicalBERT-ModelArch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'initialized from [BioBERT](https://arxiv.org/abs/1901.08746) & trained on all MIMIC notes'}
mcro_layoutlmv3,mcro_Model,{}
mcro_layoutlmv3-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The content of this project itself is licensed under the [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/).\nPortions of the source code are based on the [transformers](https://github.com/huggingface/transformers) project.\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)'}
mcro_Llama3370BInstructAWQ-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'AWQ 4-bit quantized version'}
mcro_jonatasgrosmanwav2vec2largexlsr53dutch-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Common Voice 6.1'}
mcro_microsoftFlorence2base-ModelDetail,mcro_ModelDetailSection,{}
mcro_microsoftFlorence2base-License,mcro_LicenseInformationSection,{}
mcro_bigvganv222khz80band256x-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'universal neural vocoder'}
mcro_Yehorw2vxlsruk,mcro_Model,{}
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX4bit-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': '4-bit quantized version of DeepSeek-R1-0528-Qwen3-8B using MLX, optimized for Apple Silicon.'}"
mcro_bertsmalluncased-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""Mini BERT models from https://arxiv.org/abs/1908.08962 that the HF team didn't convert. The original [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py) is used.\n\nSee the original Google repo: [google-research/bert](https://github.com/google-research/bert)\n\nNote: it's not clear if these checkpoints have undergone knowledge distillation.""}"
mcro_testmodel-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'test citation'}
mcro_llama32CollectionOfMultilingualLargeLanguageModelsLlms-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_deepseekaiDeepSeekR10528,mcro_Model,{}
mcro_jinaaijinaembeddingsv3-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,\n      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, \n      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael Günther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},\n      year={2024},\n      eprint={2409.10173},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2409.10173}, \n}'}"
mcro_facebookmask2formerswintinycocoinstance,mcro_Model,{}
mcro_efficientnet_b3.ra2_in1k,mcro_Model,{}
mcro_intfloatmultilinguale5base-LimitationInformationSection,mcro_LimitationInformationSection,{'prov_hasTextValue': 'Long texts will be truncated to at most 512 tokens.'}
mcro_MaziyarPanahigemma31bitGGUF-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'GGUF'}
mcro_StellaEn400Mv5-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{zhang2025jasperstelladistillationsota,\n      title={Jasper and Stella: distillation of SOTA embedding models}, \n      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},\n      year={2025},\n      eprint={2412.19048},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2412.19048}, \n}'}"
mcro_oliverguhrfullstoppunctuationmultilanglarge-QuantativeAnalysis,mcro_QuantativeAnalysisSection,"{'prov_hasTextValue': 'precision recall f1-score support 0 0.99 0.99 0.99 73317475 . 0.94 0.95 0.95 4484845 , 0.86 0.86 0.86 6100650 ? 0.88 0.85 0.86 136479 - 0.60 0.29 0.39 233630 : 0.71 0.49 0.58 152424 accuracy 0.98 84425503 macro avg 0.83 0.74 0.77 84425503 weighted avg 0.98 0.98 0.98 84425503'}"
mcro_CLIPViTB32LAION2B-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'LAION-2B'}
mcro_owlv2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training.'}"
mcro_siglipso400mpatch14384-ModelDetail,mcro_ModelDetailSection,{}
mcro_siglipso400mpatch14384-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training}, \n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_sdxl10refinermodel-Limitation,mcro_LimitationInformationSection,{}
mcro_t5base-Consideration,mcro_ConsiderationInformationSection,{'prov_hasTextValue': 'More information needed.'}
mcro_FacebookAIxlmrobertabase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_bartbase-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_llavahfllavav16mistral7bhf,mcro_Model,{}
mcro_deepseekaiDeepSeekR1DistillQwen7B-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.'}"
mcro_visiontransformerbasesizedmodel-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224.; Vision Transformer (ViT) model trained using the DINOv2 method. It was introduced in the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Oquab et al. and first released in [this repository](https://github.com/facebookresearch/dinov2).\n\nDisclaimer: The team releasing DINOv2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion. \n\nImages are presented to the model as a sequence of fixed-size patches, which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nNote that this model does not include any fine-tuned heads. \n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.'}"
mcro_fnetbasemodel,mcro_Model,{}
mcro_fnetbasemodel-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task.""}"
mcro_XenovaallMiniLML6v2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_edgenextsmallusiin1k-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_Qwen257B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_xclipbasesizedmodel-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'This model was trained on [Kinetics-400](https://www.deepmind.com/open-source/kinetics).'}
mcro_vitbasepatch8224augreg2in21kftin1k-Citation,mcro_CitationInformationSection,{}
mcro_trocrsmalliam-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.'}"
mcro_Qwen2505BInstruct-ArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_xlmrobertabaselanguagedetection-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'XLM-RoBERTa transformer model with a classification head'}
mcro_microsoftdebertav3base-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_pyannotespeakerdiarization30-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{Bredin23,\n  author={Hervé Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}; @inproceedings{Plaquet23,\n  author={Alexis Plaquet and Hervé Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}'}"
mcro_resnet50.a1_in1k-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476; https://github.com/huggingface/pytorch-image-models; Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385'}
mcro_Salesforceblipvqacapfiltlarge-ModelDetail,mcro_ModelDetailSection,{}
mcro_ibmgranitegranite318binstruct-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Overall, our SFT data is largely comprised of three key sources: (1) publicly available datasets with permissive license, (2) internal synthetic data targeting specific capabilities including long-context tasks, and (3) very small amounts of human-curated data. A detailed attribution of datasets can be found in the [Granite 3.0 Technical Report](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf), [Granite 3.1 Technical Report (coming soon)](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d), and [Accompanying Author List](https://github.com/ibm-granite/granite-3.0-language-models/blob/main/author-ack.pdf).'}"
mcro_metallamaMetaLlama38B-ModelDetail,mcro_ModelDetailSection,{}
mcro_metallamaMetaLlama38B-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)'}
mcro_rbhatia46financialragmatryoshka-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'apache-2.0'}
mcro_sentencetransformersparaphrasemultilingualMiniLML12v2-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_wavlmlarge-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The official license can be found here'}
mcro_microsoftPhi3mini4kinstruct-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) that it can support.\n\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.'}"
mcro_openbmbMiniCPMo26-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_Qwen25Coder7BInstruct,mcro_Model,{}
mcro_patrickjohncyhfashionclip-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder.'}
mcro_clip-vit-large-patch14-336-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'More information needed'}
mcro_gemma2b,mcro_Model,{}
mcro_gemma2b-ModelDetail,mcro_ModelDetailSection,{}
mcro_facebookdinov2large-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision}, \n      author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_distilwhisperdistillargev3-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'encoder-decoder architecture'}
mcro_edgenextsmallusiin1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_Qwen257BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_resnet50v1.5-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{he2016deep,\n  title={Deep residual learning for image recognition},\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages={770--778},\n  year={2016}\n}'}"
mcro_microsoftdebertalargemnli,mcro_Model,{}
mcro_microsoftdebertalargemnli-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention'}
mcro_emilyalsentzerBioClinicalBERT-DatasetInfo,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. For more details on MIMIC, see [here](https://mimic.physionet.org/). All notes from the `NOTEEVENTS` table were included (~880M words).'}"
mcro_metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': '**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.'}"
mcro_Qwen2532BInstruct,mcro_Model,{}
mcro_byt5small-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'the architecture of MT5'}
mcro_atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'A tiny version of https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct'}
mcro_blackforestlabsFLUX1schnell-LimitationInformationSection,mcro_LimitationInformationSection,{}
mcro_lmmslabLLaVAVideo7BQwen2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{zhang2024videoinstructiontuningsynthetic,\n    title={Video Instruction Tuning With Synthetic Data}, \n    author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},\n    year={2024},\n    eprint={2410.02713},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV},\n    url={https://arxiv.org/abs/2410.02713}, \n}'}"
mcro_microsoftcodebertbasemlm-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'CodeBERTScore'}
mcro_patrickjohncyhfashionclip-ModelDetail,mcro_ModelDetailSection,{}
mcro_owlvitbasepatch32-ModelDetail,mcro_ModelDetailSection,{}
mcro_sat-3l-sm-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '3 Transfomer layers'}
mcro_Salesforceblipimagecaptioningbase-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': 'This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people’s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.'}"
mcro_deepseekaiDeepSeekR10528-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528.'}"
mcro_QwenQwen25VL3BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '* Streamlined and Efficient Vision Encoder; * Dynamic Resolution and Frame Rate Training for Video Understanding'}
mcro_mobilenetv3small100lambin1k-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}'}"
mcro_chronosboltsmall-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}'}"
mcro_segformerb5finetunedade20k,mcro_Model,{}
mcro_stablediffusioninpaintingmodelcard-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj""orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }'}"
mcro_segformerb2finetunedforclothessegmentation,mcro_Model,{}
mcro_segformerb2finetunedforclothessegmentation-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_microsoftdebertav3large-UseCase,mcro_UseCaseInformationSection,{}
mcro_Whispertiny-UseCase,mcro_UseCaseInformationSection,{}
mcro_gemma3-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.; Open vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.""}"
mcro_bertlargemodeluncased-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).'}"
mcro_Qwen317B,mcro_Model,{}
mcro_Qwen317B-UseCase,mcro_UseCaseInformationSection,{}
mcro_llama32CollectionOfMultilingualLlLMs,mcro_Model,{}
mcro_llama32CollectionOfMultilingualLlLMs-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_HuggingFaceH4zephyr7bbeta-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT'}
mcro_nlpconnectvitgpt2imagecaptioning-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_BAAIbgem3-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_metaLlama31CollectionOfMultilingualLargeLanguageModelsLlm-IntendedUse,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.'}"
mcro_t53b-Citation,mcro_CitationInformationSection,{}
mcro_BiomedParse-EthicalConsiderationSection,mcro_EthicalConsiderationSection,"{'prov_hasTextValue': ""Microsoft believes Responsible AI is a shared responsibility and we have identified six principles and practices to help organizations address risks, innovate, and create value: fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant use case and addresses unforeseen product misuse.\u202f\n\nWhile testing the model with images and/or text, ensure that the data is PHI free and that there are no patient information or information that can be tracked to a patient identity.\n\nThe model is not designed for the following use cases:\n\n- Use by clinicians to inform clinical decision-making, as a diagnostic tool or as a medical device - Although MedImageParse is highly accurate in parsing biomedical data, it is not desgined or intended to be deployed in clinical settings as-is not is it for use in the diagnosis, cure, mitigation, treatment, or prevention of disease or other conditions (including to support clinical decision-making), or as a substitute of professional medical advice, diagnosis, treatment, or clinical judgment of a healthcare professional.\u202f\n\n- Scenarios without consent for data -\u202fAny scenario that uses health data for a purpose for which consent was not obtained.\u202f\u202f\n\n- Use outside of health scenarios - Any scenario that uses non-medical related image and/or serving purposes outside of the healthcare domain.\u202f\n\nPlease see Microsoft's Responsible AI Principles and approach available at https://www.microsoft.com/en-us/ai/principles-and-approach/""}"
mcro_xlmrobertabaselanguagedetection-PerformanceMetric,mcro_PerformanceMetricInformationSection,{'prov_hasTextValue': 'average accuracy on the test set is **99.6%**'}
mcro_rinnajapanesecloobvitb16-License,mcro_LicenseInformationSection,{'prov_hasTextValue': '[The Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)'}
mcro_swin_transformer_v2_tiny_sized_model-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_Salesforcecodet5basemultisum,mcro_Model,{}
mcro_Salesforcecodet5basemultisum-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '[CodeT5-base](https://huggingface.co/Salesforce/codet5-base) model fine-tuned on CodeSearchNet data in a multi-lingual training setting (\nRuby/JavaScript/Go/Python/Java/PHP) for code summarization.'}
mcro_HelsinkiNLPopusmtzhen-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'translation and text-to-text generation'}
mcro_BRIA_Background_Removal_v14,mcro_Model,{}
mcro_chronos-t5-small-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}'}"
mcro_alakxendermmsttsdivfinetunedmdm01-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'MMS-TTS (VITS)'}
mcro_germansentimentbert-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Googles Bert architecture'}
mcro_wav2vec2largexlsrhindi-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_llama4Scout17B16EInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.'}
mcro_gte-base-LimitationInformationSection,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens.'}"
mcro_VitTinyVitSmall-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'vit-tiny and vit-small'}
mcro_BAAIbgem3-License,mcro_LicenseInformationSection,{}
mcro_speechbrainspkrececapavoxceleb-License,mcro_LicenseInformationSection,{}
mcro_Inteldptlarge-Consideration,mcro_ConsiderationInformationSection,{'prov_hasTextValue': 'Multiple datasets compiled together'}
mcro_NLLB200-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\n- Primary intended users: Primary users are researchers and machine translation research community.\n- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations.'}"
mcro_ibmgranitegranite318binstruct-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Granite-3.1-8B-Instruct is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.'}"
mcro_pyannotesegmentation30-Dataset,mcro_DatasetInformationSection,{}
mcro_llamaGuard38B-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n\nBelow is a response classification example for Llama Guard 3.\n\n\n  \n  \n\n\nIn order to produce classifier scores, we look at the probability for the first token, and use that as the “unsafe” class probability. We can then apply score thresholding to make binary decisions.'}"
mcro_Qwen2.5-1.5B-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_sdxlturbo-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'SDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation.'}
mcro_Distilbertbasemultilingualcasednerhrl-UseCase,mcro_UseCaseInformationSection,{}
mcro_albertbasev2-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions'}"
mcro_Qwen2514BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias'}"
mcro_sdxlturbo-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': '- The generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.'}"
mcro_testModel-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet'}
mcro_sarvamaisarvamm,mcro_Model,{}
mcro_sarvamaisarvamm-ModelDetail,mcro_ModelDetailSection,{}
mcro_banglat5_banglaparaphrase-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_microsoftcodebertbasemlm,mcro_Model,{}
mcro_microsoftcodebertbasemlm-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{zhou2023codebertscore,\n  url = {https://arxiv.org/abs/2302.05527},\n  author = {Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham},\n  title = {CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code},\n  publisher = {arXiv},\n  year = {2023},\n}'}"
mcro_flairnerenglishlarge,mcro_Model,{}
mcro_RoBERTaLargeModel-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_bertsmalluncased,mcro_Model,{}
mcro_microsoftwavlmbaseplussv-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '[Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900)\n\nAuthors: Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei\n\n**Abstract**\n*Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.*'}"
mcro_FacebookAIxlmrobertalarge-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformers'}
mcro_TheBlokephi2GGUF-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': '250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.'}"
mcro_microsoftFlorence2large-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}'}"
mcro_deepseekaiDeepSeekV30324-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3.'}
mcro_jonatasgrosmanwav2vec2largexlsr53portuguese-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_allrobertalargev1-BackgroundInformationSection,obo_http_//purl.obolibrary.org/obo/IAO_0000314,"{'prov_hasTextValue': 'The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`roberta-large`](https://huggingface.co/roberta-large) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developped this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developped this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.'}"
mcro_microsoftcodebertbasemlm-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'codeparrot/github-code-clean'}
mcro_xlmrobertabaselanguagedetection,mcro_Model,{}
mcro_Llama3370BInstructAWQ-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '`meta-llama/Llama-3.3-70B-Instruct`'}
mcro_FinBERT-Citation2,mcro_CitationInformationSection,{'prov_hasTextValue': '[FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063)'}
mcro_HuggingFaceH4zephyr7bbeta-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}'}"
mcro_FinetunedT5SmallTextSummarization,mcro_Model,{}
mcro_resnet18a1in1k-Citation3,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}'}"
mcro_khawhitemangaocrbase,mcro_Model,{}
mcro_khawhitemangaocrbase-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'general purpose printed Japanese OCR'}
mcro_mobilenetv3small100lambin1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_crossencodermsmarcoMiniLML6v2-QuantativeAnalysis,mcro_QuantativeAnalysisSection,"{'prov_hasTextValue': 'In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset.'}"
mcro_intfloatmultilinguale5largeinstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '[Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/pdf/2402.05672).\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv 2024'}"
mcro_metallamaMetaLlama38B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{llama3modelcard,\n  title={Llama 3 Model Card},\n  author={AI@Meta},\n  year={2024},\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n}'}"
mcro_cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'UMLS'}
mcro_Qwen253BInstruct,mcro_Model,{}
mcro_Qwen253BInstruct-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_pyannotespeakerdiarization30-Benchmark,mcro_QuantativeAnalysisSection,{}
mcro_phobertpretrainedlanguagemodels-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{phobert,\n    title     = {{PhoBERT: Pre-trained language models for Vietnamese}},\n    author    = {Dat Quoc Nguyen and Anh Tuan Nguyen},\n    journal   = {Findings of EMNLP},\n    year      = {2020}\n    }'}"
mcro_avsolatorioGISTsmallEmbeddingv0-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The dataset used is a compilation of the MEDI and MTEB Classification training datasets. Third-party datasets may be subject to additional terms and conditions under their associated licenses. A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:\n\n- Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets)\n- Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb\n\nThe dataset contains a `task_type` key, which can be used to select only the mteb classification tasks (prefixed with `mteb_`).\n\nThe **MEDI Dataset** is published in the following paper: [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741).\n\nThe MTEB Benchmark results of the GIST embedding model, compared with the base model, suggest that the fine-tuning dataset has perturbed the model considerably, which resulted in significant improvements in certain tasks while adversely degrading performance in some.\n\nThe retrieval performance for the TRECCOVID task is of note. The fine-tuning dataset does not contain significant knowledge about COVID-19, which could have caused the observed performance degradation. We found some evidence, detailed in the paper, that thematic coverage of the fine-tuning data can affect downstream performance.'}"
mcro_llama32CollectionOfMultilingualLlLMs-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.'}"
mcro_clip-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.'}"
mcro_bertMiniatures-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.'}"
mcro_Qwen2505BInstruct,mcro_Model,{}
mcro_deepseekaiDeepSeekR1DistillQwen14B-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'reasoning'}
mcro_microsoftPhi35visioninstruct-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) General image understanding\n4) Optical character recognition\n5) Chart and table understanding\n6) Multiple image comparison\n7) Multi-image or video clip summarization\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n### Use Case Considerations\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.***'}"
mcro_opusmtnlen-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'opus'}
mcro_modelcard123-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet'}
mcro_kokoro82M-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'permissive/non-copyrighted audio data and IPA phoneme labels'}
mcro_Salesforceblip2opt27b,mcro_Model,{}
mcro_BridgeTowerbridgetowerlargeitmmlmitc-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'The BridgeTower model was pretrained on four public image-caption datasets:\n- [Conceptual Captions (CC3M)](https://ai.google.com/research/ConceptualCaptions/)\n- [Conceptual 12M (CC12M)](https://github.com/google-research-datasets/conceptual-12m)\n- [SBU Captions](https://www.cs.rice.edu/~vo9/sbucaptions/)\n- [MSCOCO Captions](https://arxiv.org/pdf/1504.00325.pdf)\n- [Visual Genome](https://visualgenome.org/)\n  \nThe total number of unique images in the combined data is around 14M.'}
mcro_distilbertbasemultilingualcased-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.""}"
mcro_mmlwretrievalrobertalarge,mcro_Model,{}
mcro_mmlwretrievalrobertalarge-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': '[Polish MS MARCO](https://huggingface.co/datasets/clarin-knext/msmarco-pl)'}
mcro_bertmultilingualbasemodelcased-UseCase,mcro_UseCaseInformationSection,{}
mcro_FacebookAIxlmrobertabase,mcro_Model,{}
mcro_Snowflakesnowflakearcticembedm-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Arctic is licensed under the [Apache-2](https://www.apache.org/licenses/LICENSE-2.0). The released models can be used for commercial purposes free of charge.'}
mcro_deberta-v3-base-prompt-injection-v2-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'This model is a fine-tuned version of [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) specifically developed to detect and classify prompt injection attacks which can manipulate language models into producing unintended outputs.'}
mcro_graphcodebert-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages.'}"
mcro_mmlwretrievalrobertalarge-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{dadas2024pirb,\n  title={{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods}, \n  author={Sławomir Dadas and Michał Perełkiewicz and Rafał Poświata},\n  year={2024},\n  eprint={2402.13350},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}'}"
mcro_flairnerenglishlarge-PerformanceMetric,mcro_PerformanceMetricInformationSection,"{'prov_hasTextValue': '94,36 (corrected CoNLL-03)'}"
mcro_FinBERT-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '[Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014)'}
mcro_facebookwav2vec2base960h-ModelDetail,mcro_ModelDetailSection,{}
mcro_Inteldptlarge-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': ""@article{DBLP:journals/corr/abs-2103-13413,\n  author    = {Ren{\\'{e}} Ranftl and\n               Alexey Bochkovskiy and\n               Vladlen Koltun},\n  title     = {Vision Transformers for Dense Prediction},\n  journal   = {CoRR},\n  volume    = {abs/2103.13413},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2103.13413},\n  eprinttype = {arXiv},\n  eprint    = {2103.13413},\n  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13413.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}""}"
mcro_metaLlama3-OutOfScopeUseCase,mcro_OutOfScopeUseCaseSectionInformation,{'prov_hasTextValue': 'Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.'}
mcro_sentencetransformersdistilbertbasenlimeantokens-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}
mcro_deepseekaiDeepSeekR1DistillQwen7B-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_bge-m3-onnx-o4-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '- [x] Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\n- [x] Multi-Linguality: It can support more than **100** working languages.\n- [x] Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to **8192** tokens.'}"
mcro_EleutherAIpythia70mdeduped-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': 'Never rely on Pythia-70M-deduped to produce factually accurate output.'}
mcro_mask2former-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone).'}"
mcro_blackforestlabsFLUX1dev-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': '- This model is not intended or able to provide factual information.\n- As a statistical model this checkpoint might amplify existing societal biases.\n- The model may fail to generate output that matches the prompts.\n- Prompt following is heavily influenced by the prompting-style.'}
mcro_Stablediffusionsafetychecker-Citation,mcro_CitationInformationSection,{}
mcro_XenovaallMiniLML6v2-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_bert-base-uncased,mcro_Model,{}
mcro_bert-base-uncased-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CNN'}
mcro_metallamaLlama3211BVisionInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.'}"
mcro_vitbasepatch32clip448laion2bftin12kin1k-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}; @inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}; @article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}; @software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}; @article{cherti2022reproducible,\n  title={Reproducible scaling laws for contrastive language-image learning},\n  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n  journal={arXiv preprint arXiv:2212.07143},\n  year={2022}\n}'}"
mcro_tsmatzxlmrobertanerjapanese-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_microsoftPhi35visioninstruct-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': '**Architecture:** Phi-3.5-vision has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.<br>\n**Inputs:** Text and Image. It’s best suited for prompts using the chat format.<br>\n**Context length:** 128K tokens<br>\n**GPUs:** 256 A100-80G<br>\n**Training time:** 6 days<br>\n**Training data:** 500B tokens (vision tokens + text tokens)<br>\n**Outputs:** Generated text in response to the input<br>\n**Dates:** Trained between July and August 2024<br>\n**Status:** This is a static model trained on an offline text dataset with cutoff date March 15, 2024. Future versions of the tuned models may be released as we improve models.<br>\n**Release date:** August 2024<br>'}"
mcro_pyannotesegmentation30-UseCase,mcro_UseCaseInformationSection,{}
mcro_stabilityaistablediffusion35medium-Consideration,mcro_ConsiderationInformationSection,{}
mcro_RobertaLargeMnli-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT'}
mcro_Snowflakesnowflakearcticembedxs-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance.'}"
mcro_distilgpt2-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_jonatasgrosmanwav2vec2largexlsr53chinesezhcn-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech recognition in Chinese'}
mcro_IDEAResearchgroundingdinobase-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'extends a closed-set object detection model with a text encoder, enabling open-set object detection'}"
mcro_mimi-IntendedUseCase,mcro_PrimaryIntendedUseCaseInformationSection,{}
mcro_gte-large-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'text reranking; semantic textual similarity; information retrieval'}
mcro_Salesforceblipvqacapfiltlarge-EthicalConsideration,mcro_EthicalConsiderationSection,"{'prov_hasTextValue': 'This release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people’s lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.'}"
mcro_lmmslabLLaVAVideo7BQwen2-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'SO400M + Qwen2'}
mcro_deepseekaiDeepSeekR1DistillQwen7B-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'You can chat with DeepSeek-R1 on DeepSeek\'s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button ""DeepThink""'}"
mcro_bertbasechinese-ModelParameter,mcro_ModelParameterSection,{'prov_hasTextValue': '* **type_vocab_size:** 2\n* **vocab_size:** 21128\n* **num_hidden_layers:** 12'}
mcro_flan-t5-base-EvaluationData,mcro_EvaluationDataInformationSection,{}
mcro_englishnerinflairfastmodel-PerformanceMetric,mcro_PerformanceMetricInformationSection,"{'prov_hasTextValue': '92,92'}"
mcro_metallamaLlama31-ModelDetail,mcro_ModelDetailSection,{}
mcro_metallamaLlama31-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_llavahfllava157bhf,mcro_Model,{}
mcro_llavahfllava157bhf-ModelDetail,mcro_ModelDetailSection,{}
mcro_mistralaiMixtral8x7BInstructv01-UseCaseSection,mcro_UseCaseInformationSection,{}
mcro_deepseekaiDeepSeekV3-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.'}
mcro_microsoftPhi3mini128kinstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_bertmultilingualbasemodeluncased-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_t5large-ModelDetail,mcro_ModelDetailSection,{}
mcro_t5large-Citation,mcro_CitationInformationSection,{}
mcro_ESM2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'suitable for fine-tuning on a wide range of tasks that take protein sequences as input'}
mcro_gemma3-ModelDetail,mcro_ModelDetailSection,{}
mcro_gemma3-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}'}"
mcro_crossencodernlidebertav3base-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_NLLB200-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.'}
mcro_llama2-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)'}
mcro_facebookcontriever,mcro_Model,{}
mcro_faceparsing-ModelDetail,mcro_ModelDetailSection,{}
mcro_ibmgranitegranite318binstruct,mcro_Model,{}
mcro_llama32CollectionOfMultilingualLargeLanguageModelsLlms-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.'}"
mcro_flan-t5-base-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_phi4-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Our training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:\n\n1. Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code.\n\n2. Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).\n\n3. Acquired academic books and Q&A datasets.\n\n4. High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nMultilingual data constitutes about 8% of our overall data. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge.'}"
mcro_EleutherAIpythia70mdeduped,mcro_Model,{}
mcro_t53b-UseCase,mcro_UseCaseInformationSection,{}
mcro_allrobertalargev1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'We used the pretrained [`roberta-large`](https://huggingface.co/roberta-large) model and fine-tuned in on a \n1B sentence pairs dataset.'}
mcro_whisper-baseen-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'English speech recognition'}
mcro_gemma2-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Terms of Use: [Terms][terms]'}
mcro_ViTSO400M14SigLIP384-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'WebLI'}
mcro_huggingquantsMetaLlama318BInstructAWQINT4-UseCase,mcro_UseCaseInformationSection,{}
mcro_eng-zho-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'pre-processing: normalization + SentencePiece (spm32k,spm32k); source group: English; target group: Chinese'}"
mcro_emimodel-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CreativeML Open RAIL++-M License'}
mcro_T5basesummarizationclaimextractor-ModelDetail,mcro_ModelDetailSection,{}
mcro_AlibabaNLPgtemultilingualbase,mcro_Model,{}
mcro_AlibabaNLPgtemultilingualbase-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'multilingual retrieval, cross-lingual retrieval, long text retrieval, and general text representation'}"
mcro_BAAIbgesmallen-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'MTEB, C-MTEB'}"
mcro_Salesforceblipimagecaptioninglarge-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}'}"
mcro_microsoftPhi35miniinstruct,mcro_Model,{}
mcro_microsoftPhi35miniinstruct-ModelParameter,mcro_ModelParameterSection,{}
mcro_answerdotaiModernBERTbase-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'ModernBERT’s native long context length makes it ideal for tasks that require processing long documents, such as retrieval, classification, and semantic search within large corpora. The model was trained on a large corpus of text and code, making it suitable for a wide range of downstream tasks, including code retrieval and hybrid (text + code) semantic search.'}"
mcro_stablediffusioninpaintingmodelcard-ModelDetailSection,mcro_ModelDetailSection,{}
mcro_toxigen-Reference,mcro_ReferenceInformationSection,{'prov_hasTextValue': 'Please visit the [Github Repository](https://github.com/microsoft/TOXIGEN) for the training dataset and further details.'}
mcro_gtelargeenv15-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Text Embeddings'}
mcro_sentencetransformersmultiqampnetbasedotv1,mcro_Model,{}
mcro_AlibabaNLPgtebaseenv15-ModelParameter,mcro_ModelParameterSection,{}
mcro_bertbasemodelcased-ModelDetail,mcro_ModelDetailSection,{}
mcro_wav2vec2xlsr300mcv7turkish-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'The following datasets were used for finetuning:\n - [Common Voice 7.0 TR](https://huggingface.co/datasets/mozilla-foundation/common_voice_7_0) All `validated` split except `test` split was used for training.\n - [MediaSpeech](https://www.openslr.org/108/)'}
mcro_facebookmusicgenmedium-ModelDetail,mcro_ModelDetailSection,{}
mcro_rorshark-vit-base-ModelDetail,mcro_ModelDetailSection,{}
mcro_vitmatte-model-ModelDetail,mcro_ModelDetailSection,{}
mcro_vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_ChatGLM2-6B-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'GLM'}
mcro_WhereIsAIUAELargeV1-UseCase,mcro_UseCaseInformationSection,{}
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Consideration,mcro_ConsiderationInformationSection,{}
mcro_USERbgem3-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': ""We did not thoroughly evaluate the model's ability for sparse and multi-vec encoding.""}"
mcro_germanbert-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_microsoftPhi3mini128kinstruct-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Our training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of 1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.'}"
mcro_wavlmlarge-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': '- 60,000 hours of [Libri-Light](https://arxiv.org/abs/1912.07875)\n- 10,000 hours of [GigaSpeech](https://arxiv.org/abs/2106.06909)\n- 24,000 hours of [VoxPopuli](https://arxiv.org/abs/2101.00390)'}"
mcro_vit-age-classifier-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""A vision transformer finetuned to classify the age of a given person's face.""}"
mcro_gte-small-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT framework'}
mcro_debertav3basetasksourcenli-Citation,mcro_CitationInformationSection,{}
mcro_googleelectrasmalldiscriminator-ModelDetail,mcro_ModelDetailSection,{}
mcro_bertbaseuncased-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task.""}"
mcro_pyannotespeakerdiarization30-Consideration,mcro_ConsiderationInformationSection,{}
mcro_CLIPViTB32LAION2B-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'research output for research communities'}
mcro_sentencetransformersmultiqampnetbasedotv1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for **semantic search**.'}
mcro_indobenchmarkindobertbasep1-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{wilie2020indonlu,\n  title={IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding},\n  author={Bryan Wilie and Karissa Vincentio and Genta Indra Winata and Samuel Cahyawijaya and X. Li and Zhi Yuan Lim and S. Soleman and R. Mahendra and Pascale Fung and Syafri Bahar and A. Purwarianti},\n  booktitle={Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},\n  year={2020}\n}'}"
mcro_blackforestlabsFLUX1schnell-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_stabilityaisdturbo-Reference,mcro_ReferenceInformationSection,{'prov_hasTextValue': 'https://github.com/Stability-AI/generative-models; https://stability.ai/research/adversarial-diffusion-distillation'}
mcro_wav2vec2largexlsrhindi-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_wav2vec2xlsr300mhebrew,mcro_Model,{}
mcro_InternVL378B-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'InternVL3 retains the same model architecture as InternVL 2.5 and its predecessors, InternVL 1.5 and 2.0, following the ""ViT-MLP-LLM"" paradigm. In this new version, we integrate a newly incrementally pre-trained InternViT with various pre-trained LLMs, including InternLM 3 and Qwen 2.5, using a randomly initialized MLP projector.'}"
mcro_Salesforceblip2opt27b-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': ""BLIP2-OPT uses off-the-shelf OPT as the language model. It inherits the same risks and limitations as mentioned in Meta's model card.\n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models.\n>\nBLIP2 is fine-tuned on image-text datasets (e.g. [LAION](https://laion.ai/blog/laion-400-open-dataset/) ) collected from the internet.  As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\nBLIP2 has not been tested in real world applications. It should not be directly deployed in any applications. Researchers should first carefully assess the safety and fairness of the model in relation to the specific context they’re being deployed within.""}"
mcro_Auralis-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Based on [Coqui XTTS-v2](https://huggingface.co/coqui/XTTS-v2)'}
mcro_segformerb2finetunedforclothessegmentation-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ATR dataset; mattmdjaga/human_parsing_dataset'}
mcro_microsoftPhi35visioninstruct-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The model is licensed under the [MIT license](./LICENSE).'}
mcro_unik3d,mcro_Model,{}
mcro_resnet50.a1_in1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_BAAIbgem3-UseCase,mcro_UseCaseInformationSection,{}
mcro_IDEAResearchgroundingdinotiny,mcro_Model,{}
mcro_IDEAResearchgroundingdinotiny-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection.'}"
mcro_wavlmbaseplus,mcro_Model,{}
mcro_VitTinyVitSmall,mcro_Model,{}
mcro_segformerb2finetunedforclothessegmentation-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'https://github.com/NVlabs/SegFormer/blob/master/LICENSE'}
mcro_Inteldptlarge-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for zero-shot monocular depth estimation. See the [model hub](https://huggingface.co/models?search=dpt) to look for fine-tuned versions on a task that interests you.'}
mcro_canary1bflash-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': ""The canary-1b-flash model is trained on a total of 85K hrs of speech data. It consists of 31K hrs of public data, 20K hrs collected by [Suno](https://suno.ai/), and 34K hrs of in-house data. \nThe datasets below include conversations, videos from the web and audiobook recordings.\n\n**Data Collection Method:**\n* Human <br>\n\n**Labeling Method:**\n* Hybrid: Human, Automated <br>\n\nThe constituents of public data are as follows. \n\n#### English (25.5k hours)\n- Librispeech 960 hours\n- Fisher Corpus\n- Switchboard-1 Dataset\n- WSJ-0 and WSJ-1\n- National Speech Corpus (Part 1, Part 6)\n- VCTK\n- VoxPopuli (EN)\n- Europarl-ASR (EN)\n- Multilingual Librispeech (MLS EN) - 2,000 hour subset\n- Mozilla Common Voice (v7.0)\n- People's Speech - 12,000 hour subset\n- Mozilla Common Voice (v11.0)  - 1,474 hour subset\n\n#### German (2.5k hours)\n- Mozilla Common Voice (v12.0)  - 800 hour subset\n- Multilingual Librispeech (MLS DE) - 1,500 hour subset\n- VoxPopuli (DE) - 200 hr subset\n\n#### Spanish (1.4k hours)\n- Mozilla Common Voice (v12.0)  - 395 hour subset\n- Multilingual Librispeech (MLS ES) - 780 hour subset\n- VoxPopuli (ES) - 108 hour subset\n- Fisher  - 141 hour subset\n\n#### French (1.8k hours)\n- Mozilla Common Voice (v12.0)  - 708 hour subset\n- Multilingual Librispeech (MLS FR) - 926 hour subset\n- VoxPopuli (FR) - 165 hour subset""}"
mcro_speechbrainspkrececapavoxceleb-Citation-SpeechBrain,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{speechbrain,\n  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n  year={2021},\n  eprint={2106.04624},\n  archivePrefix={arXiv},\n  primaryClass={eess.AS},\n  note={arXiv:2106.04624}\n}'}"
mcro_tabletransformerpretrainedfortablestructurerecognition,mcro_Model,{}
mcro_tabletransformerpretrainedfortablestructurerecognition-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for detecting tables in documents. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer) for more info.'}
mcro_openvla7b-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT'}
mcro_unslothQwen2505BInstructbnb4bit-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_facebookesm2t363BUR50D,mcro_Model,{}
mcro_Qwen2505B-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'We do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model.'}"
mcro_NLLB200-TrainData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': '• We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.'}
mcro_emimodel-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{podell2023sdxl,\n      title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis}, \n      author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas Müller and Joe Penna and Robin Rombach},\n      year={2023},\n      eprint={2307.01952},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_hallucinationevaluationmodel-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53persian-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Common Voice 6.1'}
mcro_T5basesummarizationclaimextractor-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'N/A'}
mcro_e5smallv2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model has 12 layers and the embedding size is 384.'}
mcro_IP-Adapter-FaceID-LicenseInformationSection,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'AS InsightFace pretrained models are available for non-commercial research purposes, IP-Adapter-FaceID models are released exclusively for research purposes and is not intended for commercial use.'}"
mcro_crossencodermsmarcoTinyBERTL2v2-ModelDetail,mcro_ModelDetailSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53dutch,mcro_Model,{}
mcro_AlibabaNLPgtebaseenv15-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_CLIPViTbigG14LAION2B-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT'}
mcro_mistralaiMixtral8x7BInstructv01-LimitationSection,mcro_LimitationInformationSection,{}
mcro_sat-3l-sm,mcro_Model,{}
mcro_distilrobertabase-UseCase,mcro_UseCaseInformationSection,{}
mcro_llavahfllava157bhf-ModelArchitectureInformation,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.\nIt is an auto-regressive language model, based on the transformer architecture.'}"
mcro_Qwen306B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}'}"
mcro_mistralaiMixtral8x7BInstructv01-LicenseSection,mcro_LicenseInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53arabic,mcro_Model,{}
mcro_NLLB200-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CC-BY-NC'}
mcro_snunlpKRSBERTV40KklueNLIaugSTS,mcro_Model,{}
mcro_snunlpKRSBERTV40KklueNLIaugSTS-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}
mcro_facebookmmslid256,mcro_Model,{}
mcro_facebookmmslid256-Dataset,mcro_DatasetInformationSection,{}
mcro_owlv2-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as [COCO](https://cocodataset.org/#home) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html).\n\n(to be updated for v2)'}"
mcro_deepseekaiDeepSeekR1DistillQwen14B-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Qwen2.5 series'}
mcro_lengyue233contentvecbest-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_Florence-2Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'sequence-to-sequence architecture'}
mcro_efficientnet_b0.ra_in1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_text-detection-model-for-surya,mcro_Model,{}
mcro_microsoftdebertalargemnli-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}'}"
mcro_clip,mcro_Model,{}
mcro_cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken,mcro_Model,{}
mcro_Qwen1505BChat-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}'}"
mcro_tabletransformerfinetunedfortablestructurerecognition-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The Table Transformer is equivalent to [DETR](https://huggingface.co/docs/transformers/model_doc/detr), a Transformer-based object detection model. Note that the authors decided to use the ""normalize before"" setting of DETR, which means that layernorm is applied before self- and cross-attention.'}"
mcro_mlxcommunitygemma312bitqat4bit,mcro_Model,{}
mcro_testModel-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'intended use case'}
mcro_fnetbasemodel-ModelParameter,mcro_ModelParameterSection,"{'prov_hasTextValue': 'The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000.'}"
mcro_trocrbasesroie,mcro_Model,{}
mcro_trocrbasesroie-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for optical character recognition (OCR) on single text-line images. See the [model hub](https://huggingface.co/models?search=microsoft/trocr) to look for fine-tuned versions on a task that interests you.'}
mcro_wavlmbaseplus-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This is an English pre-trained speech model that has to be fine-tuned on a downstream task like speech recognition or audio classification before it can be \nused in inference. The model was pre-trained in English and should therefore perform well only in English. The model has been shown to work well on the [SUPERB benchmark](https://superbbenchmark.org/).'}
mcro_Stablediffusionv14ModelCard-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Diffusion-based text-to-image generation model'}
mcro_EmergentMethodsglinermediumnewsv21-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_whisper-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Alec Radford et al from OpenAI; @misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}'}"
mcro_wav2vec2xlsr300mhebrew-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_FinetunedT5SmallTextSummarization-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'a variety of documents and their corresponding human-generated summaries'}
mcro_fasttextlanguageidentification-ModelDetail,mcro_ModelDetailSection,{}
mcro_fasttextlanguageidentification-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Please cite [1] if using this code for learning word representations or [2] if using for text classification.'}
mcro_flan-t5-base-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_Qwen25Omni-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Thinker-Talker architecture'}
mcro_bartlargesizedmodelfinetunedoncnndailymail,mcro_Model,{}
mcro_gpt2medium-TrainingData,mcro_DatasetInformationSection,{}
mcro_whisper-baseen-PerformanceLimitations,mcro_LimitationInformationSection,{'prov_hasTextValue': 'hallucination'}
mcro_bertsmalluncased-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{turc2019,\n  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1908.08962v2 },\n  year={2019}\n}'}"
mcro_spladecocondenserensembledistil-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{https://doi.org/10.48550/arxiv.2205.04733,\n  doi = {10.48550/ARXIV.2205.04733},\n  url = {https://arxiv.org/abs/2205.04733},\n  author = {Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, Stéphane},\n  keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}'}"
mcro_gemma3-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'These models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.; Here are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].'}"
mcro_answerdotaiModernBERTbase-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'ModernBERT’s training data is primarily English and code, so performance may be lower for other languages. While it can handle long sequences efficiently, using the full 8,192 tokens window may be slower than short-context inference. Like any large language model, ModernBERT may produce representations that reflect biases present in its training data. Verify critical or sensitive outputs before relying on them.'}"
mcro_Distilbertbasemultilingualcasednerhrl,mcro_Model,{}
mcro_Protbertmodel-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_openbmbMiniCPMo26-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_Xlmrobertalargenerspanish-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'CoNLL-2002'}
mcro_mobilevitsmall-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for image classification.'}
mcro_gte-base-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'text reranking; semantic textual similarity; information retrieval'}
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-License,mcro_LicenseInformationSection,{}
mcro_unslothDeepSeekR10528Qwen38BGGUF-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B'}"
mcro_facebookesm2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'state-of-the-art protein model trained on a masked language modelling objective'}
mcro_ModelCardReport-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_crossencodermsmarcoMiniLML6v2-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'This model was trained on the MS Marco Passage Ranking task.'}
mcro_BiomedParse-DataSpecificationSection,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': ""- The model expect 2D 8-bit RGB or grayscale images by default, with pixel values ranging from 0 to 255 and resolution 1024*1024.\n- The model outputs pixel probabilities in the same shape as the input image. We convert the floating point probabilities to 8-bit grayscale outputs. The probability threshold for segmentation mask is 0.5, which corresponds to 127.5 in 8-bit grayscale output.\n- The model takes in text prompts for segmentation and doesn't have a fixed number of targets to handle. However, to ensure quality performance, we recommend the following tasks based on evaluation results. However, as we only evaluated the model on the test split of BiomedParseData, there is no guarantee for the same performance on external datasets even for the same task, due to variation in device, preprocessing, resolution and other distribution shifts. For best performance, we recommend finetuning on your specific tasks.\n  - CT:\n    - abdomen: adrenal gland, aorta, bladder, duodenum, esophagus, gallbladder, kidney, kidney cyst, kidney tumor, left adrenal gland, left kidney, liver, pancreas, postcava, right adrenal gland, right kidney, spleen, stomach, tumor\n    - colon: tumor\n    - liver: liver, tumor\n    - lung: COVID-19 infection, nodule\n    - pelvis: uterus\n  - MRI-FLAIR: brain: edema, lower-grade glioma, tumor, tumor core, whole tumor\n  - MRI-T1-Gd: brain: enhancing tumor, tumor core\n  - MRI-T2: prostate: prostate peripheral zone, prostate transitional zone,\n  - MRI:\n    - abdomen: aorta, esophagus, gallbladder, kidney, left kidney, liver, pancreas, postcava, right kidney, spleen, stomach\n    - brain: anterior hippocampus, posterior hippocampus\n    - heart: left heart atrium, left heart ventricle, myocardium, right heart ventricle\n    - prostate: prostate\n  - OCT: retinal: edema\n  - X-Ray: chest: COVID-19 infection, left lung, lung, lung opacity, right lung, viral pneumonia\n  - Dermoscopy: skin: lesion, melanoma\n  - Endoscope: colon: neoplastic polyp, non-neoplastic polyp, polyp\n  - Fundus: retinal: optic cup, optic disc,\n  - Pathology:\n    - bladder: neoplastic cells\n    - breast: epithelial cells, neoplastic cells\n    - cervix: neoplastic cells\n    - colon: glandular structure, neoplastic cells\n    - esophagus: neoplastic cells\n    - kidney: neoplastic cells\n    - liver: epithelial cells, neoplastic cells\n    - ovarian: epithelial cells, neoplastic cells\n    - prostate: neoplastic cells skin: neoplastic cells\n    - stomach: neoplastic cells\n    - testis: epithelial cells\n    - thyroid: epithelial cells, neoplastic cells\n    - uterus: neoplastic cells\n  - Ultrasound:\n    - breast: benign tumor, malignant tumor, tumor\n    - heart: left heart atrium, left heart ventricle\n    - transperineal: fetal head, public symphysis""}"
mcro_pyannotespeakerdiarization31-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_legalbertTheMuppetsStraightOutOfLawSchool,mcro_Model,{}
mcro_RobertaLargeMnli-Citation,mcro_CitationInformationSection,{}
mcro_metaLlama31Instruct-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.'}"
mcro_SmolDocling256Mpreview-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{nassar2025smoldoclingultracompactvisionlanguagemodel,\n      title={SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion}, \n      author={Ahmed Nassar and Andres Marafioti and Matteo Omenetti and Maksym Lysak and Nikolaos Livathinos and Christoph Auer and Lucas Morin and Rafael Teixeira de Lima and Yusik Kim and A. Said Gurbuz and Michele Dolfi and Miquel Farré and Peter W. J. Staar},\n      year={2025},\n      eprint={2503.11576},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2503.11576}, \n}'}"
mcro_nomicainomicembedtextv15-Citation,mcro_CitationInformationSection,{}
mcro_chronosboltsmall-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache-2.0 License'}
mcro_distilbertbasemultilingualcased,mcro_Model,{}
mcro_gemma2b-Citation,mcro_CitationInformationSection,{}
mcro_bertbaseNER,mcro_Model,{}
mcro_kluerobertabase-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Pretrained RoBERTa Model on Korean Language'}
mcro_FlagEmbedding-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_phi4,mcro_Model,{}
mcro_phi4-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '[Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)'}
mcro_gte-small-LimitationInformationSection,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens.'}"
mcro_t5small-UseCase,mcro_UseCaseInformationSection,{}
mcro_vitbasepatch8224augreg2in21kftin1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_opusmtruen-Citation,mcro_CitationInformationSection,{}
mcro_deepseekaiDeepSeekR1DistillLlama8B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'DeepSeek-R1'}
mcro_sentencetransformersmsmarcodistilbertbasev4,mcro_Model,{}
mcro_sentencetransformersmsmarcodistilbertbasev4-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_segformerb0finetunedade20k,mcro_Model,{}
mcro_Stablediffusionv14ModelCard,mcro_Model,{}
mcro_Qwen3-8B-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Causal Language Models'}
mcro_deepseekaiDeepSeekR1DistillQwen7B,mcro_Model,{}
mcro_bigvganv244khz128band512x,mcro_Model,{}
mcro_jbetkerwav2vec2largerobustftlibrittsvoxpopuli-PerformanceMetricInformationSection,mcro_PerformanceMetricInformationSection,"{'prov_hasTextValue': 'The model gets a respectable WER of 4.45% on the librispeech validation set. The baseline, `facebook/wav2vec2-large-robust-ft-libri-960h`, got 4.3%.'}"
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Limitation,mcro_LimitationInformationSection,{}
mcro_nlpconnectvitgpt2imagecaptioning-ModelDetail,mcro_ModelDetailSection,{}
mcro_allMiniLML6v2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}
mcro_facebookopt125m-Arch,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'OPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\nOPT belongs to the same family of decoder-only models like [GPT-3](https://arxiv.org/abs/2005.14165). As such, it was pretrained using the self-supervised causal language modedling objective.'}"
mcro_whisperlargev3-ModelDetail,mcro_ModelDetailSection,{}
mcro_whisperlargev3-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}'}"
mcro_conjunctsditre15-UseCase,mcro_UseCaseInformationSection,{}
mcro_wavlmlarge-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_germanbert-Owner,mcro_OwnerInformationSection,{}
mcro_bigvganv222khz80band256x,mcro_Model,{}
mcro_vitmatte-model-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'ViTMatte is a simple approach to image matting, the task of accurately estimating the foreground object in an image. The model consists of a Vision Transformer (ViT) with a lightweight head on top.'}"
mcro_bert-base-uncased-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet'}
mcro_jhartmannemotionenglishdistilrobertabase-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'DistilRoBERTa-base'}
mcro_jonatasgrosmanwav2vec2largexlsr53persian-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech recognition in Persian'}
mcro_modelid-UseCase,mcro_UseCaseInformationSection,{}
mcro_BRIA_Background_Removal_v14-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'designed to effectively separate foreground from background in a range of\ncategories and image types. This model has been trained on a carefully selected dataset, which includes:\ngeneral stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale.'}"
mcro_ESMFold,mcro_Model,{}
mcro_ESMFold-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'For details on the model architecture and training, please refer to the [accompanying paper](https://www.science.org/doi/10.1126/science.ade2574).'}"
mcro_kresnikwav2vec2largexlsrkorean-PerformanceMetricInformationSection,mcro_PerformanceMetricInformationSection,{'prov_hasTextValue': 'WER: 4.74%; CER: 1.78%'}
mcro_jonatasgrosmanwav2vec2largexlsr53persian-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{grosman2021xlsr53-large-persian,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ersian},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-persian}},\n  year={2021}\n}'}"
mcro_FlagEmbedding-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_bertMiniatures-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{turc2019,\n  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1908.08962v2 },\n  year={2019}\n}'}"
mcro_granitetimeseriesttmr1-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_twitterxlmrobertabasesentiment-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'sentiment analysis'}
mcro_FinetunedT5SmallTextSummarization-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'T5 transformer model'}
mcro_emrecanbertbaseturkishcasedmeannlistsbtr-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'NLI; STS-b'}
mcro_llama4Scout17B16EInstruct,mcro_Model,{}
mcro_all-distilroberta-v1,mcro_Model,{}
mcro_mxbai-rerank-xsmall-v1-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-TechnicalDetail,obo_http_//purl.obolibrary.org/obo/IAO_0000314,"{'prov_hasTextValue': '8-bit quantized version of DeepSeek-R1-0528-Qwen3-8B using MLX, optimized for Apple Silicon.'}"
mcro_FlagEmbedding-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'retromae and contrastive learning; We pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning.'}
mcro_microsoftwavlmbaseplussv,mcro_Model,{}
mcro_segformerb0finetunedade20k-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE).'}
mcro_microsoftphi2-Training,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': '* Architecture: a Transformer-based model with next-word prediction objective\n\n* Context length: 2048 tokens\n\n* Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\n\n* Training tokens: 1.4T tokens\n\n* GPUs: 96xA100-80G\n\n* Training time: 14 days'}"
mcro_facebookesm2,mcro_Model,{}
mcro_facebookesm2-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2'}
mcro_metallamaMetaLlama38B,mcro_Model,{}
mcro_metallamaMetaLlama38B-Consideration,mcro_ConsiderationInformationSection,{}
mcro_clip-OutOfScopeUseCase,mcro_OutOfScopeUseCaseSectionInformation,"{'prov_hasTextValue': '**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP’s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.\n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.'}"
mcro_bertsmalluncased-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'See other BERT model cards e.g. https://huggingface.co/bert-base-uncased'}
mcro_TheBlokephi2GGUF-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Phi-2 is intended for research purposes only. Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.'}"
mcro_vit_base_patch16_224.augreg2_in21k_ft_in1k-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_jinaaijinarerankerv2basemultilingual-LicenseInformationSection,mcro_LicenseInformationSection,"{'prov_hasTextValue': ""_This model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please [contact us](https://jina.ai/contact-sales) for any further clarifications._""}"
mcro_Qwen257BInstruct-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_CLIPSegModel-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Lüddecke et al. and first released in [this repository](https://github.com/timojl/clipseg).'}"
mcro_sentencetransformersparaphrasemultilingualMiniLML12v2-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}
mcro_llavahfllavav16mistral7bhf-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'The LLaVA-NeXT model was proposed in [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://llava-vl.github.io/blog/2024-01-30-llava-next/) by Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee. LLaVa-NeXT (also called LLaVa-1.6) improves upon [LLaVa-1.5](https://huggingface.co/transformers/main/model_doc/llava.html) by increasing the input image resolution and training on an improved visual instruction tuning dataset to improve OCR and common sense reasoning.'}"
mcro_facebookesm2t363BUR50D-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': 'For detailed information on the model architecture and training data, please refer to the [accompanying paper](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2).'}"
mcro_hubertsiuzdaksnac24khz-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'SNAC encodes audio into hierarchical tokens similarly to SoundStream, EnCodec, and DAC'}"
mcro_CLIPViTbigG14LAION2B-ModelDetail,mcro_ModelDetailSection,{}
mcro_CLIPViTbigG14LAION2B-Citation3,mcro_CitationInformationSection,"{'prov_hasTextValue': '@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}'}"
mcro_facebookencodec24khz-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Common Voice; Jamendo dataset; FSD50K; DNS Challenge 4; AudioSet'}
mcro_BilateralReferenceForHighResolutionDichotomousImageSegmentation-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT'}
mcro_BAAIbgem3-Dataset,mcro_DatasetInformationSection,{}
mcro_roberta-baseforextractiveqa-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'roberta-base'}
mcro_visiontransformerbasesizedmodel-CitationInformationSection2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_swin_transformer_v2_tiny_sized_model-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'image classification'}
mcro_TahaDouajidetrdoctabledetection-Citation,mcro_CitationInformationSection,{}
mcro_RoBERTaBaseModel-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_wav2vec2xlsr300mcv7turkish-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_sdxlturbo,mcro_Model,{}
mcro_sdxlturbo-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The model is intended for both non-commercial and commercial usage.'}
mcro_chronos-t5-tiny-ArchitectureSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The models in this repository are based on the [T5 architecture](https://arxiv.org/abs/1910.10683). The only difference is in the vocabulary size: Chronos-T5 models use 4096 different tokens, compared to 32128 of the original T5 models, resulting in fewer parameters.'}"
mcro_mixedbreadaimxbaiembedlargev1-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_AudioSpectrogramTransformerfinetunedonAudioSet-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'It was introduced in the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Gong et al. and first released in [this repository](https://github.com/YuanGongND/ast).'}
mcro_MaziyarPanahiMistral7BInstructv03GGUF,mcro_Model,{}
mcro_FsoftAICpiiphi-ConsiderationInformationSection,mcro_ConsiderationInformationSection,{'prov_hasTextValue': '* Not guaranteed to detect all forms of PII in every context.\n* May return false positives or omit contextually relevant information.'}
mcro_oliverguhrfullstoppunctuationmultilanglarge-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This model predicts the punctuation of English, Italian, French and German texts. We developed it to restore the punctuation of transcribed spoken language.'}"
mcro_gpt2-IntendedUseCase,mcro_UseCaseInformationSection,{}
mcro_ModelCardReport-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_cognitivecomputationsdolphin291yi1534b-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Yi-1.5-34B'}
mcro_QwenQwen25Coder7BInstructGPTQInt4,mcro_Model,{}
mcro_xlmrobertabaselanguagedetection-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_rinnajapanesecloobvitb16-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'The model was trained on [CC12M](https://github.com/google-research-datasets/conceptual-12m) translated the captions to Japanese.'}
mcro_clip-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.'}"
mcro_facebookcontriever-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'This model has been trained without supervision following the approach described in [Towards Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118). The associated GitHub repository is available here https://github.com/facebookresearch/contriever.'}
mcro_distilbertbasemultilingualcasedsentimentsstudent,mcro_Model,{}
mcro_SWividF5TTS,mcro_Model,{}
mcro_bertbasemodelcased-Consideration,mcro_ConsiderationInformationSection,{}
mcro_googleelectrasmalldiscriminator-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators'}
mcro_layoutlmbaseuncased-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': '12-layer, 768-hidden, 12-heads, 113M parameters'}"
mcro_crossencodermsmarcoTinyBERTL2v2,mcro_Model,{}
mcro_metallamaLlama323BInstruct-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).'}"
mcro_Llama160m-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The model is mainly developed as a base Small Speculative Model in the [SpecInfer](https://arxiv.org/abs/2305.09781) paper.'}
mcro_chronos-t5-base-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'This project is licensed under the Apache-2.0 License.'}
mcro_mistralaiMistral7BInstructv02-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': ""The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.""}"
mcro_vitlargepatch14reg4dinov2lvd142m-Citation,mcro_CitationInformationSection,{}
mcro_llavahfllavaonevisionqwen205bovhf-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{li2024llavaonevisioneasyvisualtask,\n      title={LLaVA-OneVision: Easy Visual Task Transfer}, \n      author={Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},\n      year={2024},\n      eprint={2408.03326},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2408.03326}, \n}'}"
mcro_TheBlokephi2GGUF-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'a Transformer-based model with next-word prediction objective'}
mcro_clip-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'CLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.'}"
mcro_googlest5v11,mcro_Model,{}
mcro_edgenextsmallusiin1k-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image Classification'}
mcro_microsoftdebertaxlargemnli-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}'}"
mcro_siglipso400mpatch14384-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'SoViT-400m architecture'}
mcro_bertmultilingualbasemodelcased-ModelDetail,mcro_ModelDetailSection,{}
mcro_bertmultilingualbasemodelcased-Citation,mcro_CitationInformationSection,{}
mcro_bertbasechinese-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper).\n\n- **Developed by:** HuggingFace team\n- **Model Type:** Fill-Mask\n- **Language(s):** Chinese\n- **License:** [More Information needed]\n- **Parent Model:** See the [BERT base uncased model](https://huggingface.co/bert-base-uncased) for more information about the BERT base model.'}"
mcro_chronosboltbase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}'}"
mcro_llama4Scout17B16EInstruct-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI.'}"
mcro_mbartlarge50manytomanymmt,mcro_Model,{}
mcro_tiiuaefalconrw1b-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay'}
mcro_wav2vec2largexlsr53espeakcvft-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Wav2Vec2'}
mcro_M2M100418M-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation.'}
mcro_metaLlama3-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.'}"
mcro_opusmtruen-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CC-BY-4.0'}
mcro_crossencodernlidebertav3base-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model was trained using [SentenceTransformers](https://sbert.net) [Cross-Encoder](https://www.sbert.net/examples/applications/cross-encoder/README.html) class. This model is based on [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base)'}
mcro_efficientnet_b0.ra_in1k-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_spa-eng-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'English; Spanish'}
mcro_khawhitemangaocrbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Vision Encoder Decoder framework'}
mcro_madebyollinsdxlvaefp16fix,mcro_Model,{}
mcro_madebyollinsdxlvaefp16fix-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'SDXL-VAE-FP16-Fix is the [SDXL VAE](https://huggingface.co/stabilityai/sdxl-vae)*, but modified to run in fp16 precision without generating NaNs.'}"
mcro_gemma2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Open Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains.'}
mcro_distilbertdistilbertbaseuncasedfinetunedsst2englis,mcro_Model,{}
mcro_ChatGLM2-6B-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': '中英双语对话'}
mcro_blackforestlabsFLUX1Filldev-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Generated outputs can be used for personal, scientific, and commercial purposes as described in the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).'}"
mcro_sdxlturbo-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'SDXL-Turbo is a distilled version of [SDXL 1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), trained for real-time synthesis.'}"
mcro_microsoftphi2-ModelDetail,mcro_ModelDetailSection,{}
mcro_Salesforceblipimagecaptioningbase-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'image captioning pretrained on COCO dataset'}
mcro_stablediffusioninpaintingmodelcard-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The model is intended for research purposes only.'}
mcro_modelcard123-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image classification'}
mcro_Inteldptlarge-QuantativeAnalysis,mcro_QuantativeAnalysisSection,"{'prov_hasTextValue': 'Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the\nprotocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. ([Ranftl et al., 2021](https://arxiv.org/abs/2103.13413))'}"
mcro_deepseekaiDeepSeekR10528-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}'}"
mcro_t5large-Dataset,mcro_DatasetInformationSection,{}
mcro_EmergentMethodsglinermediumnewsv21-Citation,mcro_CitationInformationSection,{}
mcro_clipvitlargepatch14-Dataset,mcro_DatasetInformationSection,{}
mcro_bertbasemodelcased-UseCase,mcro_UseCaseInformationSection,{}
mcro_cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{liu-etal-2021-self,\n    title = ""Self-Alignment Pretraining for Biomedical Entity Representations"",\n    author = ""Liu, Fangyu  and\n      Shareghi, Ehsan  and\n      Meng, Zaiqiao  and\n      Basaldella, Marco  and\n      Collier, Nigel"",\n    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",\n    month = jun,\n    year = ""2021"",\n    address = ""Online"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/2021.naacl-main.334"",\n    pages = ""4228--4238"",\n    abstract = ""Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust."",\n}'}"
mcro_Qwen2VL2BInstruct,mcro_Model,{}
mcro_sentencetransformersLaBSE-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 768, 'out_features': 768, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n  (3): Normalize()\n)""}"
mcro_microsoftwavlmbaseplussv-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Speaker Verification'}
mcro_jonatasgrosmanwav2vec2largexlsr53portuguese-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': '[Common Voice 6.1](https://huggingface.co/datasets/common_voice)'}
mcro_EmergentMethodsglinermediumnewsv21-UseCase,mcro_UseCaseInformationSection,{}
mcro_Qwen2.5-VL-32B-Instruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_hallucinationevaluationmodel-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'T5-base'}
mcro_Qwen317B-ModelDetail,mcro_ModelDetailSection,{}
mcro_HuggingFaceM4idefics28b-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'OBELICS'}
mcro_snunlpKRSBERTV40KklueNLIaugSTS-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_bertbasechinese-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': '**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).'}"
mcro_IP-Adapter-FaceID-ModelDetail,mcro_ModelDetailSection,{}
mcro_avsolatorioGISTsmallEmbeddingv0-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The model is fine-tuned on top of the [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) using the [MEDI dataset](https://github.com/xlang-ai/instructor-embedding.git) augmented with mined triplets from the [MTEB Classification](https://huggingface.co/mteb) training dataset (excluding data from the Amazon Polarity Classification task).\n\nThe model does not require any instruction for generating embeddings. This means that queries for retrieval tasks can be directly encoded without crafting instructions.\n\nTechnical paper: [GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning](https://arxiv.org/abs/2402.16829)'}
mcro_Qwen2532BInstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_distilbertbasemultilingualcased-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_byt5small-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'mC4'}
mcro_Stablediffusionv14ModelCard-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'LAION-2B (en) and subsets thereof'}
mcro_wav2vec2-baseforemotionrecognition-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{yang2021superb,\n  title={SUPERB: Speech processing Universal PERformance Benchmark},\n  author={Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y and Liu, Andy T and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and others},\n  journal={arXiv preprint arXiv:2105.01051},\n  year={2021}\n}'}"
mcro_Qwen25Coder7BInstruct-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Code Agents'}
mcro_huggingquantsMetaLlama318BInstructAWQINT4-License,mcro_LicenseInformationSection,{}
mcro_FacebookAIxlmrobertabase-ModelDetail,mcro_ModelDetailSection,{}
mcro_whisper-ModelParameter,mcro_ModelParameterSection,{}
mcro_ko-sroberta-multitask-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '- Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). Kornli and korsts: New benchmark datasets for korean natural language understanding. arXiv\npreprint arXiv:2004.03289\n- Reimers, Nils and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” ArXiv abs/1908.10084 (2019)\n- Reimers, Nils and Iryna Gurevych. “Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation.” EMNLP (2020).'}"
mcro_distilbertbasemultilingualcased-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': 'Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n## Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.'}"
mcro_nguyenvulebinhwav2vec2basevi-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Since our model has the same architecture as the English wav2vec2 version, you can use [this notebook](https://colab.research.google.com/drive/1FjTsqbYKphl9kL-eILgUc-bl4zVThL8F?usp=sharing) for more information on how to fine-tune the model.'}"
mcro_modelid-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_e5smallv2,mcro_Model,{}
mcro_SmolLM2,mcro_Model,{}
mcro_testModel-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'mit'}
mcro_TRELLISImageLarge-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'TRELLIS; large 3D genetive model'}
mcro_sarvamaisarvamm-UseCase,mcro_UseCaseInformationSection,{}
mcro_depthanythingDepthAnythingV2Smallhf-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Depth Anything V2 leverages the [DPT](https://huggingface.co/docs/transformers/model_doc/dpt) architecture with a [DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2) backbone.'}
mcro_mask2former-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/).'}
mcro_NLLB200-Metrics,mcro_PerformanceMetricInformationSection,"{'prov_hasTextValue': '• Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.'}"
mcro_gpt2-Citation,mcro_CitationInformationSection,{}
mcro_Qwen3-8B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}'}"
mcro_BilateralReferenceForHighResolutionDichotomousImageSegmentation,mcro_Model,{}
mcro_BilateralReferenceForHighResolutionDichotomousImageSegmentation-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'DIS-TR and validated on DIS-TEs and DIS-VD'}
mcro_metallamaMetaLlama38B-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.'}"
mcro_AlibabaNLPgtebaseenv15-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_flairnerfrench-PerformanceMetric,mcro_PerformanceMetricInformationSection,"{'prov_hasTextValue': '90,61'}"
WikiNER,Undefined,{}
mcro_clip-Consideration,mcro_ConsiderationInformationSection,{}
mcro_CLIPViTbigG14LAION2B-Citation1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}'}"
mcro_detrresnet50-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for object detection. See the [model hub](https://huggingface.co/models?search=facebook/detr) to look for all available DETR models.'}
mcro_wav2vec2-baseforemotionrecognition,mcro_Model,{}
mcro_codebertbase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{feng2020codebert,\n    title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages},\n    author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},\n    year={2020},\n    eprint={2002.08155},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}'}"
mcro_metallamaLlama3211BVisionInstruct-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).'}"
mcro_resnet50.a1_in1k,mcro_Model,{}
mcro_BAAIbgebaseen-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification,  clustering, or semantic search.\nAnd it also can be used in vector databases for LLMs.'}"
mcro_Dreamshaper8inpainting-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'runwayml/stable-diffusion-inpainting'}
mcro_mistralaiMistral7BInstructv02-ModelDetail,mcro_ModelDetailSection,{'prov_hasTextValue': 'The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.\n\nMistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).'}
mcro_jinaaijinarerankerv2basemultilingual-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformer-based model'}
mcro_opusmtruen-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model can be used for translation and text-to-text generation.'}
mcro_vitbasepatch8224augreg2in21kftin1k-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_microsoftPhi35miniinstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_nomicainomicembedtextv1-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_Bertbasemultilingualcasednerhrl-Limitation,mcro_LimitationInformationSection,{}
mcro_BAAIbgesmallen-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Retrieval-augmented LLMs'}
mcro_bertbasechinese-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '[BERT](https://arxiv.org/abs/1810.04805)'}
mcro_microsoftphi2,mcro_Model,{}
mcro_microsoftdebertaxlargemnli-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'fine-tuned with mnli task'}
mcro_englishnerinflairfastmodel,mcro_Model,{}
mcro_jonatasgrosmanwav2vec2largexlsr53persian-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)'}
mcro_VisionTasksUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'captioning, object detection, and segmentation'}"
mcro_jonatasgrosmanwav2vec2largexlsr53arabic-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech recognition in Arabic'}
mcro_vocos-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Vocos is a fast neural vocoder designed to synthesize audio waveforms from acoustic features. Trained using a Generative\nAdversarial Network (GAN) objective, Vocos can generate waveforms in a single forward pass. Unlike other typical\nGAN-based vocoders, Vocos does not model audio samples in the time domain. Instead, it generates spectral\ncoefficients, facilitating rapid audio reconstruction through inverse Fourier transform.'}"
mcro_albertbasev2-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.""}"
mcro_bertwithflashattention,mcro_Model,{}
mcro_mixedbreadaimxbaiembedlargev1-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""Here, we provide several ways to produce sentence embeddings. Please note that you have to provide the prompt `Represent this sentence for searching relevant passages: ` for query if you want to use it for retrieval. Besides that you don't need any prompt.""}"
mcro_BEiTbasesizedmodelfinetunedonImageNet22k-CitationInformationSection1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-2106-08254,\n  author    = {Hangbo Bao and\n               Li Dong and\n               Furu Wei},\n  title     = {BEiT: {BERT} Pre-Training of Image Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2106.08254},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2106.08254},\n  archivePrefix = {arXiv},\n  eprint    = {2106.08254},\n  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_wav2vec2largexlsr53,mcro_Model,{}
mcro_Qwen2.5-VL-32B-Instruct,mcro_Model,{}
mcro_Qwen2.5-VL-32B-Instruct-UseCase,mcro_UseCaseInformationSection,{}
mcro_XTTS-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': '[Coqui Public Model License](https://coqui.ai/cpml)'}
mcro_UFNLPgatortronbase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien C, Compas C, Martin C, Costa AB, Flores MG, Zhang Y, Magoc T, Harle CA, Lipori G, Mitchell DA, Hogan WR, Shenkman EA, Bian J, Wu Y†. A large language model for electronic health records. Npj Digit Med. Nature Publishing Group; . 2022 Dec 26;5(1):1–9. https://www.nature.com/articles/s41746-022-00742-2; Yang X, Lyu T, Li Q, Lee C-Y, Bian J, Hogan WR, Wu Y†. A study of deep learning methods for de-identification of clinical notes in cross-institute settings. BMC Med Inform Decis Mak. 2020 Dec 5;19(5):232. https://www.ncbi.nlm.nih.gov/pubmed/31801524.'}"
mcro_UFNLPgatortronbase-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': '6.1B words from PubMed CC0; 82B words of de-identified clinical notes from the University of Florida Health System; 0.5B words of de-identified clinical notes from MIMIC-III; 2.5B words from WikiText'}
mcro_QwenQwen2515BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_XuhuiToxDectrobertalarge-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{zhou-etal-2020-debiasing,\n  title = {Challenges in Automated Debiasing for Toxic Language Detection},\n  author = {Zhou, Xuhui and Sap, Maarten and Swayamdipta, Swabha and Choi, Yejin and Smith, Noah A.},\n  booktitle = {EACL},\n  abbr = {EACL},\n  html = {https://www.aclweb.org/anthology/2021.eacl-main.274.pdf},\n  code = {https://github.com/XuhuiZhou/Toxic_Debias},\n  year = {2021},\n  bibtex_show = {true},\n  selected = {true}\n}'}"
mcro_ai4bharatindictrans2indicen1B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{gala2023indictrans,\ntitle={IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},\nauthor={Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},\njournal={Transactions on Machine Learning Research},\nissn={2835-8856},\nyear={2023},\nurl={https://openreview.net/forum?id=vfT4YuzAYA},\nnote={}\n}'}"
mcro_InternVL22B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{chen2024expanding,\n  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},\n  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},\n  journal={arXiv preprint arXiv:2412.05271},\n  year={2024}\n}\n@article{gao2024mini,\n  title={Mini-internvl: A flexible-transfer pocket multimodal model with 5\\% parameters and 90\\% performance},\n  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},\n  journal={arXiv preprint arXiv:2410.16261},\n  year={2024}\n}\n@article{chen2024far,\n  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},\n  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},\n  journal={arXiv preprint arXiv:2404.16821},\n  year={2024}\n}\n@inproceedings{chen2024internvl,\n  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},\n  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={24185--24198},\n  year={2024}\n}'}"
mcro_deepseekaiDeepSeekR1-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'reasoning'}
mcro_jonatasgrosmanwav2vec2xlsr1bportuguese-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Fine-tuned [facebook/wav2vec2-xls-r-1b](https://huggingface.co/facebook/wav2vec2-xls-r-1b) on Portuguese'}
mcro_bartlargesizedmodelfinetunedoncnndailymail-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.'}"
mcro_ViTSO400M14SigLIP384-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{big_vision,\n  author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},\n  title = {Big Vision},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/google-research/big_vision}}\n}'}"
mcro_efficientnet_b0.ra_in1k,mcro_Model,{}
mcro_Salesforcecodet5basemultisum-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{\n    wang2021codet5,\n    title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, \n    author={Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi},\n    booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021},\n    year={2021},\n}'}"
mcro_Systranfasterwhispersmall,mcro_Model,{}
mcro_segformerb0finetunedade20k-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.'}"
mcro_bartlargemnli-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_nguyenvulebinhwav2vec2basevi-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'We use wav2vec2 architecture for doing Self-Supervised learning'}
mcro_QwenQwen2505BInstruct-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinze He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_gemma2b-Dataset,mcro_DatasetInformationSection,{}
mcro_gemma2b-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_Qwen317B-Citation,mcro_CitationInformationSection,{}
mcro_opusmtruen-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-align'}
mcro_InternVL378B-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_jonatasgrosmanwav2vec2largexlsr53dutch-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)'}
mcro_YOLOWorldMirror-Documentation,obo_http_//purl.obolibrary.org/obo/IAO_0000314,{'prov_hasTextValue': 'https://docs.ultralytics.com/models/yolo-world/#available-models-supported-tasks-and-operating-modes'}
mcro_distilbertdistilbertbaseuncasedfinetunedsst2englis-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'topic classification'}
mcro_owlvitbasepatch32-UseCase,mcro_UseCaseInformationSection,{}
mcro_CLIPSegModel-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model is intended for zero-shot and one-shot image segmentation.'}
mcro_FacebookAIxlmrobertalarge-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': ""@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}""}"
mcro_CLIPViTbigG14LAION2B,mcro_Model,{}
mcro_CLIPViTbigG14LAION2B-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'As per the original [OpenAI CLIP model card](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/model-card.md), this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. \n\nThe OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset.'}"
mcro_whisper-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_rorshark-vit-base-UseCase,mcro_UseCaseInformationSection,{}
mcro_granitetimeseriesttmr2model-CitationInformation,mcro_CitationInformationSection,{}
mcro_petalsteamStableBeluga2-ModelDetail2,mcro_ModelDetailSection,"{'prov_hasTextValue': '* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.\n* **Language(s)**: English\n* **Library**: [HuggingFace Transformers](https://github.com/huggingface/transformers)\n* **License**: Fine-tuned checkpoints (`Stable Beluga 2`) is licensed under the [STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt)\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`'}"
mcro_trpakovvitfaceexpression-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Facial Expression/Emotion Recognition'}
mcro_vit_base_patch16_224.augreg2_in21k_ft_in1k-PretrainDataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-21k'}
mcro_t53b-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_obideidrobertai2b2-Training,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': '* Steps on how this model was trained can be found here: [Training](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/train). The ""model_name_or_path"" was set to: ""roberta-large"".'}"
mcro_bigsciencebloomz560m-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{muennighoff2022crosslingual,\n  title={Crosslingual generalization through multitask finetuning},\n  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},\n  journal={arXiv preprint arXiv:2211.01786},\n  year={2022}\n}'}"
mcro_deepseekaiDeepSeekR1-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'MoE; DeepSeek-V3'}
mcro_AlibabaNLPgtemultilingualbase-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'encoder-only transformers architecture'}
mcro_mobilevitsmall-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{vision-transformer,\ntitle = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\nauthor = {Sachin Mehta and Mohammad Rastegari},\nyear = {2022},\nURL = {https://arxiv.org/abs/2110.02178}\n}'}"
mcro_autogluonchronosboltbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683)'}
mcro_openaiwhisperlargev3turbo,mcro_Model,{}
mcro_yuvalkirstainPickScorev1-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Pick-a-Pic dataset'}
mcro_tabletransformerfinetunedfortabledetection-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-based object detection model'}
mcro_crossencodernlidebertav3base-PerformanceMetric1,mcro_PerformanceMetricInformationSection,{'prov_hasTextValue': '- Accuracy on SNLI-test dataset: 92.38'}
mcro_Qwen2VL2BInstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '* Multimodal Rotary Position Embedding (M-ROPE); * Naive Dynamic Resolution'}
mcro_intfloatmultilinguale5small-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model has 12 layers and the embedding size is 384.'}
mcro_QwenQwen25VL3BInstruct-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Qwen2VL,'}"
mcro_mimi-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CC-BY'}
mcro_Marqo-FashionSigLIP,mcro_Model,{}
mcro_wav2vec2-baseforemotionrecognition-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': ""Emotion Recognition (ER) predicts an emotion class for each utterance. The most widely used ER dataset\n[IEMOCAP](https://sail.usc.edu/iemocap/) is adopted, and we follow the conventional evaluation protocol: \nwe drop the unbalanced emotion classes to leave the final four classes with a similar amount of data points and \ncross-validate on five folds of the standard splits.\n\nFor the original model's training and evaluation instructions refer to the \n[S3PRL downstream task README](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#er-emotion-recognition).""}"
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit,mcro_Model,{}
mcro_bertwithflashattention-ModelParameterSection,mcro_ModelParameterSection,"{'prov_hasTextValue': '- `use_flash_attn`: If `True`, always use flash attention. If `None`, use flash attention when GPU is available. If `False`, never use flash attention (works on CPU).\n- `window_size`: Size (left and right) of the local attention window. If `(-1, -1)`, use global attention\n- `dense_seq_output`: If true, we only need to pass the hidden states for the masked out token (around 15%) to the classifier heads. I set this to true for pretraining.\n- `fused_mlp`: Whether to use fused-dense. Useful to reduce VRAM in combination with activation checkpointing\n- `mlp_checkpoint_lvl`: One of `{0, 1, 2}`. Increasing this increases the amount of activation checkpointing within the MLP. Keep this at 0 for pretraining and use gradient accumulation instead. For embedding training, increase this as much as needed.\n- `last_layer_subset`: If true, we only need the compute the last layer for a subset of tokens. I left this to false.\n- `use_qk_norm`: Whether or not to use QK-normalization\n- `num_loras`: Number of LoRAs to use when initializing a `BertLoRA` model. Has no effect on other models.'}"
mcro_llavamodelcard-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': '- 558K filtered image-text pairs from LAION/CC/SBU, captioned by BLIP.\n- 158K GPT-generated multimodal instruction-following data.\n- 450K academic-task-oriented VQA data mixture.\n- 40K ShareGPT data.'}"
mcro_crossencodermsmarcoTinyBERTL2v2-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_twitterrobertabasesentiment,mcro_Model,{}
mcro_bertbasemodelcased-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_bertmultilingualbasemodeluncased-License,mcro_LicenseInformationSection,{}
mcro_bertbaseNER-Limitation,mcro_LimitationInformationSection,{}
mcro_HuggingFaceM4idefics28b-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '`idefics2-8b-base` and `idefics2-8b` can be used to perform inference on multimodal (image + text) tasks in which the input is composed of a text query along with one (or multiple) image(s). Text and images can be arbitrarily interleaved. That includes image captioning, visual question answering, etc.'}"
mcro_whisperkit,mcro_Model,{}
mcro_MaziyarPanahigemma31bitGGUF,mcro_Model,{}
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-Disclaimer,obo_http_//purl.obolibrary.org/obo/IAO_0000314,"{'prov_hasTextValue': 'LM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model.  You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models.  LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.'}"
mcro_metallamaMetaLlama38B-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_CLIPViTH14LAION2B,mcro_Model,{}
mcro_CLIPViTH14LAION2B-TrainingDetails,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'This model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/).'}
mcro_pyannotewespeakervoxcelebresnet34LM-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': ""The pretrained model in WeNet follows the license of it's corresponding dataset. For example, the pretrained model on VoxCeleb follows Creative Commons Attribution 4.0 International License., since it is used as license of the VoxCeleb dataset, see https://mm.kaist.ac.kr/datasets/voxceleb/.""}"
mcro_t5small-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_mobilenetv3small100lambin1k,mcro_Model,{}
mcro_mobilenetv3small100lambin1k-UseCase3,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image Embeddings'}
mcro_huggingquantsMetaLlama318BInstructAWQINT4,mcro_Model,{}
mcro_mistralaiMistral7Bv01,mcro_Model,{}
mcro_Snowflakesnowflakearcticembedm-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance.'}
mcro_visiontransformerbase-sizedmodel-hybrid-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_deepseekaiDeepSeekR1DistillQwen14B,mcro_Model,{}
mcro_YOLOv8DetectionModel-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_allMiniLML6v2-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Our model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.'}"
mcro_arabertv1andv2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Arabic Language Understanding'}
mcro_OrengutengLlama38BLexiUncensored-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Lexi is uncensored, which makes the model compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones.\n\nYou are responsible for any content you create using this model. Please use it responsibly.'}"
mcro_bertbaseuncased-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).'}"
mcro_visiontransformerbase-sizedmodel-hybrid-ModelDetail,mcro_ModelDetailSection,{}
mcro_Inteldptlarge-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_sdxl10base-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': '- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to “A red cube on top of a blue sphere”\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.'}"
mcro_kokoro82M-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'can be deployed anywhere from production environments to personal projects'}
mcro_fnetbasemodel-ModelDetail,mcro_ModelDetailSection,{}
mcro_metaLlama3-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)'}
mcro_all-distilroberta-v1-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 128 word pieces is truncated.'}"
mcro_llama32Collection-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.'}
mcro_kluerobertabase,mcro_Model,{}
mcro_QwenQwen2505BInstruct-ModelDetail,mcro_ModelDetailSection,{}
mcro_chronosboltmini-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'This project is licensed under the Apache-2.0 License.'}
mcro_Salesforceblipimagecaptioningbase,mcro_Model,{}
mcro_gpt2-EvaluationResults,mcro_QuantativeAnalysisSection,{}
mcro_NusaBertnerv13,mcro_Model,{}
mcro_pyannotespeakerdiarization30-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'pyannote.audio; pipeline'}
mcro_QwenQwen2VL7BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': ""@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}""}"
mcro_NusaBertnerv13-ModelArchitectureInformation,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'ModernBERT architecture'}
mcro_DFN5BCLIPViTH14378,mcro_Model,{}
mcro_gemma2b-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_xclipbasesizedmodel-QuantativeAnalysisSection,mcro_QuantativeAnalysisSection,{'prov_hasTextValue': 'This model achieves a top-1 accuracy of 80.4% and a top-5 accuracy of 95.0%.'}
mcro_bertbasechinese-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{'prov_hasTextValue': '[More Information Needed]'}
mcro_chronosboltbase-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache-2.0 License'}
mcro_multilinguale5large-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}'}"
mcro_NLLB200-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': 'In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).; • In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).'}"
mcro_facebookencodec24khz-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Audio Codec'}
mcro_sentencetransformersdistilusebasemultilingualcasedv1-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_twitterxlmrobertabasesentiment-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_googleelectrasmalldiscriminator-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformer networks'}
mcro_stabilityaisdturbo-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Generative text-to-image model'}
mcro_MahmoudAshrafmms300m1130forcedaligner,mcro_Model,{}
mcro_MahmoudAshrafmms300m1130forcedaligner-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'conversion from torchaudio to HF Transformers for the MMS-300M checkpoint trained on forced alignment dataset'}
mcro_pyannotesegmentation30-Citation,mcro_CitationInformationSection,{}
mcro_bertbasechinese-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model can be used for masked language modeling'}
mcro_gemma2-ModelDetail,mcro_ModelDetailSection,{}
mcro_MaziyarPanahiPhi35miniinstructGGUF-License,mcro_LicenseInformationSection,{}
mcro_emrecanbertbaseturkishcasedmeannlistsbtr-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'tasks like clustering or semantic search'}
mcro_germanbert-ModelDetail,mcro_ModelDetailSection,{}
mcro_distilrobertabase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}'}"
mcro_Llama160m,mcro_Model,{}
mcro_Llama160m-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{miao2023specinfer,\n      title={SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification}, \n      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},\n      year={2023},\n      eprint={2305.09781},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_jhartmannemotionenglishdistilrobertabase,mcro_Model,{}
mcro_clipvitlargepatch14-TrainingDataInformation,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'publicly available image-caption data'}
mcro_bert-base-uncased-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'citation text'}
mcro_Salesforceblipimagecaptioninglarge-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'image captioning'}
mcro_vitbasepatch32clip448laion2bftin12kin1k-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Image classification / feature backbone'}
mcro_bartbase-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_llama32CollectionOfMultilingualLargeLanguageModelsLlms-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).'}"
mcro_faceparsing-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'non-commercial research and educational purposes'}
mcro_BAAIbgereRankerBase-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_distilbartcnn126-ModelParameterSection,mcro_ModelParameterSection,{'prov_hasTextValue': 'This checkpoint should be loaded into `BartForConditionalGeneration.from_pretrained`. See the [BART docs](https://huggingface.co/transformers/model_doc/bart.html?#transformers.BartForConditionalGeneration) for more information.'}
mcro_llavamodelcard,mcro_Model,{}
mcro_TahaDouajidetrdoctabledetection-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'More information needed'}
mcro_TahaDouajidetrdoctabledetection-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_microsoftwavlmbaseplussv-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'The model was pretrained on 16kHz sampled speech audio with utterance and speaker contrastive loss.'}
mcro_intfloatmultilinguale5small,mcro_Model,{}
mcro_intfloatmultilinguale5small-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'xP3'}
mcro_sentencetransformersstsbrobertabase,mcro_Model,{}
mcro_faceparsing-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Semantic segmentation model fine-tuned from nvidia/mit-b5 with CelebAMask-HQ for face parsing'}
mcro_opusmtenfr-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformer-align'}
mcro_deepseekaiDeepSeekR1-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_wav2vec2largexlsr53-UseCase,mcro_UseCaseInformationSection,{}
mcro_wav2vec2base-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli'}"
mcro_BAAIbgereRankerBase-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_Yehorw2vxlsruk-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc {\n\tsmoliakov_2025,\n\tauthor       = { {Smoliakov} },\n\ttitle        = { w2v-xls-r-uk (Revision 55b6dc0) },\n\tyear         = 2025,\n\turl          = { https://huggingface.co/Yehor/w2v-xls-r-uk },\n\tdoi          = { 10.57967/hf/4556 },\n\tpublisher    = { Hugging Face }\n}'}"
mcro_Qwen25Omni-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{Qwen2.5-Omni,\n  title={Qwen2.5-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},\n  journal={arXiv preprint arXiv:2503.20215},\n  year={2025}\n}'}"
mcro_microsoftPhi35miniinstruct-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The model is licensed under the [MIT license](./LICENSE).'}
mcro_ibmresearchMoLFormerXLboth10pct-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{https://doi.org/10.48550/arxiv.2106.09553,\n  doi = {10.48550/ARXIV.2106.09553},\n  url = {https://arxiv.org/abs/2106.09553},\n  author = {Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and Mroueh, Youssef and Das, Payel},\n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Biomolecules (q-bio.BM), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},\n  title = {Large-Scale Chemical Language Representations Capture Molecular Structure and Properties},\n  publisher = {arXiv},\n  year = {2021},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}'}"
mcro_clip-PrimaryIntendedUseCase,mcro_PrimaryIntendedUseCaseInformationSection,"{'prov_hasTextValue': 'The primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.'}"
mcro_ruri-small-v2-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_clipvitlargepatch14-Limitation,mcro_LimitationInformationSection,{}
mcro_efficientnet_b3.ra2_in1k-Citation1,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{tan2019efficientnet,\n  title={Efficientnet: Rethinking model scaling for convolutional neural networks},\n  author={Tan, Mingxing and Le, Quoc},\n  booktitle={International conference on machine learning},\n  pages={6105--6114},\n  year={2019},\n  organization={PMLR}\n}'}"
mcro_gpt2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'transformers model'}
mcro_albertbasev2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{DBLP:journals/corr/abs-1909-11942,\n  author    = {Zhenzhong Lan and\n               Mingda Chen and\n               Sebastian Goodman and\n               Kevin Gimpel and\n               Piyush Sharma and\n               Radu Soricut},\n  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language\n               Representations},\n  journal   = {CoRR},\n  volume    = {abs/1909.11942},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1909.11942},\n  archivePrefix = {arXiv},\n  eprint    = {1909.11942},\n  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}'}"
mcro_sentencetransformersparaphrasexlmrmultilingualv1-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_whisperlargev3,mcro_Model,{}
mcro_IP-Adapter-FaceID,mcro_Model,{}
mcro_phi4-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Our model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:\n\n1. Memory/compute constrained environments.\n2. Latency bound scenarios.\n3. Reasoning and logic.'}"
mcro_visiontransformerbasesizedmodel-CitationInformationSection1,mcro_CitationInformationSection,{'prov_hasTextValue': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'}
mcro_vitbasepatch32clip448laion2bftin12kin1k,mcro_Model,{}
mcro_BAAIbgebaseen-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_petalsteamStableBeluga2-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': ""Beluga is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Beluga's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model.""}"
mcro_wav2vec2xlsr300mcv7turkish-ModelDetail,mcro_ModelDetailSection,{}
mcro_detrresnet50-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The DETR model was trained on [COCO 2017 object detection](https://cocodataset.org/#download), a dataset consisting of 118k/5k annotated images for training/validation respectively.'}"
mcro_faceparsing-Consideration,mcro_ConsiderationInformationSection,"{'prov_hasTextValue': 'While the capabilities of computer vision models are impressive, they can also reinforce or exacerbate social biases. The [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ) dataset used for fine-tuning is large but not necessarily perfectly diverse or representative. Also, they are images of.... just celebrities.'}"
mcro_whisper-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': '680k hours of labelled data'}
mcro_twitterrobertabasesentiment-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': '_TweetEval_ (Findings of EMNLP 2020)'}
mcro_TinyLlama11BChatv10-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'UltraFeedback; UltraChat'}
mcro_bigvganv244khz128band512x-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Large-scale Compilation'}
mcro_ViTSO400M14SigLIP384-UseCase,mcro_UseCaseInformationSection,{}
mcro_opusmtruen-TrainingData,mcro_DatasetInformationSection,{'prov_hasTextValue': '* Dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)'}
mcro_QwenQwen25Coder7BInstructGPTQInt4-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias'}"
mcro_microsoftFlorence2large-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'FLD-5B dataset, containing 5.4 billion annotations across 126 million images'}"
mcro_segformerb1finetunedade20k-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you.'}
mcro_stabilityaistablediffusion35medium-OutofScopeUseCase,mcro_OutofScopeUseCaseSectionInformation,"{'prov_hasTextValue': 'The model was not trained to be factual or true representations of people or events. As such, using the model to generate such content is out-of-scope of the abilities of this model.'}"
mcro_mxbai-rerank-xsmall-v1-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_opusmtenfr,mcro_Model,{}
mcro_opusmtenfr-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'opus'}
mcro_yiyanghkustfinberttone-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'financial tone analysis task'}
mcro_gpt2medium-Consideration,mcro_ConsiderationInformationSection,{}
mcro_e5smallv2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}'}"
mcro_colbertv2-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': ""* [**ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT**](https://arxiv.org/abs/2004.12832) (SIGIR'20).\n* [**Relevance-guided Supervision for OpenQA with ColBERT**](https://arxiv.org/abs/2007.00814) (TACL'21).\n* [**Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval**](https://arxiv.org/abs/2101.00436) (NeurIPS'21).\n* [**ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction**](https://arxiv.org/abs/2112.01488) (NAACL'22).\n* [**PLAID: An Efficient Engine for Late Interaction Retrieval**](https://arxiv.org/abs/2205.09707) (CIKM'22).""}"
mcro_mbartlarge50manytomanymmt-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'mBART-50 many to many multilingual machine translation'}
mcro_convnextv2_nano.fcmae_ft_in22kin1k-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet-1k'}
mcro_jbetkerwav2vec2largerobustftlibrittsvoxpopuli-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody.'}"
mcro_deepseekaiDeepSeekR10528-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_TheBlokephi2GGUF,mcro_Model,{}
mcro_trlinternaltestingtinyLlamaForCausalLM32,mcro_Model,{}
mcro_IDEAResearchgroundingdinotiny-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{liu2023grounding,\n      title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}, \n      author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},\n      year={2023},\n      eprint={2303.05499},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_visiontransformerbase-sizedmodel-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes.'}"
mcro_sentencetransformersmultiqampnetbasedotv1-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Our model is intented to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.\n\nNote that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.'}
mcro_bertlargemodeluncased-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': ""You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.""}"
mcro_MERTv195M-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer Layer-Dimension'}
mcro_facebookencodec24khz-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{}
mcro_mistralaiMistral7Bv01-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Mistral-7B-v0.1 is a transformer model, with the following architecture choices:\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer'}"
mcro_Snowflakesnowflakearcticembedm,mcro_Model,{}
mcro_prithividaparrotparaphraseronT5-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_microsoftmdebertav3base,mcro_Model,{}
mcro_llavamodelcard-EvaluationData,mcro_EvaluationDataInformationSection,"{'prov_hasTextValue': 'A collection of 12 benchmarks, including 5 academic VQA benchmarks and 7 recent benchmarks specifically proposed for instruction-following LMMs.'}"
mcro_modelid-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_YOLOWorldMirror,mcro_Model,{}
mcro_Qwen253BInstruct-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Causal Language Models'}
mcro_bartbase,mcro_Model,{}
mcro_EleutherAIpythia70mdeduped-ModelDetail,mcro_ModelDetailSection,{}
mcro_vitmatte-model,mcro_Model,{}
mcro_huggingquantsMetaLlama318BInstructAWQINT4-ModelDetail,mcro_ModelDetailSection,{}
mcro_gpt2-ModelDetail,mcro_ModelDetailSection,{}
mcro_flan-t5-large-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_microsoftdebertav3large-Dataset,mcro_DatasetInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53arabic-Dataset-2,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Arabic Speech Corpus'}
mcro_testModel,mcro_Model,{}
mcro_openaiclip-UseCase,mcro_UseCaseInformationSection,{}
mcro_openaiclip-PrimaryIntendedUseCase,mcro_PrimaryIntendedUseCaseInformationSection,{}
mcro_efficientnet_b0.ra_in1k-Citation3,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}'}"
mcro_emrecanbertbaseturkishcasedmeannlistsbtr-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. The model was trained on Turkish machine translated versions of [NLI](https://huggingface.co/datasets/nli_tr) and [STS-b](https://huggingface.co/datasets/emrecan/stsb-mt-turkish) datasets, using example [training scripts]( https://github.com/UKPLab/sentence-transformers/tree/master/examples/training) from sentence-transformers GitHub repository.'}"
mcro_stabilityaistablediffusion35medium-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': 'While this model can handle long prompts, you may observe artifacts on the edge of generations when T5 tokens go over 256. Pay attention to the token limits when using this model in your workflow, and shortern prompts if artifacts becomes too obvious.\n- The medium model has a different training data distribution than the large model, so it may not respond to the same prompt similarly.\n- We recommend sampling with **Skip Layer Guidance** for better structure and anatomy coherency.'}"
mcro_deepseekaiDeepSeekR1DistillQwen14B-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT'}
mcro_sentencetransformersdistilusebasemultilingualcasedv2-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n)""}"
mcro_speechbrainemotionrecognitionwav2vec2IEMOCAP-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_Qwen1505BChat,mcro_Model,{}
mcro_gtelargeenv15-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'mGTE: Generalized Long-Context Text Representation and Reranking\nModels for Multilingual Text Retrieval'}
mcro_llamaGuard38B-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'As outlined in the Llama 3 paper, Llama Guard 3 provides industry leading system-level safety performance and is recommended to be deployed along with Llama 3.1. Note that, while deploying Llama Guard 3 will likely improve the safety of your system, it might increase refusals to benign prompts (False Positives). Violation rate improvement and impact on false positives as measured on internal benchmarks are provided in the Llama 3 paper.'}"
mcro_microsoftPhi3mini128kinstruct,mcro_Model,{}
mcro_SmolLM2-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_wav2vec2largerobustftlibri960h-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, Michael Auli'}"
mcro_answerdotaiModernBERTbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': '- Architecture: Encoder-only, Pre-Norm Transformer with GeGLU activations.\n- Sequence Length: Pre-trained up to 1,024 tokens, then extended to 8,192 tokens.\n- Data: 2 trillion tokens of English text and code.\n- Optimizer: StableAdamW with trapezoidal LR scheduling and 1-sqrt decay.\n- Hardware: Trained on 8x H100 GPUs.\n\nSee the paper for more details.'}"
mcro_conjunctsditre15-Citation,mcro_CitationInformationSection,{}
mcro_stablediffusionv15modelcard-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.'}
mcro_jonatasgrosmanwav2vec2largexlsr53japanese-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{grosman2021xlsr53-large-japanese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {J}apanese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese}},\n  year={2021}\n}'}"
mcro_upskyybgem3korean,mcro_Model,{}
mcro_TinyLlama11BChatv10-UseCase,mcro_UseCaseInformationSection,{}
mcro_IDEAResearchgroundingdinotiny-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for zero-shot object detection (the task of detecting things in an image out-of-the-box without labeled data).'}
mcro_owlv2-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{minderer2023scaling,\n      title={Scaling Open-Vocabulary Object Detection}, \n      author={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},\n      year={2023},\n      eprint={2306.09683},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-ModelDetail,mcro_ModelDetailSection,{}
mcro_Qwen1505BChat-ModelDetail,mcro_ModelDetailSection,"{'prov_hasTextValue': 'Qwen1.5 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes. For the beta version, temporarily we did not include GQA (except for 32B) and the mixture of SWA and full attention.'}"
mcro_roberta-baseforextractiveqa-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_modelid-Citation,mcro_CitationInformationSection,{}
mcro_moondream2-Version,mcro_VersionInformationSection,"{'prov_hasTextValue': ""This repository contains the latest (**2025-04-14**) release of Moondream, as well as [historical releases](https://huggingface.co/vikhyatk/moondream2/blob/main/versions.txt). The model is updated frequently, so we recommend specifying a revision as shown below if you're using it in a production application.""}"
mcro_efficientnet_b0.ra_in1k-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Image Classification; Image Embeddings; Feature Map Extraction'}
mcro_Dreamshaper8inpainting-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Stable Diffusion Inpainting model'}
mcro_intfloatmultilinguale5largeinstruct-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model has 24 layers and the embedding size is 1024.'}
mcro_hallucinationevaluationmodel-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc {hhem-2.1-open,\n\tauthor       = {Forrest Bao and Miaoran Li and Rogger Luo and Ofer Mendelevitch},\n\ttitle        = {{HHEM-2.1-Open}},\n\tyear         = 2024,\n\turl          = { https://huggingface.co/vectara/hallucination_evaluation_model },\n\tdoi          = { 10.57967/hf/3240 },\n\tpublisher    = { Hugging Face }\n}'}"
mcro_ibmgranitegranite318binstruct-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_intfloatmultilinguale5base-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model has 12 layers and the embedding size is 768.'}
mcro_allMiniLML6v2-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.'}
mcro_vit_base_patch16_224.augreg2_in21k_ft_in1k-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{steiner2021augreg,\n  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},\n  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},\n  journal={arXiv preprint arXiv:2106.10270},\n  year={2021}\n}; @article{dosovitskiy2020vit,\n  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n  journal={ICLR},\n  year={2021}\n}; @misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}'}"
mcro_Xlmrobertalargenerspanish-ModelDetail,mcro_ModelDetailSection,{}
mcro_openaiwhisperlargev3turbo-License,mcro_LicenseInformationSection,{}
mcro_xlmrobertabaselanguagedetection-TrainingData,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Language Identification dataset, which consists of text sequences in 20 languages'}"
mcro_graphcodebert-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512.'}"
mcro_codebertbase-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'The model is trained on bi-modal data (documents & code) of [CodeSearchNet](https://github.com/github/CodeSearchNet)'}
mcro_stabilityaistablediffusion35medium-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'MMDiT-X text-to-image generative model'}
mcro_Snowflakesnowflakearcticembedxs-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'snowflake-arctic-embed is a suite of text embedding models that focuses on creating high-quality retrieval models optimized for performance.'}
mcro_cambridgeltlSapBERTfromPubMedBERTfulltext-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'UMLS'}
mcro_HelsinkiNLPopusmtzhen-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Translation'}
mcro_detrresnet50,mcro_Model,{}
mcro_wav2vec2largerobustftlibri960h-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'transcribe audio files'}
mcro_CLIPSegModel,mcro_Model,{}
mcro_visiontransformerbase-sizedmodel-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer).'}
mcro_nlpconnectvitgpt2imagecaptioning,mcro_Model,{}
mcro_obideidrobertai2b2-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': '* A demo on how the model works (using model predictions to de-identify a medical note) is on this space:'}
mcro_mobilevitsmall-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apple sample code license'}
mcro_facebookmusicgenmedium-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'MusicGen consists of an EnCodec model for audio tokenization, an auto-regressive language model based on the transformer architecture for music modeling. The model comes in different sizes: 300M, 1.5B and 3.3B parameters ; and two variants: a model trained for text-to-music generation task and a model trained for melody-guided music generation.'}"
mcro_hubertsiuzdaksnac24khz,mcro_Model,{}
mcro_RobertaLargeMnli-TrainingData,mcro_DatasetInformationSection,{}
mcro_deepseekaiDeepSeekR1-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_whisper-Limitation,mcro_LimitationInformationSection,{}
mcro_Stablediffusionsafetychecker-BiasRiskLimitationConsideration,mcro_ConsiderationInformationSection,{}
mcro_StanfordAIMIstanforddeidentifierbase-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production.'}
mcro_Qwen2514BInstruct,mcro_Model,{}
mcro_vitbasepatch32clip448laion2bftin12kin1k-ModelDetail,mcro_ModelDetailSection,{}
mcro_YOLOv8DetectionModel-ConsiderationInformationSection,mcro_ConsiderationInformationSection,{}
mcro_sentencetransformersstsbxlmrmultilingual-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_Qwen3-4B-UseCase,mcro_UseCaseInformationSection,{}
mcro_myshellaiMeloTTS-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'This library is under MIT License, which means it is free for both commercial and non-commercial use.'}"
mcro_facebookdinov2large-ModelDetail,mcro_ModelDetailSection,{}
mcro_facebookdinov2large-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'feature extraction'}
mcro_parakeettdt06bv2-Performance,mcro_PerformanceMetricInformationSection,"{'prov_hasTextValue': '#### Huggingface Open-ASR-Leaderboard Performance\nThe performance of Automatic Speech Recognition (ASR) models is measured using Word Error Rate (WER). Given that this model is trained on a large and diverse dataset spanning multiple domains, it is generally more robust and accurate across various types of audio.\n\n### Base Performance\nThe table below summarizes the WER (%) using a Transducer decoder with greedy decoding (without an external language model):\n\n| **Model** | **Avg WER** | **AMI** | **Earnings-22** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI Speech** | **TEDLIUM-v3** | **VoxPopuli** |\n|:-------------|:-------------:|:---------:|:------------------:|:----------------:|:-----------------:|:-----------------:|:------------------:|:----------------:|:---------------:|\n| parakeet-tdt-0.6b-v2 | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | -\n\n### Noise Robustness\nPerformance across different Signal-to-Noise Ratios (SNR) using MUSAN music and noise samples:\n\n| **SNR Level** | **Avg WER** | **AMI** | **Earnings** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI** | **Tedlium** | **VoxPopuli** | **Relative Change** |\n|:---------------|:-------------:|:----------:|:------------:|:----------------:|:-----------------:|:-----------------:|:-----------:|:-------------:|:---------------:|:-----------------:|\n| Clean | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |\n| SNR 50 | 6.04 | 11.11 | 11.12 | 9.74 | 1.70 | 3.18 | 2.18 | 3.34 | 5.98 | +0.25% |\n| SNR 25 | 6.50 | 12.76 | 11.50 | 9.98 | 1.78 | 3.63 | 2.54 | 3.46 | 6.34 | -7.04% |\n| SNR 5 | 8.39 | 19.33 | 13.83 | 11.28 | 2.36 | 5.50 | 3.91 | 3.91 | 6.96 | -38.11% |\n\n### Telephony Audio Performance\nPerformance comparison between standard 16kHz audio and telephony-style audio (using μ-law encoding with 16kHz→8kHz→16kHz conversion):\n\n| **Audio Format** | **Avg WER** | **AMI** | **Earnings** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI** | **Tedlium** | **VoxPopuli** | **Relative Change** |\n|:-----------------|:-------------:|:----------:|:------------:|:----------------:|:-----------------:|:-----------------:|:-----------:|:-------------:|:---------------:|:-----------------:|\n| Standard 16kHz | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |\n| μ-law 8kHz | 6.32 | 11.98 | 11.16 | 10.02 | 1.78 | 3.52 | 2.20 | 3.38 | 6.52 | -4.10% |\n\nThese WER scores were obtained using greedy decoding without an external language model. Additional evaluation details are available on the [Hugging Face ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).[6]'}"
mcro_mobilebertuncased,mcro_Model,{}
mcro_gpt2medium-EvaluationData,mcro_DatasetInformationSection,{}
mcro_distilgpt2-TrainingData,mcro_DatasetInformationSection,{}
mcro_modelid,mcro_Model,{}
mcro_modelid-Evaluation,mcro_EvaluationDataInformationSection,{}
mcro_Salesforceblipimagecaptioningbase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}'}"
mcro_BAAIbgebaseen-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'The [masive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released'}
mcro_Systranfasterwhisperbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CTranslate2'}
mcro_Qwen3-4B,mcro_Model,{}
mcro_Qwen3-4B-ModelDetail,mcro_ModelDetailSection,{}
mcro_metaLlama32Collection-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.'}"
mcro_t5base-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_scb10xtyphoon21gemma312b-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'A 12B instruct decoder-only model based on Gemma3 architecture'}
mcro_mradermacherDeepSeekV2LiteGGUF,mcro_Model,{}
mcro_mradermacherDeepSeekV2LiteGGUF-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_ProtGPT2-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'UniRef50 (version 2021_04)'}
mcro_AlibabaNLPgteQwen27Binstruct-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. This training leverages both weakly supervised and supervised data, ensuring the model's applicability across numerous languages and a wide array of downstream tasks.; Integration of bidirectional attention mechanisms, enriching its contextual understanding.; Instruction tuning, applied solely on the query side for streamlined efficiency""}"
mcro_facebookwav2vec2base960h-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_nvidiatitanetlargeenus-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'All the models in this collection are trained on a composite dataset comprising several thousand hours of English speech:\n\n- Voxceleb-1\n- Voxceleb-2\n- Fisher\n- Switchboard\n- Librispeech\n- SRE (2004-2010)'}
mcro_codebertbase-Reference,mcro_ReferenceInformationSection,"{'prov_hasTextValue': 'Please see [the official repository](https://github.com/microsoft/CodeBERT) for scripts that support ""code search"" and ""code-to-document generation"".'}"
mcro_distilbertbasemultilingualcased-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformer-based language model'}
mcro_t5base-Reference,mcro_ReferenceInformationSection,"{'prov_hasTextValue': 'Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.'}"
mcro_ProtGPT2-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'ProtGPT2 is based on the GPT2 Transformer architecture and contains 36 layers with a model dimensionality of 1280, totalling 738 million parameters.'}"
mcro_albertbasev2-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The ALBERT model was pretrained on BookCorpus, a dataset consisting of 11,038\nunpublished books and English Wikipedia (excluding lists, tables and\nheaders).'}"
mcro_llavahfllavav16mistral7bhf-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'You can use the raw model for tasks like image captioning, visual question answering, multimodal chatbot use cases. See the [model hub](https://huggingface.co/models?search=llava-hf) to look for\nother versions on a task that interests you.'}"
mcro_fnetbasemodel-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The FNet model was pretrained on [C4](https://huggingface.co/datasets/c4), a cleaned version of the Common Crawl dataset.'}"
mcro_BAAIbgem3-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_sentencetransformersmsmarcodistilbertbasev4-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}
mcro_sentencetransformersmultiqampnetbasedotv1-DatasetInformationSection,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'It has been trained on 215M (question, answer) pairs from diverse sources.'}"
mcro_stablediffusionv2-1-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The model is intended for research purposes only.'}
mcro_granitetimeseriesttmr1-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_facebookmusicgenmedium-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Code is released under MIT, model weights are released under CC-BY-NC 4.0.'}"
mcro_FacebookAIxlmrobertabase-Citation,mcro_CitationInformationSection,{}
mcro_chronosboltmini-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Chronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting.'}
mcro_BridgeTowerbridgetowerlargeitmmlmitc-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance on various downstream vision-language tasks. In particular, on the VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs. Notably, when further scaling the model, BridgeTower achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets.'}"
mcro_legalbertTheMuppetsStraightOutOfLawSchool-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""* We trained BERT using the official code provided in Google BERT's GitHub repository (https://github.com/google-research/bert).\n* We released a model similar to the English BERT-BASE model (12-layer, 768-hidden, 12-heads, 110M parameters).\n* We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4.\n* We were able to use a single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc), while also utilizing [GCP research credits](https://edu.google.com/programs/credits/research). Huge thanks to both Google programs for supporting us!\n* Part of LEGAL-BERT is a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.""}"
mcro_gpt2-Limitation,mcro_LimitationInformationSection,{}
mcro_vit-age-classifier,mcro_Model,{}
mcro_e5largev2,mcro_Model,{}
mcro_e5largev2-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\n\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef average_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n\n# Each input text should start with ""query: "" or ""passage: "".\n# For tasks other than retrieval, you can simply use the ""query: "" prefix.\ninput_texts = [\'query: how much protein should a female eat\',\n               \'query: summit define\',\n               ""passage: As a general guideline, the CDC\'s average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you\'ll need to increase that if you\'re expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day."",\n               ""passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.""]\n\ntokenizer = AutoTokenizer.from_pretrained(\'intfloat/e5-large-v2\')\nmodel = AutoModel.from_pretrained(\'intfloat/e5-large-v2\')\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors=\'pt\')\n\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict[\'attention_mask\'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())'}"
mcro_llama4Scout17B16EInstruct-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.'}"
mcro_QwenQwen2515BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_stablediffusionv15modelcard-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'The model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)'}
mcro_sdxlvae-EvaluationInformationSection,mcro_QuantativeAnalysisSection,{}
mcro_banglat5_banglaparaphrase-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_T5basesummarizationclaimextractor-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'T5'}
mcro_lengyue233contentvecbest,mcro_Model,{}
mcro_indonesianrobertabaseposptagger-ModelDetail,mcro_ModelDetailSection,{}
mcro_microsoftPhi4multimodalinstruct-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The model is licensed under the MIT license.'}
mcro_googleflant5small,mcro_Model,{}
mcro_legalbertTheMuppetsStraightOutOfLawSchool-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available.'}"
mcro_sentencetransformersparaphraseMiniLML6v2-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{reimers-2019-sentence-bert,\n    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",\n    author = ""Reimers, Nils and Gurevych, Iryna"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",\n    month = ""11"",\n    year = ""2019"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""http://arxiv.org/abs/1908.10084"",\n}'}"
mcro_clip-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.'}"
mcro_Qwen2514BInstruct-UseCase,mcro_UseCaseInformationSection,{}
mcro_jbetkerwav2vec2largerobustftlibrittsvoxpopuli,mcro_Model,{}
mcro_TheBlokephi2GGUF-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'The model is licensed under the [microsoft-research-license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE).'}
mcro_bertbasemodelcased-Citation,mcro_CitationInformationSection,{}
mcro_T5basesummarizationclaimextractor-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{scire-etal-2024-fenice,\n    title = ""{FENICE}: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction"",\n    author = ""Scir{\\`e}, Alessandro and Ghonim, Karim and Navigli, Roberto"",\n    editor = ""Ku, Lun-Wei  and Martins, Andre and Srikumar, Vivek"",\n    booktitle = ""Findings of the Association for Computational Linguistics ACL 2024"",\n    month = aug,\n    year = ""2024"",\n    address = ""Bangkok, Thailand and virtual meeting"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://aclanthology.org/2024.findings-acl.841"",\n    pages = ""14148--14161"",\n}'}"
mcro_llama2-TrainingData,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.'}"
mcro_crossencodermsmarcoMiniLML4v2-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)'}"
mcro_cambridgeltlSapBERTfromPubMedBERTfulltext-Architecture,mcro_ModelArchitectureInformationSection,{}
mcro_wavlmbaseplus-Citation,mcro_CitationInformationSection,{}
mcro_metaLlama31Instruct-IntendedUseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases.'}"
mcro_microsoftbeitlargepatch16224-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'image classification'}
mcro_owlv2-Citation,mcro_CitationInformationSection,{}
mcro_efficientnet_b3.ra2_in1k-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}'}"
mcro_dima806fairfaceageimagedetection-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Detects age group with about 59% accuracy based on an image.'}
mcro_efficientnet_b0.ra_in1k-Citation2,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}'}"
mcro_QwenQwen2505BInstruct-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_rorshark-vit-base-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_stablediffusionv2-1-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\""orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }'}"
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-UseCase,mcro_UseCaseInformationSection,{}
mcro_spa-eng,mcro_Model,{}
mcro_Qwen257BInstruct1M-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Causal Language Models'}
mcro_jonatasgrosmanwav2vec2largexlsr53dutch-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'speech recognition in Dutch'}
mcro_vitmatte-model-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{yao2023vitmatte,\n      title={ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers}, \n      author={Jingfeng Yao and Xinggang Wang and Shusheng Yang and Baoyuan Wang},\n      year={2023},\n      eprint={2305.15272},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}'}"
mcro_albertbasev2,mcro_Model,{}
mcro_QwenQwen2515BInstruct,mcro_Model,{}
mcro_whisper-QuantativeAnalysisSection,mcro_QuantativeAnalysisSection,{}
mcro_TheBlokeMistral7BInstructv01GGUF-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'This instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer'}"
mcro_multilinguale5large-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This model has 24 layers and the embedding size is 1024.'}
mcro_sentencetransformersbertbasenlimeantokens-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_siglip2so400mpatch16naflex-TrainingDataInformationSection,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'SigLIP 2 is pre-trained on the WebLI dataset [(Chen et al., 2023)](https://arxiv.org/abs/2209.06794).'}"
mcro_playgroundaiplaygroundv251024pxaesthetic-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'generates highly aesthetic images of resolution 1024x1024, as well as portrait and landscape aspect ratios'}"
mcro_alakxendermmsttsdivfinetunedmdm01,mcro_Model,{}
mcro_metallamaLlama31-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)'}"
mcro_t5large-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_sentencetransformersparaphrasemultilingualmpnetbasev2-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_GIT,mcro_Model,{}
mcro_GIT-ModelDetail,mcro_ModelDetailSection,{}
mcro_SmolLM2-Training,mcro_TrainingDataInformationSection,{}
mcro_BAAIbgereRankerBase-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_sentencetransformersparaphraseMiniLML6v2,mcro_Model,{}
mcro_microsoftphi2-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': '* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n\n* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as ""typing, math, random, collections, datetime, itertools"". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n\n* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n\n* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring training data safety. There\'s a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n\n* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n\n* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.'}"
mcro_googleelectrabasediscriminator-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/)).'}"
mcro_NusaBertnerv13-IntendedUseCase,mcro_UseCaseInformationSection,{}
mcro_NLLB200-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '- Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022; Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022'}"
mcro_JackFramllama68m,mcro_Model,{}
mcro_AlibabaNLPgtebaseenv15-Consideration,mcro_ConsiderationInformationSection,{}
mcro_example_model-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'CNN'}
mcro_openaiclip-Limitation,mcro_LimitationInformationSection,{}
mcro_openaiclip-OutOfScopeUseCase,mcro_OutOfScopeUseCaseSectionInformation,{}
mcro_jonatasgrosmanwav2vec2largexlsr53hungarian-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_vitlargepatch14reg4dinov2lvd142m-UseCase,mcro_UseCaseInformationSection,{}
mcro_CLIPViTH14LAION2B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}\n\n@inproceedings{Radford2021LearningTV,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n  booktitle={ICML},\n  year={2021}\n}\n\n@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}'}"
mcro_allmpnetbasev2,mcro_Model,{}
mcro_allmpnetbasev2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.'}"
mcro_stabilityaistablediffusion35medium,mcro_Model,{}
mcro_e5smallv2-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': 'This model only works for English texts. Long texts will be truncated to at most 512 tokens.'}
mcro_twitterxlmrobertabasesentiment-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_parakeettdt06bv2-EvaluationDataset,mcro_EvaluationDataInformationSection,"{'prov_hasTextValue': 'Huggingface Open ASR Leaderboard datasets are used to evaluate the performance of this model.\n\n**Data Collection Method by dataset**\n* Human\n\n**Labeling Method by dataset**\n* Human\n\n**Properties:**\n\n* All are commonly used for benchmarking English ASR systems.\n* Audio data is typically processed into a 16kHz mono channel format for ASR evaluation, consistent with benchmarks like the [Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).'}"
mcro_microsoftPhi35miniinstruct-Dataset,mcro_DatasetInformationSection,{}
mcro_vikplayoutsegmenter-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Segments pdf page layout into blocks.'}
mcro_SmolLM2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{allal2025smollm2smolgoesbig,\n      title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model}, \n      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martín Blázquez and Guilherme Penedo and Lewis Tunstall and Andrés Marafioti and Hynek Kydlíček and Agustín Piqueres Lajarín and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Clémentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},\n      year={2025},\n      eprint={2502.02737},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.02737}, \n}'}"
mcro_NLLB200-Arch,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': '- Information about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper.'}"
mcro_deepseekaiDeepSeekR1DistillLlama8B-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_t5large-QuantativeAnalysis,mcro_QuantativeAnalysisSection,{}
mcro_blackforestlabsFLUX1dev-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': '`FLUX.1 [dev]` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.'}
mcro_mistralaiMistral7BInstructv03-Limitation,mcro_LimitationInformationSection,"{'prov_hasTextValue': ""The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.""}"
mcro_keremberkeyolov5nlicenseplate-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_sdxl10refinermodel-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CreativeML Open RAIL++-M License'}
mcro_audeeringwav2vec2largerobust12ftemotionmspdim-DatasetInformationSection,mcro_DatasetInformationSection,{'prov_hasTextValue': 'MSP-Podcast'}
mcro_llavahfllavaonevisionqwen205bovhf-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'SO400M + Qwen2'}
mcro_AudioSpectrogramTransformerfinetunedonAudioSet-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use the raw model for classifying audio into one of the AudioSet classes.; You can use the raw model for classifying audio into one of the AudioSet classes. See the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification.forward.example) for more info.'}
mcro_sentencetransformersstsbrobertabase-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': True}) with Transformer model: RobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_mxbai-rerank-xsmall-v1-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_cointegratedruberttiny2,mcro_Model,{}
mcro_petalsteamStableBeluga2-ModelParameter,mcro_ModelParameterSection,"{'prov_hasTextValue': 'Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:\n\n| Dataset           | Batch Size | Learning Rate |Learning Rate Decay| Warm-up | Weight Decay | Betas       |\n|-------------------|------------|---------------|-------------------|---------|--------------|-------------|\n| Orca pt1 packed   | 256        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n| Orca pt2 unpacked | 512        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |'}"
mcro_GIT-TrainingData,mcro_DatasetInformationSection,{'prov_hasTextValue': '10 million image-text pairs'}
mcro_multilinguale5large-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'This model is initialized from [xlm-roberta-large](https://huggingface.co/xlm-roberta-large)\nand continually trained on a mixture of multilingual datasets.\nIt supports 100 languages from xlm-roberta,\nbut low-resource languages may see performance degradation.'}"
mcro_BAAIbgesmallen-LicenseInformationSection,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT License'}
mcro_QwenQwen2515BInstruct-UseCase,mcro_UseCaseInformationSection,{}
mcro_edgenextsmallusiin1k,mcro_Model,{}
mcro_xclipbasesizedmodel-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'It was introduced in the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Ni et al.'}
mcro_Qwen2515BInstruct-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings'}"
mcro_distilbertbasemodeluncased-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.'}"
mcro_sdxlinpainting01-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CreativeML Open RAIL++-M License'}
mcro_petalsteamStableBeluga2-Citation3,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4},\n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_sentencetransformersstsbxlmrmultilingual-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clustering or semantic search'}
mcro_clipvitlargepatch14-EvaluationDataInformation,mcro_EvaluationDataInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53polish-Citation,mcro_CitationInformationSection,{}
mcro_Qwen2514BInstruct-Citation,mcro_CitationInformationSection,{}
mcro_gpt2medium-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Modified MIT License'}
mcro_Falconsainsfwimagedetection-TrainingData,mcro_TrainingDataInformationSection,"{'prov_hasTextValue': 'The model\'s training data includes a proprietary dataset comprising approximately 80,000 images. This dataset encompasses a significant amount of variability and consists of two distinct classes: ""normal"" and ""nsfw."" The training process on this data aimed to equip the model with the ability to distinguish between safe and explicit content effectively.'}"
mcro_llavahfllavav16mistral7bhf-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'LLaVa-NeXT, leveraging [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) as LLM'}"
mcro_pyannotewespeakervoxcelebresnet34LM-UseCase,mcro_UseCaseInformationSection,{}
mcro_googleflant5small-UseCase,mcro_UseCaseInformationSection,{}
mcro_conjunctsditre15-TechnicalSpecification,obo_http_//purl.obolibrary.org/obo/IAO_0000314,{}
mcro_deepseekaiDeepSeekR1DistillQwen14B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}'}"
mcro_SWividF5TTS-Citation,mcro_CitationInformationSection,{'prov_hasTextValue': 'F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching'}
mcro_nomicainomicembedtextv15-UseCase,mcro_UseCaseInformationSection,{}
mcro_distilwhisperdistillargev3-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'MIT license'}
mcro_facebookwav2vec2base960h,mcro_Model,{}
mcro_faceparsing,mcro_Model,{}
mcro_granitetimeseriesttmr2model-UseCaseInformation,mcro_UseCaseInformationSection,{}
mcro_whisperlargev3-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'The models are primarily trained and evaluated on ASR and speech translation to English tasks.'}
mcro_nvidiatitanetlargeenus-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'TitaNet model is a depth-wise separable conv1D model [1] for Speaker Verification and diarization tasks.'}
mcro_Protbertmodel-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_wav2vec2xlsr300mhebrew-TrainingDataInformationSection,mcro_DatasetInformationSection,{}
mcro_facebookdinov2large,mcro_Model,{}
mcro_sentencetransformersparaphraseMiniLML6v2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_lmstudiocommunityDeepSeekR10528Qwen38BMLX8bit-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_FacebookAIxlmrobertalarge-ModelDetail,mcro_ModelDetailSection,{}
mcro_TinyLlama11BChatv10-ModelDetail,mcro_ModelDetailSection,{}
mcro_wav2vec2largerobustftlibri960h-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Wav2Vec2'}
mcro_petalsteamStableBeluga2-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{StableBelugaModels,\n      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)},\n      title={Stable Beluga models},\n      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}\n}'}"
mcro_metallamaLlama3370BInstruct-OutOfScopeUseCase,mcro_OutOfScopeUseCaseSectionInformation,{'prov_hasTextValue': 'Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card'}
mcro_speechbrainemotionrecognitionwav2vec2IEMOCAP-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_stabilityaistablediffusion35medium-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Free for research, non-commercial, and commercial use for organizations or individuals with less than $1M in total annual revenue. More details can be found in the Community License Agreement. Read more at https://stability.ai/license.\n- For individuals and organizations with annual revenue above $1M: please contact us to get an Enterprise License.'}"
mcro_deepseekaiDeepSeekR1DistillQwen7B-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}'}"
mcro_bertbaseNER-Citation,mcro_CitationInformationSection,{}
mcro_sentencetransformersstsbrobertabase-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.'}
mcro_XuhuiToxDectrobertalarge-IntendedUseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'toxic language detection'}
mcro_speechbrainemotionrecognitionwav2vec2IEMOCAP-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'This system is composed of an wav2vec2 model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss. Speaker Verification is performed using cosine distance between speaker embeddings.'}
mcro_germanbert-License,mcro_LicenseInformationSection,{}
mcro_jinaaijinaembeddingsv3-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': '`jina-embeddings-v3` is a **multilingual multi-task text embedding model** designed for a variety of NLP applications.\nBased on the [Jina-XLM-RoBERTa architecture](https://huggingface.co/jinaai/xlm-roberta-flash-implementation), \nthis model supports Rotary Position Embeddings to handle long input sequences up to **8192 tokens**.\nAdditionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently.'}"
mcro_distilrobertabase-Consideration,mcro_ConsiderationInformationSection,{}
mcro_deberta-v3-base-prompt-injection-v2-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'deberta-v3-base'}
mcro_openaiclip-ModelDetail,mcro_ModelDetailSection,{}
mcro_Qwen253BInstruct-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_t5large-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_sdxl10base-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'CreativeML Open RAIL++-M License'}
mcro_yiyanghkustfinberttone-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': 'Huang, Allen H., Hui Wang, and Yi Yang. ""FinBERT: A Large Language Model for Extracting Information from Financial Text."" *Contemporary Accounting Research* (2022).'}"
mcro_Qwen2532BInstruct-License,mcro_LicenseInformationSection,{}
mcro_microsoftPhi3mini128kinstruct-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Phi-3 Mini-128K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.'}
mcro_bigsciencebloomz560m-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""*Translate to English: Je t’aime.*"", the model will most likely answer ""*I love you.*"".'}"
mcro_crossencodermsmarcoTinyBERTL2v2-ModelParameter,mcro_ModelParameterSection,{}
mcro_sentencetransformersmsmarcodistilbertbasev4-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)""}"
mcro_facebookopt125m-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{zhang2022opt,\n      title={OPT: Open Pre-trained Transformer Language Models}, \n      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},\n      year={2022},\n      eprint={2205.01068},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_clip-vit-large-patch14-336-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'More information needed'}
mcro_tsmatzxlmrobertanerjapanese-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'RobertaModel'}
mcro_flan-t5-large-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_deepseekaiDeepSeekR1DistillLlama8B-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'Qwen2.5 and Llama3'}
mcro_Auralis-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache 2.0'}
mcro_jinaaijinaembeddingsv3-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Jina-XLM-RoBERTa architecture'}
mcro_CryptoBERT-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': ""CryptoBERT is a pre-trained NLP model to analyse the language and sentiments of cryptocurrency-related social media posts and messages. It was built by further training the [vinai's bertweet-base](https://huggingface.co/vinai/bertweet-base) language model on the cryptocurrency domain, using a corpus of over 3.2M unique cryptocurrency-related social media posts. (A research paper with more details will follow soon.)""}"
mcro_gemma2b-UseCase,mcro_UseCaseInformationSection,{}
mcro_blackforestlabsFLUX1Filldev-OutofScopeUseCase,mcro_OutofScopeUseCaseSectionInformation,{}
mcro_autogluonchronosboltbase-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'This project is licensed under the Apache-2.0 License.'}
mcro_visiontransformerbase-sizedmodel-hybrid-Citation,mcro_CitationInformationSection,{}
mcro_HelsinkiNLPopusmtzhen-EvaluationData,mcro_EvaluationDataInformationSection,{}
mcro_mlxcommunitygemma312bitqat4bit-License,mcro_LicenseInformationSection,{}
mcro_MERTv195M-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{li2023mert,\n      title={MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}, \n      author={Yizhi Li and Ruibin Yuan and Ge Zhang and Yinghao Ma and Xingran Chen and Hanzhi Yin and Chenghua Lin and Anton Ragni and Emmanouil Benetos and Norbert Gyenge and Roger Dannenberg and Ruibo Liu and Wenhu Chen and Gus Xia and Yemin Shi and Wenhao Huang and Yike Guo and Jie Fu},\n      year={2023},\n      eprint={2306.00107},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD}\n}'}"
mcro_bertbasemultilingualuncasedsentiment,mcro_Model,{}
mcro_opusmtruen-EvaluationData,mcro_DatasetInformationSection,{}
mcro_YOLOWorldMirror-ModelDetail,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'model weights for ultralytics yolo models'}
mcro_HuggingFaceM4idefics28b-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Multi-modal model (image+text)'}
mcro_microsoftdebertav3large-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_Qwen3-14B-UseCase,mcro_UseCaseInformationSection,{}
mcro_WhisperbaseenmodelforCTranslate2-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_Salesforceblipimagecaptioningbase-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'You can use this model for conditional and un-conditional image captioning'}
mcro_example_model-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'ImageNet'}
mcro_ruri-small-v2-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_bertlargemodeluncased-Citation,mcro_CitationInformationSection,{}
mcro_ChatGLM2-6B-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'Apache-2.0'}
mcro_Stablediffusionsafetychecker-EnvironmentalImpact,mcro_ConsiderationInformationSection,{}
mcro_lftwr4target,mcro_Model,{}
mcro_debertav3basetasksourcenli-UseCase,mcro_UseCaseInformationSection,{}
mcro_trlinternaltestingtinyT5ForConditionalGeneration,mcro_Model,{}
mcro_detoxify-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. https://github.com/unitaryai/detoxify},\n  year={2020}\n}'}"
mcro_llama32CollectionOfMultilingualLlLMs-License,mcro_LicenseInformationSection,"{'prov_hasTextValue': 'Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).'}"
mcro_openaiwhisperlargev3turbo-UseCase,mcro_UseCaseInformationSection,{}
mcro_FinetunedT5SmallTextSummarization-ModelDetail,mcro_ModelDetailSection,{}
mcro_resnet50.a1_in1k-UseCase,mcro_UseCaseInformationSection,{}
mcro_bertlargemodeluncased-Arch,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion.'}
mcro_nvidiatitanetlargeenus-UseCaseInformationSection,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'This model extracts speaker embeddings from given speech, which is the backbone for speaker verification and diarization tasks.'}"
mcro_MoritzLaurerDeBERTa-v3-base-mnli-fever-anli-Architecture,mcro_ModelArchitectureInformationSection,{}
mcro_tabletransformerfinetunedfortabledetection-CitationInformationSection,mcro_CitationInformationSection,{'prov_hasTextValue': 'PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents'}
mcro_mradermacherDeepSeekV2LiteGGUF-License,mcro_LicenseInformationSection,{}
mcro_Qwen257BInstruct-CitationInformationSection,mcro_CitationInformationSection,{}
mcro_gemma2b-License,mcro_LicenseInformationSection,{}
mcro_crossencodermsmarcoMiniLML12v2-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.'}
mcro_BilateralReferenceForHighResolutionDichotomousImageSegmentation-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{}
mcro_answerdotaiModernBERTbase,mcro_Model,{}
mcro_vitlargepatch14reg4dinov2lvd142m-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': 'LVD-142M'}
mcro_Qwen2.5-1.5B-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}'}"
mcro_nomicainomicembedtextv1-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{}
mcro_AlibabaNLPgtebaseenv15-License,mcro_LicenseInformationSection,{}
mcro_clipvitlargepatch14-License,mcro_LicenseInformationSection,{}
mcro_facebookmask2formerswintinycocoinstance-Citation,mcro_CitationInformationSection,{}
mcro_multilinguale5large-Limitation,mcro_LimitationInformationSection,{'prov_hasTextValue': 'Long texts will be truncated to at most 512 tokens.'}
mcro_gte-large-Citation,mcro_CitationInformationSection,{}
mcro_gemma3model-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.'}"
mcro_Marqo-FashionSigLIP-UseCase,mcro_UseCaseInformationSection,{}
mcro_chronosboltbase-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'T5 encoder-decoder architecture'}
mcro_facebookwav2vec2base960h-UseCase,mcro_UseCaseInformationSection,{}
mcro_layoutlmv3-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{huang2022layoutlmv3,\n  author={Yupan Huang and Tengchao Lv and Lei Cui and Yutong Lu and Furu Wei},\n  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},\n  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},\n  year={2022}\n}'}"
mcro_bertmultilingualbasemodelcased-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_wav2vec2xlsr300mcv7turkish-License,mcro_LicenseInformationSection,{}
mcro_jonatasgrosmanwav2vec2xlsr1bportuguese-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{grosman2021xlsr-1b-portuguese,\n  title={Fine-tuned {XLS-R} 1{B} model for speech recognition in {P}ortuguese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-xls-r-1b-portuguese}},\n  year={2022}\n}'}"
mcro_BAAIbgelargeen,mcro_Model,{}
mcro_wav2vec2largexlsr53-Architecture,mcro_ModelArchitectureInformationSection,{}
mcro_facebookmmslid256-Citation,mcro_CitationInformationSection,{}
mcro_ModelCardReport-LicenseInformationSection,mcro_LicenseInformationSection,{}
mcro_MyAwesomeModel-License,mcro_LicenseInformationSection,{'prov_hasTextValue': 'mit'}
mcro_owlv2-Architecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'The model uses a CLIP backbone with a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective.'}"
mcro_CLIPViTbigG14LAION2B-Evaluation,mcro_QuantativeAnalysisSection,{}
mcro_conjunctsditre15-Consideration,mcro_ConsiderationInformationSection,{}
mcro_TheBlokephi2GGUF-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': ""Microsoft's Phi 2""}"
mcro_cointegratedruberttiny2-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'a small Russian BERT-based encoder with high-quality sentence embeddings'}
mcro_pyannotesegmentation30-Architecture,mcro_ModelArchitectureInformationSection,{}
mcro_crossencodernlidebertav3base-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'The model was trained on the [SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.'}"
mcro_whisper-EthicalConsideration,mcro_EthicalConsiderationSection,{'prov_hasTextValue': 'dual use concerns'}
mcro_petalsteamStableBeluga2-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': '` Stable Beluga 2` is trained on our internal Orca-style dataset'}
mcro_flairnerenglishlarge-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}'}"
mcro_Stablediffusionsafetychecker-TrainingDetail,mcro_ModelParameterSection,{}
mcro_twitterrobertabasesentiment-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'sentiment analysis'}
mcro_upskyybgem3korean-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_distilbertdistilbertbaseuncasedfinetunedsst2englis-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Text Classification'}
mcro_AlibabaNLPgtemultilingualbase-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@inproceedings{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\n  pages={1393--1412},\n  year={2024}\n}'}"
mcro_englishnerinflairfastmodel-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Flair embeddings and LSTM-CRF'}
mcro_modelid-License,mcro_LicenseInformationSection,{}
mcro_Stablediffusionsafetychecker-License,mcro_LicenseInformationSection,{}
mcro_segformerb2finetunedforclothessegmentation-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'clothes segmentation; human segmentation'}
mcro_dima806fairfaceageimagedetection,mcro_Model,{}
mcro_flan-t5-large-Citation,mcro_CitationInformationSection,{}
mcro_parakeet_rnnt_06b-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': ""The model was trained on 64K hours of English speech collected and prepared by NVIDIA NeMo and Suno teams.\n\nThe training dataset consists of private subset with 40K hours of English speech plus 24K hours from the following public datasets:\n\n- Librispeech 960 hours of English speech\n- Fisher Corpus\n- Switchboard-1 Dataset\n- WSJ-0 and WSJ-1\n- National Speech Corpus (Part 1, Part 6)\n- VCTK\n- VoxPopuli (EN)\n- Europarl-ASR (EN)\n- Multilingual Librispeech (MLS EN) - 2,000 hour subset\n- Mozilla Common Voice (v7.0)\n- People's Speech  - 12,000 hour subset""}"
mcro_SmolLM2-Dataset,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'FineWeb-Edu, DCLM, The Stack'}"
mcro_BAAIbgererankerlarge,mcro_Model,{}
mcro_openaiclip-EthicalConsideration,mcro_EthicalConsiderationSection,{}
mcro_detoxify-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'research purposes'}
mcro_facebookmask2formerswintinycocoinstance-UseCase,mcro_UseCaseInformationSection,{}
mcro_visiontransformerbase-sizedmodel-hybrid-EvaluationResults,mcro_QuantativeAnalysisSection,{}
mcro_CLIPViTbigG14LAION2B-Architecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).'}
mcro_mlxcommunitygemma312bitqat4bit-Citation,mcro_CitationInformationSection,{}
mcro_BAAIbgererankerlarge-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 're-rank top-k documents'}
mcro_colbertv2-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'Using ColBERT on a dataset typically involves the following steps.'}
mcro_metallamaMetaLlama38B-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.'}"
mcro_owlvitbasepatch32,mcro_Model,{}
mcro_cambridgeltlSapBERTfromPubMedBERTfulltextmeantoken-ModelArchitecture,mcro_ModelArchitectureInformationSection,"{'prov_hasTextValue': 'SapBERT by [Liu et al. (2020)](https://arxiv.org/pdf/2010.11784.pdf). Trained with [UMLS](https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html) 2020AA (English only), using [microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) as the base model. Please use the mean-pooling of the output as the representation.'}"
mcro_e5largev2-CitationInformationSection,mcro_CitationInformationSection,"{'prov_hasTextValue': '@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}'}"
mcro_Whisperlargev3model,mcro_Model,{}
mcro_gemma3model,mcro_Model,{}
mcro_Systranfasterwhisperbase,mcro_Model,{}
mcro_timmViTB16SigLIPi18n256,mcro_Model,{}
mcro_NLLB200-Dataset,mcro_DatasetInformationSection,{'prov_hasTextValue': '- Datasets: Flores-200 dataset is described in Section 4\n- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\n- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200.'}
mcro_facebookwav2vec2base960h-Dataset,mcro_DatasetInformationSection,{}
mcro_Distilbertbasemultilingualcasednerhrl-ModelDetail,mcro_ModelDetailSection,{}
mcro_openaiclip-ModelArchitecture,mcro_ModelArchitectureInformationSection,{}
mcro_mistralaiMixtral8x7BInstructv01-ArchitectureSection,mcro_ModelArchitectureInformationSection,{}
mcro_detoxify-ModelArchitectureInformationSection,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Transformers'}
mcro_we_log_statistics,mcro_Model,{'prov_hasTextValue': 'We log statistics to see if any envs are breaking'}
mcro_microsoftphi2-UseCase,mcro_UseCaseInformationSection,"{'prov_hasTextValue': 'Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.'}"
mcro_allMiniLML12v2-TrainingDataInformationSection,mcro_TrainingDataInformationSection,{'prov_hasTextValue': 'We use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.'}
mcro_deepseekaiDeepSeekV3-Dataset2,mcro_DatasetInformationSection,"{'prov_hasTextValue': 'Post-Training: Knowledge Distillation from DeepSeek-R1\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.'}"
mcro_Stablediffusionsafetychecker-UseCase,mcro_UseCaseInformationSection,{}
mcro_conjunctsditre15-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_flan-t5-base-UseCase,mcro_UseCaseInformationSection,{}
mcro_bertbasemultilingualuncasedsentiment-UseCase,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above or for further finetuning on related sentiment analysis tasks.'}
mcro_jonatasgrosmanwav2vec2largexlsr53hungarian-DatasetInformationSection,mcro_DatasetInformationSection,{}
mcro_emrecanbertbaseturkishcasedmeannlistsbtr-TrainingData,mcro_TrainingDataInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53polish-UseCase,mcro_UseCaseInformationSection,{}
mcro_bertbasechinese-TrainingData,mcro_TrainingDataInformationSection,{'prov_hasTextValue': '[More Information Needed]'}
mcro_sdxlinpainting01-Limitation,mcro_LimitationInformationSection,{}
mcro_canary1bflash-UseCaseInformationSection,mcro_UseCaseInformationSection,{'prov_hasTextValue': 'None'}
mcro_nlpconnectvitgpt2imagecaptioning-License,mcro_LicenseInformationSection,{}
mcro_surya,mcro_Model,{}
mcro_openaiclip-PerformanceMetric,mcro_PerformanceMetricInformationSection,{}
mcro_jonatasgrosmanwav2vec2largexlsr53english-UseCaseInformationSection,mcro_UseCaseInformationSection,{}
mcro_atinyversionofhttpshuggingfacecommetalllamaMetaLlama38BInstruct,mcro_Model,{}
mcro_OrengutengLlama38BLexiUncensored-ModelArchitecture,mcro_ModelArchitectureInformationSection,{'prov_hasTextValue': 'Llama-3-8b-Instruct'}
mcro_openaiclip-Citation,mcro_CitationInformationSection,{}
mcro_googleflant5small-Citation,mcro_CitationInformationSection,{}
mcro_BAAIbgererankerlarge-Citation,mcro_CitationInformationSection,"{'prov_hasTextValue': '@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}'}"
mcro_distilgpt2-Citation,mcro_CitationInformationSection,{}
mcro_bge-m3-onnx-o4,mcro_Model,{}
mcro_Stablediffusionsafetychecker-Evaluation,mcro_QuantativeAnalysisSection,{}
