# Knowledge Graph RAG Pipeline

This repository contains a set of Python scripts that implement a Retrieval-Augmented Generation (RAG) pipeline integrated with a Neo4j knowledge graph. It is a subcomponent of a larger thesis project focused on enhancing transparency and accessibility in machine learning model documentation through knowledge graph technologies and generative AI. The pipeline processes RDF data, stores it in a Neo4j database, and supports natural language querying using a combination of vector search and Cypher-based graph queries.

Note: This is a subdirectory of the main thesis project repository. For additional context, resources, or related components, please refer to the main project repository.

## Table of Contents

- [Purpose](#purpose)
- [Components](#components)
- [Dependencies](#dependencies)
- [Setup](#setup)
- [Neo4j Database Setup](#neo4j-database-setup)
- [Usage](#usage)
- [Output](#output)
- [Contributing](#contributing)
- [License](#license)
- [Contact Information](#contact-information)

## Purpose

This pipeline enables intuitive querying of machine learning model metadata stored in a Neo4j knowledge graph. It processes RDF triples (generated from MCRO-based model cards), converts them into nodes and relationships for Neo4j, and supports natural language queries using a RAG approach. The system combines vector-based similarity search with structured Cypher queries, leveraging the Gemini generative AI model to enhance accessibility for non-technical users. This contributes to the thesis project's goal of promoting transparency and interoperability in AI model documentation.

## Components

The pipeline consists of four main scripts:
1. **RAG.py**: Converts RDF triples from a Turtle file into CSV files (nodes and relationships) for Neo4j import.
2. **KG_RAG.py**: Sets up the RAG pipeline, integrating Neo4j vector search and Cypher-based queries with the Gemini LLM.
3. **KG_query.py**: Generates Cypher queries from natural language questions for structured graph queries.
4. **query_runner.py**: Provides an interactive CLI for querying the RAG agent.

## Dependencies

To run the scripts, install the following dependencies:

- Python 3.8+
- `rdflib`
- `langchain-community`
- `langchain-google-genai`
- `python-dotenv`

Install them using pip:

```bash
pip install rdflib langchain-community langchain-google-genai python-dotenv
```

or

```bash
pip install -r requirements.txt
```

You will also need:
- A Neo4j database instance (local or cloud).
- A Gemini API key for query generation and embeddings.

## Setup

1. **Clone the repository**:

   ```bash
   git clone https://github.com/iamvishalsehgal/Enhancing-AI-Transparency.git
   cd Enhancing-AI-Transparency
   ```

2. **Rename `.env.copy` file to `.env`** in the root directory and add your credentials:

   ```
   GEMINI_API_KEY=your_gemini_api_key
   GEMINI_MODEL_NAME=your_gemini_model_name
   NEO4J_URI=your_neo4j_uri
   NEO4J_USER=your_neo4j_username
   NEO4J_PASSWORD=your_neo4j_password
   OUTPUT_BASE_PATH=your_output_base_path
   ```

   Replace the placeholders with your actual values. For example, `NEO4J_URI` might be `neo4j://localhost:7687` for a local Neo4j instance.

3. **Ensure the input Turtle file** (`Ontology_mapper/Output/{folder_number}/triples.ttl`) exists. This file should contain MCRO-based RDF triples, typically generated by another component of the thesis project.

**Note**: You must have access to the Google Gemini API and comply with its terms of use, which may involve setting up billing or adhering to usage limits.

## Neo4j Database Setup

To set up a Neo4j database and import the data:

1. **Install Neo4j**:
   - Download and install [Neo4j Desktop](https://neo4j.com/download/) or set up a [Neo4j Aura](https://neo4j.com/cloud/aura/) cloud instance.
   - For local installations, follow the Neo4j installation guide for your operating system.

2. **Start Neo4j**:
   - In Neo4j Desktop, create a new project and database, then start the database.
   - Note the Neo4j URI (e.g., `neo4j://localhost:7687`), username (default: `neo4j`), and password.

3. **Import Data**:
   - Run `RAG.py` to generate `nodes.csv` and `relationships.csv`:
     ```bash
     python RAG.py
     ```
   - The script will create a new output directory (e.g., `Output/{folder_number}/`) containing the CSV files.
   - Copy these CSV files to the Neo4j `import` directory:
     - For Neo4j Desktop, press ... go to open folder/import.
     - For Neo4j Aura, upload the files using the Aura web interface or Neo4j Browser.
   - In Neo4j Browser, import the data using Cypher commands:
     ```cypher
     :auto LOAD CSV WITH HEADERS FROM 'file:///nodes.csv' AS row
     CREATE (n:Node {id: row.`:ID`, label: row.`:LABEL`, properties: row.properties})
     ```
     ```cypher
     :auto LOAD CSV WITH HEADERS FROM 'file:///relationships.csv' AS row
     MATCH (start:Node {id: row.`:START_ID`}), (end:Node {id: row.`:END_ID`})
     CREATE (start)-[:RELATION {type: row.TYPE}]->(end)
     ```
   - Verify the data import by running:
     ```cypher
     MATCH (n:Node) RETURN n 
     ```

4. **Create Vector Index** (optional, for vector search):
   - After importing nodes, run `KG_RAG.py` to initialize the vector index, which embeds node properties using Gemini embeddings.

## Usage

1. **Preprocess RDF Data**:
   Run the preprocessor to convert RDF triples to Neo4j-compatible CSV files:
   ```bash
   python RAG.py
   ```
   Output CSV files are saved to a new directory under `OUTPUT_BASE_PATH` (e.g., `Output/{folder_number}/`).

2. **Import CSV Files to Neo4j**:
   Follow the [Neo4j Database Setup](#neo4j-database-setup) steps to import `nodes.csv` and `relationships.csv`.

3. **Run the RAG Pipeline**:
   Start the interactive query interface:
   ```bash
   python query_runner.py
   ```
   - Enter a natural language question (e.g., " What are the use cases for the Falconsainsfwimagedetection model?").
   - The RAG agent combines vector search and Cypher queries to generate a response.
   - Type `exit` or `quit` to terminate.

## Output

- **rag_preprocessor.py**:
  - `nodes.csv`: Contains node IDs, labels, and properties.
  - `relationships.csv`: Contains relationships between nodes (start ID, type, end ID).
  - Saved to a new directory (e.g., `Output/{folder_number}/`).

- **query_runner.py**:
  - Displays generated Cypher queries and responses to user questions.
  - Example:
    ```
    Question:  What are the use cases for the Falconsainsfwimagedetection model?

    Generating Cypher query
    > Entering new AgentExecutor chain...
   I need to find the use cases for the Falconsainsfwimagedetection model. I can use VectorSearch to find information about the model's use cases from its model card.
   Action: VectorSearch
   Action Input: "Falconsainsfwimagedetection use cases"
   Observation: [Document(metadata={'label': 'NamedIndividual;mcro_UseCaseInformationSection', 'textValue': 'NSFW Image Classification: The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications.'}, page_content="\nproperties: {'prov_hasTextValue': 'NSFW Image Classification: The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications.'}"), Document(metadata={'label': 'NamedIndividual;mcro_LimitationInformationSection', 'textValue': '- Specialized Task Fine-Tuning: While the model is adept at NSFW image classification, its performance may vary when applied to other tasks.\\n- Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.'}, page_content="\nproperties: {'prov_hasTextValue': '- Specialized Task Fine-Tuning: While the model is adept at NSFW image classification, its performance may vary when applied to other tasks.\\n- Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.'}")]
   Thought:The model is primarily intended for NSFW image classification, filtering explicit or inappropriate content in various applications. It is fine-tuned for this specific purpose. Its performance may vary when applied to other tasks, and users interested in employing it for different tasks should explore fine-tuned versions available in the model hub.
   Final Answer: The primary use case for the Falconsainsfwimagedetection model is NSFW image classification, which involves filtering explicit or inappropriate content in various applications.


   > Finished chain.

   Response:

   The primary use case for the Falconsainsfwimagedetection model is NSFW image classification, which involves filtering explicit or inappropriate content in various applications.
    ```

## Contributing

As this is part of a thesis project, contributions are limited but welcome. If you have suggestions for improvements or identify issues, please:
- Open an issue on the [main project repository](https://github.com/iamvishalsehgal/Enhancing-AI-Transparency.git).
- Follow the project's coding style and conventions.
- Provide clear and descriptive commit messages.
- Update documentation as necessary.

## License

This code is licensed under the [MIT License](LICENSE).

## Contact Information

For questions, issues, or further inquiries, please open an issue on the [main project repository](https://github.com/iamvishalsehgal/Enhancing-AI-Transparency.git).