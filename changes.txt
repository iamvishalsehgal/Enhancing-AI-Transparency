- added an output json file for triples 
- extracted 1k model cards from hugging face sorted them as downloads aseanding 
- Captured output of the 1st prompt


def extract_hf_entities(text):
    """LLM-based entity extraction using LLaMA"""
    try:
        # Create LLaMA-specific prompt
        prompt = f"""<s>[INST] <<SYS>>
You are an AI ontology expert. Extract these entities from the model card:
- Model name (string)
- License type (string)
- CO2 emitted (number)
- Training data sources (comma-separated list)
- Architecture (string)

Return ONLY valid JSON. No explanations.
<</SYS>>
{text[:1500]} [/INST]"""  # Truncate for context limits




- Saved above output as triples1.json
- then i fix padding error which i was getting on output but adding: tokenizer.pad_token = tokenizer.eos_token and tokenizer.padding_side = "left" as [no change in triples]
- added regex extraction + more ontology mapping
- change pipeline for regex - now its giving proper output not just 1 json key
- use the same approch for llms 
- added gemini removed llama
- added llama, gemini ,regex for extraction as gemini was giving api error
- still using regex for extraction even though i said use_gemini = True and giving not good output (non structured)
- changed pipeline for llm’s a bit -  using regex structure for llms 
- removed regex indika said we wont use it 
- Now i think it works properly Captured output from 2nd Prompt using SPDX license IDs both model had different prompt but mostly gemini got used file extracted 20 model cards and saved in Triples2.json



Gemini Prompt: """Extract model metadata as JSON with VALID SPDX LICENSE IDS:
{{
  "Model name": "string",
  "License type": "string (SPDX ID)",
  "Architecture": "string",
  "CO2 emitted": "float|null",
  "Training data sources": "string",
  "Datasets": ["string"],
  "Languages": ["string"],
  "Metrics": ["string"],
  "Base model": "string",
  "Pipeline tag": "string",
  "Library name": "string"
}}

Model card text:
{text[:10000]}"""
 LLama Prompt: f"""Extract metadata as JSON with SPDX licenses:
{{
  "License type": "string (SPDX ID)",
  "Architecture": "string",
  "Model name": "string",
  "Base model": "string",
  "Pipeline tag": "string",
  "Library name": "string"
}}
Text: {text[:3000]}"""




- made a one universal prompt to make it easy 
- Gemini get 10k characters llama get 3k char for memory constraint
- llama select it own character using tokenizer fuck memory contraint
- 
