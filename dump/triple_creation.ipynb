{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e5396b-a8b8-4fcd-b0b1-921f0071f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, ModelCard\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import traceback\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797e9398-e27b-4ca8-adb8-e1af99264c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LTOKEN = \"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\"\n",
    "GTOKEN = \"AIzaSyB8HMqIGvscURWPF75CwnZlXnFFsGh0Vlg\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3028e31d-2a6b-41b2-af0a-b5c7e0c582bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize APIs\n",
    "genai.configure(api_key=GTOKEN)\n",
    "api = HfApi(token=LTOKEN)\n",
    "\n",
    "# Empty mapping that will be populated dyâˆ‚modenamically\n",
    "HF_MAPPING = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22ba451-802d-4468-85e1-95206394e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction prompt template\n",
    "EXTRACTION_PROMPT = \"\"\"\n",
    "Extract ONLY valid JSON without markdown or extra text:\n",
    "{schema}\n",
    "Text:\n",
    "{text}\n",
    "\"\"\".strip()\n",
    "\n",
    "# JSON schema for extraction\n",
    "EXTRACTION_SCHEMA = {\n",
    "    \"Model name\": \"string\",\n",
    "    \"License\": \"SPDX ID\",\n",
    "    \"Architecture\": \"string\",\n",
    "    \"CO2 Emitted\": \"float|null\",\n",
    "    \"Training Data\": \"string\",\n",
    "    \"Languages\": [\"string\"],\n",
    "    \"Base Model\": \"string\",\n",
    "    \"Pipeline Tag\": \"string\",\n",
    "    \"Bias Analysis\": \"string\",\n",
    "    \"Ethical Considerations\": \"string\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfe8fa3-26e7-4461-a7bd-9b8b4767cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extraction_prompt(text):\n",
    "    \"\"\"Generate formatted extraction prompt\"\"\"\n",
    "    return EXTRACTION_PROMPT.format(\n",
    "        schema=json.dumps(EXTRACTION_SCHEMA, indent=2),\n",
    "        text=text[:8192]\n",
    "    ).replace('\\\\', '\\\\\\\\')\n",
    "\n",
    "def clean_identifier(text):\n",
    "    \"\"\"Create safe URI components\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9-]', '', str(text).replace(' ', '-').lower()[:64]) or \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5b4988-127c-4374-9b06-3fb70cb3f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hf_entities(text):\n",
    "    \"\"\"Extract metadata with strict JSON validation\"\"\"\n",
    "    prompt = create_extraction_prompt(text)\n",
    "    \n",
    "    # Gemini extraction\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "        response = model.generate_content(prompt)\n",
    "        json_str = response.text.strip()\n",
    "        \n",
    "        # Remove any markdown artifacts\n",
    "        if '```json' in json_str:\n",
    "            json_str = json_str.split('```json')[1].split('```')[0].strip()\n",
    "            \n",
    "        # Validate JSON structure\n",
    "        json_data = json.loads(json_str)\n",
    "        return json_data if isinstance(json_data, dict) else {}\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini failed: {str(e)}\")\n",
    "\n",
    "    # LLaMA fallback with improved parsing\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=LTOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=LTOKEN,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=4096, truncation=True).to(DEVICE)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract first valid JSON block\n",
    "        json_start = raw.find('{')\n",
    "        json_end = raw.rfind('}') + 1\n",
    "        json_str = raw[json_start:json_end]\n",
    "        \n",
    "        # Fix common JSON issues\n",
    "        json_str = (json_str\n",
    "            .replace(\"'\", '\"')\n",
    "            .replace('None', 'null')\n",
    "            .replace('True', 'true')\n",
    "            .replace('False', 'false'))\n",
    "        \n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"LLaMA failed: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c8434b-2feb-4579-865e-ae963aa4bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hf_triples(model_data):\n",
    "    \"\"\"Generate RDF triples with dynamic mappings\"\"\"\n",
    "    # Reset mapping for each model\n",
    "    global HF_MAPPING\n",
    "    HF_MAPPING = {\n",
    "        \"model_id\": \"ModelCard:Model\"  # Base mapping always present\n",
    "    }\n",
    "    \n",
    "    # Generate mappings based on extracted entities\n",
    "    entities = model_data.get(\"entities\", {})\n",
    "    for key in entities:\n",
    "        if key == \"Model name\":\n",
    "            continue  # Handled separately as model ID\n",
    "            \n",
    "        # Create entity mapping\n",
    "        entity_key = key.lower().replace(' ', '_')\n",
    "        HF_MAPPING[entity_key] = f\"ModelCard:{key}\"\n",
    "        \n",
    "        # Create predicate mapping\n",
    "        predicate_key = f\"{entity_key}_predicate\"\n",
    "        HF_MAPPING[predicate_key] = f\"modelcard:has{key.replace(' ', '')}\"\n",
    "\n",
    "    triples = []\n",
    "    model_uri = f\"hf:{clean_identifier(model_data['id'])}\"\n",
    "    triples.append({\"s\": model_uri, \"p\": \"rdf:type\", \"o\": HF_MAPPING[\"model_id\"]})\n",
    "    \n",
    "    # Generate triples for all detected entities\n",
    "    for key, value in entities.items():\n",
    "        if key == \"Model name\":\n",
    "            continue\n",
    "            \n",
    "        entity_key = key.lower().replace(' ', '_')\n",
    "        predicate_key = f\"{entity_key}_predicate\"\n",
    "        \n",
    "        if predicate_key not in HF_MAPPING:\n",
    "            continue  # Skip if mapping not created\n",
    "            \n",
    "        entity_uri = f\"hf:{entity_key}-{clean_identifier(value)}\"\n",
    "        triples.extend([\n",
    "            {\"s\": model_uri, \"p\": HF_MAPPING[predicate_key], \"o\": entity_uri},\n",
    "            {\"s\": entity_uri, \"p\": \"rdfs:subClassOf\", \"o\": HF_MAPPING[entity_key]},\n",
    "            {\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": str(value)}\n",
    "        ])\n",
    "\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ec551e-0084-4781-9746-7a113c08f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/100: FacebookAI/xlm-roberta-large\n",
      "  Triples: 28\n",
      "Processed 5/100: dima806/fairface_age_image_detection\n",
      "  Triples: 28\n",
      "Processed 10/100: google/electra-base-discriminator\n",
      "  Triples: 28\n",
      "Processed 15/100: FacebookAI/roberta-large\n",
      "  Triples: 28\n",
      "Processed 20/100: jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\n",
      "  Triples: 28\n",
      "Processed 25/100: FacebookAI/roberta-base\n",
      "  Triples: 28\n",
      "Processed 30/100: jonatasgrosman/wav2vec2-large-xlsr-53-portuguese\n",
      "  Triples: 28\n",
      "Processed 35/100: cross-encoder/ms-marco-MiniLM-L6-v2\n",
      "  Triples: 28\n",
      "Processed 40/100: google/vit-base-patch16-224\n",
      "  Triples: 28\n",
      "Processed 45/100: google-t5/t5-small\n",
      "  Triples: 28\n",
      "Processed 50/100: facebook/contriever-msmarco\n",
      "  Triples: 28\n",
      "Processed 55/100: jonatasgrosman/wav2vec2-large-xlsr-53-dutch\n",
      "  Triples: 28\n",
      "Processed 60/100: openai/clip-vit-base-patch16\n",
      "  Triples: 28\n",
      "Processed 65/100: intfloat/multilingual-e5-small\n",
      "  Triples: 28\n",
      "Processed 70/100: Alibaba-NLP/gte-base-en-v1.5\n",
      "  Triples: 28\n",
      "Processed 75/100: unslothai/aws\n",
      "  Triples: 28\n",
      "Processed 80/100: microsoft/deberta-base\n",
      "  Triples: 28\n",
      "Processed 85/100: pyannote/segmentation\n",
      "  Triples: 28\n",
      "Processed 90/100: ibm-granite/granite-timeseries-ttm-r1\n",
      "  Triples: 28\n",
      "Processed 95/100: imvladikon/wav2vec2-xls-r-300m-hebrew\n",
      "  Triples: 28\n",
      "\n",
      "=== STATISTICS ===\n",
      "Predicate usage:\n",
      "  rdfs:subClassOf: 900\n",
      "  dul:hasParameterDataValue: 900\n",
      "  rdf:type: 100\n",
      "  modelcard:hasLicense: 100\n",
      "  modelcard:hasArchitecture: 100\n",
      "  modelcard:hasCO2Emitted: 100\n",
      "  modelcard:hasTrainingData: 100\n",
      "  modelcard:hasLanguages: 100\n",
      "  modelcard:hasBaseModel: 100\n",
      "  modelcard:hasPipelineTag: 100\n",
      "  modelcard:hasBiasAnalysis: 100\n",
      "  modelcard:hasEthicalConsiderations: 100\n"
     ]
    }
   ],
   "source": [
    "def process_huggingface_models(limit=20):\n",
    "    \"\"\"Process models with enhanced error handling\"\"\"\n",
    "    models = list(api.list_models(sort=\"downloads\", direction=-1, limit=limit))\n",
    "    all_triples = []\n",
    "\n",
    "    for idx, model in enumerate(models):\n",
    "        try:\n",
    "            # Load model card\n",
    "            card = ModelCard.load(model.modelId, token=LTOKEN)\n",
    "            card_text = card.text\n",
    "            \n",
    "            # Extract entities\n",
    "            extracted = extract_hf_entities(card_text)\n",
    "            if not extracted:\n",
    "                print(f\"Skipping {model.modelId} - no data\")\n",
    "                continue\n",
    "            \n",
    "            # Create data structure\n",
    "            model_data = {\n",
    "                \"id\": model.modelId,\n",
    "                \"entities\": extracted\n",
    "            }\n",
    "            \n",
    "            # Generate triples with dynamic mappings\n",
    "            triples = generate_hf_triples(model_data)\n",
    "            all_triples.extend(triples)\n",
    "            \n",
    "            # Progress report\n",
    "            if idx % 5 == 0:\n",
    "                print(f\"Processed {idx}/{len(models)}: {model.modelId}\")\n",
    "                print(f\"  Triples: {len(triples)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model.modelId}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Save results\n",
    "    with open(\"model_triples.json\", \"w\") as f:\n",
    "        json.dump(all_triples, f, indent=2)\n",
    "    \n",
    "    print(\"\\n=== STATISTICS ===\")\n",
    "    predicate_counts = {}\n",
    "    for t in all_triples:\n",
    "        predicate_counts[t['p']] = predicate_counts.get(t['p'], 0) + 1\n",
    "    \n",
    "    print(\"Predicate usage:\")\n",
    "    for p, count in sorted(predicate_counts.items(), \n",
    "                         key=lambda x: x[1], \n",
    "                         reverse=True):\n",
    "        print(f\"  {p}: {count}\")\n",
    "    \n",
    "    return all_triples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_huggingface_models(limit=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
