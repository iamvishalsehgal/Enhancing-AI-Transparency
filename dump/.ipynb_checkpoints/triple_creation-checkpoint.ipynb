{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e5396b-a8b8-4fcd-b0b1-921f0071f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, ModelCard\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import traceback\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797e9398-e27b-4ca8-adb8-e1af99264c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LTOKEN = \"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\"\n",
    "GTOKEN = \"AIzaSyD-QYG2WyMAdM1uQZHMeiCqhJP61y8N9nw\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3028e31d-2a6b-41b2-af0a-b5c7e0c582bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize APIs\n",
    "genai.configure(api_key=GTOKEN)\n",
    "api = HfApi(token=LTOKEN)\n",
    "\n",
    "# Empty mapping that will be populated dynamically\n",
    "HF_MAPPING = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a22ba451-802d-4468-85e1-95206394e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_PROMPT = \"\"\"\n",
    "Extract **only the following fields** from the model card text below.  \n",
    "Populate values **only if explicitly mentioned** in the text. Omit fields with no data.  \n",
    "Return the result as a **strict JSON object** with **double quotes** and **no extra text**.\n",
    "\n",
    "### Fields to Extract:\n",
    "- Architecture (e.g., \"RoBERTa\", \"BERT\")\n",
    "- License (e.g., \"Apache-2.0\", \"MIT\")\n",
    "- Training Data (e.g., \"BookCorpus, Wikipedia\")\n",
    "- Languages (array of strings, e.g., [\"en\", \"fr\"])\n",
    "- Use Cases (array of tasks, e.g., [\"text-classification\"])\n",
    "- Model Description (brief description)\n",
    "- Dataset Construction (how data was compiled)\n",
    "- Limitations (known issues)\n",
    "\n",
    "### Rules:\n",
    "1. **No extra fields**: Only include the listed fields if present in the text.\n",
    "2. **Arrays**: Use square brackets for lists (e.g., [\"en\", \"fr\"]).\n",
    "3. **Strings**: Use double quotes for all text values.\n",
    "4. **No markdown**: Only raw JSON.\n",
    "\n",
    "### Example Output:\n",
    "{\n",
    "  \"Architecture\": \"RoBERTa\",\n",
    "  \"License\": \"MIT\",\n",
    "  \"Training Data\": \"BookCorpus, Wikipedia\",\n",
    "  \"Languages\": [\"en\"],\n",
    "  \"Use Cases\": [\"text-classification\", \"question-answering\"],\n",
    "  \"Model Description\": \"RoBERTa-base model pre-trained by Facebook AI Research...\",\n",
    "  \"Dataset Construction\": \"Combination of public datasets\",\n",
    "  \"Limitations\": \"Limited support for rare languages\"\n",
    "}\n",
    "\n",
    "### Text to Analyze:\n",
    "{{text}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfe8fa3-26e7-4461-a7bd-9b8b4767cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extraction_prompt(text):\n",
    "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
    "    schema_str = schema_str.replace(\"{\", \"{{\").replace(\"}\", \"}}\")  # Escape schema braces\n",
    "    return EXTRACTION_PROMPT.format(\n",
    "        schema=schema_str,\n",
    "        text=text[:8192]\n",
    "    ).replace(\"\\\\\", \"\\\\\\\\\")  # Keep existing backslash escaping\n",
    "\n",
    "def clean_identifier(text):\n",
    "    # Replace slashes and spaces with hyphens\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9-]', '', \n",
    "                      str(text).replace('/', '-').replace(' ', '-').lower())\n",
    "    # Collapse multiple hyphens\n",
    "    sanitized = re.sub(r'-+', '-', sanitized)\n",
    "    # Truncate to 64 characters\n",
    "    return sanitized[:64] or \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5b4988-127c-4374-9b06-3fb70cb3f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hf_entities(text):\n",
    "    prompt = create_extraction_prompt(text)\n",
    "    \n",
    "    # Gemini extraction\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "        response = model.generate_content(prompt)\n",
    "        json_str = response.text.strip()\n",
    "        \n",
    "        # Extract valid JSON block\n",
    "        match = re.search(r'\\{.*\\}', json_str, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "            try:\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Invalid JSON: {str(e)}. Raw output: {json_str[:200]}...\")\n",
    "                return {}\n",
    "        else:\n",
    "            print(\"No valid JSON found.\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini failed: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "    # LLaMA fallback\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=LTOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=LTOKEN,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"cpu\"\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
    "        inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract JSON\n",
    "        json_start = raw.find('{')\n",
    "        json_end = raw.rfind('}') + 1\n",
    "        json_str = raw[json_start:json_end].replace(\"'\", '\"')\n",
    "        \n",
    "        # Validate JSON\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Invalid LLaMA JSON: {str(e)}\")\n",
    "            print(f\"Raw output: {json_str[:100]}...\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"LLaMA failed: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c8434b-2feb-4579-865e-ae963aa4bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hf_triples(model_data):\n",
    "    global HF_MAPPING\n",
    "    HF_MAPPING = {\n",
    "        \"model_id\": \"ModelCard:Model\"  # Base mapping always present\n",
    "    }\n",
    "    \n",
    "    # Generate mappings based on extracted entities\n",
    "    entities = model_data.get(\"entities\", {})\n",
    "    for key in entities:\n",
    "        if key == \"Model name\":\n",
    "            continue  # Handled separately as model ID\n",
    "            \n",
    "        # Create entity mapping\n",
    "        entity_key = key.lower().replace(' ', '_')\n",
    "        HF_MAPPING[entity_key] = f\"ModelCard:{key}\"\n",
    "        \n",
    "        # Create predicate mapping\n",
    "        predicate_key = f\"{entity_key}_predicate\"\n",
    "        HF_MAPPING[predicate_key] = f\"modelcard:has{key.replace(' ', '')}\"\n",
    "\n",
    "    triples = []\n",
    "    model_id = model_data['id']\n",
    "    # Remove redundant parts (e.g., \"google-bert-bert-base-uncased\" â†’ \"google-bert-base-uncased\")\n",
    "    if \"/\" in model_id:\n",
    "        model_id = model_id.split(\"/\")[-1]  # Take the last segment\n",
    "    model_uri = f\"hf:{clean_identifier(model_id)}\"\n",
    "    triples.append({\"s\": model_uri, \"p\": \"rdf:type\", \"o\": HF_MAPPING[\"model_id\"]})\n",
    "    \n",
    "    # Generate triples for all detected entities\n",
    "    for key, value in entities.items():\n",
    "        if key == \"Model name\":\n",
    "            continue\n",
    "            \n",
    "        entity_key = key.lower().replace(' ', '_')\n",
    "        predicate_key = f\"{entity_key}_predicate\"\n",
    "        \n",
    "        if predicate_key not in HF_MAPPING:\n",
    "            continue  # Skip if mapping not created\n",
    "            \n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                if not item:  # Skip empty strings\n",
    "                    continue\n",
    "                entity_uri = f\"hf:{entity_key}-{clean_identifier(item)}\"\n",
    "                triples.extend([\n",
    "                    {\"s\": model_uri, \"p\": HF_MAPPING[predicate_key], \"o\": entity_uri},\n",
    "                    {\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": str(item)}\n",
    "                ])\n",
    "        else:\n",
    "            entity_uri = f\"hf:{entity_key}-{clean_identifier(str(value))}\"\n",
    "            triples.extend([\n",
    "                {\"s\": model_uri, \"p\": HF_MAPPING[predicate_key], \"o\": entity_uri},\n",
    "                {\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": str(value)}\n",
    "            ])\n",
    "\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ec551e-0084-4781-9746-7a113c08f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing FacebookAI/xlm-roberta-large: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing google-bert/bert-base-uncased: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sentence-transformers/all-MiniLM-L6-v2: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing Falconsai/nsfw_image_detection: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing dima806/fairface_age_image_detection: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing timm/mobilenetv3_small_100.lamb_in1k: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing amazon/chronos-t5-small: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing openai/clip-vit-large-patch14: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sentence-transformers/all-mpnet-base-v2: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing google/electra-base-discriminator: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Bingsu/adetailer: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing timm/resnet50.a1_in1k: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sentence-transformers/multi-qa-MiniLM-L6-cos-v1: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing openai-community/gpt2: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing openai/clip-vit-base-patch32: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing jonatasgrosman/wav2vec2-large-xlsr-53-english: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing google/vit-base-patch16-224-in21k: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Error processing facebook/esmfold_v1: name 'EXTRACTION_SCHEMA' is not defined\n",
      "\n",
      "=== STATISTICS ===\n",
      "Predicate usage:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/548826572.py\", line 13, in process_huggingface_models\n",
      "    extracted = extract_hf_entities(card_text)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/2564728930.py\", line 2, in extract_hf_entities\n",
      "    prompt = create_extraction_prompt(text)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch-node/20230112.2294791/ipykernel_3411625/1707082564.py\", line 2, in create_extraction_prompt\n",
      "    schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'EXTRACTION_SCHEMA' is not defined\n"
     ]
    }
   ],
   "source": [
    "def process_huggingface_models(limit=20):\n",
    "    \"\"\"Process models with enhanced error handling\"\"\"\n",
    "    models = list(api.list_models(sort=\"downloads\", direction=-1, limit=limit))\n",
    "    all_triples = []\n",
    "\n",
    "    for idx, model in enumerate(models):\n",
    "        try:\n",
    "            # Load model card\n",
    "            card = ModelCard.load(model.modelId, token=LTOKEN)\n",
    "            card_text = card.text\n",
    "            \n",
    "            # Extract entities\n",
    "            extracted = extract_hf_entities(card_text)\n",
    "            if not extracted:\n",
    "                print(f\"Skipping {model.modelId} - no data\")\n",
    "                continue\n",
    "            \n",
    "            # Create data structure\n",
    "            model_data = {\n",
    "                \"id\": model.modelId,\n",
    "                \"entities\": extracted\n",
    "            }\n",
    "            \n",
    "            # Generate triples with dynamic mappings\n",
    "            triples = generate_hf_triples(model_data)\n",
    "            all_triples.extend(triples)\n",
    "            \n",
    "            # Progress report\n",
    "            if idx % 5 == 0:\n",
    "                print(f\"Processed {idx}/{len(models)}: {model.modelId}\")\n",
    "                print(f\"  Triples: {len(triples)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model.modelId}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Save results\n",
    "    with open(\"model_triples.json\", \"w\") as f:\n",
    "        json.dump(all_triples, f, indent=2)\n",
    "    \n",
    "    print(\"\\n=== STATISTICS ===\")\n",
    "    predicate_counts = {}\n",
    "    for t in all_triples:\n",
    "        predicate_counts[t['p']] = predicate_counts.get(t['p'], 0) + 1\n",
    "    \n",
    "    print(\"Predicate usage:\")\n",
    "    for p, count in sorted(predicate_counts.items(), \n",
    "                         key=lambda x: x[1], \n",
    "                         reverse=True):\n",
    "        print(f\"  {p}: {count}\")\n",
    "    \n",
    "    return all_triples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_huggingface_models(limit=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
