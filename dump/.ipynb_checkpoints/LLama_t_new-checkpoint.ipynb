{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b22fa6-3344-4806-9fd1-bdef696e9afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing FacebookAI/xlm-roberta-large: name 'GTOKEN' is not defined\n",
      "Error processing google-bert/bert-base-uncased: name 'GTOKEN' is not defined\n",
      "Error processing sentence-transformers/all-MiniLM-L6-v2: name 'GTOKEN' is not defined\n",
      "Error processing Falconsai/nsfw_image_detection: name 'GTOKEN' is not defined\n",
      "Error processing dima806/fairface_age_image_detection: name 'GTOKEN' is not defined\n",
      "\n",
      "=== STATISTICS ===\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import HfApi, ModelCard\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "LTOKEN = \"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\"\n",
    "GTOKEN = \"YOUR_GEMINI_API_KEY\"  # Uncomment and add your valid Gemini API key\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Initialize APIs\n",
    "if GTOKEN:\n",
    "    genai.configure(api_key=GTOKEN)\n",
    "api = HfApi(token=LTOKEN)\n",
    "\n",
    "# Schema-Free Prompt (no hardcoded fields)\n",
    "EXTRACTION_PROMPT = \"\"\"\n",
    "Return **only a strict JSON object** with these rules:\n",
    "1. Extract **every key-value pair** explicitly mentioned in the text (e.g., \"Architecture\", \"License\", \"Training Data\").\n",
    "2. **Exact terms only**: Use phrases directly from the text (e.g., \"BERT\", \"RoBERTa\", \"Apache-2.0\").\n",
    "3. **Architecture must be explicitly stated**:\n",
    "   - Output \"BERT\" only if the text says \"BERT\".\n",
    "   - Output \"RoBERTa\" only if the text says \"RoBERTa\".\n",
    "4. Arrays: Use square brackets (e.g., [\"en\", \"fr\"]).\n",
    "5. No markdown or extra text. Only JSON.\n",
    "6. Double quotes only.\n",
    "\n",
    "### Example for BERT:\n",
    "{{\n",
    "  \"Architecture\": \"BERT\",\n",
    "  \"License\": \"Apache-2.0\",\n",
    "  \"Training Data\": [\"BooksCorpus\", \"Wikipedia\"],\n",
    "  \"Languages\": [\"en\"],\n",
    "  \"Use Cases\": [\"classification\", \"question-answering\"]\n",
    "}}\n",
    "\n",
    "### Example for RoBERTa:\n",
    "{{\n",
    "  \"Architecture\": \"RoBERTa\",\n",
    "  \"License\": \"MIT\",\n",
    "  \"Training Data\": [\"CCNet\", \"Books\"],\n",
    "  \"Languages\": [\"en\", \"fr\", \"100+ others\"],\n",
    "  \"Use Cases\": [\"translation\", \"summarization\"]\n",
    "}}\n",
    "\n",
    "### Text to Analyze:\n",
    "{{text}}\n",
    "\"\"\".strip()\n",
    "\n",
    "def create_extraction_prompt(text):\n",
    "    max_chars = 16000  # Increased to capture more details\n",
    "    sanitized_text = text[:max_chars].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    return EXTRACTION_PROMPT.format(text=sanitized_text)\n",
    "\n",
    "def clean_identifier(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9-_ ]', '', str(text)).replace(' ', '-').replace('/', '-').lower()[:64]\n",
    "\n",
    "def extract_hf_entities(text):\n",
    "    prompt = create_extraction_prompt(text)\n",
    "    \n",
    "    # Try Gemini first\n",
    "    if GTOKEN:\n",
    "        try:\n",
    "            model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "            response = model.generate_content(prompt)\n",
    "            json_str = response.text.strip()\n",
    "            \n",
    "            # Extract first JSON block\n",
    "            match = re.search(r'\\{.*?\\}', json_str, re.DOTALL)\n",
    "            if match:\n",
    "                json_str = match.group().strip()\n",
    "                try:\n",
    "                    return json.loads(json_str)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Gemini invalid JSON: {str(e)}. Falling back to LLaMA.\")\n",
    "            else:\n",
    "                print(\"Gemini returned no valid JSON. Falling back to LLaMA.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Gemini failed: {str(e)}. Falling back to LLaMA.\")\n",
    "    \n",
    "    # LLaMA fallback (CPU-only)\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=LTOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=LTOKEN,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"cpu\"\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
    "        inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                do_sample=False,  # Greedy decoding\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract and clean JSON\n",
    "        match = re.search(r'\\{.*?\\}', raw, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group().replace(\"'\", '\"').strip()\n",
    "            json_str = re.sub(r\",\\s*}\", \"}\", json_str)\n",
    "            json_str = re.sub(r\",\\s*\\]\", \"]\", json_str)\n",
    "            json_str = re.sub(r'\\n+', ' ', json_str)\n",
    "            \n",
    "            try:\n",
    "                return json.loads(json_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"LLaMA invalid JSON: {str(e)}. Raw output: {json_str[:200]}...\")\n",
    "                return {}\n",
    "        else:\n",
    "            print(\"LLaMA returned no valid JSON. Skipping.\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"LLaMA failed: {str(e)}. Skipping.\")\n",
    "        return {}\n",
    "\n",
    "def generate_hf_triples(model_data):\n",
    "    triples = []\n",
    "    model_uri = f\"hf:{clean_identifier(model_data['id'])}\"\n",
    "    triples.append({\"s\": model_uri, \"p\": \"rdf:type\", \"o\": \"ModelCard:Model\"})\n",
    "    \n",
    "    entities = model_data.get(\"entities\", {})\n",
    "    for key, value in entities.items():\n",
    "        if not key.strip():\n",
    "            continue\n",
    "            \n",
    "        entity_key = key.lower().replace(' ', '_')\n",
    "        predicate = f\"modelcard:has{entity_key[0].upper() + entity_key[1:]}\"\n",
    "        \n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                if not item.strip():\n",
    "                    continue\n",
    "                entity_uri = f\"hf:{entity_key}-{clean_identifier(str(item))}\"\n",
    "                triples.extend([\n",
    "                    {\"s\": model_uri, \"p\": predicate, \"o\": entity_uri},\n",
    "                    {\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": str(item)}\n",
    "                ])\n",
    "        else:\n",
    "            entity_uri = f\"hf:{entity_key}-{clean_identifier(str(value))}\"\n",
    "            triples.extend([\n",
    "                {\"s\": model_uri, \"p\": predicate, \"o\": entity_uri},\n",
    "                {\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": str(value)}\n",
    "            ])\n",
    "\n",
    "    return triples\n",
    "\n",
    "def process_huggingface_models(limit=20):\n",
    "    models = list(api.list_models(sort=\"downloads\", direction=-1, limit=limit))\n",
    "    all_triples = []\n",
    "\n",
    "    for idx, model in enumerate(models):\n",
    "        try:\n",
    "            card = ModelCard.load(model.modelId, token=LTOKEN)\n",
    "            card_text = card.text\n",
    "            \n",
    "            # Extract entities with Gemini/LLaMA\n",
    "            extracted = extract_hf_entities(card_text)\n",
    "            if not extracted:\n",
    "                print(f\"Skipping {model.modelId} - no data\")\n",
    "                continue\n",
    "            \n",
    "            # Generate triples dynamically\n",
    "            model_data = {\n",
    "                \"id\": model.modelId,\n",
    "                \"entities\": extracted\n",
    "            }\n",
    "            triples = generate_hf_triples(model_data)\n",
    "            all_triples.extend(triples)\n",
    "            \n",
    "            print(f\"Processed {idx}/{len(models)}: {model.modelId}\")\n",
    "            print(f\"  Triples: {len(triples)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model.modelId}: {str(e)}\")\n",
    "\n",
    "    # Save results\n",
    "    with open(\"model_triples.json\", \"w\") as f:\n",
    "        json.dump(all_triples, f, indent=2)\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\n=== STATISTICS ===\")\n",
    "    predicate_counts = {}\n",
    "    for t in all_triples:\n",
    "        predicate_counts[t['p']] = predicate_counts.get(t['p'], 0) + 1\n",
    "    for p, count in sorted(predicate_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {p}: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force CPU for LLaMA\n",
    "    process_huggingface_models(limit=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
