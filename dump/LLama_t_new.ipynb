{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d8ef9-f176-4faf-9760-b4b83220e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import HfApi, ModelCard\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "LTOKEN = \"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\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Initialize HuggingFace API\n",
    "api = HfApi(token=LTOKEN)\n",
    "\n",
    "# Enhanced prompt to extract more entities\n",
    "EXTRACTION_PROMPT = \"\"\"\n",
    "Extract ALL possible structured information from this model card. Include these entities:\n",
    "1. Architecture (e.g., BERT, GPT, ResNet, VIT, Transformer)\n",
    "2. Task (e.g., Text Classification, Object Detection, NER)\n",
    "3. Language (e.g., English, Multilingual)\n",
    "4. Dataset (e.g., COCO, ImageNet)\n",
    "5. License (e.g., MIT, Apache)\n",
    "6. Tags/Categories (as list)\n",
    "7. Framework (e.g., PyTorch, TensorFlow)\n",
    "8. Modalities (e.g., Text, Image, Audio)\n",
    "9. Metrics (e.g., Accuracy: 92%, F1: 0.85)\n",
    "10. Version\n",
    "\n",
    "### Rules:\n",
    "1. Return JSON with ALL entities (use \"Unknown\" for missing fields)\n",
    "2. No markdown or extra text\n",
    "3. Preserve all original terminology\n",
    "\n",
    "Example:\n",
    "Text: \"BERT model for Spanish NER trained on CoNLL-2002\"\n",
    "Output: {\"Architecture\": \"BERT\", \"Task\": \"Named Entity Recognition\", \"Language\": \"Spanish\", \"Dataset\": \"CoNLL-2002\"}\n",
    "\n",
    "Text to Analyze:\n",
    "{text}\n",
    "JSON Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "def create_extraction_prompt(text):\n",
    "    max_chars = 15000\n",
    "    sanitized_text = text[:max_chars].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    return EXTRACTION_PROMPT.replace(\"{text}\", sanitized_text)\n",
    "\n",
    "def clean_identifier(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9-_ ]', '', str(text)).replace(' ', '-').lower()[:64]\n",
    "\n",
    "def extract_hf_entities(text):\n",
    "    prompt = create_extraction_prompt(text)\n",
    "    extracted = {\n",
    "        \"Architecture\": \"Unknown\",\n",
    "        \"Task\": \"Unknown\",\n",
    "        \"Language\": \"Unknown\",\n",
    "        \"Dataset\": \"Unknown\",\n",
    "        \"License\": \"Unknown\",\n",
    "        \"Tags\": \"Unknown\",\n",
    "        \"Framework\": \"Unknown\",\n",
    "        \"Modalities\": \"Unknown\",\n",
    "        \"Metrics\": \"Unknown\",\n",
    "        \"Version\": \"Unknown\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=LTOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=LTOKEN,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=DEVICE\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=800,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        json_match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
    "        \n",
    "        if json_match:\n",
    "            json_str = json_match.group().strip()\n",
    "            try:\n",
    "                llm_extracted = json.loads(json_str)\n",
    "                extracted.update({k: v for k, v in llm_extracted.items() if v not in [\"\", \"Unknown\"]})\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"JSON decode error, using regex fallback\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LLM extraction failed: {str(e)}\")\n",
    "\n",
    "    # Regex fallback enhancements\n",
    "    extracted[\"Architecture\"] = detect_architecture(text) or extracted[\"Architecture\"]\n",
    "    extracted[\"Task\"] = detect_task(text) or extracted[\"Task\"]\n",
    "    extracted[\"Language\"] = detect_language(text) or extracted[\"Language\"]\n",
    "    extracted[\"License\"] = detect_license(text) or extracted[\"License\"]\n",
    "    extracted[\"Dataset\"] = detect_dataset(text) or extracted[\"Dataset\"]\n",
    "    extracted[\"Framework\"] = detect_framework(text) or extracted[\"Framework\"]\n",
    "    extracted[\"Modalities\"] = detect_modalities(text) or extracted[\"Modalities\"]\n",
    "    extracted[\"Tags\"] = detect_tags(text) or extracted[\"Tags\"]\n",
    "    extracted[\"Metrics\"] = detect_metrics(text) or extracted[\"Metrics\"]\n",
    "    extracted[\"Version\"] = detect_version(text) or extracted[\"Version\"]\n",
    "\n",
    "    # Cleanup\n",
    "    extracted = {k: v if v not in [\"\", \"Unknown\"] else \"Unknown\" for k, v in extracted.items()}\n",
    "    print(f\"Final extraction: {json.dumps(extracted, indent=2)}\")\n",
    "    return extracted\n",
    "\n",
    "# Enhanced regex detectors\n",
    "def detect_architecture(text):\n",
    "    patterns = [\n",
    "        r\"(?i)\\b(bert|roberta|gpt|vit|transformer|resnet|distilbert|albert|t5|llama|falcon|bloom|mistral|xlm|deberta|electra|mobilebert|convnext|efficientnet)\\b\",\n",
    "        r\"(?i)architecture:\\s*([A-Za-z0-9-]+)\",\n",
    "        r\"(?i)(based on|derived from)\\s+([A-Za-z0-9-]+)\",\n",
    "    ]\n",
    "    return _match_first(patterns, text)\n",
    "\n",
    "def detect_task(text):\n",
    "    patterns = [\n",
    "        r\"(?i)\\b(text classification|named entity recognition|question answering|object detection|image segmentation|machine translation|summarization|sentiment analysis|speech recognition|tabular regression)\\b\",\n",
    "        r\"(?i)task:\\s*([A-Za-z0-9 ]+)\",\n",
    "        r\"(?i)for\\s+([A-Za-z0-9 ]+)\\s+task\",\n",
    "    ]\n",
    "    return _match_first(patterns, text)\n",
    "\n",
    "def detect_language(text):\n",
    "    patterns = [\n",
    "        r\"(?i)\\b(english|french|spanish|german|chinese|japanese|arabic|russian|portuguese|italian|dutch|multilingual)\\b\",\n",
    "        r\"(?i)language:\\s*([A-Za-z]+)\",\n",
    "        r\"(?i)trained on\\s+([A-Za-z]+)\\s+language\",\n",
    "    ]\n",
    "    return _match_first(patterns, text)\n",
    "\n",
    "def detect_license(text):\n",
    "    patterns = [\n",
    "        r\"(?i)\\b(MIT|Apache|GPL|BSD|CC-BY|CC-BY-SA|CC-BY-NC|AGPL|LGPL|MPL)\\b\",\n",
    "        r\"(?i)license:\\s*([A-Za-z0-9-]+)\",\n",
    "        r\"(?i)released under\\s+([A-Za-z0-9-]+)\",\n",
    "    ]\n",
    "    return _match_first(patterns, text)\n",
    "\n",
    "def detect_dataset(text):\n",
    "    patterns = [\n",
    "        r\"(?i)\\b(Imagenet|COCO|Wikipedia|Common Crawl|GLUE|SQuAD|MNIST|CIFAR|WikiText|BookCorpus|OpenWebText|PubMed|SNLI|CoNLL)\\b\",\n",
    "        r\"(?i)dataset:\\s*([A-Za-z0-9-]+)\",\n",
    "        r\"(?i)trained on\\s+([A-Za-z0-9-]+)\\s+dataset\",\n",
    "    ]\n",
    "    return _match_first(patterns, text)\n",
    "\n",
    "def detect_framework(text):\n",
    "    patterns = [\n",
    "        r\"(?i)\\b(pytorch|tensorflow|jax|keras|sklearn|fastai|huggingface)\\b\",\n",
    "        r\"(?i)framework:\\s*([A-Za-z0-9]+)\",\n",
    "    ]\n",
    "    return _match_first(patterns, text)\n",
    "\n",
    "def detect_modalities(text):\n",
    "    patterns = [\n",
    "        r\"(?i)\\b(text|image|audio|video|tabular|multimodal)\\b\",\n",
    "        r\"(?i)modality:\\s*([A-Za-z0-9]+)\",\n",
    "    ]\n",
    "    return _match_first(patterns, text)\n",
    "\n",
    "def detect_tags(text):\n",
    "    patterns = [\n",
    "        r\"(?i)tags:\\s*([A-Za-z0-9, ]+)\",\n",
    "        r\"(?i)categor(y|ies):\\s*([A-Za-z0-9, ]+)\",\n",
    "    ]\n",
    "    match = _match_first(patterns, text)\n",
    "    return [t.strip() for t in match.split(',')] if match else \"Unknown\"\n",
    "\n",
    "def detect_metrics(text):\n",
    "    patterns = [\n",
    "        r\"(?i)(accuracy|f1|precision|recall|bleu|rouge|loss|perplexity):\\s*([\\d.]+%?)\",\n",
    "        r\"(?i)metrics:\\s*([A-Za-z0-9:., ]+)\",\n",
    "    ]\n",
    "    metrics = {}\n",
    "    for pattern in patterns:\n",
    "        for m in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            if len(m.groups()) == 2:\n",
    "                metrics[m.group(1).lower()] = m.group(2)\n",
    "    return metrics if metrics else \"Unknown\"\n",
    "\n",
    "def detect_version(text):\n",
    "    patterns = [\n",
    "        r\"(?i)version:\\s*(\\d+\\.\\d+)\",\n",
    "        r\"(?i)v(\\d+\\.\\d+)\",\n",
    "    ]\n",
    "    return _match_first(patterns, text)\n",
    "\n",
    "def _match_first(patterns, text):\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.groups()[-1].strip().title()\n",
    "    return None\n",
    "\n",
    "def generate_hf_triples(model_data):\n",
    "    triples = []\n",
    "    model_id = model_data[\"id\"]\n",
    "    model_uri = f\"hf:{clean_identifier(model_id)}\"\n",
    "    triples.append({\"s\": model_uri, \"p\": \"rdf:type\", \"o\": \"ModelCard:Model\"})\n",
    "    \n",
    "    for key, value in model_data[\"entities\"].items():\n",
    "        if value == \"Unknown\":\n",
    "            continue\n",
    "            \n",
    "        predicate = f\"modelcard:has{key.title().replace(' ', '')}\"\n",
    "        clean_key = key.lower().replace(' ', '_')\n",
    "        \n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                entity_uri = f\"hf:{clean_key}-{clean_identifier(str(item))}\"\n",
    "                triples.append({\"s\": model_uri, \"p\": predicate, \"o\": entity_uri})\n",
    "                triples.append({\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": str(item)})\n",
    "        elif isinstance(value, dict):\n",
    "            for subkey, subvalue in value.items():\n",
    "                entity_uri = f\"hf:{clean_key}-{subkey}-{clean_identifier(str(subvalue))}\"\n",
    "                triples.append({\"s\": model_uri, \"p\": predicate, \"o\": entity_uri})\n",
    "                triples.append({\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": f\"{subkey}: {subvalue}\"})\n",
    "        else:\n",
    "            entity_uri = f\"hf:{clean_key}-{clean_identifier(str(value))}\"\n",
    "            triples.append({\"s\": model_uri, \"p\": predicate, \"o\": entity_uri})\n",
    "            triples.append({\"s\": entity_uri, \"p\": \"dul:hasParameterDataValue\", \"o\": str(value)})\n",
    "            \n",
    "    return triples\n",
    "\n",
    "def process_huggingface_models(limit=20):\n",
    "    models = list(api.list_models(sort=\"downloads\", direction=-1, limit=limit))\n",
    "    all_triples = []\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        try:\n",
    "            print(f\"\\nProcessing {idx+1}/{len(models)}: {model.modelId}\")\n",
    "            card = ModelCard.load(model.modelId, token=LTOKEN)\n",
    "            card_text = card.text or \"No text available\"\n",
    "            \n",
    "            extracted = extract_hf_entities(card_text)\n",
    "            if all(v == \"Unknown\" for v in extracted.values()):\n",
    "                print(f\"Skipping {model.modelId} - no data\")\n",
    "                continue\n",
    "                \n",
    "            model_data = {\n",
    "                \"id\": model.modelId,\n",
    "                \"entities\": extracted\n",
    "            }\n",
    "            \n",
    "            triples = generate_hf_triples(model_data)\n",
    "            all_triples.extend(triples)\n",
    "            print(f\"  Triples generated: {len(triples)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model.modelId}: {str(e)}\")\n",
    "    \n",
    "    with open(\"model_triples.json\", \"w\") as f:\n",
    "        json.dump(all_triples, f, indent=2)\n",
    "        \n",
    "    print(\"\\n=== STATISTICS ===\")\n",
    "    print(f\"Total models processed: {len(models)}\")\n",
    "    print(f\"Total triples generated: {len(all_triples)}\")\n",
    "    print(f\"Average triples per model: {len(all_triples)/max(1, len(models)):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "    process_huggingface_models(limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96891558-3d6d-4795-8fc4-cba31738f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "New code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc46b0a-4c3b-4e5c-a6d8-d5168cb772e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e33286f47844fd7abaebea7b798d83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "\n",
      "Processing 1/5: FacebookAI/xlm-roberta-large\n",
      "Raw output sample: Extract structured information from this model card text.\n",
      "\n",
      "Rules:\n",
      "1. Return ONLY a valid JSON object...\n",
      "JSON extraction failed, creating default extraction\n",
      "  Triples generated: 2\n",
      "\n",
      "Processing 2/5: Falconsai/nsfw_image_detection\n",
      "Raw output sample: Extract structured information from this model card text.\n",
      "\n",
      "Rules:\n",
      "1. Return ONLY a valid JSON object...\n",
      "Successfully extracted 9 fields\n",
      "  Triples generated: 31\n",
      "\n",
      "Processing 3/5: sentence-transformers/all-MiniLM-L6-v2\n",
      "Raw output sample: Extract structured information from this model card text.\n",
      "\n",
      "Rules:\n",
      "1. Return ONLY a valid JSON object...\n",
      "JSON extraction failed, creating default extraction\n",
      "  Triples generated: 2\n",
      "\n",
      "Processing 4/5: google-bert/bert-base-uncased\n",
      "Raw output sample: Extract structured information from this model card text.\n",
      "\n",
      "Rules:\n",
      "1. Return ONLY a valid JSON object...\n",
      "JSON extraction failed, creating default extraction\n",
      "  Triples generated: 2\n",
      "\n",
      "Processing 5/5: dima806/fairface_age_image_detection\n",
      "Raw output sample: Extract structured information from this model card text.\n",
      "\n",
      "Rules:\n",
      "1. Return ONLY a valid JSON object...\n",
      "Successfully extracted 9 fields\n",
      "  Triples generated: 37\n",
      "Saved 74 triples to model_triples.json\n",
      "Saved 5 model data entries to extracted_model_data.json\n",
      "\n",
      "=== STATISTICS ===\n",
      "Total models processed: 5\n",
      "Total triples generated: 74\n",
      "Average triples per model: 14.80\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import HfApi, ModelCard\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Configuration\n",
    "LTOKEN = \"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\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Initialize HuggingFace API\n",
    "api = HfApi(token=LTOKEN)\n",
    "\n",
    "# Global tokenizer and model variables\n",
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "# Simplified extraction prompt focused on JSON format\n",
    "EXTRACTION_PROMPT = \"\"\"\n",
    "Extract structured information from this model card text.\n",
    "\n",
    "Rules:\n",
    "1. Return ONLY a valid JSON object with properties of the model\n",
    "2. Use double quotes for keys and string values\n",
    "3. Format the JSON properly with correct syntax\n",
    "\n",
    "Extract these properties if available:\n",
    "- architecture (model architecture like Transformer, BERT, etc.)\n",
    "- task (model's purpose like classification, translation, etc.)\n",
    "- license (MIT, Apache, etc.)\n",
    "- language (English, Chinese, etc.)\n",
    "- framework (PyTorch, TensorFlow, etc.)\n",
    "- dataset (training data like COCO, ImageNet, etc.)\n",
    "- modality (text, image, audio, etc.)\n",
    "- size (parameter count)\n",
    "- metrics (performance)\n",
    "\n",
    "Text to Analyze:\n",
    "{text}\n",
    "\n",
    "Return VALID JSON only:\n",
    "\"\"\".strip()\n",
    "\n",
    "def initialize_model():\n",
    "    \"\"\"Initialize the LLM model once\"\"\"\n",
    "    global tokenizer, model\n",
    "    \n",
    "    if tokenizer is None or model is None:\n",
    "        print(\"Loading LLaMA model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=LTOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            token=LTOKEN,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=DEVICE\n",
    "        )\n",
    "        print(\"Model loaded successfully\")\n",
    "\n",
    "def create_extraction_prompt(text):\n",
    "    \"\"\"Create a prompt for extraction\"\"\"\n",
    "    # Truncate to avoid exceeding context length\n",
    "    max_chars = 12000\n",
    "    if len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "    \n",
    "    # Format for the prompt\n",
    "    sanitized_text = text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    return EXTRACTION_PROMPT.replace(\"{text}\", sanitized_text)\n",
    "\n",
    "def clean_identifier(text):\n",
    "    \"\"\"Clean identifier for URI creation\"\"\"\n",
    "    if not text:\n",
    "        return \"unknown\"\n",
    "    return re.sub(r'[^a-zA-Z0-9-_ ]', '', str(text)).replace(' ', '-').lower()[:64]\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extract JSON from text in multiple ways\"\"\"\n",
    "    # Try to find JSON pattern\n",
    "    json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if not json_match:\n",
    "        return None\n",
    "    \n",
    "    json_str = json_match.group()\n",
    "    \n",
    "    # Try direct parsing\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try with single quotes replaced\n",
    "    try:\n",
    "        fixed = json_str.replace(\"'\", '\"')\n",
    "        return json.loads(fixed)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try with more aggressive cleaning\n",
    "    try:\n",
    "        # Replace single-quoted keys and values with double quotes\n",
    "        pattern = r\"'([^']*)'(\\s*):(\\s*)'([^']*)'\"\n",
    "        replacement = r'\"\\1\"\\2:\\3\"\\4\"'\n",
    "        fixed = re.sub(pattern, replacement, json_str)\n",
    "        fixed = fixed.replace(\"'\", '\"')\n",
    "        return json.loads(fixed)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_hf_entities(text):\n",
    "    \"\"\"Extract all entities using LLM approach with better JSON handling\"\"\"\n",
    "    initialize_model()\n",
    "    \n",
    "    prompt = create_extraction_prompt(text)\n",
    "    \n",
    "    try:\n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=3072, truncation=True)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1000,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the output\n",
    "        raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Raw output sample: {raw[:100]}...\")\n",
    "        \n",
    "        # Extract and parse JSON\n",
    "        extracted = extract_json_from_text(raw)\n",
    "        \n",
    "        if extracted:\n",
    "            print(f\"Successfully extracted {len(extracted)} fields\")\n",
    "            return extracted\n",
    "        \n",
    "        # If all JSON extraction attempts failed, create default extraction\n",
    "        print(\"JSON extraction failed, creating default extraction\")\n",
    "        return {\"extraction_error\": True, \"raw_sample\": raw[:200]}\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"LLM extraction failed: {str(e)}\")\n",
    "        return {\"extraction_error\": True, \"error\": str(e)}\n",
    "\n",
    "def generate_hf_triples(model_data):\n",
    "    \"\"\"Generate RDF-like triples with proper class hierarchies\"\"\"\n",
    "    triples = []\n",
    "    model_id = model_data[\"id\"]\n",
    "    model_uri = f\"hf:{clean_identifier(model_id)}\"\n",
    "    \n",
    "    # Basic model type triple\n",
    "    triples.append({\n",
    "        \"s\": model_uri, \n",
    "        \"p\": \"rdf:type\", \n",
    "        \"o\": \"ModelCard:Model\"\n",
    "    })\n",
    "    \n",
    "    # Handle extraction errors\n",
    "    if model_data[\"entities\"].get(\"extraction_error\", False):\n",
    "        triples.append({\n",
    "            \"s\": model_uri, \n",
    "            \"p\": \"modelcard:extractionStatus\", \n",
    "            \"o\": \"Failed\"\n",
    "        })\n",
    "        return triples\n",
    "    \n",
    "    # Define property mapping to ontology classes\n",
    "    property_classes = {\n",
    "        \"architecture\": \"ModelCard:Architecture\",\n",
    "        \"task\": \"ModelCard:Task\",\n",
    "        \"license\": \"ModelCard:License\",\n",
    "        \"language\": \"ModelCard:Language\",\n",
    "        \"framework\": \"ModelCard:Framework\",\n",
    "        \"dataset\": \"ModelCard:Dataset\",\n",
    "        \"modality\": \"ModelCard:Modality\",\n",
    "        \"size\": \"ModelCard:Size\",\n",
    "        \"metrics\": \"ModelCard:Metrics\",\n",
    "        \"tags\": \"ModelCard:Tag\",\n",
    "        \"version\": \"ModelCard:Version\",\n",
    "    }\n",
    "    \n",
    "    # Process extracted fields\n",
    "    for key, value in model_data[\"entities\"].items():\n",
    "        if value is None or value == \"\":\n",
    "            continue\n",
    "            \n",
    "        # Normalize key name\n",
    "        clean_key = key.lower()\n",
    "        \n",
    "        # Get property class for this key\n",
    "        property_class = property_classes.get(clean_key, \"ModelCard:Property\")\n",
    "        \n",
    "        # Generate predicate name\n",
    "        predicate = f\"modelcard:has{key.title().replace(' ', '').replace('-', '')}\"\n",
    "        \n",
    "        # Handle different value types\n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                if item:  # Skip empty items\n",
    "                    # Create clean identifier for this value\n",
    "                    clean_value = clean_identifier(str(item))\n",
    "                    \n",
    "                    # Entity URI for this value\n",
    "                    entity_uri = f\"hf:{clean_key}-{clean_value}\"\n",
    "                    \n",
    "                    # Triple connecting model to property value\n",
    "                    triples.append({\n",
    "                        \"s\": model_uri, \n",
    "                        \"p\": predicate, \n",
    "                        \"o\": entity_uri\n",
    "                    })\n",
    "                    \n",
    "                    # Triple classifying the property value\n",
    "                    triples.append({\n",
    "                        \"s\": entity_uri, \n",
    "                        \"p\": \"rdfs:subClassOf\", \n",
    "                        \"o\": property_class\n",
    "                    })\n",
    "                    \n",
    "                    # Triple with actual value\n",
    "                    triples.append({\n",
    "                        \"s\": entity_uri, \n",
    "                        \"p\": \"dul:hasParameterDataValue\", \n",
    "                        \"o\": str(item)\n",
    "                    })\n",
    "                    \n",
    "        elif isinstance(value, dict):\n",
    "            for subkey, subvalue in value.items():\n",
    "                if subvalue:  # Skip empty values\n",
    "                    # Create clean identifier for this subkey\n",
    "                    clean_subkey = clean_identifier(subkey)\n",
    "                    \n",
    "                    # Entity URI for this subkey-value pair\n",
    "                    entity_uri = f\"hf:{clean_key}-{clean_subkey}\"\n",
    "                    \n",
    "                    # Triple connecting model to property value\n",
    "                    triples.append({\n",
    "                        \"s\": model_uri, \n",
    "                        \"p\": predicate, \n",
    "                        \"o\": entity_uri\n",
    "                    })\n",
    "                    \n",
    "                    # Triple classifying the property value\n",
    "                    triples.append({\n",
    "                        \"s\": entity_uri, \n",
    "                        \"p\": \"rdfs:subClassOf\", \n",
    "                        \"o\": property_class\n",
    "                    })\n",
    "                    \n",
    "                    # Triple with actual value\n",
    "                    triples.append({\n",
    "                        \"s\": entity_uri, \n",
    "                        \"p\": \"dul:hasParameterDataValue\", \n",
    "                        \"o\": f\"{subkey}: {subvalue}\"\n",
    "                    })\n",
    "        else:\n",
    "            # Create clean identifier for this value\n",
    "            clean_value = clean_identifier(str(value))\n",
    "            \n",
    "            # Entity URI for this value\n",
    "            entity_uri = f\"hf:{clean_key}-{clean_value}\"\n",
    "            \n",
    "            # Triple connecting model to property value\n",
    "            triples.append({\n",
    "                \"s\": model_uri, \n",
    "                \"p\": predicate, \n",
    "                \"o\": entity_uri\n",
    "            })\n",
    "            \n",
    "            # Triple classifying the property value\n",
    "            triples.append({\n",
    "                \"s\": entity_uri, \n",
    "                \"p\": \"rdfs:subClassOf\", \n",
    "                \"o\": property_class\n",
    "            })\n",
    "            \n",
    "            # Triple with actual value\n",
    "            triples.append({\n",
    "                \"s\": entity_uri, \n",
    "                \"p\": \"dul:hasParameterDataValue\", \n",
    "                \"o\": str(value)\n",
    "            })\n",
    "            \n",
    "    return triples\n",
    "\n",
    "def process_huggingface_models(limit=20):\n",
    "    \"\"\"Process HuggingFace models and extract information\"\"\"\n",
    "    models = list(api.list_models(sort=\"downloads\", direction=-1, limit=limit))\n",
    "    all_triples = []\n",
    "    extracted_data = []\n",
    "    \n",
    "    # Initialize model once before processing\n",
    "    initialize_model()\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        try:\n",
    "            print(f\"\\nProcessing {idx+1}/{len(models)}: {model.modelId}\")\n",
    "            card = ModelCard.load(model.modelId, token=LTOKEN)\n",
    "            card_text = card.text or \"No text available\"\n",
    "            \n",
    "            extracted = extract_hf_entities(card_text)\n",
    "            \n",
    "            model_data = {\n",
    "                \"id\": model.modelId,\n",
    "                \"entities\": extracted\n",
    "            }\n",
    "            \n",
    "            # Save extracted data after each model (incremental saving)\n",
    "            extracted_data.append(model_data)\n",
    "            with open(\"extracted_model_data_incremental.json\", \"w\") as f:\n",
    "                json.dump(extracted_data, f, indent=2)\n",
    "            \n",
    "            # Generate and save triples after each model\n",
    "            triples = generate_hf_triples(model_data)\n",
    "            all_triples.extend(triples)\n",
    "            with open(\"model_triples_incremental.json\", \"w\") as f:\n",
    "                json.dump(all_triples, f, indent=2)\n",
    "                \n",
    "            print(f\"  Triples generated: {len(triples)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"Error processing {model.modelId}: {str(e)}\")\n",
    "    \n",
    "    # Save final files\n",
    "    try:\n",
    "        with open(\"model_triples.json\", \"w\") as f:\n",
    "            json.dump(all_triples, f, indent=2)\n",
    "        print(f\"Saved {len(all_triples)} triples to model_triples.json\")\n",
    "        \n",
    "        with open(\"extracted_model_data.json\", \"w\") as f:\n",
    "            json.dump(extracted_data, f, indent=2)\n",
    "        print(f\"Saved {len(extracted_data)} model data entries to extracted_model_data.json\")\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"Error saving final files: {str(e)}\")\n",
    "        \n",
    "    print(\"\\n=== STATISTICS ===\")\n",
    "    print(f\"Total models processed: {len(models)}\")\n",
    "    print(f\"Total triples generated: {len(all_triples)}\")\n",
    "    print(f\"Average triples per model: {len(all_triples)/max(1, len(models)):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n",
    "    process_huggingface_models(limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde25b87-85cf-4fbe-81e0-cb59bb7fa63f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
